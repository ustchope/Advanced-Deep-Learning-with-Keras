{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动计算cell的计算时间\n",
    "%load_ext autotime\n",
    "\n",
    "%config InlineBackend.figure_format='svg' #矢量图设置，让绘图更清晰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# 增加更新\n",
    "git add *.ipynb\n",
    "\n",
    "git remote -v\n",
    "\n",
    "git commit -m '更新 ch4 #2 change Aug 12, 2021'\n",
    "\n",
    "git push origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "positive-banner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.68 s (started: 2021-08-12 16:45:11 +08:00)\n"
     ]
    }
   ],
   "source": [
    "#设置使用的gpu\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "   \n",
    "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-louis",
   "metadata": {},
   "source": [
    "# 生成对抗网络 (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-cross",
   "metadata": {},
   "source": [
    "在本章中，我们将研究生成对抗网络 (GAN)。 GAN 属于生成模型家族。 然而，与自动编码器不同，生成模型能够在给定任意编码的情况下创建新的、有意义的输出。\n",
    "\n",
    "在本章中，将讨论 GAN 的工作原理。 我们还将回顾使用 tf.keras 的几个早期 GAN 的实现，而在本章的后面，我们将演示实现稳定训练所需的技术。 本章的范围涵盖了 GAN 实现的两个流行示例，深度卷积 GAN (DCGAN) 和条件 GAN (CGAN)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-working",
   "metadata": {},
   "source": [
    "总之，本章的目标是：\n",
    "* 介绍 GAN 的原理\n",
    "* 展示 GAN 的早期工作实现之一，称为DCGAN\n",
    "* 一种称为 CGAN 的改进 DCGAN，它使用条件\n",
    "* 在 tf.keras 中实现 DCGAN 和 CGAN\n",
    "\n",
    "让我们从 GAN 的概述开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-killing",
   "metadata": {},
   "source": [
    "## GAN 概述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-emergency",
   "metadata": {},
   "source": [
    "在我们进入 GAN 的更高级概念之前，让我们先回顾一下 GAN 并介绍它们背后的基本概念。 GAN 非常强大； 这个简单的陈述得到了以下事实的证明：它们可以通过执行潜在空间插值来生成不是真人的新人脸。\n",
    "\n",
    "在这些 YouTube 视频中可以看到 GAN 的高级功能：\n",
    "* 渐进式 GAN [4]：https://youtu.be/G06dEcZ-QTg\n",
    "* StyleGAN v1 [5]：https://youtu.be/kSLJriaOumA\n",
    "* StyleGAN v2 [6]：https://youtu.be/c-NJtV9Jvp0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-cosmetic",
   "metadata": {},
   "source": [
    "展示如何利用 GAN 生成逼真人脸的视频展示了它们的强大功能。这个主题比我们之前在本书中看到的任何内容都要先进得多。例如，上面的视频演示了自动编码器无法轻松完成的事情，我们在第 3 章，自动编码器中介绍了这些内容。\n",
    "\n",
    "GAN 能够通过训练两个被称为生成器和鉴别器（有时称为评论家）的竞争（和合作）网络来学习如何对输入分布进行建模。生成器的作用是不断弄清楚如何生成可以欺骗鉴别器的虚假数据或信号（包括音频和图像）。同时，鉴别器被训练来区分假信号和真实信号。随着训练的进行，判别器将不再能够看到合成生成的数据和真实数据之间的差异。从那里，可以丢弃鉴别器，然后可以使用生成器来创建以前从未观察到的新的真实数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-greenhouse",
   "metadata": {},
   "source": [
    "GAN 的基本概念很简单。 然而，我们会发现的一件事是，最具挑战性的问题是我们如何实现生成器-鉴别器网络的稳定训练？ 生成器和鉴别器之间必须存在良性竞争，以便两个网络能够同时学习。 由于损失函数是根据鉴别器的输出计算的，因此其参数更新很快。 当判别器收敛得更快时，生成器不再为其参数接收足够的梯度更新并且无法收敛。 除了难以训练之外，GAN 还可能遭受部分或全部模态崩溃，在这种情况下，生成器为不同的潜在编码产生几乎相似的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-supply",
   "metadata": {},
   "source": [
    "### GANs原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-premiere",
   "metadata": {},
   "source": [
    "如图 4.1.1 所示，GAN 类似于造假者（生成器）-警察（鉴别器）的场景。 在学院里，警察被教导如何确定一美元钞票是真还是假。 来自银行的真美元钞票样本和来自伪造者的假币样本被用来训练警察。 然而，有时，造假者会试图假装他印的是真钞票。 最初，警察不会上当，会告诉造假者为什么钱是假的。 考虑到这种反馈，造假者再次磨练自己的技能，并试图制造新的假美元钞票。 正如预期的那样，警方将能够发现这些钱是假的，并证明美元钞票是假的："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-broadcast",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte4futnx5j618q0u079802.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-inquiry",
   "metadata": {},
   "source": [
    "这个过程会无限期地持续下去，但最终造假者已经掌握了制造假币的能力，以至于假币与真钱无法区分——即使是最专业的警察也是如此。 然后，造假者可以无限印刷美元钞票而不会被警察抓住，因为它们不再被识别为伪造品。\n",
    "\n",
    "如图 4.1.2 所示，一个 GAN 由两个网络组成，一个生成器和一个判别器："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-perspective",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte4j3gtpdj61ag0ooju302.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-bermuda",
   "metadata": {},
   "source": [
    "发生器的输入是噪声，输出是合成数据。同时，鉴别器的输入将是真实数据或合成数据。真实数据来自真实采样数据，而假数据来自生成器。所有有效数据都标记为 1.0（即 100% 的真实概率），而所有合成数据都标记为 0.0（即真实概率为 0%）。由于标记过程是自动化的，GAN 仍然被认为是深度学习中无监督学习方法的一部分。\n",
    "\n",
    "鉴别器的目标是从这个提供的数据集中学习如何区分真实数据和虚假数据。在这部分 GAN 训练期间，只会更新鉴别器参数。与典型的二元分类器一样，鉴别器被训练以在 0.0 到 1.0 的范围内预测给定输入数据与真实数据的接近程度。然而，这只是故事的一半。\n",
    "\n",
    "每隔一段时间，生成器会假装其输出是真实数据，并要求 GAN 将其标记为 1.0。当假数据被呈现给鉴别器时，自然会被归类为标签接近 0.0 的假数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-stations",
   "metadata": {},
   "source": [
    "优化器根据显示的标签（即 1.0）计算生成器参数更新。在对这些新数据进行训练时，它还考虑了自己的预测。换句话说，判别器对其预测有一些怀疑，因此，GAN 会考虑到这一点。这一次，GAN 会让梯度从鉴别器的最后一层向下传播到生成器的第一层。然而，在大多数实践中，在这个训练阶段，鉴别器参数会被暂时冻结。生成器将使用梯度更新其参数并提高其合成假数据的能力。\n",
    "\n",
    "总的来说，整个过程类似于两个网络相互竞争，同时仍然合作。当 GAN 训练收敛时，最终结果是一个可以合成看起来真实的数据的生成器。鉴别器认为这个合成数据是真实的或者标签接近 1.0，这意味着鉴别器可以被丢弃。发生器部分将有助于从任意噪声输入产生有意义的输出。\n",
    "\n",
    "下面的图 4.1.3 概述了该过程："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-equilibrium",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte4nf7lqvj61as0m0gon02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-future",
   "metadata": {},
   "source": [
    "如上图所示，可以通过最小化以下等式中的损失函数来训练判别器："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-kernel",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte4rsbhgzj618004cwet02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-watershed",
   "metadata": {},
   "source": [
    "该方程只是标准的二元交叉熵成本函数。 损失是正确识别真实数据的期望值 𝒟(𝒙) 和 1.0 减去正确识别合成数据的期望值 1 − 𝒟(𝒢(𝒛)) 的负和。 日志不会更改局部最小值的位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-orbit",
   "metadata": {},
   "source": [
    "训练期间向判别器提供两个小批量数据：\n",
    "1. x，来自采样数据的真实数据（换句话说，$𝒙 \\sim p_{data}$），标签为 1.0\n",
    "2. 𝒙′ = 𝒢(𝒛)，来自生成器的假数据，标签为 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-musical",
   "metadata": {},
   "source": [
    "为了最小化损失函数，鉴别器参数 𝜽(𝐷) 将通过反向传播通过正确识别真实数据 𝒟(𝒙) 和合成数据 1 − 𝒟(𝒢(𝒛)) 进行更新。 正确识别真实数据相当于 𝒟(𝒙) → 1.0，而正确识别虚假数据相当于 𝒟(𝒢(𝒛)) → 0.0 或 (1 − 𝒟(𝒢(𝒛))) → 1.0。 在这个等式中，z 是生成器用来合成新信号的任意编码或噪声向量。 两者都有助于最小化损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-adult",
   "metadata": {},
   "source": [
    "为了训练生成器，GAN 将鉴别器和生成器损失的总和视为零和游戏。 生成器损失函数只是判别器损失函数的负值："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-caution",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte5dpon0rj617u0420sv02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-stomach",
   "metadata": {},
   "source": [
    "然后可以更恰当地将其重写为值函数："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-thinking",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte5h7dz59j617o03gt8u02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-progressive",
   "metadata": {},
   "source": [
    "从生成器的角度来看，公式 4.1.3 应该最小化。 从判别器的角度来看，应该最大化价值函数。 因此，生成器训练准则可以写成一个极大极小问题："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-gasoline",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte5i3hssoj617i04074h02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-carbon",
   "metadata": {},
   "source": [
    "有时，我们会假装合成数据是真实的，标签为 1.0，以此来欺骗鉴别器。 通过最大化 𝜽(𝐷)，优化器向鉴别器参数发送梯度更新，以将这些合成数据视为真实数据。 同时，通过关于 𝜽(𝐺) 的最小化，优化器将训练生成器的参数如何欺骗鉴别器。 然而，在实践中，鉴别器对其将合成数据分类为假数据的预测充满信心，并且不会更新 GAN 参数。 此外，梯度更新很小，并且随着它们传播到生成器层而显着减少。 结果，生成器无法收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-ordering",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte5kt2i6kj61c60kkgnw02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-matrix",
   "metadata": {},
   "source": [
    "解决方案是将生成器的损失函数重新表述为以下形式："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-silver",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte5lllabvj619203u3yn02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-namibia",
   "metadata": {},
   "source": [
    "损失函数只是通过训练生成器来最大化鉴别器相信合成数据是真实的机会。新公式不再是零和，而是纯粹的启发式驱动。图 4.1.4 显示了训练期间的生成器。在这个图中，生成器参数只有在整个对抗网络都被训练时才会更新。这是因为梯度从鉴别器传递到生成器。然而，在实践中，鉴别器权重只是在对抗训练期间暂时冻结。\n",
    "\n",
    "在深度学习中，生成器和鉴别器都可以使用合适的神经网络架构来实现。如果数据或信号是图像，生成器和鉴别器网络都将使用 CNN。对于音频等一维序列，两个网络通常都是循环的（RNN、LSTM 或 GRU）。\n",
    "\n",
    "在本节中，我们了解到 GAN 背后的原理很简单。我们还学习了如何通过熟悉的网络层实现 GAN。 GAN 与其他网络的不同之处在于它们很难训练。像层中的微小变化这样简单的事情就会导致网络训练不稳定。在下一节中，我们将研究使用深度 CNN 的 GAN 早期成功实现之一。它被称为 DCGAN [3]。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-throw",
   "metadata": {},
   "source": [
    "## 在 Keras 中实现 DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-austria",
   "metadata": {},
   "source": [
    "图 4.2.1 显示了用于生成假 MNIST 图像的 DCGAN："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-tribute",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte5po6v6yj61840u0q5y02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-inflation",
   "metadata": {},
   "source": [
    "DCGAN 实现了以下设计原则：\n",
    "* 使用strides > 1 和卷积而不是MaxPooling2D 或UpSampling2D。 随着步幅 > 1，CNN 学习如何调整特征图的大小。\n",
    "* 避免使用密集层。 在所有层中使用 CNN。 密集层仅用作生成器的第一层以接受 z 向量。 Dense 层的输出被调整大小并成为后续 CNN 层的输入。\n",
    "* 使用批量归一化 (BN) 通过将每一层的输入归一化为零均值和单位方差来稳定学习。 生成器输出层和鉴别器输入层没有BN。 在此处介绍的实现示例中，鉴别器中没有使用批量归一化。\n",
    "* 整流线性单元 (ReLU) 用于生成器的所有层，输出层除外，其中使用了 tanh 激活。 在此处介绍的实现示例中，在生成器的输出中使用 sigmoid 代替 tanh，因为它通常会导致对 MNIST 数字的更稳定的训练。\n",
    "* 在鉴别器的所有层中使用Leaky ReLU。 与 ReLU 不同的是，Leaky ReLU 不会在输入小于零时将所有输出归零，而是生成一个与 alpha x 输入相等的小梯度。 在以下示例中，alpha = 0.2。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-feeding",
   "metadata": {},
   "source": [
    "生成器学习从 100 维输入向量（[-1.0, 1.0] 范围内 100 维均匀分布的随机噪声）生成假图像。鉴别器将真实图像与虚假图像进行分类，但在训练对抗网络时无意中指导生成器如何生成真实图像。我们的 DCGAN 实现中使用的内核大小为 5。这是为了增加卷积的感受野大小和表达能力。\n",
    "\n",
    "生成器接受由范围为 -1.0 到 1.0 的均匀分布生成的 100 维 z 向量。生成器的第一层是一个 7 x 7 x 128 = 6,272 单元的 Dense 层。单位数是根据输出图像的预期最终尺寸（28 x 28 x 1，28 是 7 的倍数）和第一个 Conv2DTranspose 的滤波器数量计算的，该数量等于 128。\n",
    "\n",
    "我们可以将转置 CNN（Conv2DTranspose）想象成 CNN 的逆过程。在一个简单的例子中，如果 CNN 将图像转换为特征图，则转置的 CNN 将生成给定特征图的图像。因此，前一章的解码器和本章的生成器都使用了转置 CNN。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-cologne",
   "metadata": {},
   "source": [
    "> 清单 4.2.1：dcgan-mnist-4.2.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hairy-potato",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 397 ms (started: 2021-08-12 18:49:39 +08:00)\n"
     ]
    }
   ],
   "source": [
    "'''Trains DCGAN on MNIST using Keras\n",
    "\n",
    "DCGAN 是使用 CNN 的生成对抗网络 (GAN)。 生成器试图通过生成假图像来欺骗鉴别器。 \n",
    "鉴别器学习区分真假图像。 生成器+鉴别器形成对抗网络。 DCGAN 交替训练鉴别器和对抗网络。 \n",
    "在训练期间，鉴别器不仅学习区分真假图像，还指导对抗的生成器部分如何提高其生成假图像的能力。\n",
    "\n",
    "[1] Radford, Alec, Luke Metz, and Soumith Chintala.\n",
    "\"Unsupervised representation learning with deep convolutional\n",
    "generative adversarial networks.\" arXiv preprint arXiv:1511.06434 (2015).\n",
    "'''\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Activation, Dense, Input\n",
    "from tensorflow.keras.layers import Conv2D, Flatten\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "def build_generator(inputs, image_size):\n",
    "    \"\"\"Build a Generator Model\n",
    "\n",
    "    BN-ReLU-Conv2DTranpose堆栈生成假图像\n",
    "    输出激活是 sigmoid 而不是 [1] 中的 tanh。\n",
    "    Sigmoid 很容易收敛。\n",
    "\n",
    "    参数：\n",
    "         输入（层）：生成器的输入层，z 向量）\n",
    "         image_size（张量）：一侧的目标大小，（假设方形图像）\n",
    "\n",
    "    Returns:\n",
    "        generator (Model): Generator Model\n",
    "    \"\"\"\n",
    "\n",
    "    image_resize = image_size // 4\n",
    "    # network parameters \n",
    "    kernel_size = 5\n",
    "    layer_filters = [128, 64, 32, 1]\n",
    "\n",
    "    x = Dense(image_resize * image_resize * layer_filters[0])(inputs)\n",
    "    x = Reshape((image_resize, image_resize, layer_filters[0]))(x)\n",
    "\n",
    "    for filters in layer_filters:\n",
    "        # 前两个卷积层使用 strides = 2\n",
    "        # 最后两个使用strides = 1\n",
    "        if filters > layer_filters[-2]:\n",
    "            strides = 2\n",
    "        else:\n",
    "            strides = 1\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2DTranspose(filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            strides=strides,\n",
    "                            padding='same')(x)\n",
    "\n",
    "    x = Activation('sigmoid')(x)\n",
    "    generator = Model(inputs, x, name='generator')\n",
    "#     generator.summary()\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "synthetic-rachel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DT (None, 14, 14, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DT (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DT (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_19 (Conv2DT (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,301,505\n",
      "Trainable params: 1,300,801\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n",
      "time: 152 ms (started: 2021-08-12 19:14:36 +08:00)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "build_generator(layers.Input(shape=(100,)), 28).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "buried-somewhere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 530 ms (started: 2021-08-12 19:15:08 +08:00)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(build_generator(layers.Input(shape=(100,)), 28), show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "brown-clearance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.62 ms (started: 2021-08-12 19:11:05 +08:00)\n"
     ]
    }
   ],
   "source": [
    "def build_discriminator(inputs):\n",
    "    \"\"\"Build a Discriminator Model\n",
    "\n",
    "    用于区分真假的 LeakyReLU-Conv2D 堆栈。\n",
    "     网络不与 BN 收敛所以这里不使用\n",
    "     与 [1] 或原始论文不同。\n",
    "\n",
    "     参数：\n",
    "         输入（层）：鉴别器的输入层（图像）\n",
    "\n",
    "    Returns:\n",
    "        discriminator (Model): Discriminator Model\n",
    "    \"\"\"\n",
    "    kernel_size = 5\n",
    "    layer_filters = [32, 64, 128, 256]\n",
    "\n",
    "    x = inputs\n",
    "    for filters in layer_filters:\n",
    "        # first 3 convolution layers use strides = 2\n",
    "        # last one uses strides = 1\n",
    "        if filters == layer_filters[-1]:\n",
    "            strides = 1\n",
    "        else:\n",
    "            strides = 2\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Conv2D(filters=filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   strides=strides,\n",
    "                   padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    discriminator = Model(inputs, x, name='discriminator')\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "steady-medium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 32)        832       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 64)          51264     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 4097      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,080,577\n",
      "Trainable params: 1,080,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "time: 103 ms (started: 2021-08-12 19:13:30 +08:00)\n"
     ]
    }
   ],
   "source": [
    "build_discriminator(layers.Input(shape=(28,28,1))).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "standard-factory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 352 ms (started: 2021-08-12 19:15:59 +08:00)\n"
     ]
    }
   ],
   "source": [
    "plot_model(build_discriminator(layers.Input(shape=(28,28,1))), show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dynamic-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.84 ms (started: 2021-08-12 21:41:27 +08:00)\n"
     ]
    }
   ],
   "source": [
    "def train(models, x_train, params):\n",
    "    \"\"\"Train the Discriminator and Adversarial Networks\n",
    "\n",
    "    交替训练判别器和对抗网络。\n",
    "    首先用正确的真假图像训练鉴别器。\n",
    "    接下来用假装是真实的假图像训练对抗\n",
    "    每个 save_interval 生成示例图像。\n",
    "\n",
    "    Arguments:\n",
    "        models (list): Generator, Discriminator, Adversarial models\n",
    "        x_train (tensor): Train images\n",
    "        params (list) : Networks parameters\n",
    "\n",
    "    \"\"\"\n",
    "    # the GAN component models\n",
    "    generator, discriminator, adversarial = models\n",
    "    # network parameters\n",
    "    batch_size, latent_size, train_steps, model_name = params\n",
    "    # the generator image is saved every 500 steps\n",
    "    save_interval = 500\n",
    "    # noise vector to see how the generator output evolves during training\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
    "    # number of elements in train dataset\n",
    "    train_size = x_train.shape[0]\n",
    "    for i in range(train_steps):\n",
    "        \n",
    "        # 训练 1 个批次的鉴别器\n",
    "        # 1 批真实（标签=1.0）和假图像（标签=0.0）\n",
    "        # 从数据集中随机选择真实图像\n",
    "        rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
    "        real_images = x_train[rand_indexes]\n",
    "        # generate fake images from noise using generator \n",
    "        # generate noise using uniform distribution\n",
    "        noise = np.random.uniform(-1.0,\n",
    "                                  1.0,\n",
    "                                  size=[batch_size, latent_size])\n",
    "        # generate fake images\n",
    "        fake_images = generator.predict(noise)\n",
    "        # real + fake images = 1 batch of train data\n",
    "        x = np.concatenate((real_images, fake_images))\n",
    "        # label real and fake images\n",
    "        # real images label is 1.0\n",
    "        y = np.ones([2 * batch_size, 1])\n",
    "        # fake images label is 0.0\n",
    "        y[batch_size:, :] = 0.0\n",
    "        # 训练鉴别器网络，记录损失和准确性\n",
    "        loss, acc = discriminator.train_on_batch(x, y)\n",
    "        log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss, acc)\n",
    "\n",
    "        # 训练 1 个批次的对抗网络\n",
    "        # 1 批带有 label=1.0 的假图像\n",
    "        # 因为判别器权重被冻结在对抗网络中\n",
    "        # 只使用均匀分布生成噪声训练生成器\n",
    "        noise = np.random.uniform(-1.0,\n",
    "                                  1.0, \n",
    "                                  size=[batch_size, latent_size])\n",
    "        # label fake images as real or 1.0\n",
    "        y = np.ones([batch_size, 1])\n",
    "        # 训练对抗网络\n",
    "        # 请注意，与判别器训练不同，\n",
    "        # 我们不将假图像保存在变量中\n",
    "        # 假图像进入对抗的鉴别器输入\n",
    "        # 用于分类\n",
    "        # 记录损失和准确率\n",
    "        loss, acc = adversarial.train_on_batch(noise, y)\n",
    "        log = \"%s [adversarial loss: %f, acc: %f]\" % (log, loss, acc)\n",
    "        print(log)\n",
    "        if (i + 1) % save_interval == 0:\n",
    "            # plot generator images on a periodic basis\n",
    "            plot_images(generator,\n",
    "                        noise_input=noise_input,\n",
    "                        show=False,\n",
    "                        step=(i + 1),\n",
    "                        model_name=model_name)\n",
    "   \n",
    "    # 训练生成器后保存模型\n",
    "    # 训练好的生成器可以重新加载\n",
    "    # 未来的 MNIST 数字生成\n",
    "    generator.save(model_name + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "apart-algorithm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.52 ms (started: 2021-08-12 21:44:01 +08:00)\n"
     ]
    }
   ],
   "source": [
    "def plot_images(generator,\n",
    "                noise_input,\n",
    "                show=False,\n",
    "                step=0,\n",
    "                model_name=\"gan\"):\n",
    "    \"\"\"Generate fake images and plot them\n",
    "        出于可视化目的，生成假图像\n",
    "        然后将它们绘制在方形网格中\n",
    "\n",
    "        参数：\n",
    "            generator（Model）：用于生成假图像的生成器模型\n",
    "            noise_input (ndarray)：z 向量数组\n",
    "            show (bool): 是否显示情节\n",
    "            step（int）：附加到保存图像的文件名\n",
    "            model_name（string）：模型名称\n",
    "\n",
    "    \"\"\"\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "    filename = os.path.join(model_name, \"%05d.png\" % step)\n",
    "    images = generator.predict(noise_input)\n",
    "    plt.figure(figsize=(2.2, 2.2))\n",
    "    num_images = images.shape[0]\n",
    "    image_size = images.shape[1]\n",
    "    rows = int(math.sqrt(noise_input.shape[0]))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(rows, rows, i + 1)\n",
    "        image = np.reshape(images[i], [image_size, image_size])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-offense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 14, 14, 32)        832       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 7, 7, 64)          51264     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 4097      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,080,577\n",
      "Trainable params: 1,080,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_input (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_28 (Conv2DT (None, 14, 14, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_29 (Conv2DT (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_30 (Conv2DT (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_31 (Conv2DT (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,301,505\n",
      "Trainable params: 1,300,801\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n",
      "Model: \"dcgan_mnist\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_input (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "generator (Functional)       (None, 28, 28, 1)         1301505   \n",
      "_________________________________________________________________\n",
      "discriminator (Functional)   (None, 1)                 1080577   \n",
      "=================================================================\n",
      "Total params: 2,382,082\n",
      "Trainable params: 1,300,801\n",
      "Non-trainable params: 1,081,281\n",
      "_________________________________________________________________\n",
      "0: [discriminator loss: 0.692629, acc: 0.429688] [adversarial loss: 0.811663, acc: 0.000000]\n",
      "1: [discriminator loss: 0.631186, acc: 0.992188] [adversarial loss: 0.890095, acc: 0.000000]\n",
      "2: [discriminator loss: 0.526727, acc: 1.000000] [adversarial loss: 0.918299, acc: 0.000000]\n",
      "3: [discriminator loss: 0.370951, acc: 1.000000] [adversarial loss: 0.606873, acc: 1.000000]\n",
      "4: [discriminator loss: 0.221915, acc: 1.000000] [adversarial loss: 0.589779, acc: 1.000000]\n",
      "5: [discriminator loss: 0.118282, acc: 1.000000] [adversarial loss: 0.045596, acc: 1.000000]\n",
      "6: [discriminator loss: 0.174475, acc: 1.000000] [adversarial loss: 1.369035, acc: 0.000000]\n",
      "7: [discriminator loss: 0.158651, acc: 0.984375] [adversarial loss: 0.181273, acc: 1.000000]\n",
      "8: [discriminator loss: 0.039685, acc: 1.000000] [adversarial loss: 0.084142, acc: 1.000000]\n",
      "9: [discriminator loss: 0.027333, acc: 1.000000] [adversarial loss: 0.056574, acc: 1.000000]\n",
      "10: [discriminator loss: 0.019674, acc: 1.000000] [adversarial loss: 0.044777, acc: 1.000000]\n",
      "11: [discriminator loss: 0.017203, acc: 1.000000] [adversarial loss: 0.035343, acc: 1.000000]\n",
      "12: [discriminator loss: 0.013786, acc: 1.000000] [adversarial loss: 0.029360, acc: 1.000000]\n",
      "13: [discriminator loss: 0.012462, acc: 1.000000] [adversarial loss: 0.022836, acc: 1.000000]\n",
      "14: [discriminator loss: 0.010414, acc: 1.000000] [adversarial loss: 0.019308, acc: 1.000000]\n",
      "15: [discriminator loss: 0.008905, acc: 1.000000] [adversarial loss: 0.016090, acc: 1.000000]\n",
      "16: [discriminator loss: 0.007579, acc: 1.000000] [adversarial loss: 0.014160, acc: 1.000000]\n",
      "17: [discriminator loss: 0.007057, acc: 1.000000] [adversarial loss: 0.011820, acc: 1.000000]\n",
      "18: [discriminator loss: 0.006857, acc: 1.000000] [adversarial loss: 0.009087, acc: 1.000000]\n",
      "19: [discriminator loss: 0.005223, acc: 1.000000] [adversarial loss: 0.008352, acc: 1.000000]\n",
      "20: [discriminator loss: 0.004537, acc: 1.000000] [adversarial loss: 0.007111, acc: 1.000000]\n",
      "21: [discriminator loss: 0.003758, acc: 1.000000] [adversarial loss: 0.006627, acc: 1.000000]\n",
      "22: [discriminator loss: 0.003598, acc: 1.000000] [adversarial loss: 0.005834, acc: 1.000000]\n",
      "23: [discriminator loss: 0.002822, acc: 1.000000] [adversarial loss: 0.005594, acc: 1.000000]\n",
      "24: [discriminator loss: 0.003253, acc: 1.000000] [adversarial loss: 0.004519, acc: 1.000000]\n",
      "25: [discriminator loss: 0.002325, acc: 1.000000] [adversarial loss: 0.004332, acc: 1.000000]\n",
      "26: [discriminator loss: 0.002210, acc: 1.000000] [adversarial loss: 0.003922, acc: 1.000000]\n",
      "27: [discriminator loss: 0.002279, acc: 1.000000] [adversarial loss: 0.003420, acc: 1.000000]\n",
      "28: [discriminator loss: 0.002033, acc: 1.000000] [adversarial loss: 0.002997, acc: 1.000000]\n",
      "29: [discriminator loss: 0.001712, acc: 1.000000] [adversarial loss: 0.002778, acc: 1.000000]\n",
      "30: [discriminator loss: 0.001634, acc: 1.000000] [adversarial loss: 0.002426, acc: 1.000000]\n",
      "31: [discriminator loss: 0.001320, acc: 1.000000] [adversarial loss: 0.002362, acc: 1.000000]\n",
      "32: [discriminator loss: 0.001265, acc: 1.000000] [adversarial loss: 0.002187, acc: 1.000000]\n",
      "33: [discriminator loss: 0.001521, acc: 1.000000] [adversarial loss: 0.001782, acc: 1.000000]\n",
      "34: [discriminator loss: 0.000981, acc: 1.000000] [adversarial loss: 0.001702, acc: 1.000000]\n",
      "35: [discriminator loss: 0.000877, acc: 1.000000] [adversarial loss: 0.001722, acc: 1.000000]\n",
      "36: [discriminator loss: 0.000789, acc: 1.000000] [adversarial loss: 0.001676, acc: 1.000000]\n",
      "37: [discriminator loss: 0.000769, acc: 1.000000] [adversarial loss: 0.001575, acc: 1.000000]\n",
      "38: [discriminator loss: 0.000787, acc: 1.000000] [adversarial loss: 0.001411, acc: 1.000000]\n",
      "39: [discriminator loss: 0.000777, acc: 1.000000] [adversarial loss: 0.001279, acc: 1.000000]\n",
      "40: [discriminator loss: 0.000596, acc: 1.000000] [adversarial loss: 0.001210, acc: 1.000000]\n",
      "41: [discriminator loss: 0.000732, acc: 1.000000] [adversarial loss: 0.001022, acc: 1.000000]\n",
      "42: [discriminator loss: 0.000593, acc: 1.000000] [adversarial loss: 0.000946, acc: 1.000000]\n",
      "43: [discriminator loss: 0.000486, acc: 1.000000] [adversarial loss: 0.000948, acc: 1.000000]\n",
      "44: [discriminator loss: 0.000559, acc: 1.000000] [adversarial loss: 0.000834, acc: 1.000000]\n",
      "45: [discriminator loss: 0.000527, acc: 1.000000] [adversarial loss: 0.000773, acc: 1.000000]\n",
      "46: [discriminator loss: 0.000482, acc: 1.000000] [adversarial loss: 0.000701, acc: 1.000000]\n",
      "47: [discriminator loss: 0.000476, acc: 1.000000] [adversarial loss: 0.000614, acc: 1.000000]\n",
      "48: [discriminator loss: 0.000362, acc: 1.000000] [adversarial loss: 0.000592, acc: 1.000000]\n",
      "49: [discriminator loss: 0.000330, acc: 1.000000] [adversarial loss: 0.000576, acc: 1.000000]\n",
      "50: [discriminator loss: 0.000315, acc: 1.000000] [adversarial loss: 0.000542, acc: 1.000000]\n",
      "51: [discriminator loss: 0.000250, acc: 1.000000] [adversarial loss: 0.000557, acc: 1.000000]\n",
      "52: [discriminator loss: 0.000300, acc: 1.000000] [adversarial loss: 0.000485, acc: 1.000000]\n",
      "53: [discriminator loss: 0.000240, acc: 1.000000] [adversarial loss: 0.000482, acc: 1.000000]\n",
      "54: [discriminator loss: 0.000253, acc: 1.000000] [adversarial loss: 0.000442, acc: 1.000000]\n",
      "55: [discriminator loss: 0.000290, acc: 1.000000] [adversarial loss: 0.000370, acc: 1.000000]\n",
      "56: [discriminator loss: 0.000195, acc: 1.000000] [adversarial loss: 0.000376, acc: 1.000000]\n",
      "57: [discriminator loss: 0.000169, acc: 1.000000] [adversarial loss: 0.000386, acc: 1.000000]\n",
      "58: [discriminator loss: 0.000181, acc: 1.000000] [adversarial loss: 0.000366, acc: 1.000000]\n",
      "59: [discriminator loss: 0.000180, acc: 1.000000] [adversarial loss: 0.000335, acc: 1.000000]\n",
      "60: [discriminator loss: 0.000146, acc: 1.000000] [adversarial loss: 0.000333, acc: 1.000000]\n",
      "61: [discriminator loss: 0.000148, acc: 1.000000] [adversarial loss: 0.000317, acc: 1.000000]\n",
      "62: [discriminator loss: 0.000163, acc: 1.000000] [adversarial loss: 0.000278, acc: 1.000000]\n",
      "63: [discriminator loss: 0.000135, acc: 1.000000] [adversarial loss: 0.000253, acc: 1.000000]\n",
      "64: [discriminator loss: 0.000102, acc: 1.000000] [adversarial loss: 0.000270, acc: 1.000000]\n",
      "65: [discriminator loss: 0.000125, acc: 1.000000] [adversarial loss: 0.000244, acc: 1.000000]\n",
      "66: [discriminator loss: 0.000099, acc: 1.000000] [adversarial loss: 0.000239, acc: 1.000000]\n",
      "67: [discriminator loss: 0.000111, acc: 1.000000] [adversarial loss: 0.000219, acc: 1.000000]\n",
      "68: [discriminator loss: 0.000091, acc: 1.000000] [adversarial loss: 0.000205, acc: 1.000000]\n",
      "69: [discriminator loss: 0.000089, acc: 1.000000] [adversarial loss: 0.000206, acc: 1.000000]\n",
      "70: [discriminator loss: 0.000076, acc: 1.000000] [adversarial loss: 0.000196, acc: 1.000000]\n",
      "71: [discriminator loss: 0.000081, acc: 1.000000] [adversarial loss: 0.000184, acc: 1.000000]\n",
      "72: [discriminator loss: 0.000064, acc: 1.000000] [adversarial loss: 0.000183, acc: 1.000000]\n",
      "73: [discriminator loss: 0.000070, acc: 1.000000] [adversarial loss: 0.000168, acc: 1.000000]\n",
      "74: [discriminator loss: 0.000068, acc: 1.000000] [adversarial loss: 0.000159, acc: 1.000000]\n",
      "75: [discriminator loss: 0.000057, acc: 1.000000] [adversarial loss: 0.000151, acc: 1.000000]\n",
      "76: [discriminator loss: 0.000061, acc: 1.000000] [adversarial loss: 0.000139, acc: 1.000000]\n",
      "77: [discriminator loss: 0.000050, acc: 1.000000] [adversarial loss: 0.000138, acc: 1.000000]\n",
      "78: [discriminator loss: 0.000048, acc: 1.000000] [adversarial loss: 0.000130, acc: 1.000000]\n",
      "79: [discriminator loss: 0.000052, acc: 1.000000] [adversarial loss: 0.000114, acc: 1.000000]\n",
      "80: [discriminator loss: 0.000066, acc: 1.000000] [adversarial loss: 0.000075, acc: 1.000000]\n",
      "81: [discriminator loss: 0.000039, acc: 1.000000] [adversarial loss: 0.000084, acc: 1.000000]\n",
      "82: [discriminator loss: 0.000042, acc: 1.000000] [adversarial loss: 0.000078, acc: 1.000000]\n",
      "83: [discriminator loss: 0.000041, acc: 1.000000] [adversarial loss: 0.000074, acc: 1.000000]\n",
      "84: [discriminator loss: 0.000032, acc: 1.000000] [adversarial loss: 0.000076, acc: 1.000000]\n",
      "85: [discriminator loss: 0.000037, acc: 1.000000] [adversarial loss: 0.000064, acc: 1.000000]\n",
      "86: [discriminator loss: 0.000036, acc: 1.000000] [adversarial loss: 0.000058, acc: 1.000000]\n",
      "87: [discriminator loss: 0.000038, acc: 1.000000] [adversarial loss: 0.000045, acc: 1.000000]\n",
      "88: [discriminator loss: 0.000024, acc: 1.000000] [adversarial loss: 0.000050, acc: 1.000000]\n",
      "89: [discriminator loss: 0.000027, acc: 1.000000] [adversarial loss: 0.000050, acc: 1.000000]\n",
      "90: [discriminator loss: 0.000025, acc: 1.000000] [adversarial loss: 0.000049, acc: 1.000000]\n",
      "91: [discriminator loss: 0.000022, acc: 1.000000] [adversarial loss: 0.000048, acc: 1.000000]\n",
      "92: [discriminator loss: 0.000020, acc: 1.000000] [adversarial loss: 0.000046, acc: 1.000000]\n",
      "93: [discriminator loss: 0.000022, acc: 1.000000] [adversarial loss: 0.000039, acc: 1.000000]\n",
      "94: [discriminator loss: 0.000017, acc: 1.000000] [adversarial loss: 0.000041, acc: 1.000000]\n",
      "95: [discriminator loss: 0.000021, acc: 1.000000] [adversarial loss: 0.000029, acc: 1.000000]\n",
      "96: [discriminator loss: 0.000022, acc: 1.000000] [adversarial loss: 0.000023, acc: 1.000000]\n",
      "97: [discriminator loss: 0.000019, acc: 1.000000] [adversarial loss: 0.000026, acc: 1.000000]\n",
      "98: [discriminator loss: 0.000016, acc: 1.000000] [adversarial loss: 0.000028, acc: 1.000000]\n",
      "99: [discriminator loss: 0.000012, acc: 1.000000] [adversarial loss: 0.000031, acc: 1.000000]\n",
      "100: [discriminator loss: 0.000023, acc: 1.000000] [adversarial loss: 0.000014, acc: 1.000000]\n",
      "101: [discriminator loss: 0.000012, acc: 1.000000] [adversarial loss: 0.000022, acc: 1.000000]\n",
      "102: [discriminator loss: 0.000015, acc: 1.000000] [adversarial loss: 0.000017, acc: 1.000000]\n",
      "103: [discriminator loss: 0.000012, acc: 1.000000] [adversarial loss: 0.000021, acc: 1.000000]\n",
      "104: [discriminator loss: 0.000011, acc: 1.000000] [adversarial loss: 0.000024, acc: 1.000000]\n",
      "105: [discriminator loss: 0.000010, acc: 1.000000] [adversarial loss: 0.000022, acc: 1.000000]\n",
      "106: [discriminator loss: 0.000009, acc: 1.000000] [adversarial loss: 0.000024, acc: 1.000000]\n",
      "107: [discriminator loss: 0.000009, acc: 1.000000] [adversarial loss: 0.000021, acc: 1.000000]\n",
      "108: [discriminator loss: 0.000013, acc: 1.000000] [adversarial loss: 0.000011, acc: 1.000000]\n",
      "109: [discriminator loss: 0.000008, acc: 1.000000] [adversarial loss: 0.000017, acc: 1.000000]\n",
      "110: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000023, acc: 1.000000]\n",
      "111: [discriminator loss: 0.000005, acc: 1.000000] [adversarial loss: 0.000027, acc: 1.000000]\n",
      "112: [discriminator loss: 0.000008, acc: 1.000000] [adversarial loss: 0.000019, acc: 1.000000]\n",
      "113: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000022, acc: 1.000000]\n",
      "114: [discriminator loss: 0.000005, acc: 1.000000] [adversarial loss: 0.000023, acc: 1.000000]\n",
      "115: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000026, acc: 1.000000]\n",
      "116: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000019, acc: 1.000000]\n",
      "117: [discriminator loss: 0.000010, acc: 1.000000] [adversarial loss: 0.000009, acc: 1.000000]\n",
      "118: [discriminator loss: 0.000005, acc: 1.000000] [adversarial loss: 0.000019, acc: 1.000000]\n",
      "119: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000016, acc: 1.000000]\n",
      "120: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000021, acc: 1.000000]\n",
      "121: [discriminator loss: 0.000005, acc: 1.000000] [adversarial loss: 0.000017, acc: 1.000000]\n",
      "122: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000017, acc: 1.000000]\n",
      "123: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000024, acc: 1.000000]\n",
      "124: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000028, acc: 1.000000]\n",
      "125: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000025, acc: 1.000000]\n",
      "126: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000012, acc: 1.000000]\n",
      "127: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000024, acc: 1.000000]\n",
      "128: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000019, acc: 1.000000]\n",
      "129: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000030, acc: 1.000000]\n",
      "130: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000032, acc: 1.000000]\n",
      "131: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000038, acc: 1.000000]\n",
      "132: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000043, acc: 1.000000]\n",
      "133: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000036, acc: 1.000000]\n",
      "134: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000028, acc: 1.000000]\n",
      "135: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000019, acc: 1.000000]\n",
      "136: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000031, acc: 1.000000]\n",
      "137: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000041, acc: 1.000000]\n",
      "138: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000047, acc: 1.000000]\n",
      "139: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000016, acc: 1.000000]\n",
      "140: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000041, acc: 1.000000]\n",
      "141: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000061, acc: 1.000000]\n",
      "142: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000059, acc: 1.000000]\n",
      "143: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000074, acc: 1.000000]\n",
      "144: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000001, acc: 1.000000]\n",
      "145: [discriminator loss: 0.000012, acc: 1.000000] [adversarial loss: 0.393000, acc: 1.000000]\n",
      "146: [discriminator loss: 0.000012, acc: 1.000000] [adversarial loss: 0.000009, acc: 1.000000]\n",
      "147: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000057, acc: 1.000000]\n",
      "148: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000093, acc: 1.000000]\n",
      "149: [discriminator loss: 0.000000, acc: 1.000000] [adversarial loss: 0.000143, acc: 1.000000]\n",
      "150: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000191, acc: 1.000000]\n",
      "151: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000242, acc: 1.000000]\n",
      "152: [discriminator loss: 0.000000, acc: 1.000000] [adversarial loss: 0.000330, acc: 1.000000]\n",
      "153: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000317, acc: 1.000000]\n",
      "154: [discriminator loss: 0.000000, acc: 1.000000] [adversarial loss: 0.000477, acc: 1.000000]\n",
      "155: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000230, acc: 1.000000]\n",
      "156: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000525, acc: 1.000000]\n",
      "157: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000858, acc: 1.000000]\n",
      "158: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000579, acc: 1.000000]\n",
      "159: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.001293, acc: 1.000000]\n",
      "160: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.001647, acc: 1.000000]\n",
      "161: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.001473, acc: 1.000000]\n",
      "162: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000169, acc: 1.000000]\n",
      "163: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.003201, acc: 1.000000]\n",
      "164: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000503, acc: 1.000000]\n",
      "165: [discriminator loss: 0.000000, acc: 1.000000] [adversarial loss: 0.000727, acc: 1.000000]\n",
      "166: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000225, acc: 1.000000]\n",
      "167: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.001447, acc: 1.000000]\n",
      "168: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.001049, acc: 1.000000]\n",
      "169: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000929, acc: 1.000000]\n",
      "170: [discriminator loss: 0.000000, acc: 1.000000] [adversarial loss: 0.002695, acc: 1.000000]\n",
      "171: [discriminator loss: 0.000000, acc: 1.000000] [adversarial loss: 0.003668, acc: 1.000000]\n",
      "172: [discriminator loss: 0.000000, acc: 1.000000] [adversarial loss: 0.005107, acc: 1.000000]\n",
      "173: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.003954, acc: 1.000000]\n",
      "174: [discriminator loss: 0.000013, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "175: [discriminator loss: 3.074631, acc: 0.500000] [adversarial loss: 4.469131, acc: 0.000000]\n",
      "176: [discriminator loss: 0.000089, acc: 1.000000] [adversarial loss: 2.066056, acc: 0.000000]\n",
      "177: [discriminator loss: 0.000034, acc: 1.000000] [adversarial loss: 0.632038, acc: 0.718750]\n",
      "178: [discriminator loss: 0.000017, acc: 1.000000] [adversarial loss: 0.381725, acc: 1.000000]\n",
      "179: [discriminator loss: 0.000016, acc: 1.000000] [adversarial loss: 0.306640, acc: 1.000000]\n",
      "180: [discriminator loss: 0.000025, acc: 1.000000] [adversarial loss: 0.261930, acc: 1.000000]\n",
      "181: [discriminator loss: 0.000152, acc: 1.000000] [adversarial loss: 0.227509, acc: 1.000000]\n",
      "182: [discriminator loss: 0.000026, acc: 1.000000] [adversarial loss: 0.202063, acc: 1.000000]\n",
      "183: [discriminator loss: 0.000009, acc: 1.000000] [adversarial loss: 0.181091, acc: 1.000000]\n",
      "184: [discriminator loss: 0.000266, acc: 1.000000] [adversarial loss: 0.163089, acc: 1.000000]\n",
      "185: [discriminator loss: 0.000036, acc: 1.000000] [adversarial loss: 0.147243, acc: 1.000000]\n",
      "186: [discriminator loss: 0.000133, acc: 1.000000] [adversarial loss: 0.134891, acc: 1.000000]\n",
      "187: [discriminator loss: 0.000828, acc: 1.000000] [adversarial loss: 0.118054, acc: 1.000000]\n",
      "188: [discriminator loss: 0.000636, acc: 1.000000] [adversarial loss: 0.105946, acc: 1.000000]\n",
      "189: [discriminator loss: 0.000005, acc: 1.000000] [adversarial loss: 0.099219, acc: 1.000000]\n",
      "190: [discriminator loss: 0.000126, acc: 1.000000] [adversarial loss: 0.090394, acc: 1.000000]\n",
      "191: [discriminator loss: 0.000012, acc: 1.000000] [adversarial loss: 0.085255, acc: 1.000000]\n",
      "192: [discriminator loss: 0.000066, acc: 1.000000] [adversarial loss: 0.080108, acc: 1.000000]\n",
      "193: [discriminator loss: 0.000130, acc: 1.000000] [adversarial loss: 0.074605, acc: 1.000000]\n",
      "194: [discriminator loss: 0.000022, acc: 1.000000] [adversarial loss: 0.070802, acc: 1.000000]\n",
      "195: [discriminator loss: 0.000053, acc: 1.000000] [adversarial loss: 0.066564, acc: 1.000000]\n",
      "196: [discriminator loss: 0.000042, acc: 1.000000] [adversarial loss: 0.062368, acc: 1.000000]\n",
      "197: [discriminator loss: 0.000093, acc: 1.000000] [adversarial loss: 0.058161, acc: 1.000000]\n",
      "198: [discriminator loss: 0.000018, acc: 1.000000] [adversarial loss: 0.054978, acc: 1.000000]\n",
      "199: [discriminator loss: 0.000013, acc: 1.000000] [adversarial loss: 0.051900, acc: 1.000000]\n",
      "200: [discriminator loss: 0.005818, acc: 0.992188] [adversarial loss: 0.022905, acc: 1.000000]\n",
      "201: [discriminator loss: 0.000096, acc: 1.000000] [adversarial loss: 0.022164, acc: 1.000000]\n",
      "202: [discriminator loss: 0.000065, acc: 1.000000] [adversarial loss: 0.021260, acc: 1.000000]\n",
      "203: [discriminator loss: 0.000025, acc: 1.000000] [adversarial loss: 0.020720, acc: 1.000000]\n",
      "204: [discriminator loss: 0.000119, acc: 1.000000] [adversarial loss: 0.020006, acc: 1.000000]\n",
      "205: [discriminator loss: 0.000040, acc: 1.000000] [adversarial loss: 0.019543, acc: 1.000000]\n",
      "206: [discriminator loss: 0.000037, acc: 1.000000] [adversarial loss: 0.019217, acc: 1.000000]\n",
      "207: [discriminator loss: 0.000055, acc: 1.000000] [adversarial loss: 0.018677, acc: 1.000000]\n",
      "208: [discriminator loss: 0.000061, acc: 1.000000] [adversarial loss: 0.018593, acc: 1.000000]\n",
      "209: [discriminator loss: 0.000129, acc: 1.000000] [adversarial loss: 0.017584, acc: 1.000000]\n",
      "210: [discriminator loss: 0.000069, acc: 1.000000] [adversarial loss: 0.017185, acc: 1.000000]\n",
      "211: [discriminator loss: 0.000084, acc: 1.000000] [adversarial loss: 0.016733, acc: 1.000000]\n",
      "212: [discriminator loss: 0.000086, acc: 1.000000] [adversarial loss: 0.016817, acc: 1.000000]\n",
      "213: [discriminator loss: 0.000108, acc: 1.000000] [adversarial loss: 0.016609, acc: 1.000000]\n",
      "214: [discriminator loss: 0.000110, acc: 1.000000] [adversarial loss: 0.016511, acc: 1.000000]\n",
      "215: [discriminator loss: 0.000137, acc: 1.000000] [adversarial loss: 0.015916, acc: 1.000000]\n",
      "216: [discriminator loss: 0.000319, acc: 1.000000] [adversarial loss: 0.014903, acc: 1.000000]\n",
      "217: [discriminator loss: 0.003257, acc: 1.000000] [adversarial loss: 0.005091, acc: 1.000000]\n",
      "218: [discriminator loss: 0.000542, acc: 1.000000] [adversarial loss: 0.006446, acc: 1.000000]\n",
      "219: [discriminator loss: 0.000488, acc: 1.000000] [adversarial loss: 0.007944, acc: 1.000000]\n",
      "220: [discriminator loss: 0.000447, acc: 1.000000] [adversarial loss: 0.009904, acc: 1.000000]\n",
      "221: [discriminator loss: 0.000416, acc: 1.000000] [adversarial loss: 0.010968, acc: 1.000000]\n",
      "222: [discriminator loss: 0.000507, acc: 1.000000] [adversarial loss: 0.010871, acc: 1.000000]\n",
      "223: [discriminator loss: 0.000398, acc: 1.000000] [adversarial loss: 0.010461, acc: 1.000000]\n",
      "224: [discriminator loss: 0.000410, acc: 1.000000] [adversarial loss: 0.009787, acc: 1.000000]\n",
      "225: [discriminator loss: 0.000453, acc: 1.000000] [adversarial loss: 0.009764, acc: 1.000000]\n",
      "226: [discriminator loss: 0.000532, acc: 1.000000] [adversarial loss: 0.010422, acc: 1.000000]\n",
      "227: [discriminator loss: 0.001475, acc: 1.000000] [adversarial loss: 0.006993, acc: 1.000000]\n",
      "228: [discriminator loss: 0.000989, acc: 1.000000] [adversarial loss: 0.016782, acc: 1.000000]\n",
      "229: [discriminator loss: 0.000879, acc: 1.000000] [adversarial loss: 0.033202, acc: 1.000000]\n",
      "230: [discriminator loss: 0.001501, acc: 1.000000] [adversarial loss: 0.044340, acc: 1.000000]\n",
      "231: [discriminator loss: 0.004072, acc: 1.000000] [adversarial loss: 0.041254, acc: 1.000000]\n",
      "232: [discriminator loss: 0.006392, acc: 1.000000] [adversarial loss: 3.278895, acc: 0.000000]\n",
      "233: [discriminator loss: 0.000699, acc: 1.000000] [adversarial loss: 1.221486, acc: 0.000000]\n",
      "234: [discriminator loss: 0.001699, acc: 1.000000] [adversarial loss: 0.016269, acc: 1.000000]\n",
      "235: [discriminator loss: 0.003160, acc: 1.000000] [adversarial loss: 0.001468, acc: 1.000000]\n",
      "236: [discriminator loss: 0.003864, acc: 1.000000] [adversarial loss: 0.484180, acc: 0.984375]\n",
      "237: [discriminator loss: 0.001097, acc: 1.000000] [adversarial loss: 0.008840, acc: 1.000000]\n",
      "238: [discriminator loss: 0.000772, acc: 1.000000] [adversarial loss: 0.011124, acc: 1.000000]\n",
      "239: [discriminator loss: 0.000508, acc: 1.000000] [adversarial loss: 0.018507, acc: 1.000000]\n",
      "240: [discriminator loss: 0.004028, acc: 1.000000] [adversarial loss: 0.000010, acc: 1.000000]\n",
      "241: [discriminator loss: 0.040712, acc: 1.000000] [adversarial loss: 18.892662, acc: 0.000000]\n",
      "242: [discriminator loss: 0.621940, acc: 0.843750] [adversarial loss: 0.000001, acc: 1.000000]\n",
      "243: [discriminator loss: 0.161439, acc: 0.984375] [adversarial loss: 4.696508, acc: 0.000000]\n",
      "244: [discriminator loss: 0.027216, acc: 0.992188] [adversarial loss: 0.136349, acc: 1.000000]\n",
      "245: [discriminator loss: 0.000182, acc: 1.000000] [adversarial loss: 0.075143, acc: 1.000000]\n",
      "246: [discriminator loss: 0.012189, acc: 0.992188] [adversarial loss: 0.010782, acc: 1.000000]\n",
      "247: [discriminator loss: 0.000608, acc: 1.000000] [adversarial loss: 0.009782, acc: 1.000000]\n",
      "248: [discriminator loss: 0.000622, acc: 1.000000] [adversarial loss: 0.009255, acc: 1.000000]\n",
      "249: [discriminator loss: 0.000632, acc: 1.000000] [adversarial loss: 0.009846, acc: 1.000000]\n",
      "250: [discriminator loss: 0.000688, acc: 1.000000] [adversarial loss: 0.008143, acc: 1.000000]\n",
      "251: [discriminator loss: 0.000664, acc: 1.000000] [adversarial loss: 0.007791, acc: 1.000000]\n",
      "252: [discriminator loss: 0.000745, acc: 1.000000] [adversarial loss: 0.007432, acc: 1.000000]\n",
      "253: [discriminator loss: 0.000725, acc: 1.000000] [adversarial loss: 0.006502, acc: 1.000000]\n",
      "254: [discriminator loss: 0.000790, acc: 1.000000] [adversarial loss: 0.006305, acc: 1.000000]\n",
      "255: [discriminator loss: 0.000892, acc: 1.000000] [adversarial loss: 0.006284, acc: 1.000000]\n",
      "256: [discriminator loss: 0.000737, acc: 1.000000] [adversarial loss: 0.006284, acc: 1.000000]\n",
      "257: [discriminator loss: 0.000805, acc: 1.000000] [adversarial loss: 0.005561, acc: 1.000000]\n",
      "258: [discriminator loss: 0.000864, acc: 1.000000] [adversarial loss: 0.005529, acc: 1.000000]\n",
      "259: [discriminator loss: 0.000914, acc: 1.000000] [adversarial loss: 0.005045, acc: 1.000000]\n",
      "260: [discriminator loss: 0.000743, acc: 1.000000] [adversarial loss: 0.005277, acc: 1.000000]\n",
      "261: [discriminator loss: 0.000901, acc: 1.000000] [adversarial loss: 0.005270, acc: 1.000000]\n",
      "262: [discriminator loss: 0.000825, acc: 1.000000] [adversarial loss: 0.005131, acc: 1.000000]\n",
      "263: [discriminator loss: 0.000779, acc: 1.000000] [adversarial loss: 0.005779, acc: 1.000000]\n",
      "264: [discriminator loss: 0.007284, acc: 0.992188] [adversarial loss: 0.000488, acc: 1.000000]\n",
      "265: [discriminator loss: 0.002164, acc: 1.000000] [adversarial loss: 0.001438, acc: 1.000000]\n",
      "266: [discriminator loss: 0.001560, acc: 1.000000] [adversarial loss: 0.002484, acc: 1.000000]\n",
      "267: [discriminator loss: 0.001080, acc: 1.000000] [adversarial loss: 0.003504, acc: 1.000000]\n",
      "268: [discriminator loss: 0.001043, acc: 1.000000] [adversarial loss: 0.004467, acc: 1.000000]\n",
      "269: [discriminator loss: 0.000965, acc: 1.000000] [adversarial loss: 0.005472, acc: 1.000000]\n",
      "270: [discriminator loss: 0.001112, acc: 1.000000] [adversarial loss: 0.005207, acc: 1.000000]\n",
      "271: [discriminator loss: 0.001231, acc: 1.000000] [adversarial loss: 0.005397, acc: 1.000000]\n",
      "272: [discriminator loss: 0.003754, acc: 1.000000] [adversarial loss: 0.001163, acc: 1.000000]\n",
      "273: [discriminator loss: 0.002145, acc: 1.000000] [adversarial loss: 0.005785, acc: 1.000000]\n",
      "274: [discriminator loss: 0.001131, acc: 1.000000] [adversarial loss: 0.010113, acc: 1.000000]\n",
      "275: [discriminator loss: 0.001501, acc: 1.000000] [adversarial loss: 0.019239, acc: 1.000000]\n",
      "276: [discriminator loss: 0.007628, acc: 0.992188] [adversarial loss: 0.000275, acc: 1.000000]\n",
      "277: [discriminator loss: 0.011370, acc: 1.000000] [adversarial loss: 2.729088, acc: 0.000000]\n",
      "278: [discriminator loss: 0.235505, acc: 0.929688] [adversarial loss: 20.770042, acc: 0.000000]\n",
      "279: [discriminator loss: 3.783760, acc: 0.609375] [adversarial loss: 0.000089, acc: 1.000000]\n",
      "280: [discriminator loss: 0.288025, acc: 0.875000] [adversarial loss: 3.096408, acc: 0.000000]\n",
      "281: [discriminator loss: 0.196604, acc: 0.960938] [adversarial loss: 0.131280, acc: 1.000000]\n",
      "282: [discriminator loss: 0.074799, acc: 0.976562] [adversarial loss: 0.023171, acc: 1.000000]\n",
      "283: [discriminator loss: 0.083570, acc: 0.976562] [adversarial loss: 0.005685, acc: 1.000000]\n",
      "284: [discriminator loss: 0.013722, acc: 1.000000] [adversarial loss: 0.009932, acc: 1.000000]\n",
      "285: [discriminator loss: 0.090662, acc: 0.992188] [adversarial loss: 0.007801, acc: 1.000000]\n",
      "286: [discriminator loss: 0.026725, acc: 0.992188] [adversarial loss: 0.008639, acc: 1.000000]\n",
      "287: [discriminator loss: 0.013772, acc: 1.000000] [adversarial loss: 0.015355, acc: 1.000000]\n",
      "288: [discriminator loss: 0.008979, acc: 1.000000] [adversarial loss: 0.027882, acc: 1.000000]\n",
      "289: [discriminator loss: 0.020343, acc: 0.992188] [adversarial loss: 0.020636, acc: 1.000000]\n",
      "290: [discriminator loss: 0.041154, acc: 0.992188] [adversarial loss: 0.013732, acc: 1.000000]\n",
      "291: [discriminator loss: 0.045994, acc: 0.992188] [adversarial loss: 0.009620, acc: 1.000000]\n",
      "292: [discriminator loss: 0.020056, acc: 1.000000] [adversarial loss: 0.021732, acc: 1.000000]\n",
      "293: [discriminator loss: 0.011006, acc: 1.000000] [adversarial loss: 0.034923, acc: 1.000000]\n",
      "294: [discriminator loss: 0.046853, acc: 0.984375] [adversarial loss: 0.005502, acc: 1.000000]\n",
      "295: [discriminator loss: 0.019860, acc: 1.000000] [adversarial loss: 0.044791, acc: 1.000000]\n",
      "296: [discriminator loss: 0.005979, acc: 1.000000] [adversarial loss: 0.054498, acc: 1.000000]\n",
      "297: [discriminator loss: 0.004267, acc: 1.000000] [adversarial loss: 0.061151, acc: 1.000000]\n",
      "298: [discriminator loss: 0.003795, acc: 1.000000] [adversarial loss: 0.051682, acc: 1.000000]\n",
      "299: [discriminator loss: 0.003188, acc: 1.000000] [adversarial loss: 0.042174, acc: 1.000000]\n",
      "300: [discriminator loss: 0.044003, acc: 0.992188] [adversarial loss: 0.006411, acc: 1.000000]\n",
      "301: [discriminator loss: 0.006404, acc: 1.000000] [adversarial loss: 0.015376, acc: 1.000000]\n",
      "302: [discriminator loss: 0.003783, acc: 1.000000] [adversarial loss: 0.023835, acc: 1.000000]\n",
      "303: [discriminator loss: 0.003263, acc: 1.000000] [adversarial loss: 0.030540, acc: 1.000000]\n",
      "304: [discriminator loss: 0.010216, acc: 1.000000] [adversarial loss: 0.005417, acc: 1.000000]\n",
      "305: [discriminator loss: 0.006025, acc: 1.000000] [adversarial loss: 0.014934, acc: 1.000000]\n",
      "306: [discriminator loss: 0.003705, acc: 1.000000] [adversarial loss: 0.024400, acc: 1.000000]\n",
      "307: [discriminator loss: 0.054153, acc: 0.984375] [adversarial loss: 0.000144, acc: 1.000000]\n",
      "308: [discriminator loss: 0.043063, acc: 1.000000] [adversarial loss: 0.730242, acc: 0.453125]\n",
      "309: [discriminator loss: 0.001214, acc: 1.000000] [adversarial loss: 0.014022, acc: 1.000000]\n",
      "310: [discriminator loss: 0.003712, acc: 1.000000] [adversarial loss: 0.005351, acc: 1.000000]\n",
      "311: [discriminator loss: 0.001529, acc: 1.000000] [adversarial loss: 0.006991, acc: 1.000000]\n",
      "312: [discriminator loss: 0.004736, acc: 1.000000] [adversarial loss: 0.002332, acc: 1.000000]\n",
      "313: [discriminator loss: 0.002814, acc: 1.000000] [adversarial loss: 0.004702, acc: 1.000000]\n",
      "314: [discriminator loss: 0.003416, acc: 1.000000] [adversarial loss: 0.002997, acc: 1.000000]\n",
      "315: [discriminator loss: 0.002656, acc: 1.000000] [adversarial loss: 0.004422, acc: 1.000000]\n",
      "316: [discriminator loss: 0.005638, acc: 1.000000] [adversarial loss: 0.001342, acc: 1.000000]\n",
      "317: [discriminator loss: 0.004027, acc: 1.000000] [adversarial loss: 0.005021, acc: 1.000000]\n",
      "318: [discriminator loss: 0.002892, acc: 1.000000] [adversarial loss: 0.004070, acc: 1.000000]\n",
      "319: [discriminator loss: 0.004644, acc: 1.000000] [adversarial loss: 0.002063, acc: 1.000000]\n",
      "320: [discriminator loss: 0.003017, acc: 1.000000] [adversarial loss: 0.005719, acc: 1.000000]\n",
      "321: [discriminator loss: 0.002068, acc: 1.000000] [adversarial loss: 0.008707, acc: 1.000000]\n",
      "322: [discriminator loss: 0.002205, acc: 1.000000] [adversarial loss: 0.007025, acc: 1.000000]\n",
      "323: [discriminator loss: 0.001944, acc: 1.000000] [adversarial loss: 0.006568, acc: 1.000000]\n",
      "324: [discriminator loss: 0.027530, acc: 0.992188] [adversarial loss: 0.000006, acc: 1.000000]\n",
      "325: [discriminator loss: 0.062060, acc: 1.000000] [adversarial loss: 7.444027, acc: 0.000000]\n",
      "326: [discriminator loss: 0.195373, acc: 0.937500] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "327: [discriminator loss: 0.819653, acc: 0.507812] [adversarial loss: 6.404305, acc: 0.000000]\n",
      "328: [discriminator loss: 0.187879, acc: 0.968750] [adversarial loss: 0.000153, acc: 1.000000]\n",
      "329: [discriminator loss: 0.054350, acc: 0.992188] [adversarial loss: 0.000323, acc: 1.000000]\n",
      "330: [discriminator loss: 0.023204, acc: 1.000000] [adversarial loss: 0.000520, acc: 1.000000]\n",
      "331: [discriminator loss: 0.026304, acc: 0.992188] [adversarial loss: 0.000466, acc: 1.000000]\n",
      "332: [discriminator loss: 0.022007, acc: 1.000000] [adversarial loss: 0.001043, acc: 1.000000]\n",
      "333: [discriminator loss: 0.024873, acc: 0.992188] [adversarial loss: 0.000677, acc: 1.000000]\n",
      "334: [discriminator loss: 0.019023, acc: 1.000000] [adversarial loss: 0.001452, acc: 1.000000]\n",
      "335: [discriminator loss: 0.012977, acc: 1.000000] [adversarial loss: 0.002282, acc: 1.000000]\n",
      "336: [discriminator loss: 0.044246, acc: 0.992188] [adversarial loss: 0.000503, acc: 1.000000]\n",
      "337: [discriminator loss: 0.017893, acc: 1.000000] [adversarial loss: 0.002377, acc: 1.000000]\n",
      "338: [discriminator loss: 0.028065, acc: 0.992188] [adversarial loss: 0.001005, acc: 1.000000]\n",
      "339: [discriminator loss: 0.013656, acc: 1.000000] [adversarial loss: 0.002624, acc: 1.000000]\n",
      "340: [discriminator loss: 0.008930, acc: 1.000000] [adversarial loss: 0.004160, acc: 1.000000]\n",
      "341: [discriminator loss: 0.007695, acc: 1.000000] [adversarial loss: 0.007397, acc: 1.000000]\n",
      "342: [discriminator loss: 0.005849, acc: 1.000000] [adversarial loss: 0.007244, acc: 1.000000]\n",
      "343: [discriminator loss: 0.094022, acc: 0.984375] [adversarial loss: 0.000060, acc: 1.000000]\n",
      "344: [discriminator loss: 0.056279, acc: 1.000000] [adversarial loss: 0.037783, acc: 1.000000]\n",
      "345: [discriminator loss: 0.004813, acc: 1.000000] [adversarial loss: 0.013578, acc: 1.000000]\n",
      "346: [discriminator loss: 0.049302, acc: 0.984375] [adversarial loss: 0.000065, acc: 1.000000]\n",
      "347: [discriminator loss: 0.042609, acc: 1.000000] [adversarial loss: 0.038119, acc: 1.000000]\n",
      "348: [discriminator loss: 0.003397, acc: 1.000000] [adversarial loss: 0.023486, acc: 1.000000]\n",
      "349: [discriminator loss: 0.011407, acc: 0.992188] [adversarial loss: 0.002014, acc: 1.000000]\n",
      "350: [discriminator loss: 0.010885, acc: 1.000000] [adversarial loss: 0.021308, acc: 1.000000]\n",
      "351: [discriminator loss: 0.009907, acc: 0.992188] [adversarial loss: 0.003036, acc: 1.000000]\n",
      "352: [discriminator loss: 0.009781, acc: 1.000000] [adversarial loss: 0.038284, acc: 1.000000]\n",
      "353: [discriminator loss: 0.004582, acc: 1.000000] [adversarial loss: 0.051579, acc: 1.000000]\n",
      "354: [discriminator loss: 0.005821, acc: 1.000000] [adversarial loss: 0.077984, acc: 1.000000]\n",
      "355: [discriminator loss: 0.006856, acc: 1.000000] [adversarial loss: 0.121063, acc: 1.000000]\n",
      "356: [discriminator loss: 0.051797, acc: 0.992188] [adversarial loss: 0.002088, acc: 1.000000]\n",
      "357: [discriminator loss: 0.050562, acc: 1.000000] [adversarial loss: 9.303889, acc: 0.000000]\n",
      "358: [discriminator loss: 0.114171, acc: 0.968750] [adversarial loss: 0.000947, acc: 1.000000]\n",
      "359: [discriminator loss: 0.057435, acc: 1.000000] [adversarial loss: 0.814167, acc: 0.218750]\n",
      "360: [discriminator loss: 0.009786, acc: 1.000000] [adversarial loss: 1.239618, acc: 0.000000]\n",
      "361: [discriminator loss: 0.021487, acc: 0.992188] [adversarial loss: 0.098430, acc: 1.000000]\n",
      "362: [discriminator loss: 0.082956, acc: 1.000000] [adversarial loss: 10.940429, acc: 0.000000]\n",
      "363: [discriminator loss: 0.474956, acc: 0.875000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "364: [discriminator loss: 2.975321, acc: 0.500000] [adversarial loss: 1.737488, acc: 0.000000]\n",
      "365: [discriminator loss: 1.562104, acc: 0.460938] [adversarial loss: 4.298094, acc: 0.000000]\n",
      "366: [discriminator loss: 1.072625, acc: 0.742188] [adversarial loss: 0.001905, acc: 1.000000]\n",
      "367: [discriminator loss: 0.951105, acc: 0.500000] [adversarial loss: 0.854441, acc: 0.140625]\n",
      "368: [discriminator loss: 0.094710, acc: 0.976562] [adversarial loss: 0.362557, acc: 1.000000]\n",
      "369: [discriminator loss: 0.067801, acc: 0.984375] [adversarial loss: 0.315370, acc: 1.000000]\n",
      "370: [discriminator loss: 0.073858, acc: 0.992188] [adversarial loss: 0.349328, acc: 1.000000]\n",
      "371: [discriminator loss: 0.129005, acc: 0.976562] [adversarial loss: 0.147485, acc: 1.000000]\n",
      "372: [discriminator loss: 0.061705, acc: 1.000000] [adversarial loss: 0.422297, acc: 1.000000]\n",
      "373: [discriminator loss: 0.077444, acc: 0.992188] [adversarial loss: 0.221904, acc: 1.000000]\n",
      "374: [discriminator loss: 0.037463, acc: 1.000000] [adversarial loss: 0.413774, acc: 1.000000]\n",
      "375: [discriminator loss: 0.028262, acc: 1.000000] [adversarial loss: 0.370822, acc: 1.000000]\n",
      "376: [discriminator loss: 0.019065, acc: 1.000000] [adversarial loss: 0.397247, acc: 1.000000]\n",
      "377: [discriminator loss: 0.071746, acc: 0.976562] [adversarial loss: 0.040265, acc: 1.000000]\n",
      "378: [discriminator loss: 0.046206, acc: 1.000000] [adversarial loss: 0.187539, acc: 1.000000]\n",
      "379: [discriminator loss: 0.018176, acc: 1.000000] [adversarial loss: 0.154998, acc: 1.000000]\n",
      "380: [discriminator loss: 0.021575, acc: 1.000000] [adversarial loss: 0.074275, acc: 1.000000]\n",
      "381: [discriminator loss: 0.066894, acc: 0.984375] [adversarial loss: 0.015464, acc: 1.000000]\n",
      "382: [discriminator loss: 0.039432, acc: 1.000000] [adversarial loss: 0.164460, acc: 1.000000]\n",
      "383: [discriminator loss: 0.052791, acc: 0.992188] [adversarial loss: 0.024741, acc: 1.000000]\n",
      "384: [discriminator loss: 0.032195, acc: 1.000000] [adversarial loss: 0.122601, acc: 1.000000]\n",
      "385: [discriminator loss: 0.025298, acc: 1.000000] [adversarial loss: 0.175721, acc: 1.000000]\n",
      "386: [discriminator loss: 0.044952, acc: 1.000000] [adversarial loss: 0.212748, acc: 1.000000]\n",
      "387: [discriminator loss: 0.103574, acc: 0.984375] [adversarial loss: 1.469934, acc: 0.000000]\n",
      "388: [discriminator loss: 0.134410, acc: 0.968750] [adversarial loss: 0.016719, acc: 1.000000]\n",
      "389: [discriminator loss: 0.278037, acc: 0.828125] [adversarial loss: 9.454012, acc: 0.000000]\n",
      "390: [discriminator loss: 1.983835, acc: 0.546875] [adversarial loss: 0.003800, acc: 1.000000]\n",
      "391: [discriminator loss: 0.369232, acc: 0.773438] [adversarial loss: 0.492568, acc: 0.890625]\n",
      "392: [discriminator loss: 0.051624, acc: 1.000000] [adversarial loss: 0.403376, acc: 1.000000]\n",
      "393: [discriminator loss: 0.029255, acc: 1.000000] [adversarial loss: 0.436864, acc: 0.984375]\n",
      "394: [discriminator loss: 0.032259, acc: 0.992188] [adversarial loss: 0.218803, acc: 1.000000]\n",
      "395: [discriminator loss: 0.033756, acc: 0.992188] [adversarial loss: 0.176370, acc: 1.000000]\n",
      "396: [discriminator loss: 0.024654, acc: 1.000000] [adversarial loss: 0.199825, acc: 1.000000]\n",
      "397: [discriminator loss: 0.039595, acc: 0.992188] [adversarial loss: 0.115088, acc: 1.000000]\n",
      "398: [discriminator loss: 0.039777, acc: 0.992188] [adversarial loss: 0.082239, acc: 1.000000]\n",
      "399: [discriminator loss: 0.044352, acc: 0.992188] [adversarial loss: 0.169900, acc: 1.000000]\n",
      "400: [discriminator loss: 0.049870, acc: 0.992188] [adversarial loss: 0.051359, acc: 1.000000]\n",
      "401: [discriminator loss: 0.112513, acc: 0.984375] [adversarial loss: 0.036512, acc: 1.000000]\n",
      "402: [discriminator loss: 0.118134, acc: 0.984375] [adversarial loss: 0.035467, acc: 1.000000]\n",
      "403: [discriminator loss: 0.042179, acc: 1.000000] [adversarial loss: 0.194238, acc: 1.000000]\n",
      "404: [discriminator loss: 0.058655, acc: 0.992188] [adversarial loss: 0.031181, acc: 1.000000]\n",
      "405: [discriminator loss: 0.031304, acc: 1.000000] [adversarial loss: 0.359538, acc: 1.000000]\n",
      "406: [discriminator loss: 0.026109, acc: 1.000000] [adversarial loss: 0.109554, acc: 1.000000]\n",
      "407: [discriminator loss: 0.062966, acc: 0.984375] [adversarial loss: 0.026193, acc: 1.000000]\n",
      "408: [discriminator loss: 0.078395, acc: 0.984375] [adversarial loss: 0.550678, acc: 0.875000]\n",
      "409: [discriminator loss: 0.050486, acc: 0.984375] [adversarial loss: 0.025285, acc: 1.000000]\n",
      "410: [discriminator loss: 0.060182, acc: 1.000000] [adversarial loss: 3.661657, acc: 0.000000]\n",
      "411: [discriminator loss: 0.135225, acc: 0.953125] [adversarial loss: 0.002241, acc: 1.000000]\n",
      "412: [discriminator loss: 0.417988, acc: 0.695312] [adversarial loss: 9.742757, acc: 0.000000]\n",
      "413: [discriminator loss: 1.598039, acc: 0.562500] [adversarial loss: 0.024596, acc: 1.000000]\n",
      "414: [discriminator loss: 0.232434, acc: 0.898438] [adversarial loss: 0.614227, acc: 0.750000]\n",
      "415: [discriminator loss: 0.063578, acc: 1.000000] [adversarial loss: 0.942721, acc: 0.000000]\n",
      "416: [discriminator loss: 0.048382, acc: 1.000000] [adversarial loss: 1.423146, acc: 0.000000]\n",
      "417: [discriminator loss: 0.214359, acc: 0.992188] [adversarial loss: 4.891989, acc: 0.000000]\n",
      "418: [discriminator loss: 0.210070, acc: 0.945312] [adversarial loss: 0.629723, acc: 0.781250]\n",
      "419: [discriminator loss: 0.392839, acc: 0.734375] [adversarial loss: 5.933758, acc: 0.000000]\n",
      "420: [discriminator loss: 0.464666, acc: 0.828125] [adversarial loss: 0.024726, acc: 1.000000]\n",
      "421: [discriminator loss: 0.656098, acc: 0.539062] [adversarial loss: 4.551244, acc: 0.000000]\n",
      "422: [discriminator loss: 0.355209, acc: 0.906250] [adversarial loss: 0.418911, acc: 0.953125]\n",
      "423: [discriminator loss: 0.574904, acc: 0.484375] [adversarial loss: 4.900224, acc: 0.000000]\n",
      "424: [discriminator loss: 0.649304, acc: 0.781250] [adversarial loss: 0.224575, acc: 1.000000]\n",
      "425: [discriminator loss: 0.662036, acc: 0.500000] [adversarial loss: 3.826555, acc: 0.000000]\n",
      "426: [discriminator loss: 0.147645, acc: 0.960938] [adversarial loss: 1.864281, acc: 0.000000]\n",
      "427: [discriminator loss: 0.225644, acc: 0.984375] [adversarial loss: 1.662691, acc: 0.000000]\n",
      "428: [discriminator loss: 0.263842, acc: 0.960938] [adversarial loss: 1.274570, acc: 0.000000]\n",
      "429: [discriminator loss: 0.197137, acc: 0.976562] [adversarial loss: 1.665349, acc: 0.000000]\n",
      "430: [discriminator loss: 0.208680, acc: 0.960938] [adversarial loss: 1.386126, acc: 0.000000]\n",
      "431: [discriminator loss: 0.260422, acc: 0.921875] [adversarial loss: 0.643849, acc: 0.812500]\n",
      "432: [discriminator loss: 0.276395, acc: 0.937500] [adversarial loss: 3.678306, acc: 0.000000]\n",
      "433: [discriminator loss: 0.508041, acc: 0.781250] [adversarial loss: 0.029852, acc: 1.000000]\n",
      "434: [discriminator loss: 0.859526, acc: 0.515625] [adversarial loss: 4.570858, acc: 0.000000]\n",
      "435: [discriminator loss: 0.371746, acc: 0.867188] [adversarial loss: 0.572891, acc: 0.906250]\n",
      "436: [discriminator loss: 0.198449, acc: 0.992188] [adversarial loss: 2.007240, acc: 0.000000]\n",
      "437: [discriminator loss: 0.125979, acc: 0.984375] [adversarial loss: 1.259125, acc: 0.000000]\n",
      "438: [discriminator loss: 0.207180, acc: 0.976562] [adversarial loss: 1.848841, acc: 0.000000]\n",
      "439: [discriminator loss: 0.203214, acc: 0.929688] [adversarial loss: 0.688241, acc: 0.656250]\n",
      "440: [discriminator loss: 0.200825, acc: 0.992188] [adversarial loss: 2.461768, acc: 0.000000]\n",
      "441: [discriminator loss: 0.156233, acc: 0.968750] [adversarial loss: 0.572083, acc: 0.921875]\n",
      "442: [discriminator loss: 0.169986, acc: 1.000000] [adversarial loss: 3.106322, acc: 0.000000]\n",
      "443: [discriminator loss: 0.221280, acc: 0.921875] [adversarial loss: 0.135253, acc: 1.000000]\n",
      "444: [discriminator loss: 0.422887, acc: 0.625000] [adversarial loss: 5.940551, acc: 0.000000]\n",
      "445: [discriminator loss: 0.792870, acc: 0.617188] [adversarial loss: 0.060979, acc: 1.000000]\n",
      "446: [discriminator loss: 0.676593, acc: 0.507812] [adversarial loss: 3.539319, acc: 0.000000]\n",
      "447: [discriminator loss: 0.108753, acc: 0.976562] [adversarial loss: 1.094451, acc: 0.000000]\n",
      "448: [discriminator loss: 0.291471, acc: 0.882812] [adversarial loss: 3.239732, acc: 0.000000]\n",
      "449: [discriminator loss: 0.442393, acc: 0.781250] [adversarial loss: 0.048554, acc: 1.000000]\n",
      "450: [discriminator loss: 0.753209, acc: 0.507812] [adversarial loss: 5.563288, acc: 0.000000]\n",
      "451: [discriminator loss: 0.645132, acc: 0.640625] [adversarial loss: 0.367266, acc: 1.000000]\n",
      "452: [discriminator loss: 0.299419, acc: 0.906250] [adversarial loss: 2.899138, acc: 0.000000]\n",
      "453: [discriminator loss: 0.105952, acc: 0.968750] [adversarial loss: 1.867128, acc: 0.000000]\n",
      "454: [discriminator loss: 0.134189, acc: 0.984375] [adversarial loss: 1.601196, acc: 0.000000]\n",
      "455: [discriminator loss: 0.152945, acc: 0.984375] [adversarial loss: 1.497471, acc: 0.000000]\n",
      "456: [discriminator loss: 0.163234, acc: 0.976562] [adversarial loss: 1.285751, acc: 0.000000]\n",
      "457: [discriminator loss: 0.279826, acc: 0.976562] [adversarial loss: 2.395972, acc: 0.000000]\n",
      "458: [discriminator loss: 0.408671, acc: 0.851562] [adversarial loss: 0.091381, acc: 1.000000]\n",
      "459: [discriminator loss: 0.633315, acc: 0.500000] [adversarial loss: 6.593889, acc: 0.000000]\n",
      "460: [discriminator loss: 1.331397, acc: 0.515625] [adversarial loss: 0.139389, acc: 1.000000]\n",
      "461: [discriminator loss: 0.627692, acc: 0.515625] [adversarial loss: 4.539612, acc: 0.000000]\n",
      "462: [discriminator loss: 0.257419, acc: 0.859375] [adversarial loss: 1.758141, acc: 0.000000]\n",
      "463: [discriminator loss: 0.092704, acc: 1.000000] [adversarial loss: 2.179989, acc: 0.000000]\n",
      "464: [discriminator loss: 0.114908, acc: 1.000000] [adversarial loss: 2.184076, acc: 0.000000]\n",
      "465: [discriminator loss: 0.141419, acc: 0.984375] [adversarial loss: 2.098587, acc: 0.000000]\n",
      "466: [discriminator loss: 0.112049, acc: 0.992188] [adversarial loss: 2.541091, acc: 0.000000]\n",
      "467: [discriminator loss: 0.111491, acc: 0.984375] [adversarial loss: 1.897644, acc: 0.000000]\n",
      "468: [discriminator loss: 0.121442, acc: 1.000000] [adversarial loss: 3.536767, acc: 0.000000]\n",
      "469: [discriminator loss: 0.144886, acc: 0.937500] [adversarial loss: 1.571054, acc: 0.000000]\n",
      "470: [discriminator loss: 0.175133, acc: 1.000000] [adversarial loss: 6.274005, acc: 0.000000]\n",
      "471: [discriminator loss: 0.178974, acc: 0.929688] [adversarial loss: 2.458483, acc: 0.000000]\n",
      "472: [discriminator loss: 0.088551, acc: 1.000000] [adversarial loss: 3.922923, acc: 0.000000]\n",
      "473: [discriminator loss: 0.104161, acc: 0.968750] [adversarial loss: 1.993696, acc: 0.000000]\n",
      "474: [discriminator loss: 0.144448, acc: 0.992188] [adversarial loss: 4.523427, acc: 0.000000]\n",
      "475: [discriminator loss: 0.081686, acc: 0.976562] [adversarial loss: 1.996454, acc: 0.000000]\n",
      "476: [discriminator loss: 0.207242, acc: 1.000000] [adversarial loss: 6.212444, acc: 0.000000]\n",
      "477: [discriminator loss: 0.653736, acc: 0.718750] [adversarial loss: 0.063800, acc: 1.000000]\n",
      "478: [discriminator loss: 1.281138, acc: 0.500000] [adversarial loss: 6.373859, acc: 0.000000]\n",
      "479: [discriminator loss: 0.597060, acc: 0.742188] [adversarial loss: 0.381181, acc: 0.953125]\n",
      "480: [discriminator loss: 0.842213, acc: 0.523438] [adversarial loss: 5.409376, acc: 0.000000]\n",
      "481: [discriminator loss: 0.275850, acc: 0.859375] [adversarial loss: 1.368458, acc: 0.000000]\n",
      "482: [discriminator loss: 0.639225, acc: 0.507812] [adversarial loss: 5.022152, acc: 0.000000]\n",
      "483: [discriminator loss: 0.520691, acc: 0.703125] [adversarial loss: 0.800130, acc: 0.437500]\n",
      "484: [discriminator loss: 0.494865, acc: 0.570312] [adversarial loss: 4.600883, acc: 0.000000]\n",
      "485: [discriminator loss: 0.273619, acc: 0.890625] [adversarial loss: 1.608028, acc: 0.000000]\n",
      "486: [discriminator loss: 0.275105, acc: 0.976562] [adversarial loss: 3.483012, acc: 0.000000]\n",
      "487: [discriminator loss: 0.157070, acc: 0.960938] [adversarial loss: 1.559644, acc: 0.000000]\n",
      "488: [discriminator loss: 0.331941, acc: 0.937500] [adversarial loss: 4.260785, acc: 0.000000]\n",
      "489: [discriminator loss: 0.381996, acc: 0.828125] [adversarial loss: 0.664862, acc: 0.671875]\n",
      "490: [discriminator loss: 0.431881, acc: 0.617188] [adversarial loss: 4.877232, acc: 0.000000]\n",
      "491: [discriminator loss: 0.356559, acc: 0.820312] [adversarial loss: 1.800425, acc: 0.000000]\n",
      "492: [discriminator loss: 0.176202, acc: 1.000000] [adversarial loss: 3.256076, acc: 0.000000]\n",
      "493: [discriminator loss: 0.097876, acc: 0.992188] [adversarial loss: 2.563628, acc: 0.000000]\n",
      "494: [discriminator loss: 0.203597, acc: 0.976562] [adversarial loss: 2.757192, acc: 0.000000]\n",
      "495: [discriminator loss: 0.191412, acc: 0.984375] [adversarial loss: 2.589987, acc: 0.000000]\n",
      "496: [discriminator loss: 0.193816, acc: 0.953125] [adversarial loss: 1.554632, acc: 0.000000]\n",
      "497: [discriminator loss: 0.235298, acc: 0.992188] [adversarial loss: 4.677064, acc: 0.000000]\n",
      "498: [discriminator loss: 0.348542, acc: 0.812500] [adversarial loss: 0.327738, acc: 1.000000]\n",
      "499: [discriminator loss: 0.858552, acc: 0.500000] [adversarial loss: 6.440858, acc: 0.000000]\n",
      "500: [discriminator loss: 0.815740, acc: 0.601562] [adversarial loss: 0.330620, acc: 0.953125]\n",
      "501: [discriminator loss: 0.817501, acc: 0.500000] [adversarial loss: 3.968276, acc: 0.000000]\n",
      "502: [discriminator loss: 0.233278, acc: 0.882812] [adversarial loss: 1.859623, acc: 0.000000]\n",
      "503: [discriminator loss: 0.154474, acc: 0.992188] [adversarial loss: 2.376101, acc: 0.000000]\n",
      "504: [discriminator loss: 0.178215, acc: 0.953125] [adversarial loss: 1.831046, acc: 0.000000]\n",
      "505: [discriminator loss: 0.182798, acc: 0.968750] [adversarial loss: 2.433367, acc: 0.000000]\n",
      "506: [discriminator loss: 0.116898, acc: 0.984375] [adversarial loss: 2.292285, acc: 0.000000]\n",
      "507: [discriminator loss: 0.154596, acc: 0.945312] [adversarial loss: 2.207249, acc: 0.000000]\n",
      "508: [discriminator loss: 0.101285, acc: 0.984375] [adversarial loss: 2.501711, acc: 0.000000]\n",
      "509: [discriminator loss: 0.095949, acc: 0.992188] [adversarial loss: 2.496600, acc: 0.000000]\n",
      "510: [discriminator loss: 0.132096, acc: 0.976562] [adversarial loss: 2.385074, acc: 0.000000]\n",
      "511: [discriminator loss: 0.131962, acc: 0.968750] [adversarial loss: 2.485626, acc: 0.000000]\n",
      "512: [discriminator loss: 0.156122, acc: 1.000000] [adversarial loss: 4.120680, acc: 0.000000]\n",
      "513: [discriminator loss: 0.166722, acc: 0.953125] [adversarial loss: 1.140857, acc: 0.000000]\n",
      "514: [discriminator loss: 0.246857, acc: 0.992188] [adversarial loss: 7.508399, acc: 0.000000]\n",
      "515: [discriminator loss: 0.424630, acc: 0.812500] [adversarial loss: 1.997049, acc: 0.000000]\n",
      "516: [discriminator loss: 0.057206, acc: 1.000000] [adversarial loss: 3.260293, acc: 0.000000]\n",
      "517: [discriminator loss: 0.022311, acc: 1.000000] [adversarial loss: 3.307182, acc: 0.000000]\n",
      "518: [discriminator loss: 0.024052, acc: 0.992188] [adversarial loss: 3.314438, acc: 0.000000]\n",
      "519: [discriminator loss: 0.018390, acc: 1.000000] [adversarial loss: 3.293231, acc: 0.000000]\n",
      "520: [discriminator loss: 0.020006, acc: 1.000000] [adversarial loss: 3.274003, acc: 0.000000]\n",
      "521: [discriminator loss: 0.026519, acc: 0.992188] [adversarial loss: 3.128353, acc: 0.000000]\n",
      "522: [discriminator loss: 0.033309, acc: 0.992188] [adversarial loss: 2.696363, acc: 0.000000]\n",
      "523: [discriminator loss: 0.053483, acc: 0.992188] [adversarial loss: 2.739949, acc: 0.000000]\n",
      "524: [discriminator loss: 0.121033, acc: 1.000000] [adversarial loss: 5.319775, acc: 0.000000]\n",
      "525: [discriminator loss: 0.160574, acc: 0.937500] [adversarial loss: 1.316409, acc: 0.000000]\n",
      "526: [discriminator loss: 0.195348, acc: 0.984375] [adversarial loss: 6.069361, acc: 0.000000]\n",
      "527: [discriminator loss: 0.204545, acc: 0.906250] [adversarial loss: 2.578614, acc: 0.000000]\n",
      "528: [discriminator loss: 0.034768, acc: 1.000000] [adversarial loss: 3.467626, acc: 0.000000]\n",
      "529: [discriminator loss: 0.014154, acc: 1.000000] [adversarial loss: 3.670934, acc: 0.000000]\n",
      "530: [discriminator loss: 0.018957, acc: 1.000000] [adversarial loss: 3.618222, acc: 0.000000]\n",
      "531: [discriminator loss: 0.012516, acc: 1.000000] [adversarial loss: 3.723858, acc: 0.000000]\n",
      "532: [discriminator loss: 0.021972, acc: 1.000000] [adversarial loss: 3.632298, acc: 0.000000]\n",
      "533: [discriminator loss: 0.020886, acc: 1.000000] [adversarial loss: 3.876225, acc: 0.000000]\n",
      "534: [discriminator loss: 0.027342, acc: 0.992188] [adversarial loss: 3.723305, acc: 0.000000]\n",
      "535: [discriminator loss: 0.039579, acc: 0.992188] [adversarial loss: 3.784998, acc: 0.000000]\n",
      "536: [discriminator loss: 0.025534, acc: 1.000000] [adversarial loss: 4.609008, acc: 0.000000]\n",
      "537: [discriminator loss: 0.024354, acc: 0.992188] [adversarial loss: 3.744696, acc: 0.000000]\n",
      "538: [discriminator loss: 0.044578, acc: 0.984375] [adversarial loss: 2.499501, acc: 0.000000]\n",
      "539: [discriminator loss: 0.222196, acc: 0.976562] [adversarial loss: 10.551624, acc: 0.000000]\n",
      "540: [discriminator loss: 1.452564, acc: 0.539062] [adversarial loss: 0.150146, acc: 1.000000]\n",
      "541: [discriminator loss: 0.381385, acc: 0.734375] [adversarial loss: 3.663448, acc: 0.000000]\n",
      "542: [discriminator loss: 0.023355, acc: 1.000000] [adversarial loss: 2.771887, acc: 0.000000]\n",
      "543: [discriminator loss: 0.135753, acc: 0.984375] [adversarial loss: 2.977518, acc: 0.000000]\n",
      "544: [discriminator loss: 0.149477, acc: 0.976562] [adversarial loss: 3.288496, acc: 0.000000]\n",
      "545: [discriminator loss: 0.105929, acc: 0.976562] [adversarial loss: 2.242178, acc: 0.000000]\n",
      "546: [discriminator loss: 0.177449, acc: 0.968750] [adversarial loss: 3.460279, acc: 0.000000]\n",
      "547: [discriminator loss: 0.152285, acc: 0.960938] [adversarial loss: 1.564632, acc: 0.031250]\n",
      "548: [discriminator loss: 0.262502, acc: 0.875000] [adversarial loss: 5.565670, acc: 0.000000]\n",
      "549: [discriminator loss: 0.174536, acc: 0.929688] [adversarial loss: 1.354392, acc: 0.171875]\n",
      "550: [discriminator loss: 0.107264, acc: 0.953125] [adversarial loss: 1.298026, acc: 0.296875]\n",
      "551: [discriminator loss: 0.047989, acc: 0.992188] [adversarial loss: 0.631315, acc: 0.656250]\n",
      "552: [discriminator loss: 0.064678, acc: 0.992188] [adversarial loss: 0.440139, acc: 0.734375]\n",
      "553: [discriminator loss: 0.065332, acc: 0.976562] [adversarial loss: 0.341461, acc: 0.859375]\n",
      "554: [discriminator loss: 0.045939, acc: 0.984375] [adversarial loss: 0.140631, acc: 0.953125]\n",
      "555: [discriminator loss: 0.062904, acc: 0.984375] [adversarial loss: 0.265734, acc: 0.921875]\n",
      "556: [discriminator loss: 0.030869, acc: 1.000000] [adversarial loss: 0.113474, acc: 1.000000]\n",
      "557: [discriminator loss: 0.035187, acc: 0.992188] [adversarial loss: 0.077900, acc: 1.000000]\n",
      "558: [discriminator loss: 0.056232, acc: 0.992188] [adversarial loss: 0.133881, acc: 1.000000]\n",
      "559: [discriminator loss: 0.065327, acc: 0.984375] [adversarial loss: 0.094800, acc: 1.000000]\n",
      "560: [discriminator loss: 0.069371, acc: 0.984375] [adversarial loss: 0.212321, acc: 0.953125]\n",
      "561: [discriminator loss: 0.066599, acc: 0.992188] [adversarial loss: 0.370005, acc: 0.906250]\n",
      "562: [discriminator loss: 0.098373, acc: 0.968750] [adversarial loss: 0.069524, acc: 1.000000]\n",
      "563: [discriminator loss: 0.053818, acc: 0.992188] [adversarial loss: 0.284541, acc: 0.953125]\n",
      "564: [discriminator loss: 0.043876, acc: 0.992188] [adversarial loss: 0.342296, acc: 0.937500]\n",
      "565: [discriminator loss: 0.043130, acc: 1.000000] [adversarial loss: 1.016773, acc: 0.171875]\n",
      "566: [discriminator loss: 0.050672, acc: 0.992188] [adversarial loss: 0.793209, acc: 0.484375]\n",
      "567: [discriminator loss: 0.076171, acc: 0.968750] [adversarial loss: 1.328794, acc: 0.000000]\n",
      "568: [discriminator loss: 0.068363, acc: 0.976562] [adversarial loss: 1.787180, acc: 0.000000]\n",
      "569: [discriminator loss: 0.122221, acc: 0.953125] [adversarial loss: 0.067734, acc: 1.000000]\n",
      "570: [discriminator loss: 0.289390, acc: 0.835938] [adversarial loss: 9.598520, acc: 0.000000]\n",
      "571: [discriminator loss: 1.105969, acc: 0.632812] [adversarial loss: 0.045839, acc: 1.000000]\n",
      "572: [discriminator loss: 0.400274, acc: 0.804688] [adversarial loss: 3.244041, acc: 0.000000]\n",
      "573: [discriminator loss: 0.127921, acc: 0.960938] [adversarial loss: 1.674759, acc: 0.093750]\n",
      "574: [discriminator loss: 0.219696, acc: 0.945312] [adversarial loss: 5.367281, acc: 0.000000]\n",
      "575: [discriminator loss: 0.386951, acc: 0.859375] [adversarial loss: 0.799772, acc: 0.515625]\n",
      "576: [discriminator loss: 0.637703, acc: 0.617188] [adversarial loss: 8.920336, acc: 0.000000]\n",
      "577: [discriminator loss: 0.964975, acc: 0.687500] [adversarial loss: 1.064116, acc: 0.234375]\n",
      "578: [discriminator loss: 0.448855, acc: 0.671875] [adversarial loss: 6.555633, acc: 0.000000]\n",
      "579: [discriminator loss: 0.329258, acc: 0.828125] [adversarial loss: 2.510720, acc: 0.000000]\n",
      "580: [discriminator loss: 0.107680, acc: 0.984375] [adversarial loss: 3.212485, acc: 0.000000]\n",
      "581: [discriminator loss: 0.080714, acc: 0.984375] [adversarial loss: 3.024553, acc: 0.000000]\n",
      "582: [discriminator loss: 0.054647, acc: 0.992188] [adversarial loss: 3.107005, acc: 0.000000]\n",
      "583: [discriminator loss: 0.094154, acc: 0.960938] [adversarial loss: 2.260256, acc: 0.000000]\n",
      "584: [discriminator loss: 0.120062, acc: 0.976562] [adversarial loss: 3.242858, acc: 0.000000]\n",
      "585: [discriminator loss: 0.039803, acc: 1.000000] [adversarial loss: 2.828028, acc: 0.000000]\n",
      "586: [discriminator loss: 0.054228, acc: 0.992188] [adversarial loss: 3.292327, acc: 0.000000]\n",
      "587: [discriminator loss: 0.059974, acc: 0.984375] [adversarial loss: 3.285880, acc: 0.000000]\n",
      "588: [discriminator loss: 0.051224, acc: 0.992188] [adversarial loss: 3.554932, acc: 0.000000]\n",
      "589: [discriminator loss: 0.048387, acc: 0.992188] [adversarial loss: 3.557211, acc: 0.000000]\n",
      "590: [discriminator loss: 0.036110, acc: 1.000000] [adversarial loss: 4.605100, acc: 0.000000]\n",
      "591: [discriminator loss: 0.069617, acc: 0.968750] [adversarial loss: 2.217418, acc: 0.000000]\n",
      "592: [discriminator loss: 0.067786, acc: 1.000000] [adversarial loss: 9.160743, acc: 0.000000]\n",
      "593: [discriminator loss: 0.092446, acc: 0.968750] [adversarial loss: 3.881941, acc: 0.000000]\n",
      "594: [discriminator loss: 0.022014, acc: 1.000000] [adversarial loss: 4.909363, acc: 0.000000]\n",
      "595: [discriminator loss: 0.023199, acc: 0.992188] [adversarial loss: 4.584148, acc: 0.000000]\n",
      "596: [discriminator loss: 0.014779, acc: 1.000000] [adversarial loss: 4.505072, acc: 0.000000]\n",
      "597: [discriminator loss: 0.017737, acc: 1.000000] [adversarial loss: 4.959888, acc: 0.000000]\n",
      "598: [discriminator loss: 0.032476, acc: 1.000000] [adversarial loss: 6.907685, acc: 0.000000]\n",
      "599: [discriminator loss: 0.063821, acc: 0.968750] [adversarial loss: 2.456898, acc: 0.000000]\n",
      "600: [discriminator loss: 0.065218, acc: 1.000000] [adversarial loss: 8.528324, acc: 0.000000]\n",
      "601: [discriminator loss: 0.081165, acc: 0.976562] [adversarial loss: 3.344819, acc: 0.000000]\n",
      "602: [discriminator loss: 0.166436, acc: 0.929688] [adversarial loss: 10.360740, acc: 0.000000]\n",
      "603: [discriminator loss: 0.220958, acc: 0.906250] [adversarial loss: 1.123005, acc: 0.187500]\n",
      "604: [discriminator loss: 0.037095, acc: 0.992188] [adversarial loss: 1.003778, acc: 0.234375]\n",
      "605: [discriminator loss: 0.029767, acc: 0.992188] [adversarial loss: 0.418990, acc: 0.859375]\n",
      "606: [discriminator loss: 0.024799, acc: 1.000000] [adversarial loss: 0.689295, acc: 0.531250]\n",
      "607: [discriminator loss: 0.046955, acc: 0.976562] [adversarial loss: 0.096353, acc: 1.000000]\n",
      "608: [discriminator loss: 0.021459, acc: 0.992188] [adversarial loss: 0.146595, acc: 1.000000]\n",
      "609: [discriminator loss: 0.016385, acc: 1.000000] [adversarial loss: 0.113863, acc: 1.000000]\n",
      "610: [discriminator loss: 0.009465, acc: 1.000000] [adversarial loss: 0.079505, acc: 1.000000]\n",
      "611: [discriminator loss: 0.011383, acc: 1.000000] [adversarial loss: 0.105430, acc: 1.000000]\n",
      "612: [discriminator loss: 0.009564, acc: 1.000000] [adversarial loss: 0.072748, acc: 1.000000]\n",
      "613: [discriminator loss: 0.022790, acc: 0.992188] [adversarial loss: 0.056288, acc: 1.000000]\n",
      "614: [discriminator loss: 0.016242, acc: 1.000000] [adversarial loss: 0.042804, acc: 1.000000]\n",
      "615: [discriminator loss: 0.022054, acc: 0.992188] [adversarial loss: 0.412173, acc: 0.875000]\n",
      "616: [discriminator loss: 0.033924, acc: 0.976562] [adversarial loss: 0.009135, acc: 1.000000]\n",
      "617: [discriminator loss: 0.020284, acc: 1.000000] [adversarial loss: 0.064877, acc: 1.000000]\n",
      "618: [discriminator loss: 0.016153, acc: 1.000000] [adversarial loss: 0.048771, acc: 1.000000]\n",
      "619: [discriminator loss: 0.028900, acc: 0.984375] [adversarial loss: 0.002516, acc: 1.000000]\n",
      "620: [discriminator loss: 0.043950, acc: 0.992188] [adversarial loss: 0.603606, acc: 0.671875]\n",
      "621: [discriminator loss: 0.053406, acc: 0.984375] [adversarial loss: 0.816574, acc: 0.500000]\n",
      "622: [discriminator loss: 0.021900, acc: 1.000000] [adversarial loss: 2.678936, acc: 0.000000]\n",
      "623: [discriminator loss: 0.120170, acc: 0.960938] [adversarial loss: 0.166688, acc: 1.000000]\n",
      "624: [discriminator loss: 0.195680, acc: 0.929688] [adversarial loss: 14.978979, acc: 0.000000]\n",
      "625: [discriminator loss: 1.146311, acc: 0.593750] [adversarial loss: 0.426896, acc: 0.765625]\n",
      "626: [discriminator loss: 0.214236, acc: 0.914062] [adversarial loss: 5.905338, acc: 0.000000]\n",
      "627: [discriminator loss: 0.047330, acc: 0.984375] [adversarial loss: 4.514596, acc: 0.000000]\n",
      "628: [discriminator loss: 0.032595, acc: 0.992188] [adversarial loss: 3.574907, acc: 0.000000]\n",
      "629: [discriminator loss: 0.154338, acc: 0.945312] [adversarial loss: 7.854375, acc: 0.000000]\n",
      "630: [discriminator loss: 0.077494, acc: 0.976562] [adversarial loss: 4.581448, acc: 0.000000]\n",
      "631: [discriminator loss: 0.065599, acc: 0.992188] [adversarial loss: 3.646532, acc: 0.000000]\n",
      "632: [discriminator loss: 0.055349, acc: 0.992188] [adversarial loss: 3.312963, acc: 0.000000]\n",
      "633: [discriminator loss: 0.041926, acc: 0.992188] [adversarial loss: 3.766643, acc: 0.000000]\n",
      "634: [discriminator loss: 0.059565, acc: 0.984375] [adversarial loss: 1.938609, acc: 0.000000]\n",
      "635: [discriminator loss: 0.102939, acc: 0.976562] [adversarial loss: 3.756079, acc: 0.000000]\n",
      "636: [discriminator loss: 0.081032, acc: 0.968750] [adversarial loss: 0.289544, acc: 0.921875]\n",
      "637: [discriminator loss: 0.043093, acc: 0.992188] [adversarial loss: 0.194414, acc: 0.984375]\n",
      "638: [discriminator loss: 0.019488, acc: 0.992188] [adversarial loss: 0.174495, acc: 0.984375]\n",
      "639: [discriminator loss: 0.011553, acc: 1.000000] [adversarial loss: 0.154584, acc: 0.984375]\n",
      "640: [discriminator loss: 0.038061, acc: 0.984375] [adversarial loss: 0.053251, acc: 1.000000]\n",
      "641: [discriminator loss: 0.012966, acc: 1.000000] [adversarial loss: 0.097291, acc: 1.000000]\n",
      "642: [discriminator loss: 0.045093, acc: 0.992188] [adversarial loss: 0.041986, acc: 1.000000]\n",
      "643: [discriminator loss: 0.011568, acc: 1.000000] [adversarial loss: 0.045232, acc: 1.000000]\n",
      "644: [discriminator loss: 0.007296, acc: 1.000000] [adversarial loss: 0.052038, acc: 1.000000]\n",
      "645: [discriminator loss: 0.005665, acc: 1.000000] [adversarial loss: 0.049152, acc: 1.000000]\n",
      "646: [discriminator loss: 0.046061, acc: 0.984375] [adversarial loss: 0.016243, acc: 1.000000]\n",
      "647: [discriminator loss: 0.021301, acc: 0.992188] [adversarial loss: 0.047854, acc: 1.000000]\n",
      "648: [discriminator loss: 0.019348, acc: 1.000000] [adversarial loss: 0.039343, acc: 1.000000]\n",
      "649: [discriminator loss: 0.012037, acc: 1.000000] [adversarial loss: 0.036694, acc: 1.000000]\n",
      "650: [discriminator loss: 0.005266, acc: 1.000000] [adversarial loss: 0.062390, acc: 1.000000]\n",
      "651: [discriminator loss: 0.006834, acc: 1.000000] [adversarial loss: 0.041435, acc: 1.000000]\n",
      "652: [discriminator loss: 0.007614, acc: 1.000000] [adversarial loss: 0.070747, acc: 1.000000]\n",
      "653: [discriminator loss: 0.002614, acc: 1.000000] [adversarial loss: 0.074492, acc: 1.000000]\n",
      "654: [discriminator loss: 0.007375, acc: 0.992188] [adversarial loss: 0.033698, acc: 1.000000]\n",
      "655: [discriminator loss: 0.002694, acc: 1.000000] [adversarial loss: 0.020862, acc: 1.000000]\n",
      "656: [discriminator loss: 0.003779, acc: 1.000000] [adversarial loss: 0.009964, acc: 1.000000]\n",
      "657: [discriminator loss: 0.011183, acc: 0.992188] [adversarial loss: 0.006519, acc: 1.000000]\n",
      "658: [discriminator loss: 0.002542, acc: 1.000000] [adversarial loss: 0.015572, acc: 1.000000]\n",
      "659: [discriminator loss: 0.003395, acc: 1.000000] [adversarial loss: 0.033647, acc: 1.000000]\n",
      "660: [discriminator loss: 0.007654, acc: 1.000000] [adversarial loss: 0.007751, acc: 1.000000]\n",
      "661: [discriminator loss: 0.005431, acc: 1.000000] [adversarial loss: 0.014606, acc: 1.000000]\n",
      "662: [discriminator loss: 0.001428, acc: 1.000000] [adversarial loss: 0.020241, acc: 1.000000]\n",
      "663: [discriminator loss: 0.002338, acc: 1.000000] [adversarial loss: 0.022944, acc: 1.000000]\n",
      "664: [discriminator loss: 0.002073, acc: 1.000000] [adversarial loss: 0.009911, acc: 1.000000]\n",
      "665: [discriminator loss: 0.001985, acc: 1.000000] [adversarial loss: 0.009018, acc: 1.000000]\n",
      "666: [discriminator loss: 0.000657, acc: 1.000000] [adversarial loss: 0.008010, acc: 1.000000]\n",
      "667: [discriminator loss: 0.001604, acc: 1.000000] [adversarial loss: 0.011498, acc: 1.000000]\n",
      "668: [discriminator loss: 0.003193, acc: 1.000000] [adversarial loss: 0.015836, acc: 1.000000]\n",
      "669: [discriminator loss: 0.001384, acc: 1.000000] [adversarial loss: 0.008638, acc: 1.000000]\n",
      "670: [discriminator loss: 0.000480, acc: 1.000000] [adversarial loss: 0.005256, acc: 1.000000]\n",
      "671: [discriminator loss: 0.023886, acc: 0.992188] [adversarial loss: 0.000387, acc: 1.000000]\n",
      "672: [discriminator loss: 0.002310, acc: 1.000000] [adversarial loss: 0.003223, acc: 1.000000]\n",
      "673: [discriminator loss: 0.002925, acc: 1.000000] [adversarial loss: 0.002542, acc: 1.000000]\n",
      "674: [discriminator loss: 0.001280, acc: 1.000000] [adversarial loss: 0.007589, acc: 1.000000]\n",
      "675: [discriminator loss: 0.001907, acc: 1.000000] [adversarial loss: 0.004558, acc: 1.000000]\n",
      "676: [discriminator loss: 0.027743, acc: 0.992188] [adversarial loss: 0.000377, acc: 1.000000]\n",
      "677: [discriminator loss: 0.002971, acc: 1.000000] [adversarial loss: 0.003239, acc: 1.000000]\n",
      "678: [discriminator loss: 0.005275, acc: 1.000000] [adversarial loss: 0.000842, acc: 1.000000]\n",
      "679: [discriminator loss: 0.001229, acc: 1.000000] [adversarial loss: 0.002450, acc: 1.000000]\n",
      "680: [discriminator loss: 0.001168, acc: 1.000000] [adversarial loss: 0.004507, acc: 1.000000]\n",
      "681: [discriminator loss: 0.003556, acc: 1.000000] [adversarial loss: 0.212754, acc: 0.984375]\n",
      "682: [discriminator loss: 0.000446, acc: 1.000000] [adversarial loss: 0.002063, acc: 1.000000]\n",
      "683: [discriminator loss: 0.000088, acc: 1.000000] [adversarial loss: 0.001042, acc: 1.000000]\n",
      "684: [discriminator loss: 0.000141, acc: 1.000000] [adversarial loss: 0.001102, acc: 1.000000]\n",
      "685: [discriminator loss: 0.000101, acc: 1.000000] [adversarial loss: 0.000617, acc: 1.000000]\n",
      "686: [discriminator loss: 0.001728, acc: 1.000000] [adversarial loss: 0.000172, acc: 1.000000]\n",
      "687: [discriminator loss: 0.000677, acc: 1.000000] [adversarial loss: 0.000075, acc: 1.000000]\n",
      "688: [discriminator loss: 0.001064, acc: 1.000000] [adversarial loss: 0.000025, acc: 1.000000]\n",
      "689: [discriminator loss: 0.001153, acc: 1.000000] [adversarial loss: 0.000008, acc: 1.000000]\n",
      "690: [discriminator loss: 0.000326, acc: 1.000000] [adversarial loss: 0.000008, acc: 1.000000]\n",
      "691: [discriminator loss: 0.000221, acc: 1.000000] [adversarial loss: 0.000010, acc: 1.000000]\n",
      "692: [discriminator loss: 0.000302, acc: 1.000000] [adversarial loss: 0.000014, acc: 1.000000]\n",
      "693: [discriminator loss: 0.000198, acc: 1.000000] [adversarial loss: 0.000017, acc: 1.000000]\n",
      "694: [discriminator loss: 0.000154, acc: 1.000000] [adversarial loss: 0.000014, acc: 1.000000]\n",
      "695: [discriminator loss: 0.000235, acc: 1.000000] [adversarial loss: 0.000025, acc: 1.000000]\n",
      "696: [discriminator loss: 0.000235, acc: 1.000000] [adversarial loss: 0.000025, acc: 1.000000]\n",
      "697: [discriminator loss: 0.000496, acc: 1.000000] [adversarial loss: 0.000089, acc: 1.000000]\n",
      "698: [discriminator loss: 0.000170, acc: 1.000000] [adversarial loss: 0.000073, acc: 1.000000]\n",
      "699: [discriminator loss: 0.013831, acc: 0.992188] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "700: [discriminator loss: 0.005103, acc: 1.000000] [adversarial loss: 0.000074, acc: 1.000000]\n",
      "701: [discriminator loss: 0.000218, acc: 1.000000] [adversarial loss: 0.000089, acc: 1.000000]\n",
      "702: [discriminator loss: 0.000194, acc: 1.000000] [adversarial loss: 0.000128, acc: 1.000000]\n",
      "703: [discriminator loss: 0.000273, acc: 1.000000] [adversarial loss: 0.000185, acc: 1.000000]\n",
      "704: [discriminator loss: 0.002296, acc: 1.000000] [adversarial loss: 0.000004, acc: 1.000000]\n",
      "705: [discriminator loss: 0.000573, acc: 1.000000] [adversarial loss: 0.000010, acc: 1.000000]\n",
      "706: [discriminator loss: 0.000582, acc: 1.000000] [adversarial loss: 0.000052, acc: 1.000000]\n",
      "707: [discriminator loss: 0.000569, acc: 1.000000] [adversarial loss: 0.000336, acc: 1.000000]\n",
      "708: [discriminator loss: 0.000268, acc: 1.000000] [adversarial loss: 0.000512, acc: 1.000000]\n",
      "709: [discriminator loss: 0.000577, acc: 1.000000] [adversarial loss: 0.003339, acc: 1.000000]\n",
      "710: [discriminator loss: 0.000733, acc: 1.000000] [adversarial loss: 0.016915, acc: 1.000000]\n",
      "711: [discriminator loss: 0.007308, acc: 1.000000] [adversarial loss: 11.669692, acc: 0.000000]\n",
      "712: [discriminator loss: 0.256091, acc: 0.929688] [adversarial loss: 0.000055, acc: 1.000000]\n",
      "713: [discriminator loss: 0.000025, acc: 1.000000] [adversarial loss: 0.000035, acc: 1.000000]\n",
      "714: [discriminator loss: 0.000026, acc: 1.000000] [adversarial loss: 0.000045, acc: 1.000000]\n",
      "715: [discriminator loss: 0.000027, acc: 1.000000] [adversarial loss: 0.000038, acc: 1.000000]\n",
      "716: [discriminator loss: 0.000027, acc: 1.000000] [adversarial loss: 0.000048, acc: 1.000000]\n",
      "717: [discriminator loss: 0.000030, acc: 1.000000] [adversarial loss: 0.000045, acc: 1.000000]\n",
      "718: [discriminator loss: 0.000028, acc: 1.000000] [adversarial loss: 0.000055, acc: 1.000000]\n",
      "719: [discriminator loss: 0.000031, acc: 1.000000] [adversarial loss: 0.000050, acc: 1.000000]\n",
      "720: [discriminator loss: 0.000032, acc: 1.000000] [adversarial loss: 0.000033, acc: 1.000000]\n",
      "721: [discriminator loss: 0.000031, acc: 1.000000] [adversarial loss: 0.000048, acc: 1.000000]\n",
      "722: [discriminator loss: 0.000083, acc: 1.000000] [adversarial loss: 0.000025, acc: 1.000000]\n",
      "723: [discriminator loss: 0.000034, acc: 1.000000] [adversarial loss: 0.000055, acc: 1.000000]\n",
      "724: [discriminator loss: 0.000036, acc: 1.000000] [adversarial loss: 0.000045, acc: 1.000000]\n",
      "725: [discriminator loss: 0.000038, acc: 1.000000] [adversarial loss: 0.000038, acc: 1.000000]\n",
      "726: [discriminator loss: 0.000039, acc: 1.000000] [adversarial loss: 0.000041, acc: 1.000000]\n",
      "727: [discriminator loss: 0.000045, acc: 1.000000] [adversarial loss: 0.000050, acc: 1.000000]\n",
      "728: [discriminator loss: 0.000042, acc: 1.000000] [adversarial loss: 0.000035, acc: 1.000000]\n",
      "729: [discriminator loss: 0.000052, acc: 1.000000] [adversarial loss: 0.000025, acc: 1.000000]\n",
      "730: [discriminator loss: 0.000047, acc: 1.000000] [adversarial loss: 0.000037, acc: 1.000000]\n",
      "731: [discriminator loss: 0.000054, acc: 1.000000] [adversarial loss: 0.000029, acc: 1.000000]\n",
      "732: [discriminator loss: 0.000055, acc: 1.000000] [adversarial loss: 0.000040, acc: 1.000000]\n",
      "733: [discriminator loss: 0.000062, acc: 1.000000] [adversarial loss: 0.000058, acc: 1.000000]\n",
      "734: [discriminator loss: 0.000068, acc: 1.000000] [adversarial loss: 0.000043, acc: 1.000000]\n",
      "735: [discriminator loss: 0.000075, acc: 1.000000] [adversarial loss: 0.000043, acc: 1.000000]\n",
      "736: [discriminator loss: 0.000072, acc: 1.000000] [adversarial loss: 0.000046, acc: 1.000000]\n",
      "737: [discriminator loss: 0.000079, acc: 1.000000] [adversarial loss: 0.000037, acc: 1.000000]\n",
      "738: [discriminator loss: 0.000093, acc: 1.000000] [adversarial loss: 0.000045, acc: 1.000000]\n",
      "739: [discriminator loss: 0.000095, acc: 1.000000] [adversarial loss: 0.000050, acc: 1.000000]\n",
      "740: [discriminator loss: 0.000109, acc: 1.000000] [adversarial loss: 0.000049, acc: 1.000000]\n",
      "741: [discriminator loss: 0.000108, acc: 1.000000] [adversarial loss: 0.000041, acc: 1.000000]\n",
      "742: [discriminator loss: 0.000120, acc: 1.000000] [adversarial loss: 0.000034, acc: 1.000000]\n",
      "743: [discriminator loss: 0.000141, acc: 1.000000] [adversarial loss: 0.000056, acc: 1.000000]\n",
      "744: [discriminator loss: 0.000157, acc: 1.000000] [adversarial loss: 0.000041, acc: 1.000000]\n",
      "745: [discriminator loss: 0.000175, acc: 1.000000] [adversarial loss: 0.000040, acc: 1.000000]\n",
      "746: [discriminator loss: 0.000200, acc: 1.000000] [adversarial loss: 0.000071, acc: 1.000000]\n",
      "747: [discriminator loss: 0.000196, acc: 1.000000] [adversarial loss: 0.000063, acc: 1.000000]\n",
      "748: [discriminator loss: 0.000198, acc: 1.000000] [adversarial loss: 0.000072, acc: 1.000000]\n",
      "749: [discriminator loss: 0.000225, acc: 1.000000] [adversarial loss: 0.000060, acc: 1.000000]\n",
      "750: [discriminator loss: 0.000217, acc: 1.000000] [adversarial loss: 0.000141, acc: 1.000000]\n",
      "751: [discriminator loss: 0.000292, acc: 1.000000] [adversarial loss: 0.000085, acc: 1.000000]\n",
      "752: [discriminator loss: 0.000312, acc: 1.000000] [adversarial loss: 0.000102, acc: 1.000000]\n",
      "753: [discriminator loss: 0.000293, acc: 1.000000] [adversarial loss: 0.000141, acc: 1.000000]\n",
      "754: [discriminator loss: 0.000345, acc: 1.000000] [adversarial loss: 0.000152, acc: 1.000000]\n",
      "755: [discriminator loss: 0.000311, acc: 1.000000] [adversarial loss: 0.000181, acc: 1.000000]\n",
      "756: [discriminator loss: 0.000370, acc: 1.000000] [adversarial loss: 0.000326, acc: 1.000000]\n",
      "757: [discriminator loss: 0.000443, acc: 1.000000] [adversarial loss: 0.000396, acc: 1.000000]\n",
      "758: [discriminator loss: 0.000626, acc: 1.000000] [adversarial loss: 0.000817, acc: 1.000000]\n",
      "759: [discriminator loss: 0.000704, acc: 1.000000] [adversarial loss: 0.002451, acc: 1.000000]\n",
      "760: [discriminator loss: 0.004861, acc: 1.000000] [adversarial loss: 1.062725, acc: 0.375000]\n",
      "761: [discriminator loss: 2.147281, acc: 0.500000] [adversarial loss: 22.012512, acc: 0.000000]\n",
      "762: [discriminator loss: 1.871903, acc: 0.664062] [adversarial loss: 4.179253, acc: 0.000000]\n",
      "763: [discriminator loss: 0.008961, acc: 1.000000] [adversarial loss: 0.537178, acc: 0.750000]\n",
      "764: [discriminator loss: 0.033516, acc: 1.000000] [adversarial loss: 0.252380, acc: 0.921875]\n",
      "765: [discriminator loss: 0.045539, acc: 0.992188] [adversarial loss: 0.246567, acc: 0.937500]\n",
      "766: [discriminator loss: 0.041842, acc: 0.992188] [adversarial loss: 0.262078, acc: 0.937500]\n",
      "767: [discriminator loss: 0.081032, acc: 0.960938] [adversarial loss: 0.909388, acc: 0.484375]\n",
      "768: [discriminator loss: 0.079814, acc: 0.976562] [adversarial loss: 1.146257, acc: 0.281250]\n",
      "769: [discriminator loss: 0.132562, acc: 0.960938] [adversarial loss: 0.149941, acc: 0.953125]\n",
      "770: [discriminator loss: 0.143936, acc: 0.945312] [adversarial loss: 1.484864, acc: 0.218750]\n",
      "771: [discriminator loss: 0.082249, acc: 0.968750] [adversarial loss: 0.318502, acc: 0.843750]\n",
      "772: [discriminator loss: 0.162873, acc: 0.945312] [adversarial loss: 1.519213, acc: 0.218750]\n",
      "773: [discriminator loss: 0.106130, acc: 0.937500] [adversarial loss: 0.234434, acc: 0.921875]\n",
      "774: [discriminator loss: 0.085440, acc: 0.953125] [adversarial loss: 0.816504, acc: 0.593750]\n",
      "775: [discriminator loss: 0.074977, acc: 0.976562] [adversarial loss: 0.370156, acc: 0.796875]\n",
      "776: [discriminator loss: 0.031508, acc: 0.992188] [adversarial loss: 0.248541, acc: 0.906250]\n",
      "777: [discriminator loss: 0.063330, acc: 0.976562] [adversarial loss: 0.299735, acc: 0.890625]\n",
      "778: [discriminator loss: 0.056095, acc: 0.976562] [adversarial loss: 0.576120, acc: 0.671875]\n",
      "779: [discriminator loss: 0.036633, acc: 0.976562] [adversarial loss: 0.060456, acc: 1.000000]\n",
      "780: [discriminator loss: 0.042512, acc: 0.984375] [adversarial loss: 0.253906, acc: 0.843750]\n",
      "781: [discriminator loss: 0.096439, acc: 0.968750] [adversarial loss: 0.259494, acc: 0.875000]\n",
      "782: [discriminator loss: 0.192275, acc: 0.953125] [adversarial loss: 0.004735, acc: 1.000000]\n",
      "783: [discriminator loss: 0.109477, acc: 0.937500] [adversarial loss: 0.845441, acc: 0.593750]\n",
      "784: [discriminator loss: 0.114347, acc: 0.976562] [adversarial loss: 0.015293, acc: 1.000000]\n",
      "785: [discriminator loss: 0.126417, acc: 0.937500] [adversarial loss: 0.601173, acc: 0.718750]\n",
      "786: [discriminator loss: 0.234068, acc: 0.898438] [adversarial loss: 3.224964, acc: 0.078125]\n",
      "787: [discriminator loss: 0.234731, acc: 0.890625] [adversarial loss: 0.000322, acc: 1.000000]\n",
      "788: [discriminator loss: 0.000479, acc: 1.000000] [adversarial loss: 0.005909, acc: 1.000000]\n",
      "789: [discriminator loss: 0.029166, acc: 0.984375] [adversarial loss: 0.004206, acc: 1.000000]\n",
      "790: [discriminator loss: 0.010386, acc: 1.000000] [adversarial loss: 0.000680, acc: 1.000000]\n",
      "791: [discriminator loss: 0.006288, acc: 1.000000] [adversarial loss: 0.000683, acc: 1.000000]\n",
      "792: [discriminator loss: 0.026285, acc: 0.992188] [adversarial loss: 0.003716, acc: 1.000000]\n",
      "793: [discriminator loss: 0.035983, acc: 0.992188] [adversarial loss: 0.004347, acc: 1.000000]\n",
      "794: [discriminator loss: 0.036302, acc: 0.984375] [adversarial loss: 0.018495, acc: 1.000000]\n",
      "795: [discriminator loss: 0.100327, acc: 0.976562] [adversarial loss: 0.010524, acc: 1.000000]\n",
      "796: [discriminator loss: 0.064262, acc: 0.976562] [adversarial loss: 0.024918, acc: 1.000000]\n",
      "797: [discriminator loss: 0.076997, acc: 0.968750] [adversarial loss: 0.069376, acc: 0.953125]\n",
      "798: [discriminator loss: 0.208559, acc: 0.929688] [adversarial loss: 2.788737, acc: 0.125000]\n",
      "799: [discriminator loss: 0.213765, acc: 0.914062] [adversarial loss: 0.000106, acc: 1.000000]\n",
      "800: [discriminator loss: 0.369876, acc: 0.890625] [adversarial loss: 0.940944, acc: 0.531250]\n",
      "801: [discriminator loss: 0.423273, acc: 0.835938] [adversarial loss: 4.148181, acc: 0.000000]\n",
      "802: [discriminator loss: 0.464887, acc: 0.843750] [adversarial loss: 0.001170, acc: 1.000000]\n",
      "803: [discriminator loss: 0.022751, acc: 0.984375] [adversarial loss: 0.001518, acc: 1.000000]\n",
      "804: [discriminator loss: 0.035420, acc: 0.984375] [adversarial loss: 0.010086, acc: 1.000000]\n",
      "805: [discriminator loss: 0.018642, acc: 0.992188] [adversarial loss: 0.032591, acc: 0.984375]\n",
      "806: [discriminator loss: 0.085312, acc: 0.953125] [adversarial loss: 0.037538, acc: 1.000000]\n",
      "807: [discriminator loss: 0.055387, acc: 0.968750] [adversarial loss: 0.105834, acc: 0.984375]\n",
      "808: [discriminator loss: 0.210907, acc: 0.929688] [adversarial loss: 1.006287, acc: 0.343750]\n",
      "809: [discriminator loss: 0.425949, acc: 0.828125] [adversarial loss: 6.328991, acc: 0.000000]\n",
      "810: [discriminator loss: 0.590531, acc: 0.820312] [adversarial loss: 0.014616, acc: 1.000000]\n",
      "811: [discriminator loss: 0.511152, acc: 0.820312] [adversarial loss: 4.766585, acc: 0.000000]\n",
      "812: [discriminator loss: 0.376041, acc: 0.843750] [adversarial loss: 0.085920, acc: 1.000000]\n",
      "813: [discriminator loss: 0.328362, acc: 0.867188] [adversarial loss: 3.323578, acc: 0.078125]\n",
      "814: [discriminator loss: 0.419885, acc: 0.820312] [adversarial loss: 0.058014, acc: 1.000000]\n",
      "815: [discriminator loss: 0.590220, acc: 0.726562] [adversarial loss: 5.245323, acc: 0.000000]\n",
      "816: [discriminator loss: 0.585817, acc: 0.765625] [adversarial loss: 0.242555, acc: 0.921875]\n",
      "817: [discriminator loss: 0.319573, acc: 0.843750] [adversarial loss: 1.798435, acc: 0.296875]\n",
      "818: [discriminator loss: 0.390574, acc: 0.843750] [adversarial loss: 0.921934, acc: 0.515625]\n",
      "819: [discriminator loss: 0.352327, acc: 0.890625] [adversarial loss: 2.533272, acc: 0.078125]\n",
      "820: [discriminator loss: 0.336556, acc: 0.859375] [adversarial loss: 0.342600, acc: 0.875000]\n",
      "821: [discriminator loss: 0.302305, acc: 0.890625] [adversarial loss: 1.871087, acc: 0.093750]\n",
      "822: [discriminator loss: 0.196821, acc: 0.898438] [adversarial loss: 0.596069, acc: 0.640625]\n",
      "823: [discriminator loss: 0.254110, acc: 0.906250] [adversarial loss: 3.384018, acc: 0.015625]\n",
      "824: [discriminator loss: 0.333927, acc: 0.859375] [adversarial loss: 0.339071, acc: 0.859375]\n",
      "825: [discriminator loss: 0.485475, acc: 0.789062] [adversarial loss: 5.321266, acc: 0.000000]\n",
      "826: [discriminator loss: 0.411242, acc: 0.828125] [adversarial loss: 0.673092, acc: 0.640625]\n",
      "827: [discriminator loss: 0.358183, acc: 0.828125] [adversarial loss: 4.612427, acc: 0.000000]\n",
      "828: [discriminator loss: 0.474476, acc: 0.851562] [adversarial loss: 1.089481, acc: 0.265625]\n",
      "829: [discriminator loss: 0.295809, acc: 0.898438] [adversarial loss: 3.605023, acc: 0.000000]\n",
      "830: [discriminator loss: 0.180031, acc: 0.929688] [adversarial loss: 1.235592, acc: 0.375000]\n",
      "831: [discriminator loss: 0.178949, acc: 0.937500] [adversarial loss: 0.649476, acc: 0.609375]\n",
      "832: [discriminator loss: 0.196671, acc: 0.898438] [adversarial loss: 0.392603, acc: 0.781250]\n",
      "833: [discriminator loss: 0.400375, acc: 0.867188] [adversarial loss: 0.834160, acc: 0.562500]\n",
      "834: [discriminator loss: 0.385256, acc: 0.867188] [adversarial loss: 0.138294, acc: 0.953125]\n",
      "835: [discriminator loss: 0.349065, acc: 0.859375] [adversarial loss: 0.309808, acc: 0.859375]\n",
      "836: [discriminator loss: 0.396449, acc: 0.843750] [adversarial loss: 0.197931, acc: 0.968750]\n",
      "837: [discriminator loss: 0.201278, acc: 0.914062] [adversarial loss: 0.078362, acc: 1.000000]\n",
      "838: [discriminator loss: 0.229813, acc: 0.914062] [adversarial loss: 0.041926, acc: 1.000000]\n",
      "839: [discriminator loss: 0.232990, acc: 0.898438] [adversarial loss: 0.260556, acc: 0.937500]\n",
      "840: [discriminator loss: 0.260420, acc: 0.921875] [adversarial loss: 0.063589, acc: 1.000000]\n",
      "841: [discriminator loss: 0.251861, acc: 0.914062] [adversarial loss: 0.066250, acc: 1.000000]\n",
      "842: [discriminator loss: 0.116714, acc: 0.960938] [adversarial loss: 0.097822, acc: 0.984375]\n",
      "843: [discriminator loss: 0.132937, acc: 0.945312] [adversarial loss: 0.049082, acc: 1.000000]\n",
      "844: [discriminator loss: 0.402222, acc: 0.851562] [adversarial loss: 1.055219, acc: 0.359375]\n",
      "845: [discriminator loss: 0.367140, acc: 0.835938] [adversarial loss: 0.004609, acc: 1.000000]\n",
      "846: [discriminator loss: 0.478420, acc: 0.796875] [adversarial loss: 1.021284, acc: 0.359375]\n",
      "847: [discriminator loss: 0.364807, acc: 0.804688] [adversarial loss: 0.024320, acc: 1.000000]\n",
      "848: [discriminator loss: 0.684747, acc: 0.726562] [adversarial loss: 1.833515, acc: 0.046875]\n",
      "849: [discriminator loss: 0.519761, acc: 0.773438] [adversarial loss: 0.055946, acc: 1.000000]\n",
      "850: [discriminator loss: 0.647859, acc: 0.710938] [adversarial loss: 0.994406, acc: 0.343750]\n",
      "851: [discriminator loss: 0.481033, acc: 0.812500] [adversarial loss: 0.771501, acc: 0.468750]\n",
      "852: [discriminator loss: 0.543189, acc: 0.765625] [adversarial loss: 2.365899, acc: 0.062500]\n",
      "853: [discriminator loss: 0.498809, acc: 0.750000] [adversarial loss: 0.169159, acc: 0.953125]\n",
      "854: [discriminator loss: 0.428389, acc: 0.804688] [adversarial loss: 1.706628, acc: 0.125000]\n",
      "855: [discriminator loss: 0.337915, acc: 0.843750] [adversarial loss: 0.790112, acc: 0.453125]\n",
      "856: [discriminator loss: 0.358937, acc: 0.859375] [adversarial loss: 1.447295, acc: 0.156250]\n",
      "857: [discriminator loss: 0.370217, acc: 0.843750] [adversarial loss: 0.567983, acc: 0.671875]\n",
      "858: [discriminator loss: 0.340086, acc: 0.867188] [adversarial loss: 1.204941, acc: 0.265625]\n",
      "859: [discriminator loss: 0.360985, acc: 0.820312] [adversarial loss: 0.193717, acc: 0.953125]\n",
      "860: [discriminator loss: 0.413434, acc: 0.804688] [adversarial loss: 1.689097, acc: 0.156250]\n",
      "861: [discriminator loss: 0.419846, acc: 0.828125] [adversarial loss: 0.419139, acc: 0.843750]\n",
      "862: [discriminator loss: 0.380024, acc: 0.828125] [adversarial loss: 0.980346, acc: 0.390625]\n",
      "863: [discriminator loss: 0.324689, acc: 0.867188] [adversarial loss: 0.233757, acc: 0.953125]\n",
      "864: [discriminator loss: 0.293823, acc: 0.851562] [adversarial loss: 0.669656, acc: 0.578125]\n",
      "865: [discriminator loss: 0.198456, acc: 0.921875] [adversarial loss: 0.326353, acc: 0.859375]\n",
      "866: [discriminator loss: 0.187053, acc: 0.937500] [adversarial loss: 0.178991, acc: 1.000000]\n",
      "867: [discriminator loss: 0.269451, acc: 0.875000] [adversarial loss: 0.316549, acc: 0.921875]\n",
      "868: [discriminator loss: 0.251906, acc: 0.882812] [adversarial loss: 0.133747, acc: 0.984375]\n",
      "869: [discriminator loss: 0.204167, acc: 0.929688] [adversarial loss: 0.294669, acc: 0.906250]\n",
      "870: [discriminator loss: 0.239902, acc: 0.906250] [adversarial loss: 0.176331, acc: 0.968750]\n",
      "871: [discriminator loss: 0.232678, acc: 0.921875] [adversarial loss: 0.158306, acc: 0.968750]\n",
      "872: [discriminator loss: 0.281592, acc: 0.898438] [adversarial loss: 0.933141, acc: 0.390625]\n",
      "873: [discriminator loss: 0.330890, acc: 0.820312] [adversarial loss: 0.019689, acc: 1.000000]\n",
      "874: [discriminator loss: 0.471114, acc: 0.835938] [adversarial loss: 0.734784, acc: 0.531250]\n",
      "875: [discriminator loss: 0.425029, acc: 0.757812] [adversarial loss: 0.008872, acc: 1.000000]\n",
      "876: [discriminator loss: 0.273057, acc: 0.875000] [adversarial loss: 0.084980, acc: 1.000000]\n",
      "877: [discriminator loss: 0.216940, acc: 0.921875] [adversarial loss: 0.131963, acc: 0.984375]\n",
      "878: [discriminator loss: 0.353190, acc: 0.835938] [adversarial loss: 0.598078, acc: 0.671875]\n",
      "879: [discriminator loss: 0.304177, acc: 0.867188] [adversarial loss: 0.067689, acc: 1.000000]\n",
      "880: [discriminator loss: 0.409067, acc: 0.820312] [adversarial loss: 0.860698, acc: 0.437500]\n",
      "881: [discriminator loss: 0.467995, acc: 0.812500] [adversarial loss: 0.040267, acc: 1.000000]\n",
      "882: [discriminator loss: 0.582653, acc: 0.750000] [adversarial loss: 1.248843, acc: 0.187500]\n",
      "883: [discriminator loss: 0.582874, acc: 0.734375] [adversarial loss: 0.100396, acc: 1.000000]\n",
      "884: [discriminator loss: 0.577708, acc: 0.710938] [adversarial loss: 1.479086, acc: 0.125000]\n",
      "885: [discriminator loss: 0.486774, acc: 0.734375] [adversarial loss: 0.127918, acc: 1.000000]\n",
      "886: [discriminator loss: 0.249783, acc: 0.921875] [adversarial loss: 0.192142, acc: 0.968750]\n",
      "887: [discriminator loss: 0.208682, acc: 0.945312] [adversarial loss: 0.181529, acc: 0.953125]\n",
      "888: [discriminator loss: 0.261369, acc: 0.898438] [adversarial loss: 0.218090, acc: 0.968750]\n",
      "889: [discriminator loss: 0.331276, acc: 0.835938] [adversarial loss: 0.279196, acc: 0.859375]\n",
      "890: [discriminator loss: 0.217164, acc: 0.945312] [adversarial loss: 0.199924, acc: 0.953125]\n",
      "891: [discriminator loss: 0.275637, acc: 0.898438] [adversarial loss: 0.330990, acc: 0.906250]\n",
      "892: [discriminator loss: 0.214515, acc: 0.906250] [adversarial loss: 0.284639, acc: 0.937500]\n",
      "893: [discriminator loss: 0.303471, acc: 0.867188] [adversarial loss: 0.363024, acc: 0.843750]\n",
      "894: [discriminator loss: 0.267766, acc: 0.898438] [adversarial loss: 0.101394, acc: 1.000000]\n",
      "895: [discriminator loss: 0.425627, acc: 0.828125] [adversarial loss: 0.722933, acc: 0.609375]\n",
      "896: [discriminator loss: 0.297628, acc: 0.843750] [adversarial loss: 0.028100, acc: 1.000000]\n",
      "897: [discriminator loss: 0.156935, acc: 0.929688] [adversarial loss: 0.081515, acc: 1.000000]\n",
      "898: [discriminator loss: 0.204607, acc: 0.921875] [adversarial loss: 0.187277, acc: 1.000000]\n",
      "899: [discriminator loss: 0.262844, acc: 0.937500] [adversarial loss: 0.156924, acc: 1.000000]\n",
      "900: [discriminator loss: 0.221409, acc: 0.945312] [adversarial loss: 0.158967, acc: 0.968750]\n",
      "901: [discriminator loss: 0.238137, acc: 0.921875] [adversarial loss: 0.415744, acc: 0.781250]\n",
      "902: [discriminator loss: 0.304934, acc: 0.875000] [adversarial loss: 0.190983, acc: 0.984375]\n",
      "903: [discriminator loss: 0.305774, acc: 0.898438] [adversarial loss: 0.578084, acc: 0.734375]\n",
      "904: [discriminator loss: 0.452642, acc: 0.796875] [adversarial loss: 0.806427, acc: 0.500000]\n",
      "905: [discriminator loss: 0.318104, acc: 0.843750] [adversarial loss: 0.021868, acc: 1.000000]\n",
      "906: [discriminator loss: 0.681897, acc: 0.640625] [adversarial loss: 2.196673, acc: 0.015625]\n",
      "907: [discriminator loss: 0.641413, acc: 0.687500] [adversarial loss: 0.087588, acc: 1.000000]\n",
      "908: [discriminator loss: 0.732773, acc: 0.601562] [adversarial loss: 1.476422, acc: 0.062500]\n",
      "909: [discriminator loss: 0.486770, acc: 0.734375] [adversarial loss: 0.259729, acc: 0.968750]\n",
      "910: [discriminator loss: 0.414666, acc: 0.765625] [adversarial loss: 0.933993, acc: 0.312500]\n",
      "911: [discriminator loss: 0.338029, acc: 0.835938] [adversarial loss: 0.313637, acc: 0.890625]\n",
      "912: [discriminator loss: 0.590402, acc: 0.632812] [adversarial loss: 1.729992, acc: 0.000000]\n",
      "913: [discriminator loss: 0.465122, acc: 0.742188] [adversarial loss: 0.262869, acc: 0.968750]\n",
      "914: [discriminator loss: 0.409167, acc: 0.804688] [adversarial loss: 0.934529, acc: 0.359375]\n",
      "915: [discriminator loss: 0.310065, acc: 0.890625] [adversarial loss: 0.457856, acc: 0.875000]\n",
      "916: [discriminator loss: 0.339694, acc: 0.867188] [adversarial loss: 1.363652, acc: 0.031250]\n",
      "917: [discriminator loss: 0.413517, acc: 0.820312] [adversarial loss: 0.468270, acc: 0.796875]\n",
      "918: [discriminator loss: 0.412641, acc: 0.789062] [adversarial loss: 1.447781, acc: 0.062500]\n",
      "919: [discriminator loss: 0.364872, acc: 0.835938] [adversarial loss: 0.298356, acc: 0.953125]\n",
      "920: [discriminator loss: 0.432641, acc: 0.804688] [adversarial loss: 1.468319, acc: 0.031250]\n",
      "921: [discriminator loss: 0.388734, acc: 0.796875] [adversarial loss: 0.570917, acc: 0.656250]\n",
      "922: [discriminator loss: 0.432723, acc: 0.773438] [adversarial loss: 1.692190, acc: 0.046875]\n",
      "923: [discriminator loss: 0.446626, acc: 0.773438] [adversarial loss: 0.345185, acc: 0.859375]\n",
      "924: [discriminator loss: 0.477679, acc: 0.750000] [adversarial loss: 1.652229, acc: 0.031250]\n",
      "925: [discriminator loss: 0.440278, acc: 0.773438] [adversarial loss: 0.575334, acc: 0.671875]\n",
      "926: [discriminator loss: 0.462863, acc: 0.765625] [adversarial loss: 2.073088, acc: 0.000000]\n",
      "927: [discriminator loss: 0.420823, acc: 0.789062] [adversarial loss: 0.426156, acc: 0.828125]\n",
      "928: [discriminator loss: 0.470785, acc: 0.742188] [adversarial loss: 1.676107, acc: 0.046875]\n",
      "929: [discriminator loss: 0.371072, acc: 0.882812] [adversarial loss: 0.958134, acc: 0.296875]\n",
      "930: [discriminator loss: 0.458302, acc: 0.796875] [adversarial loss: 2.327074, acc: 0.000000]\n",
      "931: [discriminator loss: 0.469781, acc: 0.765625] [adversarial loss: 0.858094, acc: 0.453125]\n",
      "932: [discriminator loss: 0.432732, acc: 0.781250] [adversarial loss: 1.815719, acc: 0.031250]\n",
      "933: [discriminator loss: 0.349779, acc: 0.843750] [adversarial loss: 0.670771, acc: 0.625000]\n",
      "934: [discriminator loss: 0.330548, acc: 0.882812] [adversarial loss: 1.097463, acc: 0.218750]\n",
      "935: [discriminator loss: 0.409161, acc: 0.851562] [adversarial loss: 0.747890, acc: 0.453125]\n",
      "936: [discriminator loss: 0.559477, acc: 0.726562] [adversarial loss: 2.220203, acc: 0.000000]\n",
      "937: [discriminator loss: 0.532497, acc: 0.757812] [adversarial loss: 0.593882, acc: 0.687500]\n",
      "938: [discriminator loss: 0.503643, acc: 0.695312] [adversarial loss: 2.049815, acc: 0.000000]\n",
      "939: [discriminator loss: 0.545677, acc: 0.742188] [adversarial loss: 0.885515, acc: 0.250000]\n",
      "940: [discriminator loss: 0.475876, acc: 0.781250] [adversarial loss: 2.065851, acc: 0.046875]\n",
      "941: [discriminator loss: 0.403886, acc: 0.812500] [adversarial loss: 1.080455, acc: 0.218750]\n",
      "942: [discriminator loss: 0.378690, acc: 0.851562] [adversarial loss: 1.659027, acc: 0.093750]\n",
      "943: [discriminator loss: 0.345564, acc: 0.859375] [adversarial loss: 1.216109, acc: 0.234375]\n",
      "944: [discriminator loss: 0.392865, acc: 0.820312] [adversarial loss: 1.429468, acc: 0.125000]\n",
      "945: [discriminator loss: 0.435265, acc: 0.804688] [adversarial loss: 0.951344, acc: 0.390625]\n",
      "946: [discriminator loss: 0.420390, acc: 0.796875] [adversarial loss: 2.106426, acc: 0.015625]\n",
      "947: [discriminator loss: 0.434722, acc: 0.812500] [adversarial loss: 0.487706, acc: 0.718750]\n",
      "948: [discriminator loss: 0.581889, acc: 0.656250] [adversarial loss: 2.036662, acc: 0.000000]\n",
      "949: [discriminator loss: 0.434930, acc: 0.828125] [adversarial loss: 0.715210, acc: 0.515625]\n",
      "950: [discriminator loss: 0.493276, acc: 0.718750] [adversarial loss: 1.985587, acc: 0.000000]\n",
      "951: [discriminator loss: 0.369094, acc: 0.875000] [adversarial loss: 0.938025, acc: 0.343750]\n",
      "952: [discriminator loss: 0.442344, acc: 0.812500] [adversarial loss: 1.863592, acc: 0.000000]\n",
      "953: [discriminator loss: 0.448018, acc: 0.828125] [adversarial loss: 0.763473, acc: 0.484375]\n",
      "954: [discriminator loss: 0.422366, acc: 0.804688] [adversarial loss: 1.323210, acc: 0.093750]\n",
      "955: [discriminator loss: 0.563861, acc: 0.687500] [adversarial loss: 1.167023, acc: 0.218750]\n",
      "956: [discriminator loss: 0.542967, acc: 0.757812] [adversarial loss: 1.146077, acc: 0.218750]\n",
      "957: [discriminator loss: 0.337603, acc: 0.890625] [adversarial loss: 0.588689, acc: 0.656250]\n",
      "958: [discriminator loss: 0.416551, acc: 0.804688] [adversarial loss: 1.101925, acc: 0.281250]\n",
      "959: [discriminator loss: 0.371413, acc: 0.867188] [adversarial loss: 0.472590, acc: 0.750000]\n",
      "960: [discriminator loss: 0.535622, acc: 0.710938] [adversarial loss: 1.572834, acc: 0.046875]\n",
      "961: [discriminator loss: 0.553938, acc: 0.742188] [adversarial loss: 0.405107, acc: 0.859375]\n",
      "962: [discriminator loss: 0.662405, acc: 0.554688] [adversarial loss: 2.677749, acc: 0.000000]\n",
      "963: [discriminator loss: 0.623589, acc: 0.609375] [adversarial loss: 0.576193, acc: 0.687500]\n",
      "964: [discriminator loss: 0.522420, acc: 0.640625] [adversarial loss: 2.810937, acc: 0.000000]\n",
      "965: [discriminator loss: 0.449990, acc: 0.765625] [adversarial loss: 0.843672, acc: 0.390625]\n",
      "966: [discriminator loss: 0.257031, acc: 0.945312] [adversarial loss: 0.584958, acc: 0.687500]\n",
      "967: [discriminator loss: 0.211120, acc: 0.960938] [adversarial loss: 0.553474, acc: 0.687500]\n",
      "968: [discriminator loss: 0.144485, acc: 0.992188] [adversarial loss: 0.376932, acc: 0.875000]\n",
      "969: [discriminator loss: 0.269123, acc: 0.914062] [adversarial loss: 0.496222, acc: 0.765625]\n",
      "970: [discriminator loss: 0.273470, acc: 0.906250] [adversarial loss: 0.621677, acc: 0.671875]\n",
      "971: [discriminator loss: 0.391664, acc: 0.851562] [adversarial loss: 1.242463, acc: 0.156250]\n",
      "972: [discriminator loss: 0.375262, acc: 0.804688] [adversarial loss: 0.223834, acc: 0.984375]\n",
      "973: [discriminator loss: 0.735494, acc: 0.601562] [adversarial loss: 2.132461, acc: 0.015625]\n",
      "974: [discriminator loss: 0.702181, acc: 0.570312] [adversarial loss: 0.356134, acc: 0.890625]\n",
      "975: [discriminator loss: 0.760798, acc: 0.539062] [adversarial loss: 2.023458, acc: 0.000000]\n",
      "976: [discriminator loss: 0.694654, acc: 0.625000] [adversarial loss: 0.689596, acc: 0.625000]\n",
      "977: [discriminator loss: 0.554584, acc: 0.671875] [adversarial loss: 1.425541, acc: 0.031250]\n",
      "978: [discriminator loss: 0.369543, acc: 0.804688] [adversarial loss: 0.548417, acc: 0.718750]\n",
      "979: [discriminator loss: 0.384534, acc: 0.828125] [adversarial loss: 0.573470, acc: 0.687500]\n",
      "980: [discriminator loss: 0.341900, acc: 0.890625] [adversarial loss: 0.675064, acc: 0.578125]\n",
      "981: [discriminator loss: 0.312830, acc: 0.945312] [adversarial loss: 0.462570, acc: 0.812500]\n",
      "982: [discriminator loss: 0.525405, acc: 0.750000] [adversarial loss: 0.996426, acc: 0.281250]\n",
      "983: [discriminator loss: 0.432361, acc: 0.804688] [adversarial loss: 0.612742, acc: 0.703125]\n",
      "984: [discriminator loss: 0.505387, acc: 0.695312] [adversarial loss: 1.568546, acc: 0.031250]\n",
      "985: [discriminator loss: 0.518517, acc: 0.742188] [adversarial loss: 0.421176, acc: 0.875000]\n",
      "986: [discriminator loss: 0.676403, acc: 0.585938] [adversarial loss: 1.964717, acc: 0.000000]\n",
      "987: [discriminator loss: 0.547519, acc: 0.679688] [adversarial loss: 0.788839, acc: 0.468750]\n",
      "988: [discriminator loss: 0.587004, acc: 0.648438] [adversarial loss: 1.909211, acc: 0.000000]\n",
      "989: [discriminator loss: 0.540639, acc: 0.703125] [adversarial loss: 0.845723, acc: 0.375000]\n",
      "990: [discriminator loss: 0.425099, acc: 0.812500] [adversarial loss: 0.962698, acc: 0.312500]\n",
      "991: [discriminator loss: 0.426298, acc: 0.820312] [adversarial loss: 0.841595, acc: 0.359375]\n",
      "992: [discriminator loss: 0.474740, acc: 0.789062] [adversarial loss: 0.982177, acc: 0.281250]\n",
      "993: [discriminator loss: 0.580305, acc: 0.695312] [adversarial loss: 1.081821, acc: 0.218750]\n",
      "994: [discriminator loss: 0.604214, acc: 0.679688] [adversarial loss: 1.814491, acc: 0.015625]\n",
      "995: [discriminator loss: 0.416923, acc: 0.843750] [adversarial loss: 0.619517, acc: 0.640625]\n",
      "996: [discriminator loss: 0.525190, acc: 0.710938] [adversarial loss: 1.929163, acc: 0.000000]\n",
      "997: [discriminator loss: 0.530358, acc: 0.742188] [adversarial loss: 0.699451, acc: 0.515625]\n",
      "998: [discriminator loss: 0.547555, acc: 0.656250] [adversarial loss: 2.007710, acc: 0.000000]\n",
      "999: [discriminator loss: 0.480087, acc: 0.726562] [adversarial loss: 0.745304, acc: 0.531250]\n",
      "1000: [discriminator loss: 0.504681, acc: 0.742188] [adversarial loss: 1.447005, acc: 0.015625]\n",
      "1001: [discriminator loss: 0.508136, acc: 0.757812] [adversarial loss: 0.523143, acc: 0.781250]\n",
      "1002: [discriminator loss: 0.306041, acc: 0.882812] [adversarial loss: 0.444899, acc: 0.875000]\n",
      "1003: [discriminator loss: 0.431545, acc: 0.804688] [adversarial loss: 0.870414, acc: 0.421875]\n",
      "1004: [discriminator loss: 0.367225, acc: 0.882812] [adversarial loss: 1.100678, acc: 0.203125]\n",
      "1005: [discriminator loss: 0.526873, acc: 0.726562] [adversarial loss: 0.431949, acc: 0.859375]\n",
      "1006: [discriminator loss: 0.596312, acc: 0.609375] [adversarial loss: 1.661216, acc: 0.000000]\n",
      "1007: [discriminator loss: 0.570844, acc: 0.695312] [adversarial loss: 0.706590, acc: 0.625000]\n",
      "1008: [discriminator loss: 0.558326, acc: 0.671875] [adversarial loss: 1.792913, acc: 0.015625]\n",
      "1009: [discriminator loss: 0.502842, acc: 0.750000] [adversarial loss: 1.113303, acc: 0.156250]\n",
      "1010: [discriminator loss: 0.431938, acc: 0.773438] [adversarial loss: 0.865507, acc: 0.468750]\n",
      "1011: [discriminator loss: 0.377400, acc: 0.851562] [adversarial loss: 0.962422, acc: 0.359375]\n",
      "1012: [discriminator loss: 0.417404, acc: 0.789062] [adversarial loss: 0.770503, acc: 0.500000]\n",
      "1013: [discriminator loss: 0.501486, acc: 0.734375] [adversarial loss: 1.034963, acc: 0.250000]\n",
      "1014: [discriminator loss: 0.516240, acc: 0.781250] [adversarial loss: 0.624768, acc: 0.718750]\n",
      "1015: [discriminator loss: 0.638965, acc: 0.609375] [adversarial loss: 2.561543, acc: 0.000000]\n",
      "1016: [discriminator loss: 0.736875, acc: 0.609375] [adversarial loss: 0.509540, acc: 0.718750]\n",
      "1017: [discriminator loss: 0.571369, acc: 0.695312] [adversarial loss: 0.987831, acc: 0.250000]\n",
      "1018: [discriminator loss: 0.568838, acc: 0.726562] [adversarial loss: 0.930521, acc: 0.312500]\n",
      "1019: [discriminator loss: 0.499820, acc: 0.789062] [adversarial loss: 0.630572, acc: 0.625000]\n",
      "1020: [discriminator loss: 0.575409, acc: 0.679688] [adversarial loss: 1.476829, acc: 0.031250]\n",
      "1021: [discriminator loss: 0.535993, acc: 0.742188] [adversarial loss: 0.540796, acc: 0.859375]\n",
      "1022: [discriminator loss: 0.637275, acc: 0.664062] [adversarial loss: 1.806448, acc: 0.000000]\n",
      "1023: [discriminator loss: 0.588526, acc: 0.664062] [adversarial loss: 0.648141, acc: 0.625000]\n",
      "1024: [discriminator loss: 0.417954, acc: 0.835938] [adversarial loss: 0.468121, acc: 0.812500]\n",
      "1025: [discriminator loss: 0.595531, acc: 0.679688] [adversarial loss: 1.220796, acc: 0.062500]\n",
      "1026: [discriminator loss: 0.527647, acc: 0.765625] [adversarial loss: 0.630915, acc: 0.609375]\n",
      "1027: [discriminator loss: 0.482884, acc: 0.812500] [adversarial loss: 0.616729, acc: 0.640625]\n",
      "1028: [discriminator loss: 0.462640, acc: 0.796875] [adversarial loss: 0.778862, acc: 0.515625]\n",
      "1029: [discriminator loss: 0.625786, acc: 0.625000] [adversarial loss: 1.350441, acc: 0.015625]\n",
      "1030: [discriminator loss: 0.465791, acc: 0.773438] [adversarial loss: 0.466207, acc: 0.828125]\n",
      "1031: [discriminator loss: 0.689979, acc: 0.617188] [adversarial loss: 1.283202, acc: 0.109375]\n",
      "1032: [discriminator loss: 0.551211, acc: 0.703125] [adversarial loss: 0.498602, acc: 0.781250]\n",
      "1033: [discriminator loss: 0.696614, acc: 0.546875] [adversarial loss: 1.467946, acc: 0.000000]\n",
      "1034: [discriminator loss: 0.635284, acc: 0.617188] [adversarial loss: 0.487553, acc: 0.828125]\n",
      "1035: [discriminator loss: 0.720491, acc: 0.585938] [adversarial loss: 1.416498, acc: 0.015625]\n",
      "1036: [discriminator loss: 0.658634, acc: 0.687500] [adversarial loss: 0.788965, acc: 0.390625]\n",
      "1037: [discriminator loss: 0.550135, acc: 0.710938] [adversarial loss: 1.246241, acc: 0.093750]\n",
      "1038: [discriminator loss: 0.538609, acc: 0.734375] [adversarial loss: 0.735072, acc: 0.515625]\n",
      "1039: [discriminator loss: 0.559644, acc: 0.734375] [adversarial loss: 1.362804, acc: 0.046875]\n",
      "1040: [discriminator loss: 0.520752, acc: 0.742188] [adversarial loss: 0.595124, acc: 0.656250]\n",
      "1041: [discriminator loss: 0.646291, acc: 0.585938] [adversarial loss: 1.063809, acc: 0.203125]\n",
      "1042: [discriminator loss: 0.557562, acc: 0.742188] [adversarial loss: 0.505855, acc: 0.765625]\n",
      "1043: [discriminator loss: 0.625724, acc: 0.632812] [adversarial loss: 0.930230, acc: 0.187500]\n",
      "1044: [discriminator loss: 0.443172, acc: 0.843750] [adversarial loss: 0.557201, acc: 0.828125]\n",
      "1045: [discriminator loss: 0.475162, acc: 0.757812] [adversarial loss: 0.832468, acc: 0.453125]\n",
      "1046: [discriminator loss: 0.526908, acc: 0.796875] [adversarial loss: 1.073041, acc: 0.140625]\n",
      "1047: [discriminator loss: 0.449740, acc: 0.835938] [adversarial loss: 0.936728, acc: 0.296875]\n",
      "1048: [discriminator loss: 0.495264, acc: 0.734375] [adversarial loss: 1.131880, acc: 0.078125]\n",
      "1049: [discriminator loss: 0.579140, acc: 0.703125] [adversarial loss: 0.589303, acc: 0.640625]\n",
      "1050: [discriminator loss: 0.751850, acc: 0.539062] [adversarial loss: 1.579237, acc: 0.000000]\n",
      "1051: [discriminator loss: 0.735903, acc: 0.593750] [adversarial loss: 0.665928, acc: 0.640625]\n",
      "1052: [discriminator loss: 0.702993, acc: 0.578125] [adversarial loss: 1.194379, acc: 0.078125]\n",
      "1053: [discriminator loss: 0.700767, acc: 0.617188] [adversarial loss: 0.755027, acc: 0.421875]\n",
      "1054: [discriminator loss: 0.683970, acc: 0.539062] [adversarial loss: 1.950139, acc: 0.000000]\n",
      "1055: [discriminator loss: 0.597907, acc: 0.671875] [adversarial loss: 0.561841, acc: 0.765625]\n",
      "1056: [discriminator loss: 0.517994, acc: 0.703125] [adversarial loss: 1.266965, acc: 0.109375]\n",
      "1057: [discriminator loss: 0.542602, acc: 0.781250] [adversarial loss: 0.804019, acc: 0.406250]\n",
      "1058: [discriminator loss: 0.589243, acc: 0.664062] [adversarial loss: 1.539161, acc: 0.046875]\n",
      "1059: [discriminator loss: 0.603145, acc: 0.687500] [adversarial loss: 1.039240, acc: 0.171875]\n",
      "1060: [discriminator loss: 0.466185, acc: 0.789062] [adversarial loss: 0.683027, acc: 0.562500]\n",
      "1061: [discriminator loss: 0.496635, acc: 0.789062] [adversarial loss: 0.787144, acc: 0.437500]\n",
      "1062: [discriminator loss: 0.494505, acc: 0.765625] [adversarial loss: 0.797472, acc: 0.390625]\n",
      "1063: [discriminator loss: 0.468341, acc: 0.796875] [adversarial loss: 0.880785, acc: 0.312500]\n",
      "1064: [discriminator loss: 0.435399, acc: 0.851562] [adversarial loss: 0.867234, acc: 0.343750]\n",
      "1065: [discriminator loss: 0.465683, acc: 0.796875] [adversarial loss: 1.007063, acc: 0.218750]\n",
      "1066: [discriminator loss: 0.528372, acc: 0.789062] [adversarial loss: 1.067067, acc: 0.234375]\n",
      "1067: [discriminator loss: 0.420218, acc: 0.867188] [adversarial loss: 0.682667, acc: 0.593750]\n",
      "1068: [discriminator loss: 0.622279, acc: 0.671875] [adversarial loss: 2.057741, acc: 0.000000]\n",
      "1069: [discriminator loss: 0.732938, acc: 0.554688] [adversarial loss: 0.483326, acc: 0.843750]\n",
      "1070: [discriminator loss: 0.672449, acc: 0.585938] [adversarial loss: 1.846150, acc: 0.000000]\n",
      "1071: [discriminator loss: 0.671010, acc: 0.593750] [adversarial loss: 0.764669, acc: 0.390625]\n",
      "1072: [discriminator loss: 0.528894, acc: 0.781250] [adversarial loss: 0.908402, acc: 0.296875]\n",
      "1073: [discriminator loss: 0.619496, acc: 0.640625] [adversarial loss: 1.178645, acc: 0.093750]\n",
      "1074: [discriminator loss: 0.629162, acc: 0.640625] [adversarial loss: 0.765855, acc: 0.531250]\n",
      "1075: [discriminator loss: 0.556307, acc: 0.703125] [adversarial loss: 1.427145, acc: 0.000000]\n",
      "1076: [discriminator loss: 0.549634, acc: 0.726562] [adversarial loss: 0.814075, acc: 0.406250]\n",
      "1077: [discriminator loss: 0.543065, acc: 0.726562] [adversarial loss: 1.390741, acc: 0.046875]\n",
      "1078: [discriminator loss: 0.546680, acc: 0.695312] [adversarial loss: 0.848762, acc: 0.437500]\n",
      "1079: [discriminator loss: 0.493612, acc: 0.789062] [adversarial loss: 1.011664, acc: 0.171875]\n",
      "1080: [discriminator loss: 0.558627, acc: 0.710938] [adversarial loss: 1.299319, acc: 0.015625]\n",
      "1081: [discriminator loss: 0.487053, acc: 0.781250] [adversarial loss: 0.799654, acc: 0.406250]\n",
      "1082: [discriminator loss: 0.570436, acc: 0.671875] [adversarial loss: 1.013218, acc: 0.203125]\n",
      "1083: [discriminator loss: 0.554535, acc: 0.726562] [adversarial loss: 1.020977, acc: 0.281250]\n",
      "1084: [discriminator loss: 0.551236, acc: 0.757812] [adversarial loss: 0.883839, acc: 0.281250]\n",
      "1085: [discriminator loss: 0.476702, acc: 0.812500] [adversarial loss: 1.067924, acc: 0.156250]\n",
      "1086: [discriminator loss: 0.534658, acc: 0.757812] [adversarial loss: 0.750881, acc: 0.453125]\n",
      "1087: [discriminator loss: 0.535515, acc: 0.710938] [adversarial loss: 1.743795, acc: 0.015625]\n",
      "1088: [discriminator loss: 0.652040, acc: 0.632812] [adversarial loss: 0.538537, acc: 0.734375]\n",
      "1089: [discriminator loss: 0.541341, acc: 0.710938] [adversarial loss: 1.521480, acc: 0.000000]\n",
      "1090: [discriminator loss: 0.509055, acc: 0.757812] [adversarial loss: 0.955404, acc: 0.250000]\n",
      "1091: [discriminator loss: 0.513942, acc: 0.765625] [adversarial loss: 0.693140, acc: 0.593750]\n",
      "1092: [discriminator loss: 0.602527, acc: 0.671875] [adversarial loss: 1.543970, acc: 0.046875]\n",
      "1093: [discriminator loss: 0.588050, acc: 0.671875] [adversarial loss: 0.514931, acc: 0.796875]\n",
      "1094: [discriminator loss: 0.565312, acc: 0.726562] [adversarial loss: 1.055113, acc: 0.187500]\n",
      "1095: [discriminator loss: 0.539590, acc: 0.765625] [adversarial loss: 1.025277, acc: 0.187500]\n",
      "1096: [discriminator loss: 0.571319, acc: 0.664062] [adversarial loss: 1.153144, acc: 0.125000]\n",
      "1097: [discriminator loss: 0.460747, acc: 0.789062] [adversarial loss: 0.672103, acc: 0.562500]\n",
      "1098: [discriminator loss: 0.627137, acc: 0.609375] [adversarial loss: 1.503500, acc: 0.015625]\n",
      "1099: [discriminator loss: 0.585009, acc: 0.648438] [adversarial loss: 0.618332, acc: 0.625000]\n",
      "1100: [discriminator loss: 0.555895, acc: 0.742188] [adversarial loss: 1.419325, acc: 0.031250]\n",
      "1101: [discriminator loss: 0.497127, acc: 0.812500] [adversarial loss: 1.004075, acc: 0.140625]\n",
      "1102: [discriminator loss: 0.521963, acc: 0.773438] [adversarial loss: 1.122219, acc: 0.156250]\n",
      "1103: [discriminator loss: 0.422368, acc: 0.835938] [adversarial loss: 0.552647, acc: 0.718750]\n",
      "1104: [discriminator loss: 0.507761, acc: 0.726562] [adversarial loss: 0.951527, acc: 0.343750]\n",
      "1105: [discriminator loss: 0.538283, acc: 0.765625] [adversarial loss: 0.555695, acc: 0.718750]\n",
      "1106: [discriminator loss: 0.664355, acc: 0.609375] [adversarial loss: 1.395700, acc: 0.015625]\n",
      "1107: [discriminator loss: 0.566515, acc: 0.664062] [adversarial loss: 0.561046, acc: 0.656250]\n",
      "1108: [discriminator loss: 0.676799, acc: 0.617188] [adversarial loss: 1.689267, acc: 0.000000]\n",
      "1109: [discriminator loss: 0.660957, acc: 0.593750] [adversarial loss: 0.722415, acc: 0.453125]\n",
      "1110: [discriminator loss: 0.565721, acc: 0.710938] [adversarial loss: 0.918868, acc: 0.281250]\n",
      "1111: [discriminator loss: 0.522918, acc: 0.781250] [adversarial loss: 1.095489, acc: 0.125000]\n",
      "1112: [discriminator loss: 0.463428, acc: 0.835938] [adversarial loss: 1.169653, acc: 0.078125]\n",
      "1113: [discriminator loss: 0.473065, acc: 0.796875] [adversarial loss: 1.009635, acc: 0.234375]\n",
      "1114: [discriminator loss: 0.455661, acc: 0.843750] [adversarial loss: 0.740499, acc: 0.484375]\n",
      "1115: [discriminator loss: 0.535774, acc: 0.734375] [adversarial loss: 1.045639, acc: 0.187500]\n",
      "1116: [discriminator loss: 0.560865, acc: 0.687500] [adversarial loss: 1.078442, acc: 0.062500]\n",
      "1117: [discriminator loss: 0.572983, acc: 0.679688] [adversarial loss: 1.190702, acc: 0.031250]\n",
      "1118: [discriminator loss: 0.574280, acc: 0.695312] [adversarial loss: 0.794270, acc: 0.453125]\n",
      "1119: [discriminator loss: 0.553145, acc: 0.695312] [adversarial loss: 1.547582, acc: 0.015625]\n",
      "1120: [discriminator loss: 0.608622, acc: 0.617188] [adversarial loss: 0.531825, acc: 0.781250]\n",
      "1121: [discriminator loss: 0.635691, acc: 0.617188] [adversarial loss: 1.640292, acc: 0.015625]\n",
      "1122: [discriminator loss: 0.634884, acc: 0.625000] [adversarial loss: 0.752529, acc: 0.468750]\n",
      "1123: [discriminator loss: 0.511275, acc: 0.687500] [adversarial loss: 1.803684, acc: 0.000000]\n",
      "1124: [discriminator loss: 0.545336, acc: 0.710938] [adversarial loss: 0.678245, acc: 0.500000]\n",
      "1125: [discriminator loss: 0.505012, acc: 0.742188] [adversarial loss: 0.925902, acc: 0.281250]\n",
      "1126: [discriminator loss: 0.551424, acc: 0.687500] [adversarial loss: 1.114231, acc: 0.125000]\n",
      "1127: [discriminator loss: 0.547250, acc: 0.718750] [adversarial loss: 0.563807, acc: 0.625000]\n",
      "1128: [discriminator loss: 0.600301, acc: 0.640625] [adversarial loss: 1.708779, acc: 0.015625]\n",
      "1129: [discriminator loss: 0.573205, acc: 0.671875] [adversarial loss: 0.639899, acc: 0.578125]\n",
      "1130: [discriminator loss: 0.559752, acc: 0.710938] [adversarial loss: 1.280138, acc: 0.031250]\n",
      "1131: [discriminator loss: 0.527351, acc: 0.726562] [adversarial loss: 0.926600, acc: 0.234375]\n",
      "1132: [discriminator loss: 0.518710, acc: 0.718750] [adversarial loss: 0.823360, acc: 0.359375]\n",
      "1133: [discriminator loss: 0.532224, acc: 0.750000] [adversarial loss: 0.675008, acc: 0.546875]\n",
      "1134: [discriminator loss: 0.479725, acc: 0.789062] [adversarial loss: 0.947382, acc: 0.203125]\n",
      "1135: [discriminator loss: 0.562382, acc: 0.671875] [adversarial loss: 0.957104, acc: 0.281250]\n",
      "1136: [discriminator loss: 0.532202, acc: 0.757812] [adversarial loss: 1.480078, acc: 0.015625]\n",
      "1137: [discriminator loss: 0.509713, acc: 0.742188] [adversarial loss: 0.607767, acc: 0.640625]\n",
      "1138: [discriminator loss: 0.525091, acc: 0.726562] [adversarial loss: 1.594212, acc: 0.046875]\n",
      "1139: [discriminator loss: 0.557114, acc: 0.679688] [adversarial loss: 0.647676, acc: 0.562500]\n",
      "1140: [discriminator loss: 0.601535, acc: 0.625000] [adversarial loss: 1.671642, acc: 0.031250]\n",
      "1141: [discriminator loss: 0.466048, acc: 0.765625] [adversarial loss: 0.599664, acc: 0.609375]\n",
      "1142: [discriminator loss: 0.487938, acc: 0.781250] [adversarial loss: 0.818547, acc: 0.437500]\n",
      "1143: [discriminator loss: 0.486266, acc: 0.773438] [adversarial loss: 0.665211, acc: 0.640625]\n",
      "1144: [discriminator loss: 0.503013, acc: 0.789062] [adversarial loss: 0.926137, acc: 0.140625]\n",
      "1145: [discriminator loss: 0.521999, acc: 0.765625] [adversarial loss: 0.904517, acc: 0.265625]\n",
      "1146: [discriminator loss: 0.583511, acc: 0.664062] [adversarial loss: 1.910509, acc: 0.000000]\n",
      "1147: [discriminator loss: 0.485243, acc: 0.742188] [adversarial loss: 0.369221, acc: 0.921875]\n",
      "1148: [discriminator loss: 0.342760, acc: 0.875000] [adversarial loss: 0.510145, acc: 0.734375]\n",
      "1149: [discriminator loss: 0.367793, acc: 0.851562] [adversarial loss: 0.684875, acc: 0.562500]\n",
      "1150: [discriminator loss: 0.375206, acc: 0.890625] [adversarial loss: 0.511932, acc: 0.734375]\n",
      "1151: [discriminator loss: 0.427852, acc: 0.796875] [adversarial loss: 1.038015, acc: 0.234375]\n",
      "1152: [discriminator loss: 0.513110, acc: 0.773438] [adversarial loss: 0.479384, acc: 0.843750]\n",
      "1153: [discriminator loss: 0.494592, acc: 0.734375] [adversarial loss: 1.228323, acc: 0.062500]\n",
      "1154: [discriminator loss: 0.594574, acc: 0.664062] [adversarial loss: 0.321579, acc: 0.953125]\n",
      "1155: [discriminator loss: 0.616406, acc: 0.664062] [adversarial loss: 1.724731, acc: 0.000000]\n",
      "1156: [discriminator loss: 0.667588, acc: 0.625000] [adversarial loss: 0.409033, acc: 0.890625]\n",
      "1157: [discriminator loss: 0.644073, acc: 0.585938] [adversarial loss: 1.400335, acc: 0.046875]\n",
      "1158: [discriminator loss: 0.542180, acc: 0.718750] [adversarial loss: 0.858667, acc: 0.265625]\n",
      "1159: [discriminator loss: 0.544281, acc: 0.710938] [adversarial loss: 1.462620, acc: 0.031250]\n",
      "1160: [discriminator loss: 0.433970, acc: 0.781250] [adversarial loss: 0.657240, acc: 0.656250]\n",
      "1161: [discriminator loss: 0.528080, acc: 0.703125] [adversarial loss: 1.376950, acc: 0.140625]\n",
      "1162: [discriminator loss: 0.567056, acc: 0.710938] [adversarial loss: 0.789184, acc: 0.468750]\n",
      "1163: [discriminator loss: 0.522193, acc: 0.742188] [adversarial loss: 0.960537, acc: 0.203125]\n",
      "1164: [discriminator loss: 0.499315, acc: 0.789062] [adversarial loss: 0.707828, acc: 0.546875]\n",
      "1165: [discriminator loss: 0.555879, acc: 0.679688] [adversarial loss: 1.867600, acc: 0.000000]\n",
      "1166: [discriminator loss: 0.601980, acc: 0.648438] [adversarial loss: 0.650836, acc: 0.656250]\n",
      "1167: [discriminator loss: 0.580606, acc: 0.656250] [adversarial loss: 1.682495, acc: 0.000000]\n",
      "1168: [discriminator loss: 0.537066, acc: 0.726562] [adversarial loss: 0.700465, acc: 0.562500]\n",
      "1169: [discriminator loss: 0.531264, acc: 0.742188] [adversarial loss: 1.362965, acc: 0.062500]\n",
      "1170: [discriminator loss: 0.532972, acc: 0.734375] [adversarial loss: 0.906281, acc: 0.328125]\n",
      "1171: [discriminator loss: 0.518922, acc: 0.726562] [adversarial loss: 1.617748, acc: 0.015625]\n",
      "1172: [discriminator loss: 0.534958, acc: 0.687500] [adversarial loss: 0.887499, acc: 0.375000]\n",
      "1173: [discriminator loss: 0.539849, acc: 0.687500] [adversarial loss: 1.572551, acc: 0.015625]\n",
      "1174: [discriminator loss: 0.627801, acc: 0.679688] [adversarial loss: 1.020298, acc: 0.156250]\n",
      "1175: [discriminator loss: 0.509291, acc: 0.750000] [adversarial loss: 1.641962, acc: 0.000000]\n",
      "1176: [discriminator loss: 0.465693, acc: 0.789062] [adversarial loss: 1.057013, acc: 0.125000]\n",
      "1177: [discriminator loss: 0.460622, acc: 0.820312] [adversarial loss: 1.193745, acc: 0.093750]\n",
      "1178: [discriminator loss: 0.400992, acc: 0.875000] [adversarial loss: 1.142826, acc: 0.109375]\n",
      "1179: [discriminator loss: 0.486256, acc: 0.773438] [adversarial loss: 0.968725, acc: 0.265625]\n",
      "1180: [discriminator loss: 0.511796, acc: 0.742188] [adversarial loss: 1.413513, acc: 0.078125]\n",
      "1181: [discriminator loss: 0.503076, acc: 0.789062] [adversarial loss: 1.475551, acc: 0.093750]\n",
      "1182: [discriminator loss: 0.488426, acc: 0.796875] [adversarial loss: 1.024578, acc: 0.218750]\n",
      "1183: [discriminator loss: 0.499848, acc: 0.742188] [adversarial loss: 1.487092, acc: 0.000000]\n",
      "1184: [discriminator loss: 0.469308, acc: 0.796875] [adversarial loss: 0.946580, acc: 0.265625]\n",
      "1185: [discriminator loss: 0.535156, acc: 0.765625] [adversarial loss: 2.198472, acc: 0.000000]\n",
      "1186: [discriminator loss: 0.628341, acc: 0.656250] [adversarial loss: 0.601027, acc: 0.671875]\n",
      "1187: [discriminator loss: 0.566755, acc: 0.679688] [adversarial loss: 1.801162, acc: 0.000000]\n",
      "1188: [discriminator loss: 0.536312, acc: 0.726562] [adversarial loss: 0.819793, acc: 0.421875]\n",
      "1189: [discriminator loss: 0.555314, acc: 0.703125] [adversarial loss: 1.785412, acc: 0.000000]\n",
      "1190: [discriminator loss: 0.624596, acc: 0.648438] [adversarial loss: 0.866461, acc: 0.359375]\n",
      "1191: [discriminator loss: 0.648279, acc: 0.617188] [adversarial loss: 1.468297, acc: 0.031250]\n",
      "1192: [discriminator loss: 0.536139, acc: 0.718750] [adversarial loss: 1.036813, acc: 0.187500]\n",
      "1193: [discriminator loss: 0.533634, acc: 0.710938] [adversarial loss: 1.509623, acc: 0.031250]\n",
      "1194: [discriminator loss: 0.531872, acc: 0.726562] [adversarial loss: 0.958822, acc: 0.296875]\n",
      "1195: [discriminator loss: 0.489207, acc: 0.789062] [adversarial loss: 1.499392, acc: 0.031250]\n",
      "1196: [discriminator loss: 0.505941, acc: 0.695312] [adversarial loss: 0.802045, acc: 0.437500]\n",
      "1197: [discriminator loss: 0.482684, acc: 0.781250] [adversarial loss: 1.065241, acc: 0.203125]\n",
      "1198: [discriminator loss: 0.456680, acc: 0.781250] [adversarial loss: 1.341083, acc: 0.046875]\n",
      "1199: [discriminator loss: 0.478482, acc: 0.796875] [adversarial loss: 1.124266, acc: 0.093750]\n",
      "1200: [discriminator loss: 0.494453, acc: 0.742188] [adversarial loss: 1.481396, acc: 0.031250]\n",
      "1201: [discriminator loss: 0.528109, acc: 0.734375] [adversarial loss: 0.753980, acc: 0.515625]\n",
      "1202: [discriminator loss: 0.522368, acc: 0.750000] [adversarial loss: 1.440758, acc: 0.093750]\n",
      "1203: [discriminator loss: 0.480020, acc: 0.789062] [adversarial loss: 0.549033, acc: 0.734375]\n",
      "1204: [discriminator loss: 0.509178, acc: 0.765625] [adversarial loss: 1.297480, acc: 0.078125]\n",
      "1205: [discriminator loss: 0.494023, acc: 0.734375] [adversarial loss: 0.450041, acc: 0.828125]\n",
      "1206: [discriminator loss: 0.714829, acc: 0.554688] [adversarial loss: 1.934400, acc: 0.000000]\n",
      "1207: [discriminator loss: 0.732809, acc: 0.593750] [adversarial loss: 0.632157, acc: 0.656250]\n",
      "1208: [discriminator loss: 0.572786, acc: 0.632812] [adversarial loss: 1.569458, acc: 0.000000]\n",
      "1209: [discriminator loss: 0.533762, acc: 0.703125] [adversarial loss: 0.949757, acc: 0.234375]\n",
      "1210: [discriminator loss: 0.506068, acc: 0.781250] [adversarial loss: 1.366690, acc: 0.031250]\n",
      "1211: [discriminator loss: 0.443542, acc: 0.835938] [adversarial loss: 1.282379, acc: 0.125000]\n",
      "1212: [discriminator loss: 0.503677, acc: 0.781250] [adversarial loss: 1.241373, acc: 0.125000]\n",
      "1213: [discriminator loss: 0.433411, acc: 0.820312] [adversarial loss: 1.204255, acc: 0.171875]\n",
      "1214: [discriminator loss: 0.456180, acc: 0.820312] [adversarial loss: 1.250633, acc: 0.125000]\n",
      "1215: [discriminator loss: 0.454671, acc: 0.789062] [adversarial loss: 1.086982, acc: 0.203125]\n",
      "1216: [discriminator loss: 0.554109, acc: 0.742188] [adversarial loss: 1.589124, acc: 0.031250]\n",
      "1217: [discriminator loss: 0.488657, acc: 0.804688] [adversarial loss: 0.962658, acc: 0.234375]\n",
      "1218: [discriminator loss: 0.553423, acc: 0.750000] [adversarial loss: 1.673828, acc: 0.046875]\n",
      "1219: [discriminator loss: 0.495374, acc: 0.750000] [adversarial loss: 0.928800, acc: 0.437500]\n",
      "1220: [discriminator loss: 0.479793, acc: 0.781250] [adversarial loss: 1.799128, acc: 0.093750]\n",
      "1221: [discriminator loss: 0.520839, acc: 0.742188] [adversarial loss: 0.716652, acc: 0.531250]\n",
      "1222: [discriminator loss: 0.540599, acc: 0.671875] [adversarial loss: 1.628395, acc: 0.093750]\n",
      "1223: [discriminator loss: 0.477360, acc: 0.812500] [adversarial loss: 1.104269, acc: 0.250000]\n",
      "1224: [discriminator loss: 0.528931, acc: 0.742188] [adversarial loss: 1.640357, acc: 0.031250]\n",
      "1225: [discriminator loss: 0.552968, acc: 0.710938] [adversarial loss: 0.799991, acc: 0.500000]\n",
      "1226: [discriminator loss: 0.509880, acc: 0.718750] [adversarial loss: 2.068850, acc: 0.015625]\n",
      "1227: [discriminator loss: 0.653174, acc: 0.640625] [adversarial loss: 0.663508, acc: 0.593750]\n",
      "1228: [discriminator loss: 0.561141, acc: 0.703125] [adversarial loss: 1.955674, acc: 0.015625]\n",
      "1229: [discriminator loss: 0.551876, acc: 0.695312] [adversarial loss: 0.915337, acc: 0.281250]\n",
      "1230: [discriminator loss: 0.440797, acc: 0.820312] [adversarial loss: 1.227635, acc: 0.125000]\n",
      "1231: [discriminator loss: 0.433020, acc: 0.843750] [adversarial loss: 0.881296, acc: 0.343750]\n",
      "1232: [discriminator loss: 0.480961, acc: 0.804688] [adversarial loss: 1.334152, acc: 0.078125]\n",
      "1233: [discriminator loss: 0.450674, acc: 0.789062] [adversarial loss: 0.874626, acc: 0.343750]\n",
      "1234: [discriminator loss: 0.535722, acc: 0.734375] [adversarial loss: 1.806466, acc: 0.015625]\n",
      "1235: [discriminator loss: 0.517848, acc: 0.765625] [adversarial loss: 0.886916, acc: 0.343750]\n",
      "1236: [discriminator loss: 0.485861, acc: 0.804688] [adversarial loss: 1.449482, acc: 0.078125]\n",
      "1237: [discriminator loss: 0.456826, acc: 0.804688] [adversarial loss: 1.034981, acc: 0.281250]\n",
      "1238: [discriminator loss: 0.496050, acc: 0.726562] [adversarial loss: 1.556095, acc: 0.015625]\n",
      "1239: [discriminator loss: 0.425887, acc: 0.804688] [adversarial loss: 1.223536, acc: 0.109375]\n",
      "1240: [discriminator loss: 0.477096, acc: 0.789062] [adversarial loss: 1.513884, acc: 0.031250]\n",
      "1241: [discriminator loss: 0.547257, acc: 0.742188] [adversarial loss: 0.819018, acc: 0.390625]\n",
      "1242: [discriminator loss: 0.469804, acc: 0.789062] [adversarial loss: 1.996382, acc: 0.015625]\n",
      "1243: [discriminator loss: 0.533859, acc: 0.718750] [adversarial loss: 0.516512, acc: 0.750000]\n",
      "1244: [discriminator loss: 0.569863, acc: 0.687500] [adversarial loss: 2.066908, acc: 0.000000]\n",
      "1245: [discriminator loss: 0.561546, acc: 0.695312] [adversarial loss: 0.822804, acc: 0.453125]\n",
      "1246: [discriminator loss: 0.513499, acc: 0.726562] [adversarial loss: 1.412014, acc: 0.125000]\n",
      "1247: [discriminator loss: 0.489942, acc: 0.742188] [adversarial loss: 0.880604, acc: 0.359375]\n",
      "1248: [discriminator loss: 0.546579, acc: 0.695312] [adversarial loss: 1.684731, acc: 0.015625]\n",
      "1249: [discriminator loss: 0.498086, acc: 0.718750] [adversarial loss: 1.042312, acc: 0.281250]\n",
      "1250: [discriminator loss: 0.514078, acc: 0.742188] [adversarial loss: 0.866603, acc: 0.343750]\n",
      "1251: [discriminator loss: 0.488496, acc: 0.757812] [adversarial loss: 1.660978, acc: 0.046875]\n",
      "1252: [discriminator loss: 0.511985, acc: 0.703125] [adversarial loss: 0.803411, acc: 0.468750]\n",
      "1253: [discriminator loss: 0.517097, acc: 0.710938] [adversarial loss: 1.796773, acc: 0.015625]\n",
      "1254: [discriminator loss: 0.531232, acc: 0.703125] [adversarial loss: 0.687067, acc: 0.609375]\n",
      "1255: [discriminator loss: 0.569949, acc: 0.679688] [adversarial loss: 1.492981, acc: 0.093750]\n",
      "1256: [discriminator loss: 0.581063, acc: 0.742188] [adversarial loss: 0.995831, acc: 0.265625]\n",
      "1257: [discriminator loss: 0.469634, acc: 0.796875] [adversarial loss: 1.215019, acc: 0.093750]\n",
      "1258: [discriminator loss: 0.459854, acc: 0.789062] [adversarial loss: 1.132756, acc: 0.171875]\n",
      "1259: [discriminator loss: 0.492094, acc: 0.742188] [adversarial loss: 1.176380, acc: 0.140625]\n",
      "1260: [discriminator loss: 0.446716, acc: 0.804688] [adversarial loss: 1.231675, acc: 0.156250]\n",
      "1261: [discriminator loss: 0.412387, acc: 0.843750] [adversarial loss: 0.942534, acc: 0.312500]\n",
      "1262: [discriminator loss: 0.452319, acc: 0.796875] [adversarial loss: 1.326805, acc: 0.171875]\n",
      "1263: [discriminator loss: 0.473311, acc: 0.828125] [adversarial loss: 1.356128, acc: 0.109375]\n",
      "1264: [discriminator loss: 0.505130, acc: 0.750000] [adversarial loss: 1.128226, acc: 0.250000]\n",
      "1265: [discriminator loss: 0.481211, acc: 0.773438] [adversarial loss: 1.811136, acc: 0.015625]\n",
      "1266: [discriminator loss: 0.445912, acc: 0.789062] [adversarial loss: 0.560011, acc: 0.703125]\n",
      "1267: [discriminator loss: 0.624952, acc: 0.656250] [adversarial loss: 2.309898, acc: 0.015625]\n",
      "1268: [discriminator loss: 0.708570, acc: 0.609375] [adversarial loss: 0.548138, acc: 0.687500]\n",
      "1269: [discriminator loss: 0.639221, acc: 0.617188] [adversarial loss: 1.606083, acc: 0.000000]\n",
      "1270: [discriminator loss: 0.494796, acc: 0.765625] [adversarial loss: 1.019364, acc: 0.218750]\n",
      "1271: [discriminator loss: 0.439763, acc: 0.835938] [adversarial loss: 1.345410, acc: 0.109375]\n",
      "1272: [discriminator loss: 0.427848, acc: 0.820312] [adversarial loss: 1.076060, acc: 0.156250]\n",
      "1273: [discriminator loss: 0.437522, acc: 0.820312] [adversarial loss: 1.224128, acc: 0.171875]\n",
      "1274: [discriminator loss: 0.431272, acc: 0.820312] [adversarial loss: 1.427709, acc: 0.078125]\n",
      "1275: [discriminator loss: 0.418247, acc: 0.820312] [adversarial loss: 1.292438, acc: 0.125000]\n",
      "1276: [discriminator loss: 0.492087, acc: 0.765625] [adversarial loss: 1.661557, acc: 0.046875]\n",
      "1277: [discriminator loss: 0.491437, acc: 0.750000] [adversarial loss: 0.629224, acc: 0.671875]\n",
      "1278: [discriminator loss: 0.589003, acc: 0.585938] [adversarial loss: 2.225395, acc: 0.015625]\n",
      "1279: [discriminator loss: 0.644397, acc: 0.609375] [adversarial loss: 0.850441, acc: 0.359375]\n",
      "1280: [discriminator loss: 0.458679, acc: 0.789062] [adversarial loss: 1.259087, acc: 0.109375]\n",
      "1281: [discriminator loss: 0.403480, acc: 0.859375] [adversarial loss: 1.069537, acc: 0.265625]\n",
      "1282: [discriminator loss: 0.521281, acc: 0.781250] [adversarial loss: 1.538008, acc: 0.078125]\n",
      "1283: [discriminator loss: 0.484798, acc: 0.781250] [adversarial loss: 1.067596, acc: 0.234375]\n",
      "1284: [discriminator loss: 0.452485, acc: 0.781250] [adversarial loss: 1.640694, acc: 0.046875]\n",
      "1285: [discriminator loss: 0.453145, acc: 0.789062] [adversarial loss: 1.051427, acc: 0.281250]\n",
      "1286: [discriminator loss: 0.420879, acc: 0.835938] [adversarial loss: 1.186277, acc: 0.171875]\n",
      "1287: [discriminator loss: 0.420144, acc: 0.812500] [adversarial loss: 1.208594, acc: 0.234375]\n",
      "1288: [discriminator loss: 0.444294, acc: 0.820312] [adversarial loss: 1.334833, acc: 0.140625]\n",
      "1289: [discriminator loss: 0.449381, acc: 0.812500] [adversarial loss: 1.184216, acc: 0.171875]\n",
      "1290: [discriminator loss: 0.371611, acc: 0.898438] [adversarial loss: 1.015548, acc: 0.250000]\n",
      "1291: [discriminator loss: 0.489383, acc: 0.734375] [adversarial loss: 2.055637, acc: 0.015625]\n",
      "1292: [discriminator loss: 0.474339, acc: 0.734375] [adversarial loss: 0.488063, acc: 0.734375]\n",
      "1293: [discriminator loss: 0.689891, acc: 0.640625] [adversarial loss: 2.194778, acc: 0.000000]\n",
      "1294: [discriminator loss: 0.567740, acc: 0.687500] [adversarial loss: 0.862986, acc: 0.484375]\n",
      "1295: [discriminator loss: 0.499025, acc: 0.750000] [adversarial loss: 1.774380, acc: 0.062500]\n",
      "1296: [discriminator loss: 0.493669, acc: 0.742188] [adversarial loss: 0.950609, acc: 0.296875]\n",
      "1297: [discriminator loss: 0.470479, acc: 0.789062] [adversarial loss: 1.549290, acc: 0.046875]\n",
      "1298: [discriminator loss: 0.500730, acc: 0.726562] [adversarial loss: 0.858920, acc: 0.328125]\n",
      "1299: [discriminator loss: 0.481762, acc: 0.757812] [adversarial loss: 1.638059, acc: 0.031250]\n",
      "1300: [discriminator loss: 0.493157, acc: 0.789062] [adversarial loss: 1.292106, acc: 0.078125]\n",
      "1301: [discriminator loss: 0.387641, acc: 0.828125] [adversarial loss: 1.172975, acc: 0.125000]\n",
      "1302: [discriminator loss: 0.488365, acc: 0.781250] [adversarial loss: 1.746784, acc: 0.078125]\n",
      "1303: [discriminator loss: 0.462240, acc: 0.773438] [adversarial loss: 0.810777, acc: 0.421875]\n",
      "1304: [discriminator loss: 0.540464, acc: 0.656250] [adversarial loss: 2.386725, acc: 0.015625]\n",
      "1305: [discriminator loss: 0.618552, acc: 0.687500] [adversarial loss: 0.871504, acc: 0.359375]\n",
      "1306: [discriminator loss: 0.500082, acc: 0.773438] [adversarial loss: 1.542624, acc: 0.093750]\n",
      "1307: [discriminator loss: 0.434133, acc: 0.820312] [adversarial loss: 0.941441, acc: 0.343750]\n",
      "1308: [discriminator loss: 0.481727, acc: 0.773438] [adversarial loss: 1.866996, acc: 0.031250]\n",
      "1309: [discriminator loss: 0.437689, acc: 0.804688] [adversarial loss: 1.038681, acc: 0.203125]\n",
      "1310: [discriminator loss: 0.371476, acc: 0.867188] [adversarial loss: 1.812832, acc: 0.031250]\n",
      "1311: [discriminator loss: 0.486442, acc: 0.765625] [adversarial loss: 0.922815, acc: 0.406250]\n",
      "1312: [discriminator loss: 0.407970, acc: 0.851562] [adversarial loss: 1.157231, acc: 0.187500]\n",
      "1313: [discriminator loss: 0.383125, acc: 0.867188] [adversarial loss: 0.894682, acc: 0.406250]\n",
      "1314: [discriminator loss: 0.531285, acc: 0.671875] [adversarial loss: 1.889396, acc: 0.046875]\n",
      "1315: [discriminator loss: 0.462590, acc: 0.757812] [adversarial loss: 0.740124, acc: 0.531250]\n",
      "1316: [discriminator loss: 0.590927, acc: 0.632812] [adversarial loss: 2.453945, acc: 0.015625]\n",
      "1317: [discriminator loss: 0.697846, acc: 0.648438] [adversarial loss: 0.679559, acc: 0.562500]\n",
      "1318: [discriminator loss: 0.536016, acc: 0.679688] [adversarial loss: 1.983038, acc: 0.031250]\n",
      "1319: [discriminator loss: 0.520058, acc: 0.750000] [adversarial loss: 0.961333, acc: 0.406250]\n",
      "1320: [discriminator loss: 0.448678, acc: 0.843750] [adversarial loss: 1.237122, acc: 0.140625]\n",
      "1321: [discriminator loss: 0.424524, acc: 0.828125] [adversarial loss: 1.179677, acc: 0.187500]\n",
      "1322: [discriminator loss: 0.464949, acc: 0.773438] [adversarial loss: 1.174683, acc: 0.187500]\n",
      "1323: [discriminator loss: 0.475655, acc: 0.781250] [adversarial loss: 1.382228, acc: 0.109375]\n",
      "1324: [discriminator loss: 0.439186, acc: 0.828125] [adversarial loss: 1.282311, acc: 0.203125]\n",
      "1325: [discriminator loss: 0.479912, acc: 0.804688] [adversarial loss: 1.568655, acc: 0.031250]\n",
      "1326: [discriminator loss: 0.411148, acc: 0.828125] [adversarial loss: 0.783062, acc: 0.500000]\n",
      "1327: [discriminator loss: 0.472851, acc: 0.757812] [adversarial loss: 2.095753, acc: 0.031250]\n",
      "1328: [discriminator loss: 0.575798, acc: 0.750000] [adversarial loss: 0.657889, acc: 0.625000]\n",
      "1329: [discriminator loss: 0.577453, acc: 0.625000] [adversarial loss: 2.202315, acc: 0.015625]\n",
      "1330: [discriminator loss: 0.499250, acc: 0.726562] [adversarial loss: 1.024896, acc: 0.234375]\n",
      "1331: [discriminator loss: 0.425004, acc: 0.812500] [adversarial loss: 1.978426, acc: 0.000000]\n",
      "1332: [discriminator loss: 0.433648, acc: 0.773438] [adversarial loss: 1.120658, acc: 0.250000]\n",
      "1333: [discriminator loss: 0.361875, acc: 0.867188] [adversarial loss: 1.401936, acc: 0.062500]\n",
      "1334: [discriminator loss: 0.413982, acc: 0.796875] [adversarial loss: 1.259478, acc: 0.156250]\n",
      "1335: [discriminator loss: 0.444755, acc: 0.828125] [adversarial loss: 1.493131, acc: 0.031250]\n",
      "1336: [discriminator loss: 0.470867, acc: 0.820312] [adversarial loss: 1.092148, acc: 0.203125]\n",
      "1337: [discriminator loss: 0.457674, acc: 0.750000] [adversarial loss: 1.977923, acc: 0.000000]\n",
      "1338: [discriminator loss: 0.464214, acc: 0.765625] [adversarial loss: 0.995598, acc: 0.375000]\n",
      "1339: [discriminator loss: 0.497521, acc: 0.781250] [adversarial loss: 2.132709, acc: 0.062500]\n",
      "1340: [discriminator loss: 0.516817, acc: 0.718750] [adversarial loss: 0.659783, acc: 0.500000]\n",
      "1341: [discriminator loss: 0.556674, acc: 0.687500] [adversarial loss: 1.881444, acc: 0.015625]\n",
      "1342: [discriminator loss: 0.518286, acc: 0.765625] [adversarial loss: 0.761382, acc: 0.468750]\n",
      "1343: [discriminator loss: 0.505619, acc: 0.718750] [adversarial loss: 2.037625, acc: 0.015625]\n",
      "1344: [discriminator loss: 0.467786, acc: 0.750000] [adversarial loss: 0.934969, acc: 0.312500]\n",
      "1345: [discriminator loss: 0.438881, acc: 0.789062] [adversarial loss: 1.635831, acc: 0.125000]\n",
      "1346: [discriminator loss: 0.429223, acc: 0.789062] [adversarial loss: 1.185673, acc: 0.171875]\n",
      "1347: [discriminator loss: 0.443731, acc: 0.812500] [adversarial loss: 1.517538, acc: 0.062500]\n",
      "1348: [discriminator loss: 0.373585, acc: 0.867188] [adversarial loss: 1.328415, acc: 0.140625]\n",
      "1349: [discriminator loss: 0.498458, acc: 0.765625] [adversarial loss: 1.369042, acc: 0.093750]\n",
      "1350: [discriminator loss: 0.417961, acc: 0.804688] [adversarial loss: 1.142867, acc: 0.187500]\n",
      "1351: [discriminator loss: 0.442864, acc: 0.804688] [adversarial loss: 1.726457, acc: 0.046875]\n",
      "1352: [discriminator loss: 0.400478, acc: 0.820312] [adversarial loss: 0.899263, acc: 0.390625]\n",
      "1353: [discriminator loss: 0.490835, acc: 0.773438] [adversarial loss: 2.039448, acc: 0.015625]\n",
      "1354: [discriminator loss: 0.535278, acc: 0.710938] [adversarial loss: 0.623925, acc: 0.687500]\n",
      "1355: [discriminator loss: 0.590686, acc: 0.664062] [adversarial loss: 2.239555, acc: 0.000000]\n",
      "1356: [discriminator loss: 0.520150, acc: 0.718750] [adversarial loss: 0.729775, acc: 0.625000]\n",
      "1357: [discriminator loss: 0.474592, acc: 0.734375] [adversarial loss: 1.885724, acc: 0.015625]\n",
      "1358: [discriminator loss: 0.514547, acc: 0.734375] [adversarial loss: 0.932143, acc: 0.421875]\n",
      "1359: [discriminator loss: 0.511283, acc: 0.726562] [adversarial loss: 1.917534, acc: 0.000000]\n",
      "1360: [discriminator loss: 0.445619, acc: 0.773438] [adversarial loss: 0.783344, acc: 0.468750]\n",
      "1361: [discriminator loss: 0.551014, acc: 0.703125] [adversarial loss: 1.764523, acc: 0.015625]\n",
      "1362: [discriminator loss: 0.476080, acc: 0.765625] [adversarial loss: 1.039823, acc: 0.218750]\n",
      "1363: [discriminator loss: 0.403011, acc: 0.828125] [adversarial loss: 1.602443, acc: 0.093750]\n",
      "1364: [discriminator loss: 0.454146, acc: 0.757812] [adversarial loss: 1.213707, acc: 0.218750]\n",
      "1365: [discriminator loss: 0.487656, acc: 0.773438] [adversarial loss: 1.592271, acc: 0.046875]\n",
      "1366: [discriminator loss: 0.462152, acc: 0.773438] [adversarial loss: 0.833659, acc: 0.437500]\n",
      "1367: [discriminator loss: 0.451085, acc: 0.812500] [adversarial loss: 1.885922, acc: 0.000000]\n",
      "1368: [discriminator loss: 0.427332, acc: 0.796875] [adversarial loss: 0.989629, acc: 0.328125]\n",
      "1369: [discriminator loss: 0.490590, acc: 0.710938] [adversarial loss: 2.012057, acc: 0.062500]\n",
      "1370: [discriminator loss: 0.474368, acc: 0.789062] [adversarial loss: 0.740283, acc: 0.546875]\n",
      "1371: [discriminator loss: 0.492798, acc: 0.750000] [adversarial loss: 2.254456, acc: 0.046875]\n",
      "1372: [discriminator loss: 0.492811, acc: 0.726562] [adversarial loss: 1.014132, acc: 0.250000]\n",
      "1373: [discriminator loss: 0.480015, acc: 0.789062] [adversarial loss: 2.097807, acc: 0.031250]\n",
      "1374: [discriminator loss: 0.465846, acc: 0.789062] [adversarial loss: 0.821230, acc: 0.421875]\n",
      "1375: [discriminator loss: 0.501111, acc: 0.710938] [adversarial loss: 2.228704, acc: 0.031250]\n",
      "1376: [discriminator loss: 0.510951, acc: 0.718750] [adversarial loss: 0.992379, acc: 0.281250]\n",
      "1377: [discriminator loss: 0.524144, acc: 0.703125] [adversarial loss: 1.932801, acc: 0.000000]\n",
      "1378: [discriminator loss: 0.398204, acc: 0.789062] [adversarial loss: 1.265732, acc: 0.171875]\n",
      "1379: [discriminator loss: 0.392155, acc: 0.835938] [adversarial loss: 1.454587, acc: 0.156250]\n",
      "1380: [discriminator loss: 0.416122, acc: 0.843750] [adversarial loss: 1.161044, acc: 0.296875]\n",
      "1381: [discriminator loss: 0.500128, acc: 0.757812] [adversarial loss: 1.254080, acc: 0.140625]\n",
      "1382: [discriminator loss: 0.425133, acc: 0.820312] [adversarial loss: 1.587991, acc: 0.093750]\n",
      "1383: [discriminator loss: 0.463587, acc: 0.796875] [adversarial loss: 1.042524, acc: 0.265625]\n",
      "1384: [discriminator loss: 0.400246, acc: 0.835938] [adversarial loss: 1.835568, acc: 0.031250]\n",
      "1385: [discriminator loss: 0.452489, acc: 0.804688] [adversarial loss: 1.071696, acc: 0.265625]\n",
      "1386: [discriminator loss: 0.549645, acc: 0.742188] [adversarial loss: 2.223341, acc: 0.000000]\n",
      "1387: [discriminator loss: 0.514160, acc: 0.773438] [adversarial loss: 0.894380, acc: 0.453125]\n",
      "1388: [discriminator loss: 0.493414, acc: 0.742188] [adversarial loss: 2.149688, acc: 0.000000]\n",
      "1389: [discriminator loss: 0.492238, acc: 0.726562] [adversarial loss: 0.758848, acc: 0.468750]\n",
      "1390: [discriminator loss: 0.497384, acc: 0.773438] [adversarial loss: 2.105117, acc: 0.015625]\n",
      "1391: [discriminator loss: 0.437880, acc: 0.734375] [adversarial loss: 1.076084, acc: 0.312500]\n",
      "1392: [discriminator loss: 0.486615, acc: 0.734375] [adversarial loss: 1.878526, acc: 0.062500]\n",
      "1393: [discriminator loss: 0.478815, acc: 0.742188] [adversarial loss: 0.985155, acc: 0.265625]\n",
      "1394: [discriminator loss: 0.444458, acc: 0.796875] [adversarial loss: 1.880761, acc: 0.062500]\n",
      "1395: [discriminator loss: 0.392139, acc: 0.859375] [adversarial loss: 1.103809, acc: 0.281250]\n",
      "1396: [discriminator loss: 0.422586, acc: 0.781250] [adversarial loss: 2.040268, acc: 0.015625]\n",
      "1397: [discriminator loss: 0.449823, acc: 0.726562] [adversarial loss: 0.813241, acc: 0.453125]\n",
      "1398: [discriminator loss: 0.480682, acc: 0.750000] [adversarial loss: 2.018215, acc: 0.062500]\n",
      "1399: [discriminator loss: 0.486545, acc: 0.734375] [adversarial loss: 0.919252, acc: 0.328125]\n",
      "1400: [discriminator loss: 0.502460, acc: 0.750000] [adversarial loss: 1.964138, acc: 0.015625]\n",
      "1401: [discriminator loss: 0.465391, acc: 0.773438] [adversarial loss: 0.865202, acc: 0.453125]\n",
      "1402: [discriminator loss: 0.516182, acc: 0.734375] [adversarial loss: 1.970901, acc: 0.015625]\n",
      "1403: [discriminator loss: 0.540190, acc: 0.679688] [adversarial loss: 0.964318, acc: 0.375000]\n",
      "1404: [discriminator loss: 0.488057, acc: 0.703125] [adversarial loss: 1.862712, acc: 0.062500]\n",
      "1405: [discriminator loss: 0.478325, acc: 0.734375] [adversarial loss: 0.825213, acc: 0.390625]\n",
      "1406: [discriminator loss: 0.496409, acc: 0.734375] [adversarial loss: 2.049612, acc: 0.000000]\n",
      "1407: [discriminator loss: 0.430835, acc: 0.804688] [adversarial loss: 1.140489, acc: 0.187500]\n",
      "1408: [discriminator loss: 0.484895, acc: 0.789062] [adversarial loss: 1.527385, acc: 0.140625]\n",
      "1409: [discriminator loss: 0.368448, acc: 0.851562] [adversarial loss: 1.383158, acc: 0.109375]\n",
      "1410: [discriminator loss: 0.438820, acc: 0.757812] [adversarial loss: 1.359884, acc: 0.125000]\n",
      "1411: [discriminator loss: 0.375523, acc: 0.851562] [adversarial loss: 1.601785, acc: 0.109375]\n",
      "1412: [discriminator loss: 0.405727, acc: 0.828125] [adversarial loss: 1.223613, acc: 0.218750]\n",
      "1413: [discriminator loss: 0.450138, acc: 0.781250] [adversarial loss: 1.773021, acc: 0.046875]\n",
      "1414: [discriminator loss: 0.397822, acc: 0.812500] [adversarial loss: 0.824425, acc: 0.515625]\n",
      "1415: [discriminator loss: 0.529795, acc: 0.742188] [adversarial loss: 2.584115, acc: 0.015625]\n",
      "1416: [discriminator loss: 0.603921, acc: 0.632812] [adversarial loss: 0.744666, acc: 0.531250]\n",
      "1417: [discriminator loss: 0.508439, acc: 0.734375] [adversarial loss: 1.926355, acc: 0.015625]\n",
      "1418: [discriminator loss: 0.479383, acc: 0.757812] [adversarial loss: 1.075365, acc: 0.218750]\n",
      "1419: [discriminator loss: 0.449873, acc: 0.804688] [adversarial loss: 1.797183, acc: 0.062500]\n",
      "1420: [discriminator loss: 0.462022, acc: 0.812500] [adversarial loss: 0.961695, acc: 0.343750]\n",
      "1421: [discriminator loss: 0.448391, acc: 0.828125] [adversarial loss: 2.135198, acc: 0.000000]\n",
      "1422: [discriminator loss: 0.497795, acc: 0.750000] [adversarial loss: 0.921435, acc: 0.375000]\n",
      "1423: [discriminator loss: 0.498670, acc: 0.773438] [adversarial loss: 1.743522, acc: 0.046875]\n",
      "1424: [discriminator loss: 0.473500, acc: 0.742188] [adversarial loss: 1.049404, acc: 0.281250]\n",
      "1425: [discriminator loss: 0.446939, acc: 0.804688] [adversarial loss: 1.713192, acc: 0.062500]\n",
      "1426: [discriminator loss: 0.477483, acc: 0.742188] [adversarial loss: 0.717887, acc: 0.562500]\n",
      "1427: [discriminator loss: 0.519195, acc: 0.695312] [adversarial loss: 2.071274, acc: 0.031250]\n",
      "1428: [discriminator loss: 0.514523, acc: 0.726562] [adversarial loss: 0.923993, acc: 0.406250]\n",
      "1429: [discriminator loss: 0.471178, acc: 0.765625] [adversarial loss: 1.327369, acc: 0.156250]\n",
      "1430: [discriminator loss: 0.419630, acc: 0.789062] [adversarial loss: 1.075928, acc: 0.296875]\n",
      "1431: [discriminator loss: 0.451150, acc: 0.828125] [adversarial loss: 1.848726, acc: 0.015625]\n",
      "1432: [discriminator loss: 0.425102, acc: 0.796875] [adversarial loss: 1.088671, acc: 0.265625]\n",
      "1433: [discriminator loss: 0.347853, acc: 0.914062] [adversarial loss: 1.820129, acc: 0.031250]\n",
      "1434: [discriminator loss: 0.381489, acc: 0.820312] [adversarial loss: 1.222259, acc: 0.250000]\n",
      "1435: [discriminator loss: 0.458567, acc: 0.820312] [adversarial loss: 1.786257, acc: 0.046875]\n",
      "1436: [discriminator loss: 0.421127, acc: 0.773438] [adversarial loss: 1.055813, acc: 0.281250]\n",
      "1437: [discriminator loss: 0.459610, acc: 0.781250] [adversarial loss: 1.952666, acc: 0.015625]\n",
      "1438: [discriminator loss: 0.506325, acc: 0.742188] [adversarial loss: 0.497573, acc: 0.718750]\n",
      "1439: [discriminator loss: 0.608842, acc: 0.656250] [adversarial loss: 2.291931, acc: 0.015625]\n",
      "1440: [discriminator loss: 0.576893, acc: 0.679688] [adversarial loss: 0.946216, acc: 0.250000]\n",
      "1441: [discriminator loss: 0.557551, acc: 0.640625] [adversarial loss: 1.769026, acc: 0.031250]\n",
      "1442: [discriminator loss: 0.424152, acc: 0.796875] [adversarial loss: 1.681299, acc: 0.078125]\n",
      "1443: [discriminator loss: 0.429377, acc: 0.835938] [adversarial loss: 1.209097, acc: 0.125000]\n",
      "1444: [discriminator loss: 0.437150, acc: 0.828125] [adversarial loss: 1.480300, acc: 0.171875]\n",
      "1445: [discriminator loss: 0.407139, acc: 0.851562] [adversarial loss: 1.344483, acc: 0.203125]\n",
      "1446: [discriminator loss: 0.379733, acc: 0.867188] [adversarial loss: 1.610980, acc: 0.046875]\n",
      "1447: [discriminator loss: 0.419124, acc: 0.773438] [adversarial loss: 1.367211, acc: 0.218750]\n",
      "1448: [discriminator loss: 0.426951, acc: 0.789062] [adversarial loss: 1.089865, acc: 0.296875]\n",
      "1449: [discriminator loss: 0.469310, acc: 0.796875] [adversarial loss: 2.073795, acc: 0.015625]\n",
      "1450: [discriminator loss: 0.472005, acc: 0.781250] [adversarial loss: 0.697109, acc: 0.531250]\n",
      "1451: [discriminator loss: 0.576080, acc: 0.671875] [adversarial loss: 2.460069, acc: 0.015625]\n",
      "1452: [discriminator loss: 0.625721, acc: 0.695312] [adversarial loss: 0.915471, acc: 0.453125]\n",
      "1453: [discriminator loss: 0.488156, acc: 0.765625] [adversarial loss: 1.662863, acc: 0.015625]\n",
      "1454: [discriminator loss: 0.395176, acc: 0.835938] [adversarial loss: 1.139189, acc: 0.234375]\n",
      "1455: [discriminator loss: 0.380257, acc: 0.875000] [adversarial loss: 1.796167, acc: 0.031250]\n",
      "1456: [discriminator loss: 0.428592, acc: 0.765625] [adversarial loss: 1.039020, acc: 0.296875]\n",
      "1457: [discriminator loss: 0.504410, acc: 0.796875] [adversarial loss: 2.043751, acc: 0.031250]\n",
      "1458: [discriminator loss: 0.468115, acc: 0.796875] [adversarial loss: 1.044173, acc: 0.281250]\n",
      "1459: [discriminator loss: 0.444015, acc: 0.804688] [adversarial loss: 1.836392, acc: 0.031250]\n",
      "1460: [discriminator loss: 0.404488, acc: 0.804688] [adversarial loss: 1.132896, acc: 0.265625]\n",
      "1461: [discriminator loss: 0.404815, acc: 0.820312] [adversarial loss: 1.510056, acc: 0.171875]\n",
      "1462: [discriminator loss: 0.416598, acc: 0.820312] [adversarial loss: 1.030357, acc: 0.296875]\n",
      "1463: [discriminator loss: 0.457744, acc: 0.765625] [adversarial loss: 1.776947, acc: 0.109375]\n",
      "1464: [discriminator loss: 0.459064, acc: 0.781250] [adversarial loss: 0.910187, acc: 0.437500]\n",
      "1465: [discriminator loss: 0.498054, acc: 0.773438] [adversarial loss: 2.320704, acc: 0.000000]\n",
      "1466: [discriminator loss: 0.555932, acc: 0.687500] [adversarial loss: 0.716840, acc: 0.578125]\n",
      "1467: [discriminator loss: 0.557649, acc: 0.656250] [adversarial loss: 2.076455, acc: 0.046875]\n",
      "1468: [discriminator loss: 0.642408, acc: 0.664062] [adversarial loss: 0.822746, acc: 0.437500]\n",
      "1469: [discriminator loss: 0.456585, acc: 0.734375] [adversarial loss: 1.766634, acc: 0.062500]\n",
      "1470: [discriminator loss: 0.452450, acc: 0.750000] [adversarial loss: 1.195229, acc: 0.187500]\n",
      "1471: [discriminator loss: 0.468950, acc: 0.828125] [adversarial loss: 1.363531, acc: 0.125000]\n",
      "1472: [discriminator loss: 0.447811, acc: 0.789062] [adversarial loss: 1.136376, acc: 0.218750]\n",
      "1473: [discriminator loss: 0.442342, acc: 0.796875] [adversarial loss: 1.601011, acc: 0.109375]\n",
      "1474: [discriminator loss: 0.419564, acc: 0.820312] [adversarial loss: 1.373959, acc: 0.140625]\n",
      "1475: [discriminator loss: 0.429457, acc: 0.828125] [adversarial loss: 1.489920, acc: 0.140625]\n",
      "1476: [discriminator loss: 0.453378, acc: 0.773438] [adversarial loss: 1.090313, acc: 0.328125]\n",
      "1477: [discriminator loss: 0.467760, acc: 0.796875] [adversarial loss: 1.363973, acc: 0.078125]\n",
      "1478: [discriminator loss: 0.455771, acc: 0.773438] [adversarial loss: 1.254397, acc: 0.156250]\n",
      "1479: [discriminator loss: 0.469906, acc: 0.781250] [adversarial loss: 1.443744, acc: 0.140625]\n",
      "1480: [discriminator loss: 0.454061, acc: 0.804688] [adversarial loss: 1.068253, acc: 0.281250]\n",
      "1481: [discriminator loss: 0.417594, acc: 0.828125] [adversarial loss: 1.811913, acc: 0.000000]\n",
      "1482: [discriminator loss: 0.480935, acc: 0.757812] [adversarial loss: 0.907138, acc: 0.375000]\n",
      "1483: [discriminator loss: 0.470402, acc: 0.804688] [adversarial loss: 2.116910, acc: 0.031250]\n",
      "1484: [discriminator loss: 0.521553, acc: 0.734375] [adversarial loss: 0.668453, acc: 0.609375]\n",
      "1485: [discriminator loss: 0.614393, acc: 0.671875] [adversarial loss: 2.429226, acc: 0.062500]\n",
      "1486: [discriminator loss: 0.648419, acc: 0.671875] [adversarial loss: 0.845102, acc: 0.468750]\n",
      "1487: [discriminator loss: 0.515113, acc: 0.726562] [adversarial loss: 1.226302, acc: 0.187500]\n",
      "1488: [discriminator loss: 0.423501, acc: 0.804688] [adversarial loss: 1.306880, acc: 0.171875]\n",
      "1489: [discriminator loss: 0.416031, acc: 0.835938] [adversarial loss: 1.176731, acc: 0.187500]\n",
      "1490: [discriminator loss: 0.402170, acc: 0.851562] [adversarial loss: 1.321696, acc: 0.140625]\n",
      "1491: [discriminator loss: 0.512782, acc: 0.765625] [adversarial loss: 1.446586, acc: 0.125000]\n",
      "1492: [discriminator loss: 0.450956, acc: 0.796875] [adversarial loss: 1.172468, acc: 0.140625]\n",
      "1493: [discriminator loss: 0.413344, acc: 0.789062] [adversarial loss: 1.244635, acc: 0.171875]\n",
      "1494: [discriminator loss: 0.484914, acc: 0.726562] [adversarial loss: 1.036678, acc: 0.359375]\n",
      "1495: [discriminator loss: 0.434456, acc: 0.789062] [adversarial loss: 1.953371, acc: 0.046875]\n",
      "1496: [discriminator loss: 0.544189, acc: 0.750000] [adversarial loss: 0.588523, acc: 0.671875]\n",
      "1497: [discriminator loss: 0.515935, acc: 0.679688] [adversarial loss: 2.382751, acc: 0.000000]\n",
      "1498: [discriminator loss: 0.607845, acc: 0.734375] [adversarial loss: 0.934225, acc: 0.312500]\n",
      "1499: [discriminator loss: 0.449646, acc: 0.796875] [adversarial loss: 1.301696, acc: 0.093750]\n",
      "1500: [discriminator loss: 0.396515, acc: 0.820312] [adversarial loss: 0.972752, acc: 0.296875]\n",
      "1501: [discriminator loss: 0.545309, acc: 0.695312] [adversarial loss: 1.899319, acc: 0.078125]\n",
      "1502: [discriminator loss: 0.561191, acc: 0.703125] [adversarial loss: 0.860853, acc: 0.375000]\n",
      "1503: [discriminator loss: 0.438205, acc: 0.804688] [adversarial loss: 1.556587, acc: 0.093750]\n",
      "1504: [discriminator loss: 0.511822, acc: 0.734375] [adversarial loss: 1.058018, acc: 0.234375]\n",
      "1505: [discriminator loss: 0.503004, acc: 0.757812] [adversarial loss: 1.806995, acc: 0.046875]\n",
      "1506: [discriminator loss: 0.559363, acc: 0.703125] [adversarial loss: 0.952060, acc: 0.375000]\n",
      "1507: [discriminator loss: 0.479445, acc: 0.765625] [adversarial loss: 1.725074, acc: 0.031250]\n",
      "1508: [discriminator loss: 0.506006, acc: 0.734375] [adversarial loss: 0.771253, acc: 0.406250]\n",
      "1509: [discriminator loss: 0.551392, acc: 0.695312] [adversarial loss: 1.848742, acc: 0.031250]\n",
      "1510: [discriminator loss: 0.538221, acc: 0.710938] [adversarial loss: 0.907477, acc: 0.390625]\n",
      "1511: [discriminator loss: 0.465728, acc: 0.742188] [adversarial loss: 1.520313, acc: 0.062500]\n",
      "1512: [discriminator loss: 0.405940, acc: 0.828125] [adversarial loss: 1.437285, acc: 0.062500]\n",
      "1513: [discriminator loss: 0.504138, acc: 0.742188] [adversarial loss: 1.293032, acc: 0.187500]\n",
      "1514: [discriminator loss: 0.525631, acc: 0.710938] [adversarial loss: 0.980057, acc: 0.265625]\n",
      "1515: [discriminator loss: 0.445459, acc: 0.820312] [adversarial loss: 1.643996, acc: 0.093750]\n",
      "1516: [discriminator loss: 0.492235, acc: 0.742188] [adversarial loss: 1.128134, acc: 0.234375]\n",
      "1517: [discriminator loss: 0.443476, acc: 0.812500] [adversarial loss: 1.565888, acc: 0.078125]\n",
      "1518: [discriminator loss: 0.421615, acc: 0.820312] [adversarial loss: 0.881982, acc: 0.375000]\n",
      "1519: [discriminator loss: 0.441095, acc: 0.789062] [adversarial loss: 2.139351, acc: 0.046875]\n",
      "1520: [discriminator loss: 0.564434, acc: 0.726562] [adversarial loss: 0.812946, acc: 0.437500]\n",
      "1521: [discriminator loss: 0.505673, acc: 0.750000] [adversarial loss: 1.798820, acc: 0.015625]\n",
      "1522: [discriminator loss: 0.547801, acc: 0.734375] [adversarial loss: 0.923584, acc: 0.406250]\n",
      "1523: [discriminator loss: 0.405300, acc: 0.843750] [adversarial loss: 1.469653, acc: 0.125000]\n",
      "1524: [discriminator loss: 0.417712, acc: 0.859375] [adversarial loss: 1.292239, acc: 0.171875]\n",
      "1525: [discriminator loss: 0.355495, acc: 0.882812] [adversarial loss: 1.428201, acc: 0.187500]\n",
      "1526: [discriminator loss: 0.512467, acc: 0.734375] [adversarial loss: 0.999861, acc: 0.312500]\n",
      "1527: [discriminator loss: 0.405430, acc: 0.812500] [adversarial loss: 1.514763, acc: 0.062500]\n",
      "1528: [discriminator loss: 0.516685, acc: 0.710938] [adversarial loss: 0.802790, acc: 0.515625]\n",
      "1529: [discriminator loss: 0.540364, acc: 0.695312] [adversarial loss: 2.292501, acc: 0.000000]\n",
      "1530: [discriminator loss: 0.592340, acc: 0.648438] [adversarial loss: 0.734589, acc: 0.609375]\n",
      "1531: [discriminator loss: 0.532393, acc: 0.726562] [adversarial loss: 2.228011, acc: 0.062500]\n",
      "1532: [discriminator loss: 0.620473, acc: 0.671875] [adversarial loss: 0.889534, acc: 0.406250]\n",
      "1533: [discriminator loss: 0.480691, acc: 0.757812] [adversarial loss: 1.386781, acc: 0.140625]\n",
      "1534: [discriminator loss: 0.432736, acc: 0.796875] [adversarial loss: 1.008876, acc: 0.265625]\n",
      "1535: [discriminator loss: 0.474488, acc: 0.789062] [adversarial loss: 1.413225, acc: 0.078125]\n",
      "1536: [discriminator loss: 0.445231, acc: 0.804688] [adversarial loss: 1.266462, acc: 0.171875]\n",
      "1537: [discriminator loss: 0.463280, acc: 0.773438] [adversarial loss: 1.177757, acc: 0.234375]\n",
      "1538: [discriminator loss: 0.455620, acc: 0.750000] [adversarial loss: 1.727546, acc: 0.093750]\n",
      "1539: [discriminator loss: 0.536210, acc: 0.742188] [adversarial loss: 0.790877, acc: 0.515625]\n",
      "1540: [discriminator loss: 0.508201, acc: 0.742188] [adversarial loss: 1.936648, acc: 0.031250]\n",
      "1541: [discriminator loss: 0.511846, acc: 0.718750] [adversarial loss: 0.930932, acc: 0.328125]\n",
      "1542: [discriminator loss: 0.494476, acc: 0.734375] [adversarial loss: 1.604435, acc: 0.062500]\n",
      "1543: [discriminator loss: 0.503053, acc: 0.757812] [adversarial loss: 1.152419, acc: 0.265625]\n",
      "1544: [discriminator loss: 0.524899, acc: 0.773438] [adversarial loss: 1.487662, acc: 0.093750]\n",
      "1545: [discriminator loss: 0.533079, acc: 0.726562] [adversarial loss: 0.816515, acc: 0.468750]\n",
      "1546: [discriminator loss: 0.497932, acc: 0.750000] [adversarial loss: 1.722691, acc: 0.062500]\n",
      "1547: [discriminator loss: 0.515197, acc: 0.718750] [adversarial loss: 1.025916, acc: 0.265625]\n",
      "1548: [discriminator loss: 0.479417, acc: 0.750000] [adversarial loss: 1.532725, acc: 0.156250]\n",
      "1549: [discriminator loss: 0.562056, acc: 0.765625] [adversarial loss: 0.737163, acc: 0.531250]\n",
      "1550: [discriminator loss: 0.553060, acc: 0.703125] [adversarial loss: 1.842782, acc: 0.046875]\n",
      "1551: [discriminator loss: 0.579726, acc: 0.671875] [adversarial loss: 0.712334, acc: 0.546875]\n",
      "1552: [discriminator loss: 0.431840, acc: 0.789062] [adversarial loss: 1.623944, acc: 0.046875]\n",
      "1553: [discriminator loss: 0.477781, acc: 0.757812] [adversarial loss: 0.981631, acc: 0.328125]\n",
      "1554: [discriminator loss: 0.536091, acc: 0.742188] [adversarial loss: 1.709742, acc: 0.031250]\n",
      "1555: [discriminator loss: 0.450573, acc: 0.789062] [adversarial loss: 1.019792, acc: 0.312500]\n",
      "1556: [discriminator loss: 0.499167, acc: 0.796875] [adversarial loss: 1.722203, acc: 0.062500]\n",
      "1557: [discriminator loss: 0.541130, acc: 0.734375] [adversarial loss: 0.800246, acc: 0.406250]\n",
      "1558: [discriminator loss: 0.502947, acc: 0.773438] [adversarial loss: 1.912420, acc: 0.046875]\n",
      "1559: [discriminator loss: 0.498618, acc: 0.726562] [adversarial loss: 0.888752, acc: 0.359375]\n",
      "1560: [discriminator loss: 0.540208, acc: 0.781250] [adversarial loss: 1.572969, acc: 0.062500]\n",
      "1561: [discriminator loss: 0.467961, acc: 0.781250] [adversarial loss: 1.238110, acc: 0.234375]\n",
      "1562: [discriminator loss: 0.423816, acc: 0.835938] [adversarial loss: 1.269216, acc: 0.203125]\n",
      "1563: [discriminator loss: 0.481118, acc: 0.765625] [adversarial loss: 0.887318, acc: 0.437500]\n",
      "1564: [discriminator loss: 0.434238, acc: 0.812500] [adversarial loss: 1.618681, acc: 0.031250]\n",
      "1565: [discriminator loss: 0.513848, acc: 0.750000] [adversarial loss: 1.025612, acc: 0.234375]\n",
      "1566: [discriminator loss: 0.471472, acc: 0.804688] [adversarial loss: 1.777565, acc: 0.062500]\n",
      "1567: [discriminator loss: 0.519496, acc: 0.781250] [adversarial loss: 0.936782, acc: 0.296875]\n",
      "1568: [discriminator loss: 0.509804, acc: 0.742188] [adversarial loss: 1.619102, acc: 0.046875]\n",
      "1569: [discriminator loss: 0.591614, acc: 0.656250] [adversarial loss: 0.618262, acc: 0.625000]\n",
      "1570: [discriminator loss: 0.571924, acc: 0.679688] [adversarial loss: 2.021212, acc: 0.046875]\n",
      "1571: [discriminator loss: 0.644867, acc: 0.609375] [adversarial loss: 0.934181, acc: 0.421875]\n",
      "1572: [discriminator loss: 0.544519, acc: 0.750000] [adversarial loss: 1.661528, acc: 0.031250]\n",
      "1573: [discriminator loss: 0.547909, acc: 0.750000] [adversarial loss: 0.880879, acc: 0.328125]\n",
      "1574: [discriminator loss: 0.528618, acc: 0.718750] [adversarial loss: 1.423377, acc: 0.156250]\n",
      "1575: [discriminator loss: 0.507830, acc: 0.773438] [adversarial loss: 0.964785, acc: 0.328125]\n",
      "1576: [discriminator loss: 0.478806, acc: 0.835938] [adversarial loss: 1.212910, acc: 0.234375]\n",
      "1577: [discriminator loss: 0.498875, acc: 0.781250] [adversarial loss: 1.213250, acc: 0.250000]\n",
      "1578: [discriminator loss: 0.520503, acc: 0.726562] [adversarial loss: 1.414813, acc: 0.109375]\n",
      "1579: [discriminator loss: 0.455269, acc: 0.781250] [adversarial loss: 1.065249, acc: 0.296875]\n",
      "1580: [discriminator loss: 0.522343, acc: 0.718750] [adversarial loss: 1.753609, acc: 0.015625]\n",
      "1581: [discriminator loss: 0.620170, acc: 0.648438] [adversarial loss: 0.800372, acc: 0.484375]\n",
      "1582: [discriminator loss: 0.565378, acc: 0.664062] [adversarial loss: 1.413479, acc: 0.125000]\n",
      "1583: [discriminator loss: 0.568172, acc: 0.734375] [adversarial loss: 0.849477, acc: 0.421875]\n",
      "1584: [discriminator loss: 0.508941, acc: 0.734375] [adversarial loss: 2.081514, acc: 0.015625]\n",
      "1585: [discriminator loss: 0.599654, acc: 0.671875] [adversarial loss: 0.749416, acc: 0.578125]\n",
      "1586: [discriminator loss: 0.577253, acc: 0.718750] [adversarial loss: 1.781388, acc: 0.031250]\n",
      "1587: [discriminator loss: 0.539096, acc: 0.703125] [adversarial loss: 0.826662, acc: 0.421875]\n",
      "1588: [discriminator loss: 0.556190, acc: 0.718750] [adversarial loss: 1.580423, acc: 0.078125]\n",
      "1589: [discriminator loss: 0.475713, acc: 0.781250] [adversarial loss: 0.980137, acc: 0.281250]\n",
      "1590: [discriminator loss: 0.474054, acc: 0.750000] [adversarial loss: 1.290690, acc: 0.203125]\n",
      "1591: [discriminator loss: 0.505940, acc: 0.765625] [adversarial loss: 0.924071, acc: 0.312500]\n",
      "1592: [discriminator loss: 0.480743, acc: 0.757812] [adversarial loss: 1.635421, acc: 0.062500]\n",
      "1593: [discriminator loss: 0.563307, acc: 0.664062] [adversarial loss: 0.879784, acc: 0.437500]\n",
      "1594: [discriminator loss: 0.629592, acc: 0.640625] [adversarial loss: 1.572976, acc: 0.109375]\n",
      "1595: [discriminator loss: 0.591003, acc: 0.773438] [adversarial loss: 0.875934, acc: 0.390625]\n",
      "1596: [discriminator loss: 0.474124, acc: 0.789062] [adversarial loss: 1.429148, acc: 0.125000]\n",
      "1597: [discriminator loss: 0.421991, acc: 0.843750] [adversarial loss: 1.238377, acc: 0.171875]\n",
      "1598: [discriminator loss: 0.455689, acc: 0.804688] [adversarial loss: 1.410395, acc: 0.125000]\n",
      "1599: [discriminator loss: 0.489390, acc: 0.773438] [adversarial loss: 0.904835, acc: 0.375000]\n",
      "1600: [discriminator loss: 0.523884, acc: 0.742188] [adversarial loss: 1.695165, acc: 0.031250]\n",
      "1601: [discriminator loss: 0.575706, acc: 0.679688] [adversarial loss: 0.760359, acc: 0.453125]\n",
      "1602: [discriminator loss: 0.587232, acc: 0.687500] [adversarial loss: 1.915759, acc: 0.062500]\n",
      "1603: [discriminator loss: 0.569126, acc: 0.726562] [adversarial loss: 0.938621, acc: 0.312500]\n",
      "1604: [discriminator loss: 0.483779, acc: 0.773438] [adversarial loss: 1.464236, acc: 0.109375]\n",
      "1605: [discriminator loss: 0.452828, acc: 0.773438] [adversarial loss: 1.147670, acc: 0.218750]\n",
      "1606: [discriminator loss: 0.524062, acc: 0.710938] [adversarial loss: 1.563359, acc: 0.078125]\n",
      "1607: [discriminator loss: 0.505879, acc: 0.757812] [adversarial loss: 0.888459, acc: 0.265625]\n",
      "1608: [discriminator loss: 0.562274, acc: 0.679688] [adversarial loss: 2.077930, acc: 0.046875]\n",
      "1609: [discriminator loss: 0.561615, acc: 0.679688] [adversarial loss: 0.841219, acc: 0.390625]\n",
      "1610: [discriminator loss: 0.463462, acc: 0.796875] [adversarial loss: 1.541298, acc: 0.125000]\n",
      "1611: [discriminator loss: 0.578864, acc: 0.687500] [adversarial loss: 0.938741, acc: 0.343750]\n",
      "1612: [discriminator loss: 0.545407, acc: 0.671875] [adversarial loss: 1.332557, acc: 0.140625]\n",
      "1613: [discriminator loss: 0.504036, acc: 0.750000] [adversarial loss: 1.069955, acc: 0.234375]\n",
      "1614: [discriminator loss: 0.525934, acc: 0.750000] [adversarial loss: 1.259418, acc: 0.140625]\n",
      "1615: [discriminator loss: 0.521280, acc: 0.789062] [adversarial loss: 1.040620, acc: 0.250000]\n",
      "1616: [discriminator loss: 0.432875, acc: 0.835938] [adversarial loss: 1.376527, acc: 0.125000]\n",
      "1617: [discriminator loss: 0.574255, acc: 0.671875] [adversarial loss: 0.832647, acc: 0.390625]\n",
      "1618: [discriminator loss: 0.571692, acc: 0.679688] [adversarial loss: 2.043872, acc: 0.046875]\n",
      "1619: [discriminator loss: 0.610337, acc: 0.687500] [adversarial loss: 0.730205, acc: 0.484375]\n",
      "1620: [discriminator loss: 0.560799, acc: 0.718750] [adversarial loss: 1.647668, acc: 0.046875]\n",
      "1621: [discriminator loss: 0.618188, acc: 0.632812] [adversarial loss: 0.830442, acc: 0.390625]\n",
      "1622: [discriminator loss: 0.531095, acc: 0.742188] [adversarial loss: 1.510747, acc: 0.156250]\n",
      "1623: [discriminator loss: 0.531397, acc: 0.757812] [adversarial loss: 0.962268, acc: 0.343750]\n",
      "1624: [discriminator loss: 0.498931, acc: 0.734375] [adversarial loss: 1.478356, acc: 0.109375]\n",
      "1625: [discriminator loss: 0.528886, acc: 0.734375] [adversarial loss: 1.105983, acc: 0.203125]\n",
      "1626: [discriminator loss: 0.483986, acc: 0.773438] [adversarial loss: 1.474309, acc: 0.078125]\n",
      "1627: [discriminator loss: 0.563605, acc: 0.726562] [adversarial loss: 1.300781, acc: 0.171875]\n",
      "1628: [discriminator loss: 0.501276, acc: 0.757812] [adversarial loss: 1.459484, acc: 0.031250]\n",
      "1629: [discriminator loss: 0.456435, acc: 0.781250] [adversarial loss: 0.952259, acc: 0.312500]\n",
      "1630: [discriminator loss: 0.506439, acc: 0.718750] [adversarial loss: 1.489735, acc: 0.031250]\n",
      "1631: [discriminator loss: 0.487260, acc: 0.750000] [adversarial loss: 1.007269, acc: 0.328125]\n",
      "1632: [discriminator loss: 0.470180, acc: 0.765625] [adversarial loss: 1.533206, acc: 0.109375]\n",
      "1633: [discriminator loss: 0.458230, acc: 0.781250] [adversarial loss: 0.919153, acc: 0.281250]\n",
      "1634: [discriminator loss: 0.578718, acc: 0.695312] [adversarial loss: 1.928726, acc: 0.000000]\n",
      "1635: [discriminator loss: 0.588871, acc: 0.687500] [adversarial loss: 0.742458, acc: 0.593750]\n",
      "1636: [discriminator loss: 0.565245, acc: 0.679688] [adversarial loss: 1.831582, acc: 0.046875]\n",
      "1637: [discriminator loss: 0.636418, acc: 0.664062] [adversarial loss: 0.856266, acc: 0.406250]\n",
      "1638: [discriminator loss: 0.523958, acc: 0.703125] [adversarial loss: 1.541014, acc: 0.078125]\n",
      "1639: [discriminator loss: 0.560567, acc: 0.703125] [adversarial loss: 0.771241, acc: 0.468750]\n",
      "1640: [discriminator loss: 0.452389, acc: 0.804688] [adversarial loss: 1.323141, acc: 0.171875]\n",
      "1641: [discriminator loss: 0.492293, acc: 0.750000] [adversarial loss: 1.034254, acc: 0.281250]\n",
      "1642: [discriminator loss: 0.465206, acc: 0.789062] [adversarial loss: 1.319470, acc: 0.109375]\n",
      "1643: [discriminator loss: 0.558245, acc: 0.679688] [adversarial loss: 1.124282, acc: 0.187500]\n",
      "1644: [discriminator loss: 0.487857, acc: 0.781250] [adversarial loss: 1.493478, acc: 0.109375]\n",
      "1645: [discriminator loss: 0.520603, acc: 0.757812] [adversarial loss: 0.705523, acc: 0.625000]\n",
      "1646: [discriminator loss: 0.562450, acc: 0.734375] [adversarial loss: 1.980264, acc: 0.031250]\n",
      "1647: [discriminator loss: 0.518501, acc: 0.789062] [adversarial loss: 0.943592, acc: 0.390625]\n",
      "1648: [discriminator loss: 0.552879, acc: 0.710938] [adversarial loss: 1.574817, acc: 0.078125]\n",
      "1649: [discriminator loss: 0.665755, acc: 0.632812] [adversarial loss: 0.679155, acc: 0.546875]\n",
      "1650: [discriminator loss: 0.512668, acc: 0.726562] [adversarial loss: 1.656068, acc: 0.031250]\n",
      "1651: [discriminator loss: 0.547141, acc: 0.734375] [adversarial loss: 0.768892, acc: 0.421875]\n",
      "1652: [discriminator loss: 0.455789, acc: 0.843750] [adversarial loss: 1.390637, acc: 0.140625]\n",
      "1653: [discriminator loss: 0.498966, acc: 0.789062] [adversarial loss: 0.968738, acc: 0.281250]\n",
      "1654: [discriminator loss: 0.486549, acc: 0.750000] [adversarial loss: 1.601827, acc: 0.046875]\n",
      "1655: [discriminator loss: 0.527004, acc: 0.765625] [adversarial loss: 1.060896, acc: 0.250000]\n",
      "1656: [discriminator loss: 0.559967, acc: 0.679688] [adversarial loss: 1.214286, acc: 0.187500]\n",
      "1657: [discriminator loss: 0.525831, acc: 0.750000] [adversarial loss: 1.160311, acc: 0.125000]\n",
      "1658: [discriminator loss: 0.445732, acc: 0.835938] [adversarial loss: 1.236615, acc: 0.171875]\n",
      "1659: [discriminator loss: 0.454114, acc: 0.820312] [adversarial loss: 1.368577, acc: 0.078125]\n",
      "1660: [discriminator loss: 0.530478, acc: 0.750000] [adversarial loss: 1.112394, acc: 0.234375]\n",
      "1661: [discriminator loss: 0.427560, acc: 0.843750] [adversarial loss: 1.520528, acc: 0.109375]\n",
      "1662: [discriminator loss: 0.546447, acc: 0.726562] [adversarial loss: 1.061725, acc: 0.328125]\n",
      "1663: [discriminator loss: 0.568518, acc: 0.718750] [adversarial loss: 1.838650, acc: 0.031250]\n",
      "1664: [discriminator loss: 0.552635, acc: 0.710938] [adversarial loss: 0.800986, acc: 0.437500]\n",
      "1665: [discriminator loss: 0.510338, acc: 0.718750] [adversarial loss: 1.994232, acc: 0.046875]\n",
      "1666: [discriminator loss: 0.650923, acc: 0.648438] [adversarial loss: 0.908250, acc: 0.343750]\n",
      "1667: [discriminator loss: 0.533050, acc: 0.726562] [adversarial loss: 1.624165, acc: 0.046875]\n",
      "1668: [discriminator loss: 0.493929, acc: 0.757812] [adversarial loss: 0.949730, acc: 0.343750]\n",
      "1669: [discriminator loss: 0.437523, acc: 0.804688] [adversarial loss: 1.605715, acc: 0.093750]\n",
      "1670: [discriminator loss: 0.567478, acc: 0.695312] [adversarial loss: 0.963338, acc: 0.265625]\n",
      "1671: [discriminator loss: 0.499301, acc: 0.734375] [adversarial loss: 1.658092, acc: 0.062500]\n",
      "1672: [discriminator loss: 0.512073, acc: 0.710938] [adversarial loss: 0.954102, acc: 0.359375]\n",
      "1673: [discriminator loss: 0.458512, acc: 0.804688] [adversarial loss: 1.572589, acc: 0.078125]\n",
      "1674: [discriminator loss: 0.501491, acc: 0.765625] [adversarial loss: 1.069730, acc: 0.281250]\n",
      "1675: [discriminator loss: 0.443085, acc: 0.843750] [adversarial loss: 1.385012, acc: 0.109375]\n",
      "1676: [discriminator loss: 0.563886, acc: 0.695312] [adversarial loss: 0.896441, acc: 0.515625]\n",
      "1677: [discriminator loss: 0.492415, acc: 0.718750] [adversarial loss: 1.531713, acc: 0.078125]\n",
      "1678: [discriminator loss: 0.493367, acc: 0.710938] [adversarial loss: 1.015055, acc: 0.375000]\n",
      "1679: [discriminator loss: 0.481012, acc: 0.773438] [adversarial loss: 1.751877, acc: 0.062500]\n",
      "1680: [discriminator loss: 0.577724, acc: 0.671875] [adversarial loss: 0.719509, acc: 0.531250]\n",
      "1681: [discriminator loss: 0.562634, acc: 0.695312] [adversarial loss: 1.897834, acc: 0.000000]\n",
      "1682: [discriminator loss: 0.491537, acc: 0.734375] [adversarial loss: 1.028621, acc: 0.250000]\n",
      "1683: [discriminator loss: 0.524481, acc: 0.820312] [adversarial loss: 1.347480, acc: 0.093750]\n",
      "1684: [discriminator loss: 0.442825, acc: 0.820312] [adversarial loss: 1.057066, acc: 0.250000]\n",
      "1685: [discriminator loss: 0.425783, acc: 0.828125] [adversarial loss: 1.519400, acc: 0.062500]\n",
      "1686: [discriminator loss: 0.454270, acc: 0.804688] [adversarial loss: 0.915856, acc: 0.328125]\n",
      "1687: [discriminator loss: 0.591368, acc: 0.656250] [adversarial loss: 2.249485, acc: 0.031250]\n",
      "1688: [discriminator loss: 0.566405, acc: 0.671875] [adversarial loss: 0.908796, acc: 0.390625]\n",
      "1689: [discriminator loss: 0.563601, acc: 0.687500] [adversarial loss: 1.578510, acc: 0.078125]\n",
      "1690: [discriminator loss: 0.501119, acc: 0.773438] [adversarial loss: 0.964085, acc: 0.312500]\n",
      "1691: [discriminator loss: 0.511676, acc: 0.734375] [adversarial loss: 1.798524, acc: 0.062500]\n",
      "1692: [discriminator loss: 0.495364, acc: 0.765625] [adversarial loss: 0.738485, acc: 0.515625]\n",
      "1693: [discriminator loss: 0.544360, acc: 0.710938] [adversarial loss: 1.784265, acc: 0.046875]\n",
      "1694: [discriminator loss: 0.585954, acc: 0.695312] [adversarial loss: 0.811523, acc: 0.484375]\n",
      "1695: [discriminator loss: 0.553384, acc: 0.695312] [adversarial loss: 1.555920, acc: 0.109375]\n",
      "1696: [discriminator loss: 0.457824, acc: 0.789062] [adversarial loss: 0.885424, acc: 0.421875]\n",
      "1697: [discriminator loss: 0.501314, acc: 0.742188] [adversarial loss: 1.422784, acc: 0.140625]\n",
      "1698: [discriminator loss: 0.466153, acc: 0.812500] [adversarial loss: 1.067044, acc: 0.234375]\n",
      "1699: [discriminator loss: 0.440136, acc: 0.859375] [adversarial loss: 1.572564, acc: 0.078125]\n",
      "1700: [discriminator loss: 0.504706, acc: 0.757812] [adversarial loss: 0.959892, acc: 0.296875]\n",
      "1701: [discriminator loss: 0.446160, acc: 0.789062] [adversarial loss: 1.651619, acc: 0.046875]\n",
      "1702: [discriminator loss: 0.532129, acc: 0.679688] [adversarial loss: 0.786829, acc: 0.484375]\n",
      "1703: [discriminator loss: 0.505674, acc: 0.750000] [adversarial loss: 2.011127, acc: 0.031250]\n",
      "1704: [discriminator loss: 0.509645, acc: 0.757812] [adversarial loss: 0.881683, acc: 0.421875]\n",
      "1705: [discriminator loss: 0.496301, acc: 0.789062] [adversarial loss: 1.516396, acc: 0.093750]\n",
      "1706: [discriminator loss: 0.544186, acc: 0.703125] [adversarial loss: 0.753980, acc: 0.437500]\n",
      "1707: [discriminator loss: 0.549639, acc: 0.710938] [adversarial loss: 1.502403, acc: 0.156250]\n",
      "1708: [discriminator loss: 0.533506, acc: 0.718750] [adversarial loss: 0.825271, acc: 0.453125]\n",
      "1709: [discriminator loss: 0.521763, acc: 0.726562] [adversarial loss: 1.759914, acc: 0.031250]\n",
      "1710: [discriminator loss: 0.550378, acc: 0.718750] [adversarial loss: 0.632314, acc: 0.656250]\n",
      "1711: [discriminator loss: 0.634279, acc: 0.617188] [adversarial loss: 2.070497, acc: 0.015625]\n",
      "1712: [discriminator loss: 0.534495, acc: 0.734375] [adversarial loss: 1.037974, acc: 0.296875]\n",
      "1713: [discriminator loss: 0.455160, acc: 0.843750] [adversarial loss: 1.550725, acc: 0.140625]\n",
      "1714: [discriminator loss: 0.394201, acc: 0.843750] [adversarial loss: 1.087637, acc: 0.312500]\n",
      "1715: [discriminator loss: 0.452410, acc: 0.781250] [adversarial loss: 1.133402, acc: 0.171875]\n",
      "1716: [discriminator loss: 0.447601, acc: 0.789062] [adversarial loss: 1.470277, acc: 0.140625]\n",
      "1717: [discriminator loss: 0.430763, acc: 0.843750] [adversarial loss: 0.889378, acc: 0.406250]\n",
      "1718: [discriminator loss: 0.473509, acc: 0.765625] [adversarial loss: 1.917080, acc: 0.062500]\n",
      "1719: [discriminator loss: 0.563002, acc: 0.679688] [adversarial loss: 0.823096, acc: 0.421875]\n",
      "1720: [discriminator loss: 0.506595, acc: 0.765625] [adversarial loss: 1.608962, acc: 0.062500]\n",
      "1721: [discriminator loss: 0.487234, acc: 0.757812] [adversarial loss: 0.993849, acc: 0.359375]\n",
      "1722: [discriminator loss: 0.498024, acc: 0.750000] [adversarial loss: 1.541220, acc: 0.078125]\n",
      "1723: [discriminator loss: 0.490570, acc: 0.765625] [adversarial loss: 0.936715, acc: 0.343750]\n",
      "1724: [discriminator loss: 0.473425, acc: 0.781250] [adversarial loss: 1.745728, acc: 0.109375]\n",
      "1725: [discriminator loss: 0.559191, acc: 0.671875] [adversarial loss: 0.773697, acc: 0.453125]\n",
      "1726: [discriminator loss: 0.500487, acc: 0.703125] [adversarial loss: 1.742349, acc: 0.109375]\n",
      "1727: [discriminator loss: 0.560780, acc: 0.726562] [adversarial loss: 0.764932, acc: 0.484375]\n",
      "1728: [discriminator loss: 0.501119, acc: 0.765625] [adversarial loss: 1.576049, acc: 0.093750]\n",
      "1729: [discriminator loss: 0.471958, acc: 0.796875] [adversarial loss: 1.001019, acc: 0.250000]\n",
      "1730: [discriminator loss: 0.461425, acc: 0.781250] [adversarial loss: 1.675511, acc: 0.062500]\n",
      "1731: [discriminator loss: 0.481635, acc: 0.757812] [adversarial loss: 1.034253, acc: 0.375000]\n",
      "1732: [discriminator loss: 0.483738, acc: 0.734375] [adversarial loss: 1.605531, acc: 0.093750]\n",
      "1733: [discriminator loss: 0.480052, acc: 0.781250] [adversarial loss: 1.201754, acc: 0.234375]\n",
      "1734: [discriminator loss: 0.434343, acc: 0.804688] [adversarial loss: 1.262552, acc: 0.234375]\n",
      "1735: [discriminator loss: 0.479699, acc: 0.781250] [adversarial loss: 1.222550, acc: 0.156250]\n",
      "1736: [discriminator loss: 0.475642, acc: 0.773438] [adversarial loss: 1.246161, acc: 0.203125]\n",
      "1737: [discriminator loss: 0.447872, acc: 0.796875] [adversarial loss: 0.824934, acc: 0.468750]\n",
      "1738: [discriminator loss: 0.503168, acc: 0.750000] [adversarial loss: 1.922585, acc: 0.078125]\n",
      "1739: [discriminator loss: 0.535858, acc: 0.695312] [adversarial loss: 0.575716, acc: 0.718750]\n",
      "1740: [discriminator loss: 0.606899, acc: 0.632812] [adversarial loss: 1.988887, acc: 0.062500]\n",
      "1741: [discriminator loss: 0.665577, acc: 0.671875] [adversarial loss: 0.938799, acc: 0.343750]\n",
      "1742: [discriminator loss: 0.525294, acc: 0.718750] [adversarial loss: 1.622684, acc: 0.109375]\n",
      "1743: [discriminator loss: 0.534709, acc: 0.687500] [adversarial loss: 1.067089, acc: 0.328125]\n",
      "1744: [discriminator loss: 0.444898, acc: 0.820312] [adversarial loss: 1.621283, acc: 0.046875]\n",
      "1745: [discriminator loss: 0.477319, acc: 0.750000] [adversarial loss: 0.810289, acc: 0.453125]\n",
      "1746: [discriminator loss: 0.481589, acc: 0.734375] [adversarial loss: 1.464065, acc: 0.156250]\n",
      "1747: [discriminator loss: 0.476786, acc: 0.773438] [adversarial loss: 1.241129, acc: 0.203125]\n",
      "1748: [discriminator loss: 0.510077, acc: 0.750000] [adversarial loss: 1.449017, acc: 0.156250]\n",
      "1749: [discriminator loss: 0.454775, acc: 0.796875] [adversarial loss: 1.131790, acc: 0.281250]\n",
      "1750: [discriminator loss: 0.447260, acc: 0.828125] [adversarial loss: 1.492809, acc: 0.140625]\n",
      "1751: [discriminator loss: 0.505868, acc: 0.726562] [adversarial loss: 0.808305, acc: 0.484375]\n",
      "1752: [discriminator loss: 0.546305, acc: 0.757812] [adversarial loss: 1.779658, acc: 0.046875]\n",
      "1753: [discriminator loss: 0.538334, acc: 0.710938] [adversarial loss: 0.739361, acc: 0.531250]\n",
      "1754: [discriminator loss: 0.609923, acc: 0.617188] [adversarial loss: 1.800717, acc: 0.062500]\n",
      "1755: [discriminator loss: 0.484577, acc: 0.773438] [adversarial loss: 0.871299, acc: 0.453125]\n",
      "1756: [discriminator loss: 0.529581, acc: 0.757812] [adversarial loss: 1.586116, acc: 0.140625]\n",
      "1757: [discriminator loss: 0.521388, acc: 0.750000] [adversarial loss: 1.123926, acc: 0.234375]\n",
      "1758: [discriminator loss: 0.454854, acc: 0.796875] [adversarial loss: 1.378872, acc: 0.156250]\n",
      "1759: [discriminator loss: 0.563420, acc: 0.695312] [adversarial loss: 1.315819, acc: 0.156250]\n",
      "1760: [discriminator loss: 0.471023, acc: 0.820312] [adversarial loss: 1.330198, acc: 0.140625]\n",
      "1761: [discriminator loss: 0.475259, acc: 0.781250] [adversarial loss: 1.136798, acc: 0.250000]\n",
      "1762: [discriminator loss: 0.470262, acc: 0.804688] [adversarial loss: 1.406663, acc: 0.109375]\n",
      "1763: [discriminator loss: 0.486716, acc: 0.757812] [adversarial loss: 1.298496, acc: 0.140625]\n",
      "1764: [discriminator loss: 0.489195, acc: 0.773438] [adversarial loss: 1.556636, acc: 0.078125]\n",
      "1765: [discriminator loss: 0.428088, acc: 0.796875] [adversarial loss: 1.024712, acc: 0.281250]\n",
      "1766: [discriminator loss: 0.610051, acc: 0.640625] [adversarial loss: 1.727622, acc: 0.093750]\n",
      "1767: [discriminator loss: 0.497941, acc: 0.726562] [adversarial loss: 0.541426, acc: 0.718750]\n",
      "1768: [discriminator loss: 0.689877, acc: 0.601562] [adversarial loss: 1.893070, acc: 0.046875]\n",
      "1769: [discriminator loss: 0.647946, acc: 0.671875] [adversarial loss: 0.948262, acc: 0.406250]\n",
      "1770: [discriminator loss: 0.594382, acc: 0.671875] [adversarial loss: 1.716967, acc: 0.093750]\n",
      "1771: [discriminator loss: 0.488066, acc: 0.812500] [adversarial loss: 1.096760, acc: 0.296875]\n",
      "1772: [discriminator loss: 0.464576, acc: 0.812500] [adversarial loss: 1.440287, acc: 0.125000]\n",
      "1773: [discriminator loss: 0.493262, acc: 0.757812] [adversarial loss: 1.183472, acc: 0.218750]\n",
      "1774: [discriminator loss: 0.508021, acc: 0.757812] [adversarial loss: 1.515478, acc: 0.093750]\n",
      "1775: [discriminator loss: 0.406282, acc: 0.835938] [adversarial loss: 0.890827, acc: 0.484375]\n",
      "1776: [discriminator loss: 0.592904, acc: 0.648438] [adversarial loss: 2.045904, acc: 0.046875]\n",
      "1777: [discriminator loss: 0.526777, acc: 0.734375] [adversarial loss: 0.670377, acc: 0.500000]\n",
      "1778: [discriminator loss: 0.625907, acc: 0.640625] [adversarial loss: 1.987076, acc: 0.078125]\n",
      "1779: [discriminator loss: 0.598319, acc: 0.695312] [adversarial loss: 0.767909, acc: 0.546875]\n",
      "1780: [discriminator loss: 0.506302, acc: 0.757812] [adversarial loss: 1.733647, acc: 0.078125]\n",
      "1781: [discriminator loss: 0.513318, acc: 0.773438] [adversarial loss: 0.825084, acc: 0.390625]\n",
      "1782: [discriminator loss: 0.518426, acc: 0.734375] [adversarial loss: 1.337616, acc: 0.203125]\n",
      "1783: [discriminator loss: 0.473841, acc: 0.796875] [adversarial loss: 1.071030, acc: 0.234375]\n",
      "1784: [discriminator loss: 0.494302, acc: 0.773438] [adversarial loss: 0.875899, acc: 0.375000]\n",
      "1785: [discriminator loss: 0.504924, acc: 0.742188] [adversarial loss: 1.215918, acc: 0.265625]\n",
      "1786: [discriminator loss: 0.531367, acc: 0.757812] [adversarial loss: 1.206242, acc: 0.156250]\n",
      "1787: [discriminator loss: 0.513520, acc: 0.789062] [adversarial loss: 1.049010, acc: 0.234375]\n",
      "1788: [discriminator loss: 0.436515, acc: 0.828125] [adversarial loss: 1.585583, acc: 0.078125]\n",
      "1789: [discriminator loss: 0.421215, acc: 0.835938] [adversarial loss: 0.979458, acc: 0.359375]\n",
      "1790: [discriminator loss: 0.519466, acc: 0.773438] [adversarial loss: 1.611057, acc: 0.093750]\n",
      "1791: [discriminator loss: 0.488177, acc: 0.773438] [adversarial loss: 0.754338, acc: 0.578125]\n",
      "1792: [discriminator loss: 0.565742, acc: 0.679688] [adversarial loss: 1.850996, acc: 0.062500]\n",
      "1793: [discriminator loss: 0.578481, acc: 0.695312] [adversarial loss: 0.723362, acc: 0.593750]\n",
      "1794: [discriminator loss: 0.541255, acc: 0.695312] [adversarial loss: 1.914553, acc: 0.000000]\n",
      "1795: [discriminator loss: 0.536647, acc: 0.679688] [adversarial loss: 0.998951, acc: 0.390625]\n",
      "1796: [discriminator loss: 0.452372, acc: 0.820312] [adversarial loss: 1.302861, acc: 0.125000]\n",
      "1797: [discriminator loss: 0.514058, acc: 0.757812] [adversarial loss: 1.390525, acc: 0.171875]\n",
      "1798: [discriminator loss: 0.487838, acc: 0.789062] [adversarial loss: 0.933911, acc: 0.437500]\n",
      "1799: [discriminator loss: 0.494616, acc: 0.742188] [adversarial loss: 1.184947, acc: 0.218750]\n",
      "1800: [discriminator loss: 0.490868, acc: 0.789062] [adversarial loss: 0.955179, acc: 0.359375]\n",
      "1801: [discriminator loss: 0.500183, acc: 0.781250] [adversarial loss: 1.560039, acc: 0.062500]\n",
      "1802: [discriminator loss: 0.483264, acc: 0.765625] [adversarial loss: 1.226521, acc: 0.140625]\n",
      "1803: [discriminator loss: 0.545954, acc: 0.726562] [adversarial loss: 1.491281, acc: 0.140625]\n",
      "1804: [discriminator loss: 0.547951, acc: 0.734375] [adversarial loss: 1.234830, acc: 0.281250]\n",
      "1805: [discriminator loss: 0.424574, acc: 0.859375] [adversarial loss: 1.344111, acc: 0.156250]\n",
      "1806: [discriminator loss: 0.423729, acc: 0.796875] [adversarial loss: 1.276100, acc: 0.171875]\n",
      "1807: [discriminator loss: 0.444885, acc: 0.796875] [adversarial loss: 1.206102, acc: 0.203125]\n",
      "1808: [discriminator loss: 0.498291, acc: 0.789062] [adversarial loss: 1.642550, acc: 0.078125]\n",
      "1809: [discriminator loss: 0.501504, acc: 0.765625] [adversarial loss: 0.789965, acc: 0.484375]\n",
      "1810: [discriminator loss: 0.537845, acc: 0.734375] [adversarial loss: 1.794767, acc: 0.078125]\n",
      "1811: [discriminator loss: 0.549586, acc: 0.671875] [adversarial loss: 0.673275, acc: 0.625000]\n",
      "1812: [discriminator loss: 0.547992, acc: 0.710938] [adversarial loss: 2.162709, acc: 0.031250]\n",
      "1813: [discriminator loss: 0.521535, acc: 0.726562] [adversarial loss: 0.874332, acc: 0.406250]\n",
      "1814: [discriminator loss: 0.545109, acc: 0.726562] [adversarial loss: 1.670250, acc: 0.031250]\n",
      "1815: [discriminator loss: 0.580512, acc: 0.718750] [adversarial loss: 1.051684, acc: 0.250000]\n",
      "1816: [discriminator loss: 0.534768, acc: 0.742188] [adversarial loss: 1.672734, acc: 0.062500]\n",
      "1817: [discriminator loss: 0.515210, acc: 0.757812] [adversarial loss: 0.942528, acc: 0.343750]\n",
      "1818: [discriminator loss: 0.564905, acc: 0.687500] [adversarial loss: 1.504354, acc: 0.125000]\n",
      "1819: [discriminator loss: 0.509809, acc: 0.765625] [adversarial loss: 0.951986, acc: 0.375000]\n",
      "1820: [discriminator loss: 0.515071, acc: 0.718750] [adversarial loss: 1.862814, acc: 0.046875]\n",
      "1821: [discriminator loss: 0.594832, acc: 0.671875] [adversarial loss: 0.715769, acc: 0.578125]\n",
      "1822: [discriminator loss: 0.566125, acc: 0.703125] [adversarial loss: 1.558291, acc: 0.078125]\n",
      "1823: [discriminator loss: 0.504873, acc: 0.734375] [adversarial loss: 0.896445, acc: 0.406250]\n",
      "1824: [discriminator loss: 0.515414, acc: 0.734375] [adversarial loss: 1.773797, acc: 0.046875]\n",
      "1825: [discriminator loss: 0.519496, acc: 0.687500] [adversarial loss: 1.001765, acc: 0.312500]\n",
      "1826: [discriminator loss: 0.476958, acc: 0.773438] [adversarial loss: 1.588521, acc: 0.046875]\n",
      "1827: [discriminator loss: 0.496228, acc: 0.765625] [adversarial loss: 1.233823, acc: 0.171875]\n",
      "1828: [discriminator loss: 0.442663, acc: 0.796875] [adversarial loss: 1.110152, acc: 0.281250]\n",
      "1829: [discriminator loss: 0.461170, acc: 0.804688] [adversarial loss: 1.235254, acc: 0.171875]\n",
      "1830: [discriminator loss: 0.498756, acc: 0.734375] [adversarial loss: 1.417036, acc: 0.140625]\n",
      "1831: [discriminator loss: 0.515838, acc: 0.757812] [adversarial loss: 0.908411, acc: 0.453125]\n",
      "1832: [discriminator loss: 0.504801, acc: 0.710938] [adversarial loss: 1.963178, acc: 0.046875]\n",
      "1833: [discriminator loss: 0.615488, acc: 0.656250] [adversarial loss: 0.614176, acc: 0.625000]\n",
      "1834: [discriminator loss: 0.539253, acc: 0.687500] [adversarial loss: 1.916666, acc: 0.015625]\n",
      "1835: [discriminator loss: 0.605367, acc: 0.687500] [adversarial loss: 0.750569, acc: 0.515625]\n",
      "1836: [discriminator loss: 0.514710, acc: 0.742188] [adversarial loss: 1.378622, acc: 0.187500]\n",
      "1837: [discriminator loss: 0.469068, acc: 0.812500] [adversarial loss: 1.205788, acc: 0.265625]\n",
      "1838: [discriminator loss: 0.439137, acc: 0.820312] [adversarial loss: 1.130863, acc: 0.265625]\n",
      "1839: [discriminator loss: 0.502002, acc: 0.804688] [adversarial loss: 1.093920, acc: 0.281250]\n",
      "1840: [discriminator loss: 0.452079, acc: 0.781250] [adversarial loss: 1.175541, acc: 0.156250]\n",
      "1841: [discriminator loss: 0.449535, acc: 0.804688] [adversarial loss: 1.557257, acc: 0.187500]\n",
      "1842: [discriminator loss: 0.531542, acc: 0.750000] [adversarial loss: 0.926213, acc: 0.375000]\n",
      "1843: [discriminator loss: 0.440599, acc: 0.757812] [adversarial loss: 1.885005, acc: 0.031250]\n",
      "1844: [discriminator loss: 0.488159, acc: 0.773438] [adversarial loss: 0.720727, acc: 0.515625]\n",
      "1845: [discriminator loss: 0.528721, acc: 0.703125] [adversarial loss: 2.004397, acc: 0.046875]\n",
      "1846: [discriminator loss: 0.569430, acc: 0.687500] [adversarial loss: 0.725618, acc: 0.531250]\n",
      "1847: [discriminator loss: 0.564860, acc: 0.687500] [adversarial loss: 1.694453, acc: 0.046875]\n",
      "1848: [discriminator loss: 0.449832, acc: 0.765625] [adversarial loss: 1.123281, acc: 0.281250]\n",
      "1849: [discriminator loss: 0.524746, acc: 0.695312] [adversarial loss: 1.346640, acc: 0.140625]\n",
      "1850: [discriminator loss: 0.503758, acc: 0.726562] [adversarial loss: 0.915937, acc: 0.468750]\n",
      "1851: [discriminator loss: 0.534681, acc: 0.734375] [adversarial loss: 1.847203, acc: 0.093750]\n",
      "1852: [discriminator loss: 0.507082, acc: 0.726562] [adversarial loss: 0.800533, acc: 0.453125]\n",
      "1853: [discriminator loss: 0.456811, acc: 0.789062] [adversarial loss: 1.538910, acc: 0.109375]\n",
      "1854: [discriminator loss: 0.543773, acc: 0.750000] [adversarial loss: 0.829375, acc: 0.421875]\n",
      "1855: [discriminator loss: 0.495974, acc: 0.734375] [adversarial loss: 1.272052, acc: 0.125000]\n",
      "1856: [discriminator loss: 0.394230, acc: 0.851562] [adversarial loss: 1.141688, acc: 0.312500]\n",
      "1857: [discriminator loss: 0.444610, acc: 0.804688] [adversarial loss: 1.328218, acc: 0.203125]\n",
      "1858: [discriminator loss: 0.554849, acc: 0.765625] [adversarial loss: 1.915653, acc: 0.062500]\n",
      "1859: [discriminator loss: 0.574177, acc: 0.710938] [adversarial loss: 0.651717, acc: 0.656250]\n",
      "1860: [discriminator loss: 0.629078, acc: 0.625000] [adversarial loss: 2.156175, acc: 0.046875]\n",
      "1861: [discriminator loss: 0.638184, acc: 0.648438] [adversarial loss: 0.670606, acc: 0.640625]\n",
      "1862: [discriminator loss: 0.523407, acc: 0.718750] [adversarial loss: 1.670148, acc: 0.078125]\n",
      "1863: [discriminator loss: 0.524007, acc: 0.726562] [adversarial loss: 0.939375, acc: 0.328125]\n",
      "1864: [discriminator loss: 0.441298, acc: 0.828125] [adversarial loss: 1.510171, acc: 0.062500]\n",
      "1865: [discriminator loss: 0.496695, acc: 0.789062] [adversarial loss: 1.041911, acc: 0.296875]\n",
      "1866: [discriminator loss: 0.493618, acc: 0.765625] [adversarial loss: 1.458023, acc: 0.015625]\n",
      "1867: [discriminator loss: 0.471303, acc: 0.773438] [adversarial loss: 1.123220, acc: 0.203125]\n",
      "1868: [discriminator loss: 0.424264, acc: 0.835938] [adversarial loss: 1.305781, acc: 0.109375]\n",
      "1869: [discriminator loss: 0.483386, acc: 0.773438] [adversarial loss: 1.171419, acc: 0.234375]\n",
      "1870: [discriminator loss: 0.509679, acc: 0.773438] [adversarial loss: 1.410041, acc: 0.140625]\n",
      "1871: [discriminator loss: 0.444679, acc: 0.804688] [adversarial loss: 0.808569, acc: 0.515625]\n",
      "1872: [discriminator loss: 0.505762, acc: 0.765625] [adversarial loss: 1.376427, acc: 0.140625]\n",
      "1873: [discriminator loss: 0.540021, acc: 0.718750] [adversarial loss: 0.756991, acc: 0.531250]\n",
      "1874: [discriminator loss: 0.572414, acc: 0.703125] [adversarial loss: 1.682443, acc: 0.046875]\n",
      "1875: [discriminator loss: 0.542241, acc: 0.695312] [adversarial loss: 0.893952, acc: 0.406250]\n",
      "1876: [discriminator loss: 0.494450, acc: 0.789062] [adversarial loss: 1.437272, acc: 0.156250]\n",
      "1877: [discriminator loss: 0.495161, acc: 0.757812] [adversarial loss: 1.262865, acc: 0.140625]\n",
      "1878: [discriminator loss: 0.465117, acc: 0.796875] [adversarial loss: 1.255351, acc: 0.171875]\n",
      "1879: [discriminator loss: 0.485693, acc: 0.750000] [adversarial loss: 1.196589, acc: 0.218750]\n",
      "1880: [discriminator loss: 0.426983, acc: 0.859375] [adversarial loss: 1.114232, acc: 0.296875]\n",
      "1881: [discriminator loss: 0.456270, acc: 0.789062] [adversarial loss: 1.491809, acc: 0.093750]\n",
      "1882: [discriminator loss: 0.460796, acc: 0.781250] [adversarial loss: 0.937354, acc: 0.375000]\n",
      "1883: [discriminator loss: 0.506276, acc: 0.789062] [adversarial loss: 1.517067, acc: 0.125000]\n",
      "1884: [discriminator loss: 0.478714, acc: 0.804688] [adversarial loss: 0.781993, acc: 0.546875]\n",
      "1885: [discriminator loss: 0.604029, acc: 0.671875] [adversarial loss: 2.277999, acc: 0.015625]\n",
      "1886: [discriminator loss: 0.695711, acc: 0.632812] [adversarial loss: 0.706372, acc: 0.578125]\n",
      "1887: [discriminator loss: 0.594715, acc: 0.664062] [adversarial loss: 1.853077, acc: 0.109375]\n",
      "1888: [discriminator loss: 0.581167, acc: 0.703125] [adversarial loss: 0.778346, acc: 0.484375]\n",
      "1889: [discriminator loss: 0.599774, acc: 0.679688] [adversarial loss: 1.239660, acc: 0.187500]\n",
      "1890: [discriminator loss: 0.477065, acc: 0.765625] [adversarial loss: 1.510627, acc: 0.078125]\n",
      "1891: [discriminator loss: 0.541556, acc: 0.687500] [adversarial loss: 0.921939, acc: 0.375000]\n",
      "1892: [discriminator loss: 0.551278, acc: 0.750000] [adversarial loss: 1.542709, acc: 0.125000]\n",
      "1893: [discriminator loss: 0.459897, acc: 0.796875] [adversarial loss: 1.097584, acc: 0.265625]\n",
      "1894: [discriminator loss: 0.608603, acc: 0.640625] [adversarial loss: 1.543603, acc: 0.093750]\n",
      "1895: [discriminator loss: 0.481054, acc: 0.773438] [adversarial loss: 1.014502, acc: 0.328125]\n",
      "1896: [discriminator loss: 0.503837, acc: 0.726562] [adversarial loss: 1.339692, acc: 0.218750]\n",
      "1897: [discriminator loss: 0.536184, acc: 0.687500] [adversarial loss: 1.474433, acc: 0.125000]\n",
      "1898: [discriminator loss: 0.447631, acc: 0.773438] [adversarial loss: 1.222624, acc: 0.281250]\n",
      "1899: [discriminator loss: 0.451040, acc: 0.789062] [adversarial loss: 1.242270, acc: 0.203125]\n",
      "1900: [discriminator loss: 0.447559, acc: 0.789062] [adversarial loss: 1.762703, acc: 0.093750]\n",
      "1901: [discriminator loss: 0.507453, acc: 0.757812] [adversarial loss: 0.795190, acc: 0.453125]\n",
      "1902: [discriminator loss: 0.567298, acc: 0.710938] [adversarial loss: 1.696261, acc: 0.062500]\n",
      "1903: [discriminator loss: 0.596004, acc: 0.687500] [adversarial loss: 0.905648, acc: 0.390625]\n",
      "1904: [discriminator loss: 0.540568, acc: 0.695312] [adversarial loss: 1.594570, acc: 0.093750]\n",
      "1905: [discriminator loss: 0.510068, acc: 0.796875] [adversarial loss: 1.025486, acc: 0.296875]\n",
      "1906: [discriminator loss: 0.493040, acc: 0.757812] [adversarial loss: 1.337103, acc: 0.109375]\n",
      "1907: [discriminator loss: 0.499638, acc: 0.843750] [adversarial loss: 1.381716, acc: 0.078125]\n",
      "1908: [discriminator loss: 0.476521, acc: 0.804688] [adversarial loss: 1.229902, acc: 0.250000]\n",
      "1909: [discriminator loss: 0.459254, acc: 0.773438] [adversarial loss: 1.024472, acc: 0.359375]\n",
      "1910: [discriminator loss: 0.468053, acc: 0.789062] [adversarial loss: 1.686475, acc: 0.046875]\n",
      "1911: [discriminator loss: 0.549035, acc: 0.687500] [adversarial loss: 0.959528, acc: 0.359375]\n",
      "1912: [discriminator loss: 0.466429, acc: 0.781250] [adversarial loss: 1.651955, acc: 0.125000]\n",
      "1913: [discriminator loss: 0.488843, acc: 0.742188] [adversarial loss: 0.937018, acc: 0.390625]\n",
      "1914: [discriminator loss: 0.466650, acc: 0.765625] [adversarial loss: 1.775066, acc: 0.046875]\n",
      "1915: [discriminator loss: 0.530567, acc: 0.726562] [adversarial loss: 1.064735, acc: 0.281250]\n",
      "1916: [discriminator loss: 0.501630, acc: 0.796875] [adversarial loss: 1.653394, acc: 0.125000]\n",
      "1917: [discriminator loss: 0.472867, acc: 0.796875] [adversarial loss: 1.063107, acc: 0.312500]\n",
      "1918: [discriminator loss: 0.562187, acc: 0.726562] [adversarial loss: 1.574667, acc: 0.125000]\n",
      "1919: [discriminator loss: 0.479144, acc: 0.789062] [adversarial loss: 0.884492, acc: 0.406250]\n",
      "1920: [discriminator loss: 0.472164, acc: 0.796875] [adversarial loss: 1.484363, acc: 0.109375]\n",
      "1921: [discriminator loss: 0.529349, acc: 0.718750] [adversarial loss: 0.765791, acc: 0.562500]\n",
      "1922: [discriminator loss: 0.507827, acc: 0.695312] [adversarial loss: 1.971957, acc: 0.046875]\n",
      "1923: [discriminator loss: 0.583699, acc: 0.750000] [adversarial loss: 0.925018, acc: 0.375000]\n",
      "1924: [discriminator loss: 0.582871, acc: 0.664062] [adversarial loss: 1.668139, acc: 0.093750]\n",
      "1925: [discriminator loss: 0.529819, acc: 0.757812] [adversarial loss: 1.028115, acc: 0.296875]\n",
      "1926: [discriminator loss: 0.490343, acc: 0.742188] [adversarial loss: 1.585247, acc: 0.109375]\n",
      "1927: [discriminator loss: 0.415440, acc: 0.851562] [adversarial loss: 1.033142, acc: 0.328125]\n",
      "1928: [discriminator loss: 0.541920, acc: 0.734375] [adversarial loss: 1.569600, acc: 0.093750]\n",
      "1929: [discriminator loss: 0.527600, acc: 0.734375] [adversarial loss: 1.065602, acc: 0.281250]\n",
      "1930: [discriminator loss: 0.521874, acc: 0.726562] [adversarial loss: 1.723516, acc: 0.078125]\n",
      "1931: [discriminator loss: 0.556173, acc: 0.726562] [adversarial loss: 0.806849, acc: 0.484375]\n",
      "1932: [discriminator loss: 0.491665, acc: 0.695312] [adversarial loss: 1.799511, acc: 0.062500]\n",
      "1933: [discriminator loss: 0.489867, acc: 0.789062] [adversarial loss: 1.147024, acc: 0.265625]\n",
      "1934: [discriminator loss: 0.538226, acc: 0.750000] [adversarial loss: 1.557111, acc: 0.109375]\n",
      "1935: [discriminator loss: 0.499654, acc: 0.742188] [adversarial loss: 1.005440, acc: 0.343750]\n",
      "1936: [discriminator loss: 0.495126, acc: 0.750000] [adversarial loss: 1.764875, acc: 0.062500]\n",
      "1937: [discriminator loss: 0.486118, acc: 0.742188] [adversarial loss: 0.980127, acc: 0.328125]\n",
      "1938: [discriminator loss: 0.499701, acc: 0.750000] [adversarial loss: 1.203060, acc: 0.281250]\n",
      "1939: [discriminator loss: 0.471920, acc: 0.796875] [adversarial loss: 1.297657, acc: 0.187500]\n",
      "1940: [discriminator loss: 0.541273, acc: 0.718750] [adversarial loss: 1.200577, acc: 0.234375]\n",
      "1941: [discriminator loss: 0.536319, acc: 0.750000] [adversarial loss: 1.598675, acc: 0.109375]\n",
      "1942: [discriminator loss: 0.492316, acc: 0.773438] [adversarial loss: 1.063673, acc: 0.296875]\n",
      "1943: [discriminator loss: 0.459047, acc: 0.804688] [adversarial loss: 1.575401, acc: 0.078125]\n",
      "1944: [discriminator loss: 0.460920, acc: 0.757812] [adversarial loss: 0.976865, acc: 0.312500]\n",
      "1945: [discriminator loss: 0.514472, acc: 0.742188] [adversarial loss: 1.390959, acc: 0.109375]\n",
      "1946: [discriminator loss: 0.454252, acc: 0.796875] [adversarial loss: 1.045774, acc: 0.328125]\n",
      "1947: [discriminator loss: 0.591669, acc: 0.664062] [adversarial loss: 2.059247, acc: 0.078125]\n",
      "1948: [discriminator loss: 0.571403, acc: 0.671875] [adversarial loss: 0.631913, acc: 0.546875]\n",
      "1949: [discriminator loss: 0.560420, acc: 0.656250] [adversarial loss: 1.713899, acc: 0.078125]\n",
      "1950: [discriminator loss: 0.457857, acc: 0.789062] [adversarial loss: 1.116646, acc: 0.296875]\n",
      "1951: [discriminator loss: 0.529650, acc: 0.734375] [adversarial loss: 1.523483, acc: 0.109375]\n",
      "1952: [discriminator loss: 0.481072, acc: 0.820312] [adversarial loss: 1.255456, acc: 0.312500]\n",
      "1953: [discriminator loss: 0.493520, acc: 0.750000] [adversarial loss: 1.218403, acc: 0.171875]\n",
      "1954: [discriminator loss: 0.517308, acc: 0.750000] [adversarial loss: 1.384126, acc: 0.171875]\n",
      "1955: [discriminator loss: 0.509674, acc: 0.742188] [adversarial loss: 0.782672, acc: 0.531250]\n",
      "1956: [discriminator loss: 0.529468, acc: 0.695312] [adversarial loss: 1.890276, acc: 0.015625]\n",
      "1957: [discriminator loss: 0.596901, acc: 0.687500] [adversarial loss: 0.806422, acc: 0.500000]\n",
      "1958: [discriminator loss: 0.592923, acc: 0.679688] [adversarial loss: 1.944633, acc: 0.000000]\n",
      "1959: [discriminator loss: 0.508858, acc: 0.734375] [adversarial loss: 0.970476, acc: 0.328125]\n",
      "1960: [discriminator loss: 0.494678, acc: 0.812500] [adversarial loss: 1.588991, acc: 0.140625]\n",
      "1961: [discriminator loss: 0.542036, acc: 0.710938] [adversarial loss: 1.033144, acc: 0.375000]\n",
      "1962: [discriminator loss: 0.499075, acc: 0.773438] [adversarial loss: 1.649333, acc: 0.062500]\n",
      "1963: [discriminator loss: 0.449719, acc: 0.804688] [adversarial loss: 1.120746, acc: 0.218750]\n",
      "1964: [discriminator loss: 0.449598, acc: 0.789062] [adversarial loss: 1.646321, acc: 0.140625]\n",
      "1965: [discriminator loss: 0.467716, acc: 0.789062] [adversarial loss: 1.313286, acc: 0.281250]\n",
      "1966: [discriminator loss: 0.507702, acc: 0.718750] [adversarial loss: 1.629222, acc: 0.140625]\n",
      "1967: [discriminator loss: 0.427569, acc: 0.804688] [adversarial loss: 0.954935, acc: 0.359375]\n",
      "1968: [discriminator loss: 0.470423, acc: 0.750000] [adversarial loss: 1.501859, acc: 0.140625]\n",
      "1969: [discriminator loss: 0.511052, acc: 0.734375] [adversarial loss: 1.294640, acc: 0.171875]\n",
      "1970: [discriminator loss: 0.551729, acc: 0.718750] [adversarial loss: 1.239776, acc: 0.156250]\n",
      "1971: [discriminator loss: 0.545845, acc: 0.687500] [adversarial loss: 1.522222, acc: 0.125000]\n",
      "1972: [discriminator loss: 0.456759, acc: 0.742188] [adversarial loss: 1.503981, acc: 0.171875]\n",
      "1973: [discriminator loss: 0.494200, acc: 0.750000] [adversarial loss: 0.806481, acc: 0.468750]\n",
      "1974: [discriminator loss: 0.538662, acc: 0.734375] [adversarial loss: 2.058074, acc: 0.031250]\n",
      "1975: [discriminator loss: 0.569154, acc: 0.703125] [adversarial loss: 0.647151, acc: 0.593750]\n",
      "1976: [discriminator loss: 0.515348, acc: 0.750000] [adversarial loss: 1.773616, acc: 0.046875]\n",
      "1977: [discriminator loss: 0.549409, acc: 0.695312] [adversarial loss: 0.809845, acc: 0.500000]\n",
      "1978: [discriminator loss: 0.551381, acc: 0.687500] [adversarial loss: 1.762245, acc: 0.046875]\n",
      "1979: [discriminator loss: 0.516643, acc: 0.757812] [adversarial loss: 0.988993, acc: 0.296875]\n",
      "1980: [discriminator loss: 0.470475, acc: 0.773438] [adversarial loss: 1.506333, acc: 0.171875]\n",
      "1981: [discriminator loss: 0.513809, acc: 0.726562] [adversarial loss: 1.031947, acc: 0.296875]\n",
      "1982: [discriminator loss: 0.513394, acc: 0.765625] [adversarial loss: 1.324836, acc: 0.171875]\n",
      "1983: [discriminator loss: 0.491156, acc: 0.757812] [adversarial loss: 1.358513, acc: 0.187500]\n",
      "1984: [discriminator loss: 0.460095, acc: 0.757812] [adversarial loss: 1.455480, acc: 0.093750]\n",
      "1985: [discriminator loss: 0.409208, acc: 0.875000] [adversarial loss: 1.211515, acc: 0.171875]\n",
      "1986: [discriminator loss: 0.494209, acc: 0.718750] [adversarial loss: 1.615150, acc: 0.093750]\n",
      "1987: [discriminator loss: 0.484206, acc: 0.757812] [adversarial loss: 1.119526, acc: 0.281250]\n",
      "1988: [discriminator loss: 0.510562, acc: 0.710938] [adversarial loss: 1.891999, acc: 0.062500]\n",
      "1989: [discriminator loss: 0.553527, acc: 0.726562] [adversarial loss: 0.935176, acc: 0.437500]\n",
      "1990: [discriminator loss: 0.526371, acc: 0.750000] [adversarial loss: 1.777149, acc: 0.062500]\n",
      "1991: [discriminator loss: 0.509556, acc: 0.742188] [adversarial loss: 0.915743, acc: 0.421875]\n",
      "1992: [discriminator loss: 0.504748, acc: 0.742188] [adversarial loss: 1.620373, acc: 0.078125]\n",
      "1993: [discriminator loss: 0.501731, acc: 0.765625] [adversarial loss: 0.909424, acc: 0.406250]\n",
      "1994: [discriminator loss: 0.554031, acc: 0.734375] [adversarial loss: 1.729133, acc: 0.125000]\n",
      "1995: [discriminator loss: 0.558321, acc: 0.695312] [adversarial loss: 0.814956, acc: 0.531250]\n",
      "1996: [discriminator loss: 0.655803, acc: 0.656250] [adversarial loss: 1.823319, acc: 0.031250]\n",
      "1997: [discriminator loss: 0.528043, acc: 0.687500] [adversarial loss: 0.892443, acc: 0.375000]\n",
      "1998: [discriminator loss: 0.514538, acc: 0.695312] [adversarial loss: 1.495250, acc: 0.093750]\n",
      "1999: [discriminator loss: 0.557991, acc: 0.742188] [adversarial loss: 0.949654, acc: 0.281250]\n",
      "2000: [discriminator loss: 0.472124, acc: 0.781250] [adversarial loss: 1.479587, acc: 0.109375]\n",
      "2001: [discriminator loss: 0.429409, acc: 0.804688] [adversarial loss: 1.086616, acc: 0.328125]\n",
      "2002: [discriminator loss: 0.412492, acc: 0.875000] [adversarial loss: 1.420716, acc: 0.171875]\n",
      "2003: [discriminator loss: 0.550697, acc: 0.718750] [adversarial loss: 0.800050, acc: 0.515625]\n",
      "2004: [discriminator loss: 0.457551, acc: 0.765625] [adversarial loss: 1.712361, acc: 0.093750]\n",
      "2005: [discriminator loss: 0.466881, acc: 0.804688] [adversarial loss: 0.997005, acc: 0.375000]\n",
      "2006: [discriminator loss: 0.568077, acc: 0.679688] [adversarial loss: 1.528716, acc: 0.093750]\n",
      "2007: [discriminator loss: 0.516285, acc: 0.750000] [adversarial loss: 0.994285, acc: 0.312500]\n",
      "2008: [discriminator loss: 0.435295, acc: 0.820312] [adversarial loss: 1.424633, acc: 0.140625]\n",
      "2009: [discriminator loss: 0.451585, acc: 0.812500] [adversarial loss: 1.219263, acc: 0.140625]\n",
      "2010: [discriminator loss: 0.466851, acc: 0.765625] [adversarial loss: 1.351763, acc: 0.203125]\n",
      "2011: [discriminator loss: 0.506943, acc: 0.757812] [adversarial loss: 1.025670, acc: 0.312500]\n",
      "2012: [discriminator loss: 0.491766, acc: 0.812500] [adversarial loss: 1.354871, acc: 0.171875]\n",
      "2013: [discriminator loss: 0.467236, acc: 0.796875] [adversarial loss: 1.279224, acc: 0.250000]\n",
      "2014: [discriminator loss: 0.531795, acc: 0.742188] [adversarial loss: 1.364692, acc: 0.140625]\n",
      "2015: [discriminator loss: 0.513343, acc: 0.773438] [adversarial loss: 1.322464, acc: 0.171875]\n",
      "2016: [discriminator loss: 0.514802, acc: 0.757812] [adversarial loss: 1.438173, acc: 0.187500]\n",
      "2017: [discriminator loss: 0.478815, acc: 0.718750] [adversarial loss: 1.144865, acc: 0.234375]\n",
      "2018: [discriminator loss: 0.398324, acc: 0.828125] [adversarial loss: 1.541094, acc: 0.140625]\n",
      "2019: [discriminator loss: 0.506034, acc: 0.734375] [adversarial loss: 0.882892, acc: 0.437500]\n",
      "2020: [discriminator loss: 0.556106, acc: 0.687500] [adversarial loss: 2.135593, acc: 0.062500]\n",
      "2021: [discriminator loss: 0.640122, acc: 0.671875] [adversarial loss: 0.617056, acc: 0.687500]\n",
      "2022: [discriminator loss: 0.517193, acc: 0.687500] [adversarial loss: 1.711639, acc: 0.093750]\n",
      "2023: [discriminator loss: 0.534406, acc: 0.710938] [adversarial loss: 0.969327, acc: 0.421875]\n",
      "2024: [discriminator loss: 0.447896, acc: 0.820312] [adversarial loss: 1.189522, acc: 0.234375]\n",
      "2025: [discriminator loss: 0.473146, acc: 0.789062] [adversarial loss: 1.329636, acc: 0.187500]\n",
      "2026: [discriminator loss: 0.516667, acc: 0.710938] [adversarial loss: 1.389310, acc: 0.171875]\n",
      "2027: [discriminator loss: 0.482883, acc: 0.765625] [adversarial loss: 1.528756, acc: 0.078125]\n",
      "2028: [discriminator loss: 0.561695, acc: 0.710938] [adversarial loss: 1.084558, acc: 0.312500]\n",
      "2029: [discriminator loss: 0.472980, acc: 0.796875] [adversarial loss: 1.515293, acc: 0.171875]\n",
      "2030: [discriminator loss: 0.472240, acc: 0.773438] [adversarial loss: 1.334863, acc: 0.203125]\n",
      "2031: [discriminator loss: 0.482662, acc: 0.796875] [adversarial loss: 1.823259, acc: 0.078125]\n",
      "2032: [discriminator loss: 0.527773, acc: 0.757812] [adversarial loss: 0.801168, acc: 0.500000]\n",
      "2033: [discriminator loss: 0.549325, acc: 0.726562] [adversarial loss: 2.003642, acc: 0.031250]\n",
      "2034: [discriminator loss: 0.540633, acc: 0.726562] [adversarial loss: 0.968882, acc: 0.343750]\n",
      "2035: [discriminator loss: 0.456918, acc: 0.757812] [adversarial loss: 1.492990, acc: 0.156250]\n",
      "2036: [discriminator loss: 0.544937, acc: 0.726562] [adversarial loss: 1.146934, acc: 0.265625]\n",
      "2037: [discriminator loss: 0.556446, acc: 0.710938] [adversarial loss: 1.857372, acc: 0.015625]\n",
      "2038: [discriminator loss: 0.484497, acc: 0.781250] [adversarial loss: 1.012372, acc: 0.343750]\n",
      "2039: [discriminator loss: 0.543321, acc: 0.710938] [adversarial loss: 1.815270, acc: 0.062500]\n",
      "2040: [discriminator loss: 0.585790, acc: 0.710938] [adversarial loss: 0.958746, acc: 0.359375]\n",
      "2041: [discriminator loss: 0.442233, acc: 0.812500] [adversarial loss: 1.161892, acc: 0.296875]\n",
      "2042: [discriminator loss: 0.462603, acc: 0.781250] [adversarial loss: 1.299129, acc: 0.234375]\n",
      "2043: [discriminator loss: 0.429645, acc: 0.812500] [adversarial loss: 1.111325, acc: 0.250000]\n",
      "2044: [discriminator loss: 0.496783, acc: 0.796875] [adversarial loss: 1.520558, acc: 0.125000]\n",
      "2045: [discriminator loss: 0.539907, acc: 0.757812] [adversarial loss: 1.191392, acc: 0.281250]\n",
      "2046: [discriminator loss: 0.553763, acc: 0.687500] [adversarial loss: 1.298517, acc: 0.187500]\n",
      "2047: [discriminator loss: 0.546027, acc: 0.703125] [adversarial loss: 1.316453, acc: 0.093750]\n",
      "2048: [discriminator loss: 0.511876, acc: 0.781250] [adversarial loss: 1.259225, acc: 0.203125]\n",
      "2049: [discriminator loss: 0.474733, acc: 0.804688] [adversarial loss: 1.149158, acc: 0.296875]\n",
      "2050: [discriminator loss: 0.499783, acc: 0.765625] [adversarial loss: 1.475591, acc: 0.093750]\n",
      "2051: [discriminator loss: 0.510691, acc: 0.765625] [adversarial loss: 1.172607, acc: 0.359375]\n",
      "2052: [discriminator loss: 0.578241, acc: 0.695312] [adversarial loss: 1.680739, acc: 0.109375]\n",
      "2053: [discriminator loss: 0.549734, acc: 0.757812] [adversarial loss: 0.919078, acc: 0.359375]\n",
      "2054: [discriminator loss: 0.518151, acc: 0.710938] [adversarial loss: 2.147591, acc: 0.000000]\n",
      "2055: [discriminator loss: 0.628030, acc: 0.648438] [adversarial loss: 0.713480, acc: 0.531250]\n",
      "2056: [discriminator loss: 0.652728, acc: 0.648438] [adversarial loss: 2.177320, acc: 0.046875]\n",
      "2057: [discriminator loss: 0.509810, acc: 0.757812] [adversarial loss: 0.883244, acc: 0.437500]\n",
      "2058: [discriminator loss: 0.495415, acc: 0.789062] [adversarial loss: 1.552993, acc: 0.093750]\n",
      "2059: [discriminator loss: 0.516232, acc: 0.773438] [adversarial loss: 0.888867, acc: 0.421875]\n",
      "2060: [discriminator loss: 0.540157, acc: 0.718750] [adversarial loss: 1.477161, acc: 0.156250]\n",
      "2061: [discriminator loss: 0.452874, acc: 0.828125] [adversarial loss: 0.947877, acc: 0.375000]\n",
      "2062: [discriminator loss: 0.466456, acc: 0.804688] [adversarial loss: 1.734936, acc: 0.046875]\n",
      "2063: [discriminator loss: 0.543055, acc: 0.710938] [adversarial loss: 0.815278, acc: 0.500000]\n",
      "2064: [discriminator loss: 0.531183, acc: 0.726562] [adversarial loss: 1.685896, acc: 0.031250]\n",
      "2065: [discriminator loss: 0.499989, acc: 0.750000] [adversarial loss: 1.196490, acc: 0.218750]\n",
      "2066: [discriminator loss: 0.415215, acc: 0.851562] [adversarial loss: 1.224972, acc: 0.203125]\n",
      "2067: [discriminator loss: 0.461037, acc: 0.820312] [adversarial loss: 1.393305, acc: 0.171875]\n",
      "2068: [discriminator loss: 0.464246, acc: 0.789062] [adversarial loss: 1.371287, acc: 0.187500]\n",
      "2069: [discriminator loss: 0.508689, acc: 0.750000] [adversarial loss: 1.003873, acc: 0.312500]\n",
      "2070: [discriminator loss: 0.485228, acc: 0.734375] [adversarial loss: 1.981344, acc: 0.093750]\n",
      "2071: [discriminator loss: 0.569639, acc: 0.718750] [adversarial loss: 0.930615, acc: 0.406250]\n",
      "2072: [discriminator loss: 0.502210, acc: 0.742188] [adversarial loss: 1.532291, acc: 0.078125]\n",
      "2073: [discriminator loss: 0.466180, acc: 0.820312] [adversarial loss: 1.284511, acc: 0.156250]\n",
      "2074: [discriminator loss: 0.536573, acc: 0.726562] [adversarial loss: 1.068067, acc: 0.328125]\n",
      "2075: [discriminator loss: 0.546994, acc: 0.710938] [adversarial loss: 1.598818, acc: 0.046875]\n",
      "2076: [discriminator loss: 0.510046, acc: 0.765625] [adversarial loss: 0.912487, acc: 0.375000]\n",
      "2077: [discriminator loss: 0.566902, acc: 0.742188] [adversarial loss: 1.784155, acc: 0.046875]\n",
      "2078: [discriminator loss: 0.551028, acc: 0.750000] [adversarial loss: 1.054819, acc: 0.328125]\n",
      "2079: [discriminator loss: 0.550672, acc: 0.679688] [adversarial loss: 1.951558, acc: 0.078125]\n",
      "2080: [discriminator loss: 0.578250, acc: 0.773438] [adversarial loss: 1.043274, acc: 0.359375]\n",
      "2081: [discriminator loss: 0.500532, acc: 0.757812] [adversarial loss: 1.503874, acc: 0.093750]\n",
      "2082: [discriminator loss: 0.533440, acc: 0.757812] [adversarial loss: 1.286717, acc: 0.093750]\n",
      "2083: [discriminator loss: 0.487963, acc: 0.789062] [adversarial loss: 1.195449, acc: 0.296875]\n",
      "2084: [discriminator loss: 0.488836, acc: 0.765625] [adversarial loss: 1.365986, acc: 0.171875]\n",
      "2085: [discriminator loss: 0.473254, acc: 0.812500] [adversarial loss: 1.286617, acc: 0.265625]\n",
      "2086: [discriminator loss: 0.412098, acc: 0.843750] [adversarial loss: 1.536796, acc: 0.125000]\n",
      "2087: [discriminator loss: 0.459637, acc: 0.734375] [adversarial loss: 1.267252, acc: 0.156250]\n",
      "2088: [discriminator loss: 0.532725, acc: 0.734375] [adversarial loss: 1.056383, acc: 0.390625]\n",
      "2089: [discriminator loss: 0.594245, acc: 0.679688] [adversarial loss: 1.609655, acc: 0.078125]\n",
      "2090: [discriminator loss: 0.454707, acc: 0.812500] [adversarial loss: 1.003011, acc: 0.406250]\n",
      "2091: [discriminator loss: 0.518173, acc: 0.718750] [adversarial loss: 1.786767, acc: 0.062500]\n",
      "2092: [discriminator loss: 0.507422, acc: 0.781250] [adversarial loss: 1.036986, acc: 0.296875]\n",
      "2093: [discriminator loss: 0.509023, acc: 0.750000] [adversarial loss: 1.424267, acc: 0.171875]\n",
      "2094: [discriminator loss: 0.495196, acc: 0.742188] [adversarial loss: 0.969310, acc: 0.437500]\n",
      "2095: [discriminator loss: 0.498096, acc: 0.781250] [adversarial loss: 1.941009, acc: 0.015625]\n",
      "2096: [discriminator loss: 0.573687, acc: 0.703125] [adversarial loss: 0.775565, acc: 0.515625]\n",
      "2097: [discriminator loss: 0.543796, acc: 0.703125] [adversarial loss: 1.911225, acc: 0.046875]\n",
      "2098: [discriminator loss: 0.484097, acc: 0.742188] [adversarial loss: 0.948796, acc: 0.406250]\n",
      "2099: [discriminator loss: 0.471654, acc: 0.757812] [adversarial loss: 1.588606, acc: 0.093750]\n",
      "2100: [discriminator loss: 0.485591, acc: 0.789062] [adversarial loss: 1.056806, acc: 0.328125]\n",
      "2101: [discriminator loss: 0.501608, acc: 0.726562] [adversarial loss: 1.607511, acc: 0.140625]\n",
      "2102: [discriminator loss: 0.494291, acc: 0.757812] [adversarial loss: 1.089290, acc: 0.250000]\n",
      "2103: [discriminator loss: 0.571298, acc: 0.703125] [adversarial loss: 1.591414, acc: 0.125000]\n",
      "2104: [discriminator loss: 0.591760, acc: 0.710938] [adversarial loss: 0.839385, acc: 0.453125]\n",
      "2105: [discriminator loss: 0.600835, acc: 0.695312] [adversarial loss: 1.806973, acc: 0.031250]\n",
      "2106: [discriminator loss: 0.470686, acc: 0.765625] [adversarial loss: 1.055214, acc: 0.328125]\n",
      "2107: [discriminator loss: 0.468811, acc: 0.789062] [adversarial loss: 1.643864, acc: 0.046875]\n",
      "2108: [discriminator loss: 0.497390, acc: 0.718750] [adversarial loss: 1.167554, acc: 0.203125]\n",
      "2109: [discriminator loss: 0.386412, acc: 0.867188] [adversarial loss: 1.235231, acc: 0.265625]\n",
      "2110: [discriminator loss: 0.516019, acc: 0.742188] [adversarial loss: 1.177951, acc: 0.250000]\n",
      "2111: [discriminator loss: 0.493627, acc: 0.781250] [adversarial loss: 1.387743, acc: 0.140625]\n",
      "2112: [discriminator loss: 0.443042, acc: 0.804688] [adversarial loss: 1.188031, acc: 0.312500]\n",
      "2113: [discriminator loss: 0.471167, acc: 0.820312] [adversarial loss: 1.471391, acc: 0.171875]\n",
      "2114: [discriminator loss: 0.487014, acc: 0.781250] [adversarial loss: 1.438364, acc: 0.078125]\n",
      "2115: [discriminator loss: 0.533567, acc: 0.710938] [adversarial loss: 1.243546, acc: 0.234375]\n",
      "2116: [discriminator loss: 0.412799, acc: 0.843750] [adversarial loss: 1.470680, acc: 0.109375]\n",
      "2117: [discriminator loss: 0.512268, acc: 0.734375] [adversarial loss: 1.036631, acc: 0.296875]\n",
      "2118: [discriminator loss: 0.532365, acc: 0.757812] [adversarial loss: 2.044942, acc: 0.046875]\n",
      "2119: [discriminator loss: 0.514596, acc: 0.757812] [adversarial loss: 0.883090, acc: 0.437500]\n",
      "2120: [discriminator loss: 0.548355, acc: 0.710938] [adversarial loss: 1.757341, acc: 0.093750]\n",
      "2121: [discriminator loss: 0.589628, acc: 0.695312] [adversarial loss: 0.817542, acc: 0.546875]\n",
      "2122: [discriminator loss: 0.590473, acc: 0.609375] [adversarial loss: 1.988447, acc: 0.093750]\n",
      "2123: [discriminator loss: 0.491654, acc: 0.742188] [adversarial loss: 1.150018, acc: 0.312500]\n",
      "2124: [discriminator loss: 0.536133, acc: 0.742188] [adversarial loss: 1.522467, acc: 0.109375]\n",
      "2125: [discriminator loss: 0.508076, acc: 0.789062] [adversarial loss: 0.838418, acc: 0.421875]\n",
      "2126: [discriminator loss: 0.545269, acc: 0.757812] [adversarial loss: 1.599573, acc: 0.093750]\n",
      "2127: [discriminator loss: 0.437836, acc: 0.789062] [adversarial loss: 1.102970, acc: 0.328125]\n",
      "2128: [discriminator loss: 0.387832, acc: 0.851562] [adversarial loss: 1.191969, acc: 0.187500]\n",
      "2129: [discriminator loss: 0.493180, acc: 0.781250] [adversarial loss: 1.296032, acc: 0.203125]\n",
      "2130: [discriminator loss: 0.520512, acc: 0.750000] [adversarial loss: 1.613696, acc: 0.078125]\n",
      "2131: [discriminator loss: 0.531517, acc: 0.742188] [adversarial loss: 1.436139, acc: 0.156250]\n",
      "2132: [discriminator loss: 0.484447, acc: 0.804688] [adversarial loss: 1.670553, acc: 0.109375]\n",
      "2133: [discriminator loss: 0.511717, acc: 0.734375] [adversarial loss: 0.980175, acc: 0.375000]\n",
      "2134: [discriminator loss: 0.495350, acc: 0.750000] [adversarial loss: 2.069215, acc: 0.031250]\n",
      "2135: [discriminator loss: 0.514338, acc: 0.734375] [adversarial loss: 0.984683, acc: 0.359375]\n",
      "2136: [discriminator loss: 0.505930, acc: 0.718750] [adversarial loss: 1.707234, acc: 0.078125]\n",
      "2137: [discriminator loss: 0.467168, acc: 0.757812] [adversarial loss: 0.844337, acc: 0.500000]\n",
      "2138: [discriminator loss: 0.550776, acc: 0.695312] [adversarial loss: 1.914779, acc: 0.031250]\n",
      "2139: [discriminator loss: 0.537853, acc: 0.742188] [adversarial loss: 0.880971, acc: 0.390625]\n",
      "2140: [discriminator loss: 0.544017, acc: 0.718750] [adversarial loss: 1.744644, acc: 0.140625]\n",
      "2141: [discriminator loss: 0.512048, acc: 0.726562] [adversarial loss: 1.030860, acc: 0.265625]\n",
      "2142: [discriminator loss: 0.495726, acc: 0.703125] [adversarial loss: 1.628654, acc: 0.109375]\n",
      "2143: [discriminator loss: 0.506174, acc: 0.781250] [adversarial loss: 0.940105, acc: 0.390625]\n",
      "2144: [discriminator loss: 0.527876, acc: 0.742188] [adversarial loss: 1.368329, acc: 0.156250]\n",
      "2145: [discriminator loss: 0.493128, acc: 0.781250] [adversarial loss: 1.071112, acc: 0.343750]\n",
      "2146: [discriminator loss: 0.531681, acc: 0.750000] [adversarial loss: 1.604353, acc: 0.093750]\n",
      "2147: [discriminator loss: 0.475388, acc: 0.757812] [adversarial loss: 1.244418, acc: 0.250000]\n",
      "2148: [discriminator loss: 0.412455, acc: 0.828125] [adversarial loss: 1.109064, acc: 0.281250]\n",
      "2149: [discriminator loss: 0.522286, acc: 0.757812] [adversarial loss: 1.670462, acc: 0.109375]\n",
      "2150: [discriminator loss: 0.440658, acc: 0.796875] [adversarial loss: 1.314969, acc: 0.171875]\n",
      "2151: [discriminator loss: 0.529745, acc: 0.750000] [adversarial loss: 1.972015, acc: 0.062500]\n",
      "2152: [discriminator loss: 0.491014, acc: 0.750000] [adversarial loss: 1.102016, acc: 0.281250]\n",
      "2153: [discriminator loss: 0.457006, acc: 0.781250] [adversarial loss: 1.433562, acc: 0.156250]\n",
      "2154: [discriminator loss: 0.480601, acc: 0.773438] [adversarial loss: 1.237014, acc: 0.171875]\n",
      "2155: [discriminator loss: 0.499708, acc: 0.750000] [adversarial loss: 1.258464, acc: 0.140625]\n",
      "2156: [discriminator loss: 0.484669, acc: 0.773438] [adversarial loss: 1.354936, acc: 0.171875]\n",
      "2157: [discriminator loss: 0.542359, acc: 0.742188] [adversarial loss: 1.370477, acc: 0.203125]\n",
      "2158: [discriminator loss: 0.501072, acc: 0.742188] [adversarial loss: 1.394412, acc: 0.218750]\n",
      "2159: [discriminator loss: 0.489166, acc: 0.757812] [adversarial loss: 1.266954, acc: 0.156250]\n",
      "2160: [discriminator loss: 0.486757, acc: 0.757812] [adversarial loss: 1.302997, acc: 0.265625]\n",
      "2161: [discriminator loss: 0.512537, acc: 0.757812] [adversarial loss: 1.664157, acc: 0.093750]\n",
      "2162: [discriminator loss: 0.451687, acc: 0.796875] [adversarial loss: 1.274340, acc: 0.234375]\n",
      "2163: [discriminator loss: 0.448934, acc: 0.781250] [adversarial loss: 1.637419, acc: 0.156250]\n",
      "2164: [discriminator loss: 0.435438, acc: 0.796875] [adversarial loss: 1.441228, acc: 0.218750]\n",
      "2165: [discriminator loss: 0.494224, acc: 0.710938] [adversarial loss: 1.424961, acc: 0.140625]\n",
      "2166: [discriminator loss: 0.442666, acc: 0.812500] [adversarial loss: 0.884603, acc: 0.453125]\n",
      "2167: [discriminator loss: 0.513912, acc: 0.789062] [adversarial loss: 2.159175, acc: 0.046875]\n",
      "2168: [discriminator loss: 0.589377, acc: 0.710938] [adversarial loss: 0.630610, acc: 0.656250]\n",
      "2169: [discriminator loss: 0.557489, acc: 0.687500] [adversarial loss: 2.199437, acc: 0.062500]\n",
      "2170: [discriminator loss: 0.640331, acc: 0.632812] [adversarial loss: 0.806729, acc: 0.468750]\n",
      "2171: [discriminator loss: 0.592972, acc: 0.679688] [adversarial loss: 1.574052, acc: 0.093750]\n",
      "2172: [discriminator loss: 0.539117, acc: 0.734375] [adversarial loss: 1.244002, acc: 0.203125]\n",
      "2173: [discriminator loss: 0.510026, acc: 0.773438] [adversarial loss: 1.317903, acc: 0.140625]\n",
      "2174: [discriminator loss: 0.513838, acc: 0.773438] [adversarial loss: 1.354650, acc: 0.187500]\n",
      "2175: [discriminator loss: 0.528099, acc: 0.734375] [adversarial loss: 1.359093, acc: 0.203125]\n",
      "2176: [discriminator loss: 0.531026, acc: 0.726562] [adversarial loss: 0.882282, acc: 0.484375]\n",
      "2177: [discriminator loss: 0.526625, acc: 0.710938] [adversarial loss: 1.964923, acc: 0.031250]\n",
      "2178: [discriminator loss: 0.563156, acc: 0.664062] [adversarial loss: 0.958791, acc: 0.406250]\n",
      "2179: [discriminator loss: 0.499382, acc: 0.757812] [adversarial loss: 1.605228, acc: 0.109375]\n",
      "2180: [discriminator loss: 0.483833, acc: 0.765625] [adversarial loss: 0.931681, acc: 0.312500]\n",
      "2181: [discriminator loss: 0.465791, acc: 0.765625] [adversarial loss: 1.717558, acc: 0.125000]\n",
      "2182: [discriminator loss: 0.475842, acc: 0.796875] [adversarial loss: 1.192047, acc: 0.218750]\n",
      "2183: [discriminator loss: 0.474765, acc: 0.804688] [adversarial loss: 1.507932, acc: 0.140625]\n",
      "2184: [discriminator loss: 0.517020, acc: 0.757812] [adversarial loss: 1.660887, acc: 0.093750]\n",
      "2185: [discriminator loss: 0.481212, acc: 0.789062] [adversarial loss: 1.156373, acc: 0.281250]\n",
      "2186: [discriminator loss: 0.518083, acc: 0.726562] [adversarial loss: 1.480881, acc: 0.125000]\n",
      "2187: [discriminator loss: 0.429074, acc: 0.804688] [adversarial loss: 0.921655, acc: 0.468750]\n",
      "2188: [discriminator loss: 0.518037, acc: 0.726562] [adversarial loss: 1.891220, acc: 0.109375]\n",
      "2189: [discriminator loss: 0.491810, acc: 0.765625] [adversarial loss: 0.980470, acc: 0.375000]\n",
      "2190: [discriminator loss: 0.519570, acc: 0.742188] [adversarial loss: 1.538927, acc: 0.125000]\n",
      "2191: [discriminator loss: 0.491726, acc: 0.765625] [adversarial loss: 1.069526, acc: 0.343750]\n",
      "2192: [discriminator loss: 0.507385, acc: 0.742188] [adversarial loss: 1.684641, acc: 0.125000]\n",
      "2193: [discriminator loss: 0.520417, acc: 0.757812] [adversarial loss: 1.054347, acc: 0.375000]\n",
      "2194: [discriminator loss: 0.511469, acc: 0.773438] [adversarial loss: 1.505837, acc: 0.093750]\n",
      "2195: [discriminator loss: 0.498560, acc: 0.750000] [adversarial loss: 0.975340, acc: 0.375000]\n",
      "2196: [discriminator loss: 0.528282, acc: 0.765625] [adversarial loss: 1.431919, acc: 0.203125]\n",
      "2197: [discriminator loss: 0.519763, acc: 0.734375] [adversarial loss: 0.776647, acc: 0.515625]\n",
      "2198: [discriminator loss: 0.475877, acc: 0.742188] [adversarial loss: 2.035868, acc: 0.000000]\n",
      "2199: [discriminator loss: 0.590116, acc: 0.695312] [adversarial loss: 0.693613, acc: 0.578125]\n",
      "2200: [discriminator loss: 0.563007, acc: 0.671875] [adversarial loss: 1.747092, acc: 0.062500]\n",
      "2201: [discriminator loss: 0.480883, acc: 0.773438] [adversarial loss: 1.134629, acc: 0.281250]\n",
      "2202: [discriminator loss: 0.454140, acc: 0.781250] [adversarial loss: 1.279571, acc: 0.171875]\n",
      "2203: [discriminator loss: 0.444384, acc: 0.796875] [adversarial loss: 1.373357, acc: 0.093750]\n",
      "2204: [discriminator loss: 0.474975, acc: 0.765625] [adversarial loss: 1.296739, acc: 0.203125]\n",
      "2205: [discriminator loss: 0.500297, acc: 0.757812] [adversarial loss: 1.430467, acc: 0.156250]\n",
      "2206: [discriminator loss: 0.433806, acc: 0.789062] [adversarial loss: 1.429524, acc: 0.140625]\n",
      "2207: [discriminator loss: 0.436568, acc: 0.796875] [adversarial loss: 1.592039, acc: 0.156250]\n",
      "2208: [discriminator loss: 0.525688, acc: 0.773438] [adversarial loss: 1.143085, acc: 0.296875]\n",
      "2209: [discriminator loss: 0.482203, acc: 0.804688] [adversarial loss: 1.371376, acc: 0.156250]\n",
      "2210: [discriminator loss: 0.440282, acc: 0.812500] [adversarial loss: 1.265806, acc: 0.171875]\n",
      "2211: [discriminator loss: 0.532325, acc: 0.773438] [adversarial loss: 0.958676, acc: 0.453125]\n",
      "2212: [discriminator loss: 0.510360, acc: 0.695312] [adversarial loss: 1.849166, acc: 0.046875]\n",
      "2213: [discriminator loss: 0.569324, acc: 0.734375] [adversarial loss: 0.875388, acc: 0.421875]\n",
      "2214: [discriminator loss: 0.507069, acc: 0.742188] [adversarial loss: 1.770846, acc: 0.140625]\n",
      "2215: [discriminator loss: 0.497676, acc: 0.789062] [adversarial loss: 0.993293, acc: 0.328125]\n",
      "2216: [discriminator loss: 0.510949, acc: 0.750000] [adversarial loss: 1.684350, acc: 0.093750]\n",
      "2217: [discriminator loss: 0.608269, acc: 0.695312] [adversarial loss: 0.738718, acc: 0.531250]\n",
      "2218: [discriminator loss: 0.659734, acc: 0.617188] [adversarial loss: 2.034599, acc: 0.046875]\n",
      "2219: [discriminator loss: 0.558797, acc: 0.718750] [adversarial loss: 0.924058, acc: 0.343750]\n",
      "2220: [discriminator loss: 0.478706, acc: 0.734375] [adversarial loss: 1.691509, acc: 0.109375]\n",
      "2221: [discriminator loss: 0.473691, acc: 0.773438] [adversarial loss: 1.122163, acc: 0.265625]\n",
      "2222: [discriminator loss: 0.489197, acc: 0.773438] [adversarial loss: 1.301741, acc: 0.156250]\n",
      "2223: [discriminator loss: 0.445596, acc: 0.796875] [adversarial loss: 1.375080, acc: 0.109375]\n",
      "2224: [discriminator loss: 0.498815, acc: 0.765625] [adversarial loss: 1.275505, acc: 0.218750]\n",
      "2225: [discriminator loss: 0.534272, acc: 0.718750] [adversarial loss: 1.548551, acc: 0.093750]\n",
      "2226: [discriminator loss: 0.457921, acc: 0.757812] [adversarial loss: 1.018665, acc: 0.359375]\n",
      "2227: [discriminator loss: 0.500731, acc: 0.726562] [adversarial loss: 1.924208, acc: 0.015625]\n",
      "2228: [discriminator loss: 0.547159, acc: 0.742188] [adversarial loss: 1.112172, acc: 0.265625]\n",
      "2229: [discriminator loss: 0.583171, acc: 0.726562] [adversarial loss: 1.874459, acc: 0.078125]\n",
      "2230: [discriminator loss: 0.499600, acc: 0.757812] [adversarial loss: 0.945349, acc: 0.406250]\n",
      "2231: [discriminator loss: 0.461647, acc: 0.804688] [adversarial loss: 1.608515, acc: 0.140625]\n",
      "2232: [discriminator loss: 0.487533, acc: 0.812500] [adversarial loss: 1.057274, acc: 0.296875]\n",
      "2233: [discriminator loss: 0.505491, acc: 0.757812] [adversarial loss: 1.716023, acc: 0.078125]\n",
      "2234: [discriminator loss: 0.502758, acc: 0.789062] [adversarial loss: 1.009404, acc: 0.406250]\n",
      "2235: [discriminator loss: 0.479403, acc: 0.773438] [adversarial loss: 1.681520, acc: 0.140625]\n",
      "2236: [discriminator loss: 0.520868, acc: 0.710938] [adversarial loss: 1.121436, acc: 0.265625]\n",
      "2237: [discriminator loss: 0.477444, acc: 0.796875] [adversarial loss: 1.284233, acc: 0.203125]\n",
      "2238: [discriminator loss: 0.511361, acc: 0.742188] [adversarial loss: 1.120241, acc: 0.250000]\n",
      "2239: [discriminator loss: 0.454549, acc: 0.781250] [adversarial loss: 1.657993, acc: 0.109375]\n",
      "2240: [discriminator loss: 0.520669, acc: 0.734375] [adversarial loss: 0.957282, acc: 0.359375]\n",
      "2241: [discriminator loss: 0.453734, acc: 0.789062] [adversarial loss: 1.950724, acc: 0.046875]\n",
      "2242: [discriminator loss: 0.535589, acc: 0.757812] [adversarial loss: 0.887646, acc: 0.421875]\n",
      "2243: [discriminator loss: 0.557256, acc: 0.695312] [adversarial loss: 1.833890, acc: 0.062500]\n",
      "2244: [discriminator loss: 0.528716, acc: 0.757812] [adversarial loss: 0.922114, acc: 0.437500]\n",
      "2245: [discriminator loss: 0.508608, acc: 0.734375] [adversarial loss: 1.768236, acc: 0.140625]\n",
      "2246: [discriminator loss: 0.485633, acc: 0.742188] [adversarial loss: 1.183541, acc: 0.281250]\n",
      "2247: [discriminator loss: 0.550661, acc: 0.695312] [adversarial loss: 1.709368, acc: 0.078125]\n",
      "2248: [discriminator loss: 0.417389, acc: 0.835938] [adversarial loss: 1.363064, acc: 0.109375]\n",
      "2249: [discriminator loss: 0.485272, acc: 0.773438] [adversarial loss: 1.204005, acc: 0.265625]\n",
      "2250: [discriminator loss: 0.501573, acc: 0.757812] [adversarial loss: 1.585623, acc: 0.171875]\n",
      "2251: [discriminator loss: 0.464514, acc: 0.773438] [adversarial loss: 1.374857, acc: 0.218750]\n",
      "2252: [discriminator loss: 0.578439, acc: 0.734375] [adversarial loss: 1.779301, acc: 0.078125]\n",
      "2253: [discriminator loss: 0.483472, acc: 0.750000] [adversarial loss: 1.065842, acc: 0.312500]\n",
      "2254: [discriminator loss: 0.417621, acc: 0.851562] [adversarial loss: 1.646969, acc: 0.187500]\n",
      "2255: [discriminator loss: 0.560381, acc: 0.703125] [adversarial loss: 1.169595, acc: 0.218750]\n",
      "2256: [discriminator loss: 0.486138, acc: 0.773438] [adversarial loss: 1.912316, acc: 0.093750]\n",
      "2257: [discriminator loss: 0.559481, acc: 0.695312] [adversarial loss: 0.954432, acc: 0.359375]\n",
      "2258: [discriminator loss: 0.469690, acc: 0.757812] [adversarial loss: 1.737271, acc: 0.125000]\n",
      "2259: [discriminator loss: 0.508969, acc: 0.710938] [adversarial loss: 1.030174, acc: 0.359375]\n",
      "2260: [discriminator loss: 0.554226, acc: 0.695312] [adversarial loss: 1.817266, acc: 0.046875]\n",
      "2261: [discriminator loss: 0.510008, acc: 0.742188] [adversarial loss: 1.080255, acc: 0.281250]\n",
      "2262: [discriminator loss: 0.454612, acc: 0.843750] [adversarial loss: 1.533113, acc: 0.140625]\n",
      "2263: [discriminator loss: 0.493466, acc: 0.734375] [adversarial loss: 1.138494, acc: 0.328125]\n",
      "2264: [discriminator loss: 0.472687, acc: 0.789062] [adversarial loss: 1.617102, acc: 0.093750]\n",
      "2265: [discriminator loss: 0.503927, acc: 0.765625] [adversarial loss: 1.043800, acc: 0.265625]\n",
      "2266: [discriminator loss: 0.456564, acc: 0.812500] [adversarial loss: 1.572794, acc: 0.187500]\n",
      "2267: [discriminator loss: 0.480183, acc: 0.789062] [adversarial loss: 1.211018, acc: 0.296875]\n",
      "2268: [discriminator loss: 0.514729, acc: 0.773438] [adversarial loss: 1.497933, acc: 0.125000]\n",
      "2269: [discriminator loss: 0.488185, acc: 0.773438] [adversarial loss: 1.132663, acc: 0.406250]\n",
      "2270: [discriminator loss: 0.474890, acc: 0.796875] [adversarial loss: 1.657137, acc: 0.078125]\n",
      "2271: [discriminator loss: 0.560469, acc: 0.757812] [adversarial loss: 0.657653, acc: 0.515625]\n",
      "2272: [discriminator loss: 0.512818, acc: 0.718750] [adversarial loss: 1.823981, acc: 0.125000]\n",
      "2273: [discriminator loss: 0.576137, acc: 0.750000] [adversarial loss: 0.960233, acc: 0.406250]\n",
      "2274: [discriminator loss: 0.514778, acc: 0.750000] [adversarial loss: 1.839921, acc: 0.046875]\n",
      "2275: [discriminator loss: 0.498090, acc: 0.796875] [adversarial loss: 0.867300, acc: 0.468750]\n",
      "2276: [discriminator loss: 0.507173, acc: 0.734375] [adversarial loss: 1.600471, acc: 0.140625]\n",
      "2277: [discriminator loss: 0.600798, acc: 0.695312] [adversarial loss: 0.950595, acc: 0.421875]\n",
      "2278: [discriminator loss: 0.464677, acc: 0.757812] [adversarial loss: 1.880429, acc: 0.078125]\n",
      "2279: [discriminator loss: 0.525374, acc: 0.781250] [adversarial loss: 1.067606, acc: 0.312500]\n",
      "2280: [discriminator loss: 0.536372, acc: 0.710938] [adversarial loss: 1.613708, acc: 0.156250]\n",
      "2281: [discriminator loss: 0.502257, acc: 0.765625] [adversarial loss: 1.021297, acc: 0.406250]\n",
      "2282: [discriminator loss: 0.387839, acc: 0.828125] [adversarial loss: 1.146431, acc: 0.281250]\n",
      "2283: [discriminator loss: 0.480043, acc: 0.796875] [adversarial loss: 1.564825, acc: 0.093750]\n",
      "2284: [discriminator loss: 0.512753, acc: 0.726562] [adversarial loss: 1.701493, acc: 0.046875]\n",
      "2285: [discriminator loss: 0.496041, acc: 0.734375] [adversarial loss: 1.311414, acc: 0.203125]\n",
      "2286: [discriminator loss: 0.449780, acc: 0.828125] [adversarial loss: 1.065885, acc: 0.359375]\n",
      "2287: [discriminator loss: 0.566074, acc: 0.718750] [adversarial loss: 1.503694, acc: 0.125000]\n",
      "2288: [discriminator loss: 0.484609, acc: 0.796875] [adversarial loss: 0.808576, acc: 0.468750]\n",
      "2289: [discriminator loss: 0.520430, acc: 0.773438] [adversarial loss: 1.840974, acc: 0.078125]\n",
      "2290: [discriminator loss: 0.576923, acc: 0.742188] [adversarial loss: 0.921916, acc: 0.406250]\n",
      "2291: [discriminator loss: 0.526163, acc: 0.710938] [adversarial loss: 1.742954, acc: 0.109375]\n",
      "2292: [discriminator loss: 0.477699, acc: 0.742188] [adversarial loss: 0.870856, acc: 0.390625]\n",
      "2293: [discriminator loss: 0.593116, acc: 0.687500] [adversarial loss: 1.541769, acc: 0.203125]\n",
      "2294: [discriminator loss: 0.445893, acc: 0.796875] [adversarial loss: 0.863963, acc: 0.406250]\n",
      "2295: [discriminator loss: 0.514951, acc: 0.742188] [adversarial loss: 1.869672, acc: 0.078125]\n",
      "2296: [discriminator loss: 0.503588, acc: 0.734375] [adversarial loss: 1.073432, acc: 0.281250]\n",
      "2297: [discriminator loss: 0.420946, acc: 0.812500] [adversarial loss: 1.321075, acc: 0.187500]\n",
      "2298: [discriminator loss: 0.458223, acc: 0.789062] [adversarial loss: 1.520851, acc: 0.218750]\n",
      "2299: [discriminator loss: 0.565606, acc: 0.726562] [adversarial loss: 1.341044, acc: 0.171875]\n",
      "2300: [discriminator loss: 0.512333, acc: 0.734375] [adversarial loss: 1.414262, acc: 0.093750]\n",
      "2301: [discriminator loss: 0.445874, acc: 0.789062] [adversarial loss: 1.213012, acc: 0.281250]\n",
      "2302: [discriminator loss: 0.479038, acc: 0.781250] [adversarial loss: 1.597449, acc: 0.125000]\n",
      "2303: [discriminator loss: 0.467483, acc: 0.789062] [adversarial loss: 1.388759, acc: 0.171875]\n",
      "2304: [discriminator loss: 0.403244, acc: 0.812500] [adversarial loss: 1.307897, acc: 0.218750]\n",
      "2305: [discriminator loss: 0.421122, acc: 0.851562] [adversarial loss: 1.570884, acc: 0.171875]\n",
      "2306: [discriminator loss: 0.518005, acc: 0.796875] [adversarial loss: 1.105256, acc: 0.312500]\n",
      "2307: [discriminator loss: 0.423205, acc: 0.796875] [adversarial loss: 1.203445, acc: 0.343750]\n",
      "2308: [discriminator loss: 0.470269, acc: 0.820312] [adversarial loss: 1.367812, acc: 0.234375]\n",
      "2309: [discriminator loss: 0.459947, acc: 0.796875] [adversarial loss: 1.066118, acc: 0.296875]\n",
      "2310: [discriminator loss: 0.576984, acc: 0.656250] [adversarial loss: 2.548902, acc: 0.062500]\n",
      "2311: [discriminator loss: 0.548205, acc: 0.734375] [adversarial loss: 0.792204, acc: 0.500000]\n",
      "2312: [discriminator loss: 0.626831, acc: 0.687500] [adversarial loss: 1.914572, acc: 0.015625]\n",
      "2313: [discriminator loss: 0.552377, acc: 0.757812] [adversarial loss: 0.806002, acc: 0.484375]\n",
      "2314: [discriminator loss: 0.580102, acc: 0.703125] [adversarial loss: 1.472356, acc: 0.125000]\n",
      "2315: [discriminator loss: 0.514781, acc: 0.742188] [adversarial loss: 0.920993, acc: 0.390625]\n",
      "2316: [discriminator loss: 0.554041, acc: 0.750000] [adversarial loss: 1.608506, acc: 0.156250]\n",
      "2317: [discriminator loss: 0.474131, acc: 0.765625] [adversarial loss: 1.067785, acc: 0.359375]\n",
      "2318: [discriminator loss: 0.439593, acc: 0.781250] [adversarial loss: 1.317825, acc: 0.234375]\n",
      "2319: [discriminator loss: 0.521895, acc: 0.734375] [adversarial loss: 1.151982, acc: 0.312500]\n",
      "2320: [discriminator loss: 0.476365, acc: 0.773438] [adversarial loss: 1.398378, acc: 0.203125]\n",
      "2321: [discriminator loss: 0.478198, acc: 0.757812] [adversarial loss: 1.249446, acc: 0.281250]\n",
      "2322: [discriminator loss: 0.537201, acc: 0.726562] [adversarial loss: 1.056333, acc: 0.359375]\n",
      "2323: [discriminator loss: 0.417208, acc: 0.804688] [adversarial loss: 1.851240, acc: 0.093750]\n",
      "2324: [discriminator loss: 0.475125, acc: 0.804688] [adversarial loss: 1.036226, acc: 0.296875]\n",
      "2325: [discriminator loss: 0.492071, acc: 0.796875] [adversarial loss: 1.333697, acc: 0.187500]\n",
      "2326: [discriminator loss: 0.475506, acc: 0.757812] [adversarial loss: 1.197469, acc: 0.187500]\n",
      "2327: [discriminator loss: 0.522539, acc: 0.750000] [adversarial loss: 1.702133, acc: 0.046875]\n",
      "2328: [discriminator loss: 0.449708, acc: 0.750000] [adversarial loss: 0.811260, acc: 0.484375]\n",
      "2329: [discriminator loss: 0.514145, acc: 0.726562] [adversarial loss: 2.222093, acc: 0.078125]\n",
      "2330: [discriminator loss: 0.483212, acc: 0.796875] [adversarial loss: 1.062659, acc: 0.359375]\n",
      "2331: [discriminator loss: 0.465467, acc: 0.757812] [adversarial loss: 1.519410, acc: 0.156250]\n",
      "2332: [discriminator loss: 0.466884, acc: 0.804688] [adversarial loss: 0.923717, acc: 0.390625]\n",
      "2333: [discriminator loss: 0.540504, acc: 0.742188] [adversarial loss: 1.498404, acc: 0.140625]\n",
      "2334: [discriminator loss: 0.503046, acc: 0.781250] [adversarial loss: 1.152134, acc: 0.250000]\n",
      "2335: [discriminator loss: 0.581087, acc: 0.632812] [adversarial loss: 2.019887, acc: 0.062500]\n",
      "2336: [discriminator loss: 0.544631, acc: 0.726562] [adversarial loss: 0.780762, acc: 0.515625]\n",
      "2337: [discriminator loss: 0.610833, acc: 0.695312] [adversarial loss: 2.041424, acc: 0.046875]\n",
      "2338: [discriminator loss: 0.610165, acc: 0.648438] [adversarial loss: 1.038344, acc: 0.312500]\n",
      "2339: [discriminator loss: 0.467209, acc: 0.773438] [adversarial loss: 1.630040, acc: 0.062500]\n",
      "2340: [discriminator loss: 0.525442, acc: 0.742188] [adversarial loss: 0.884940, acc: 0.437500]\n",
      "2341: [discriminator loss: 0.543055, acc: 0.695312] [adversarial loss: 1.807986, acc: 0.062500]\n",
      "2342: [discriminator loss: 0.503244, acc: 0.750000] [adversarial loss: 0.991518, acc: 0.343750]\n",
      "2343: [discriminator loss: 0.537153, acc: 0.757812] [adversarial loss: 1.363209, acc: 0.156250]\n",
      "2344: [discriminator loss: 0.421047, acc: 0.812500] [adversarial loss: 1.117866, acc: 0.203125]\n",
      "2345: [discriminator loss: 0.490315, acc: 0.804688] [adversarial loss: 1.331111, acc: 0.218750]\n",
      "2346: [discriminator loss: 0.535064, acc: 0.718750] [adversarial loss: 1.623018, acc: 0.125000]\n",
      "2347: [discriminator loss: 0.433544, acc: 0.789062] [adversarial loss: 1.292972, acc: 0.203125]\n",
      "2348: [discriminator loss: 0.456381, acc: 0.828125] [adversarial loss: 1.402870, acc: 0.187500]\n",
      "2349: [discriminator loss: 0.458769, acc: 0.820312] [adversarial loss: 1.376713, acc: 0.109375]\n",
      "2350: [discriminator loss: 0.467775, acc: 0.804688] [adversarial loss: 1.427712, acc: 0.203125]\n",
      "2351: [discriminator loss: 0.495199, acc: 0.781250] [adversarial loss: 1.395812, acc: 0.156250]\n",
      "2352: [discriminator loss: 0.435841, acc: 0.796875] [adversarial loss: 1.588591, acc: 0.140625]\n",
      "2353: [discriminator loss: 0.482645, acc: 0.773438] [adversarial loss: 0.996858, acc: 0.375000]\n",
      "2354: [discriminator loss: 0.511823, acc: 0.718750] [adversarial loss: 1.570192, acc: 0.156250]\n",
      "2355: [discriminator loss: 0.455981, acc: 0.804688] [adversarial loss: 1.192071, acc: 0.265625]\n",
      "2356: [discriminator loss: 0.492072, acc: 0.734375] [adversarial loss: 1.612532, acc: 0.156250]\n",
      "2357: [discriminator loss: 0.485965, acc: 0.726562] [adversarial loss: 1.325275, acc: 0.281250]\n",
      "2358: [discriminator loss: 0.470292, acc: 0.796875] [adversarial loss: 1.972614, acc: 0.078125]\n",
      "2359: [discriminator loss: 0.504590, acc: 0.765625] [adversarial loss: 1.039515, acc: 0.328125]\n",
      "2360: [discriminator loss: 0.480044, acc: 0.773438] [adversarial loss: 1.973108, acc: 0.062500]\n",
      "2361: [discriminator loss: 0.658071, acc: 0.671875] [adversarial loss: 0.731801, acc: 0.437500]\n",
      "2362: [discriminator loss: 0.593316, acc: 0.671875] [adversarial loss: 2.099275, acc: 0.046875]\n",
      "2363: [discriminator loss: 0.528420, acc: 0.710938] [adversarial loss: 1.135721, acc: 0.296875]\n",
      "2364: [discriminator loss: 0.436527, acc: 0.835938] [adversarial loss: 1.660023, acc: 0.062500]\n",
      "2365: [discriminator loss: 0.472152, acc: 0.804688] [adversarial loss: 1.201327, acc: 0.218750]\n",
      "2366: [discriminator loss: 0.449537, acc: 0.812500] [adversarial loss: 1.352926, acc: 0.171875]\n",
      "2367: [discriminator loss: 0.504025, acc: 0.789062] [adversarial loss: 1.473163, acc: 0.125000]\n",
      "2368: [discriminator loss: 0.522287, acc: 0.718750] [adversarial loss: 1.410250, acc: 0.187500]\n",
      "2369: [discriminator loss: 0.536282, acc: 0.718750] [adversarial loss: 1.127445, acc: 0.312500]\n",
      "2370: [discriminator loss: 0.475276, acc: 0.804688] [adversarial loss: 1.734318, acc: 0.093750]\n",
      "2371: [discriminator loss: 0.450142, acc: 0.781250] [adversarial loss: 0.936568, acc: 0.406250]\n",
      "2372: [discriminator loss: 0.533683, acc: 0.734375] [adversarial loss: 1.726127, acc: 0.125000]\n",
      "2373: [discriminator loss: 0.510031, acc: 0.750000] [adversarial loss: 0.886515, acc: 0.406250]\n",
      "2374: [discriminator loss: 0.463799, acc: 0.757812] [adversarial loss: 1.803406, acc: 0.109375]\n",
      "2375: [discriminator loss: 0.579104, acc: 0.726562] [adversarial loss: 0.950240, acc: 0.265625]\n",
      "2376: [discriminator loss: 0.535429, acc: 0.718750] [adversarial loss: 1.904196, acc: 0.031250]\n",
      "2377: [discriminator loss: 0.545709, acc: 0.726562] [adversarial loss: 0.969299, acc: 0.375000]\n",
      "2378: [discriminator loss: 0.541774, acc: 0.726562] [adversarial loss: 1.546727, acc: 0.109375]\n",
      "2379: [discriminator loss: 0.492048, acc: 0.750000] [adversarial loss: 1.540139, acc: 0.125000]\n",
      "2380: [discriminator loss: 0.439961, acc: 0.773438] [adversarial loss: 1.462913, acc: 0.125000]\n",
      "2381: [discriminator loss: 0.543444, acc: 0.757812] [adversarial loss: 1.177861, acc: 0.343750]\n",
      "2382: [discriminator loss: 0.485618, acc: 0.757812] [adversarial loss: 1.739127, acc: 0.062500]\n",
      "2383: [discriminator loss: 0.496180, acc: 0.773438] [adversarial loss: 1.033031, acc: 0.359375]\n",
      "2384: [discriminator loss: 0.618482, acc: 0.695312] [adversarial loss: 1.837119, acc: 0.093750]\n",
      "2385: [discriminator loss: 0.578122, acc: 0.687500] [adversarial loss: 1.066210, acc: 0.265625]\n",
      "2386: [discriminator loss: 0.431198, acc: 0.812500] [adversarial loss: 1.949771, acc: 0.062500]\n",
      "2387: [discriminator loss: 0.549950, acc: 0.718750] [adversarial loss: 0.890899, acc: 0.406250]\n",
      "2388: [discriminator loss: 0.514519, acc: 0.703125] [adversarial loss: 1.639403, acc: 0.125000]\n",
      "2389: [discriminator loss: 0.560401, acc: 0.750000] [adversarial loss: 0.979034, acc: 0.359375]\n",
      "2390: [discriminator loss: 0.520706, acc: 0.781250] [adversarial loss: 1.700259, acc: 0.109375]\n",
      "2391: [discriminator loss: 0.474228, acc: 0.765625] [adversarial loss: 1.044061, acc: 0.390625]\n",
      "2392: [discriminator loss: 0.465766, acc: 0.804688] [adversarial loss: 1.526944, acc: 0.171875]\n",
      "2393: [discriminator loss: 0.467075, acc: 0.796875] [adversarial loss: 1.294801, acc: 0.296875]\n",
      "2394: [discriminator loss: 0.489631, acc: 0.812500] [adversarial loss: 1.766609, acc: 0.046875]\n",
      "2395: [discriminator loss: 0.513957, acc: 0.781250] [adversarial loss: 0.986215, acc: 0.359375]\n",
      "2396: [discriminator loss: 0.593924, acc: 0.703125] [adversarial loss: 1.707743, acc: 0.093750]\n",
      "2397: [discriminator loss: 0.460997, acc: 0.820312] [adversarial loss: 1.050493, acc: 0.328125]\n",
      "2398: [discriminator loss: 0.426101, acc: 0.812500] [adversarial loss: 1.548583, acc: 0.078125]\n",
      "2399: [discriminator loss: 0.515261, acc: 0.765625] [adversarial loss: 1.152617, acc: 0.218750]\n",
      "2400: [discriminator loss: 0.458797, acc: 0.789062] [adversarial loss: 1.463569, acc: 0.140625]\n",
      "2401: [discriminator loss: 0.481332, acc: 0.773438] [adversarial loss: 1.398370, acc: 0.234375]\n",
      "2402: [discriminator loss: 0.549867, acc: 0.695312] [adversarial loss: 1.456337, acc: 0.203125]\n",
      "2403: [discriminator loss: 0.472753, acc: 0.773438] [adversarial loss: 1.327586, acc: 0.218750]\n",
      "2404: [discriminator loss: 0.513679, acc: 0.726562] [adversarial loss: 0.997680, acc: 0.359375]\n",
      "2405: [discriminator loss: 0.444056, acc: 0.820312] [adversarial loss: 1.609013, acc: 0.156250]\n",
      "2406: [discriminator loss: 0.487751, acc: 0.781250] [adversarial loss: 0.875201, acc: 0.437500]\n",
      "2407: [discriminator loss: 0.609265, acc: 0.687500] [adversarial loss: 2.183112, acc: 0.015625]\n",
      "2408: [discriminator loss: 0.504987, acc: 0.750000] [adversarial loss: 0.872359, acc: 0.375000]\n",
      "2409: [discriminator loss: 0.523715, acc: 0.726562] [adversarial loss: 1.759320, acc: 0.109375]\n",
      "2410: [discriminator loss: 0.499491, acc: 0.726562] [adversarial loss: 1.122849, acc: 0.203125]\n",
      "2411: [discriminator loss: 0.436363, acc: 0.828125] [adversarial loss: 1.544301, acc: 0.140625]\n",
      "2412: [discriminator loss: 0.489480, acc: 0.765625] [adversarial loss: 1.109624, acc: 0.265625]\n",
      "2413: [discriminator loss: 0.456394, acc: 0.781250] [adversarial loss: 1.526825, acc: 0.187500]\n",
      "2414: [discriminator loss: 0.455227, acc: 0.789062] [adversarial loss: 1.287062, acc: 0.203125]\n",
      "2415: [discriminator loss: 0.410650, acc: 0.804688] [adversarial loss: 1.520651, acc: 0.109375]\n",
      "2416: [discriminator loss: 0.517829, acc: 0.726562] [adversarial loss: 0.994172, acc: 0.437500]\n",
      "2417: [discriminator loss: 0.619541, acc: 0.656250] [adversarial loss: 1.476256, acc: 0.171875]\n",
      "2418: [discriminator loss: 0.549446, acc: 0.695312] [adversarial loss: 1.020434, acc: 0.312500]\n",
      "2419: [discriminator loss: 0.483541, acc: 0.742188] [adversarial loss: 1.281387, acc: 0.187500]\n",
      "2420: [discriminator loss: 0.466466, acc: 0.796875] [adversarial loss: 1.311843, acc: 0.171875]\n",
      "2421: [discriminator loss: 0.431905, acc: 0.781250] [adversarial loss: 1.582419, acc: 0.156250]\n",
      "2422: [discriminator loss: 0.485843, acc: 0.773438] [adversarial loss: 0.932132, acc: 0.406250]\n",
      "2423: [discriminator loss: 0.512584, acc: 0.757812] [adversarial loss: 1.580884, acc: 0.125000]\n",
      "2424: [discriminator loss: 0.477793, acc: 0.773438] [adversarial loss: 1.115478, acc: 0.296875]\n",
      "2425: [discriminator loss: 0.521478, acc: 0.679688] [adversarial loss: 1.976933, acc: 0.078125]\n",
      "2426: [discriminator loss: 0.515576, acc: 0.726562] [adversarial loss: 0.873487, acc: 0.500000]\n",
      "2427: [discriminator loss: 0.498551, acc: 0.750000] [adversarial loss: 1.875087, acc: 0.078125]\n",
      "2428: [discriminator loss: 0.536670, acc: 0.726562] [adversarial loss: 0.812542, acc: 0.546875]\n",
      "2429: [discriminator loss: 0.471381, acc: 0.796875] [adversarial loss: 1.523333, acc: 0.109375]\n",
      "2430: [discriminator loss: 0.523141, acc: 0.750000] [adversarial loss: 1.048927, acc: 0.390625]\n",
      "2431: [discriminator loss: 0.506588, acc: 0.757812] [adversarial loss: 1.588223, acc: 0.171875]\n",
      "2432: [discriminator loss: 0.529104, acc: 0.718750] [adversarial loss: 1.449273, acc: 0.171875]\n",
      "2433: [discriminator loss: 0.472656, acc: 0.773438] [adversarial loss: 0.996636, acc: 0.375000]\n",
      "2434: [discriminator loss: 0.474508, acc: 0.781250] [adversarial loss: 1.712657, acc: 0.078125]\n",
      "2435: [discriminator loss: 0.470058, acc: 0.804688] [adversarial loss: 1.237380, acc: 0.265625]\n",
      "2436: [discriminator loss: 0.554955, acc: 0.718750] [adversarial loss: 1.672601, acc: 0.062500]\n",
      "2437: [discriminator loss: 0.496306, acc: 0.750000] [adversarial loss: 0.903483, acc: 0.500000]\n",
      "2438: [discriminator loss: 0.449550, acc: 0.765625] [adversarial loss: 1.336681, acc: 0.218750]\n",
      "2439: [discriminator loss: 0.508216, acc: 0.773438] [adversarial loss: 1.390224, acc: 0.171875]\n",
      "2440: [discriminator loss: 0.461070, acc: 0.804688] [adversarial loss: 1.398989, acc: 0.250000]\n",
      "2441: [discriminator loss: 0.411068, acc: 0.851562] [adversarial loss: 1.562583, acc: 0.125000]\n",
      "2442: [discriminator loss: 0.499304, acc: 0.726562] [adversarial loss: 0.947367, acc: 0.390625]\n",
      "2443: [discriminator loss: 0.513027, acc: 0.742188] [adversarial loss: 1.740416, acc: 0.140625]\n",
      "2444: [discriminator loss: 0.550515, acc: 0.750000] [adversarial loss: 1.231969, acc: 0.296875]\n",
      "2445: [discriminator loss: 0.558239, acc: 0.757812] [adversarial loss: 1.956273, acc: 0.078125]\n",
      "2446: [discriminator loss: 0.513320, acc: 0.726562] [adversarial loss: 0.960567, acc: 0.421875]\n",
      "2447: [discriminator loss: 0.540370, acc: 0.679688] [adversarial loss: 2.045656, acc: 0.015625]\n",
      "2448: [discriminator loss: 0.461758, acc: 0.812500] [adversarial loss: 1.024789, acc: 0.484375]\n",
      "2449: [discriminator loss: 0.519861, acc: 0.734375] [adversarial loss: 1.299523, acc: 0.250000]\n",
      "2450: [discriminator loss: 0.460281, acc: 0.796875] [adversarial loss: 1.163499, acc: 0.281250]\n",
      "2451: [discriminator loss: 0.497213, acc: 0.765625] [adversarial loss: 1.268998, acc: 0.156250]\n",
      "2452: [discriminator loss: 0.412589, acc: 0.828125] [adversarial loss: 1.301252, acc: 0.203125]\n",
      "2453: [discriminator loss: 0.467091, acc: 0.804688] [adversarial loss: 1.547116, acc: 0.187500]\n",
      "2454: [discriminator loss: 0.479315, acc: 0.773438] [adversarial loss: 1.314271, acc: 0.187500]\n",
      "2455: [discriminator loss: 0.473358, acc: 0.789062] [adversarial loss: 1.151468, acc: 0.187500]\n",
      "2456: [discriminator loss: 0.566256, acc: 0.695312] [adversarial loss: 1.618089, acc: 0.140625]\n",
      "2457: [discriminator loss: 0.540689, acc: 0.726562] [adversarial loss: 1.265658, acc: 0.171875]\n",
      "2458: [discriminator loss: 0.375570, acc: 0.851562] [adversarial loss: 1.523462, acc: 0.125000]\n",
      "2459: [discriminator loss: 0.447671, acc: 0.796875] [adversarial loss: 1.341262, acc: 0.203125]\n",
      "2460: [discriminator loss: 0.494804, acc: 0.781250] [adversarial loss: 1.619358, acc: 0.125000]\n",
      "2461: [discriminator loss: 0.450982, acc: 0.781250] [adversarial loss: 1.204216, acc: 0.250000]\n",
      "2462: [discriminator loss: 0.450215, acc: 0.804688] [adversarial loss: 1.509339, acc: 0.187500]\n",
      "2463: [discriminator loss: 0.567292, acc: 0.734375] [adversarial loss: 0.917587, acc: 0.453125]\n",
      "2464: [discriminator loss: 0.608563, acc: 0.695312] [adversarial loss: 2.043740, acc: 0.078125]\n",
      "2465: [discriminator loss: 0.533320, acc: 0.742188] [adversarial loss: 1.054589, acc: 0.390625]\n",
      "2466: [discriminator loss: 0.493024, acc: 0.750000] [adversarial loss: 1.534818, acc: 0.171875]\n",
      "2467: [discriminator loss: 0.444650, acc: 0.812500] [adversarial loss: 0.942193, acc: 0.375000]\n",
      "2468: [discriminator loss: 0.532630, acc: 0.710938] [adversarial loss: 1.666364, acc: 0.093750]\n",
      "2469: [discriminator loss: 0.476677, acc: 0.757812] [adversarial loss: 1.044092, acc: 0.281250]\n",
      "2470: [discriminator loss: 0.587119, acc: 0.703125] [adversarial loss: 2.024260, acc: 0.062500]\n",
      "2471: [discriminator loss: 0.479179, acc: 0.757812] [adversarial loss: 0.952154, acc: 0.421875]\n",
      "2472: [discriminator loss: 0.521350, acc: 0.710938] [adversarial loss: 1.968293, acc: 0.093750]\n",
      "2473: [discriminator loss: 0.552338, acc: 0.726562] [adversarial loss: 0.826465, acc: 0.468750]\n",
      "2474: [discriminator loss: 0.548927, acc: 0.710938] [adversarial loss: 1.792815, acc: 0.046875]\n",
      "2475: [discriminator loss: 0.516151, acc: 0.726562] [adversarial loss: 1.175769, acc: 0.218750]\n",
      "2476: [discriminator loss: 0.446038, acc: 0.828125] [adversarial loss: 1.395242, acc: 0.140625]\n",
      "2477: [discriminator loss: 0.530530, acc: 0.726562] [adversarial loss: 1.208223, acc: 0.218750]\n",
      "2478: [discriminator loss: 0.461552, acc: 0.812500] [adversarial loss: 1.115159, acc: 0.312500]\n",
      "2479: [discriminator loss: 0.482998, acc: 0.742188] [adversarial loss: 1.557259, acc: 0.171875]\n",
      "2480: [discriminator loss: 0.459212, acc: 0.773438] [adversarial loss: 1.017782, acc: 0.359375]\n",
      "2481: [discriminator loss: 0.469283, acc: 0.765625] [adversarial loss: 1.695549, acc: 0.093750]\n",
      "2482: [discriminator loss: 0.483755, acc: 0.765625] [adversarial loss: 0.981143, acc: 0.375000]\n",
      "2483: [discriminator loss: 0.469495, acc: 0.789062] [adversarial loss: 1.480829, acc: 0.171875]\n",
      "2484: [discriminator loss: 0.486430, acc: 0.734375] [adversarial loss: 1.529185, acc: 0.093750]\n",
      "2485: [discriminator loss: 0.533061, acc: 0.710938] [adversarial loss: 0.983747, acc: 0.437500]\n",
      "2486: [discriminator loss: 0.435815, acc: 0.789062] [adversarial loss: 1.651450, acc: 0.156250]\n",
      "2487: [discriminator loss: 0.415741, acc: 0.820312] [adversarial loss: 1.291718, acc: 0.234375]\n",
      "2488: [discriminator loss: 0.521539, acc: 0.710938] [adversarial loss: 1.809022, acc: 0.078125]\n",
      "2489: [discriminator loss: 0.482417, acc: 0.781250] [adversarial loss: 0.761300, acc: 0.515625]\n",
      "2490: [discriminator loss: 0.597058, acc: 0.656250] [adversarial loss: 2.132732, acc: 0.031250]\n",
      "2491: [discriminator loss: 0.537846, acc: 0.773438] [adversarial loss: 0.857839, acc: 0.421875]\n",
      "2492: [discriminator loss: 0.530906, acc: 0.750000] [adversarial loss: 1.939277, acc: 0.078125]\n",
      "2493: [discriminator loss: 0.447856, acc: 0.750000] [adversarial loss: 1.034720, acc: 0.406250]\n",
      "2494: [discriminator loss: 0.528265, acc: 0.734375] [adversarial loss: 1.341462, acc: 0.187500]\n",
      "2495: [discriminator loss: 0.514027, acc: 0.781250] [adversarial loss: 1.609514, acc: 0.078125]\n",
      "2496: [discriminator loss: 0.539840, acc: 0.757812] [adversarial loss: 1.095926, acc: 0.281250]\n",
      "2497: [discriminator loss: 0.467337, acc: 0.804688] [adversarial loss: 1.475395, acc: 0.125000]\n",
      "2498: [discriminator loss: 0.525622, acc: 0.742188] [adversarial loss: 1.058547, acc: 0.421875]\n",
      "2499: [discriminator loss: 0.436815, acc: 0.804688] [adversarial loss: 1.519205, acc: 0.187500]\n",
      "2500: [discriminator loss: 0.523315, acc: 0.718750] [adversarial loss: 1.257951, acc: 0.234375]\n",
      "2501: [discriminator loss: 0.489982, acc: 0.804688] [adversarial loss: 1.514985, acc: 0.171875]\n",
      "2502: [discriminator loss: 0.551508, acc: 0.703125] [adversarial loss: 0.964540, acc: 0.328125]\n",
      "2503: [discriminator loss: 0.449467, acc: 0.820312] [adversarial loss: 1.765947, acc: 0.093750]\n",
      "2504: [discriminator loss: 0.545060, acc: 0.750000] [adversarial loss: 1.161293, acc: 0.281250]\n",
      "2505: [discriminator loss: 0.444259, acc: 0.820312] [adversarial loss: 1.367436, acc: 0.171875]\n",
      "2506: [discriminator loss: 0.431198, acc: 0.835938] [adversarial loss: 1.589106, acc: 0.093750]\n",
      "2507: [discriminator loss: 0.406757, acc: 0.820312] [adversarial loss: 1.368118, acc: 0.156250]\n",
      "2508: [discriminator loss: 0.522394, acc: 0.734375] [adversarial loss: 1.343578, acc: 0.265625]\n",
      "2509: [discriminator loss: 0.444801, acc: 0.812500] [adversarial loss: 1.215749, acc: 0.265625]\n",
      "2510: [discriminator loss: 0.496114, acc: 0.726562] [adversarial loss: 1.288266, acc: 0.312500]\n",
      "2511: [discriminator loss: 0.512040, acc: 0.765625] [adversarial loss: 1.457137, acc: 0.125000]\n",
      "2512: [discriminator loss: 0.442937, acc: 0.812500] [adversarial loss: 1.700611, acc: 0.093750]\n",
      "2513: [discriminator loss: 0.604863, acc: 0.664062] [adversarial loss: 1.481090, acc: 0.156250]\n",
      "2514: [discriminator loss: 0.408344, acc: 0.828125] [adversarial loss: 1.350106, acc: 0.171875]\n",
      "2515: [discriminator loss: 0.429879, acc: 0.804688] [adversarial loss: 1.819694, acc: 0.109375]\n",
      "2516: [discriminator loss: 0.426508, acc: 0.804688] [adversarial loss: 1.258877, acc: 0.234375]\n",
      "2517: [discriminator loss: 0.551064, acc: 0.734375] [adversarial loss: 2.246588, acc: 0.078125]\n",
      "2518: [discriminator loss: 0.442270, acc: 0.789062] [adversarial loss: 0.989693, acc: 0.390625]\n",
      "2519: [discriminator loss: 0.544928, acc: 0.734375] [adversarial loss: 1.873191, acc: 0.109375]\n",
      "2520: [discriminator loss: 0.508934, acc: 0.726562] [adversarial loss: 1.125450, acc: 0.359375]\n",
      "2521: [discriminator loss: 0.479584, acc: 0.773438] [adversarial loss: 1.688440, acc: 0.156250]\n",
      "2522: [discriminator loss: 0.467742, acc: 0.789062] [adversarial loss: 1.244576, acc: 0.234375]\n",
      "2523: [discriminator loss: 0.458789, acc: 0.757812] [adversarial loss: 1.567056, acc: 0.093750]\n",
      "2524: [discriminator loss: 0.486076, acc: 0.765625] [adversarial loss: 0.994479, acc: 0.375000]\n",
      "2525: [discriminator loss: 0.458856, acc: 0.750000] [adversarial loss: 1.938053, acc: 0.078125]\n",
      "2526: [discriminator loss: 0.534808, acc: 0.718750] [adversarial loss: 0.737095, acc: 0.593750]\n",
      "2527: [discriminator loss: 0.593398, acc: 0.664062] [adversarial loss: 1.988915, acc: 0.031250]\n",
      "2528: [discriminator loss: 0.524331, acc: 0.734375] [adversarial loss: 0.934782, acc: 0.437500]\n",
      "2529: [discriminator loss: 0.472098, acc: 0.765625] [adversarial loss: 1.832396, acc: 0.078125]\n",
      "2530: [discriminator loss: 0.462759, acc: 0.765625] [adversarial loss: 1.067387, acc: 0.359375]\n",
      "2531: [discriminator loss: 0.408522, acc: 0.828125] [adversarial loss: 1.359916, acc: 0.171875]\n",
      "2532: [discriminator loss: 0.487734, acc: 0.765625] [adversarial loss: 2.033958, acc: 0.062500]\n",
      "2533: [discriminator loss: 0.521981, acc: 0.726562] [adversarial loss: 0.979116, acc: 0.328125]\n",
      "2534: [discriminator loss: 0.611621, acc: 0.679688] [adversarial loss: 2.071884, acc: 0.078125]\n",
      "2535: [discriminator loss: 0.556020, acc: 0.718750] [adversarial loss: 0.903193, acc: 0.421875]\n",
      "2536: [discriminator loss: 0.520765, acc: 0.750000] [adversarial loss: 1.517964, acc: 0.109375]\n",
      "2537: [discriminator loss: 0.476480, acc: 0.773438] [adversarial loss: 1.502157, acc: 0.203125]\n",
      "2538: [discriminator loss: 0.492492, acc: 0.796875] [adversarial loss: 0.946089, acc: 0.406250]\n",
      "2539: [discriminator loss: 0.525714, acc: 0.773438] [adversarial loss: 1.423333, acc: 0.125000]\n",
      "2540: [discriminator loss: 0.497635, acc: 0.734375] [adversarial loss: 1.330227, acc: 0.203125]\n",
      "2541: [discriminator loss: 0.503690, acc: 0.781250] [adversarial loss: 1.422119, acc: 0.171875]\n",
      "2542: [discriminator loss: 0.486330, acc: 0.750000] [adversarial loss: 1.430549, acc: 0.156250]\n",
      "2543: [discriminator loss: 0.482838, acc: 0.781250] [adversarial loss: 1.172503, acc: 0.296875]\n",
      "2544: [discriminator loss: 0.532187, acc: 0.703125] [adversarial loss: 1.252449, acc: 0.187500]\n",
      "2545: [discriminator loss: 0.488272, acc: 0.765625] [adversarial loss: 0.975157, acc: 0.375000]\n",
      "2546: [discriminator loss: 0.415105, acc: 0.851562] [adversarial loss: 1.579940, acc: 0.171875]\n",
      "2547: [discriminator loss: 0.505986, acc: 0.757812] [adversarial loss: 1.072191, acc: 0.375000]\n",
      "2548: [discriminator loss: 0.612410, acc: 0.703125] [adversarial loss: 1.809124, acc: 0.156250]\n",
      "2549: [discriminator loss: 0.514169, acc: 0.726562] [adversarial loss: 0.947456, acc: 0.421875]\n",
      "2550: [discriminator loss: 0.562128, acc: 0.742188] [adversarial loss: 2.006060, acc: 0.031250]\n",
      "2551: [discriminator loss: 0.481503, acc: 0.750000] [adversarial loss: 0.954574, acc: 0.390625]\n",
      "2552: [discriminator loss: 0.549080, acc: 0.718750] [adversarial loss: 2.041020, acc: 0.031250]\n",
      "2553: [discriminator loss: 0.446876, acc: 0.781250] [adversarial loss: 1.082254, acc: 0.296875]\n",
      "2554: [discriminator loss: 0.479249, acc: 0.757812] [adversarial loss: 2.005315, acc: 0.031250]\n",
      "2555: [discriminator loss: 0.531284, acc: 0.757812] [adversarial loss: 0.875069, acc: 0.375000]\n",
      "2556: [discriminator loss: 0.517839, acc: 0.710938] [adversarial loss: 1.816100, acc: 0.125000]\n",
      "2557: [discriminator loss: 0.477610, acc: 0.757812] [adversarial loss: 1.017962, acc: 0.359375]\n",
      "2558: [discriminator loss: 0.502408, acc: 0.789062] [adversarial loss: 1.591133, acc: 0.156250]\n",
      "2559: [discriminator loss: 0.574357, acc: 0.679688] [adversarial loss: 1.479335, acc: 0.171875]\n",
      "2560: [discriminator loss: 0.530483, acc: 0.750000] [adversarial loss: 1.121279, acc: 0.265625]\n",
      "2561: [discriminator loss: 0.445700, acc: 0.789062] [adversarial loss: 1.594256, acc: 0.125000]\n",
      "2562: [discriminator loss: 0.475744, acc: 0.789062] [adversarial loss: 1.570690, acc: 0.187500]\n",
      "2563: [discriminator loss: 0.523724, acc: 0.750000] [adversarial loss: 1.304090, acc: 0.250000]\n",
      "2564: [discriminator loss: 0.482582, acc: 0.750000] [adversarial loss: 1.484353, acc: 0.125000]\n",
      "2565: [discriminator loss: 0.564880, acc: 0.703125] [adversarial loss: 1.347478, acc: 0.171875]\n",
      "2566: [discriminator loss: 0.449065, acc: 0.796875] [adversarial loss: 1.281052, acc: 0.218750]\n",
      "2567: [discriminator loss: 0.549177, acc: 0.734375] [adversarial loss: 1.662991, acc: 0.109375]\n",
      "2568: [discriminator loss: 0.517028, acc: 0.695312] [adversarial loss: 1.426366, acc: 0.234375]\n",
      "2569: [discriminator loss: 0.539186, acc: 0.750000] [adversarial loss: 1.044995, acc: 0.343750]\n",
      "2570: [discriminator loss: 0.474192, acc: 0.765625] [adversarial loss: 1.283918, acc: 0.281250]\n",
      "2571: [discriminator loss: 0.479131, acc: 0.789062] [adversarial loss: 1.256806, acc: 0.218750]\n",
      "2572: [discriminator loss: 0.482856, acc: 0.804688] [adversarial loss: 1.193969, acc: 0.281250]\n",
      "2573: [discriminator loss: 0.487518, acc: 0.781250] [adversarial loss: 1.506073, acc: 0.140625]\n",
      "2574: [discriminator loss: 0.490792, acc: 0.773438] [adversarial loss: 1.148609, acc: 0.281250]\n",
      "2575: [discriminator loss: 0.490037, acc: 0.789062] [adversarial loss: 1.113439, acc: 0.250000]\n",
      "2576: [discriminator loss: 0.450448, acc: 0.781250] [adversarial loss: 1.845370, acc: 0.125000]\n",
      "2577: [discriminator loss: 0.471069, acc: 0.781250] [adversarial loss: 0.914801, acc: 0.453125]\n",
      "2578: [discriminator loss: 0.572696, acc: 0.687500] [adversarial loss: 2.172813, acc: 0.046875]\n",
      "2579: [discriminator loss: 0.538577, acc: 0.710938] [adversarial loss: 0.816212, acc: 0.468750]\n",
      "2580: [discriminator loss: 0.487733, acc: 0.781250] [adversarial loss: 2.016211, acc: 0.015625]\n",
      "2581: [discriminator loss: 0.486636, acc: 0.765625] [adversarial loss: 0.994313, acc: 0.375000]\n",
      "2582: [discriminator loss: 0.515850, acc: 0.703125] [adversarial loss: 1.670506, acc: 0.140625]\n",
      "2583: [discriminator loss: 0.556868, acc: 0.710938] [adversarial loss: 1.047837, acc: 0.312500]\n",
      "2584: [discriminator loss: 0.518249, acc: 0.750000] [adversarial loss: 1.601340, acc: 0.203125]\n",
      "2585: [discriminator loss: 0.464435, acc: 0.796875] [adversarial loss: 1.051337, acc: 0.281250]\n",
      "2586: [discriminator loss: 0.559598, acc: 0.750000] [adversarial loss: 1.429997, acc: 0.187500]\n",
      "2587: [discriminator loss: 0.424846, acc: 0.820312] [adversarial loss: 1.493896, acc: 0.078125]\n",
      "2588: [discriminator loss: 0.459418, acc: 0.796875] [adversarial loss: 1.771282, acc: 0.109375]\n",
      "2589: [discriminator loss: 0.547257, acc: 0.734375] [adversarial loss: 0.968904, acc: 0.484375]\n",
      "2590: [discriminator loss: 0.466712, acc: 0.757812] [adversarial loss: 2.008783, acc: 0.078125]\n",
      "2591: [discriminator loss: 0.453951, acc: 0.781250] [adversarial loss: 1.079456, acc: 0.390625]\n",
      "2592: [discriminator loss: 0.551212, acc: 0.726562] [adversarial loss: 1.667170, acc: 0.125000]\n",
      "2593: [discriminator loss: 0.480730, acc: 0.789062] [adversarial loss: 1.179070, acc: 0.234375]\n",
      "2594: [discriminator loss: 0.488015, acc: 0.781250] [adversarial loss: 1.403365, acc: 0.218750]\n",
      "2595: [discriminator loss: 0.454535, acc: 0.812500] [adversarial loss: 1.178363, acc: 0.218750]\n",
      "2596: [discriminator loss: 0.481641, acc: 0.773438] [adversarial loss: 1.804017, acc: 0.031250]\n",
      "2597: [discriminator loss: 0.521100, acc: 0.750000] [adversarial loss: 1.001488, acc: 0.390625]\n",
      "2598: [discriminator loss: 0.577162, acc: 0.742188] [adversarial loss: 1.691917, acc: 0.093750]\n",
      "2599: [discriminator loss: 0.495284, acc: 0.757812] [adversarial loss: 0.858723, acc: 0.515625]\n",
      "2600: [discriminator loss: 0.503500, acc: 0.750000] [adversarial loss: 2.042858, acc: 0.031250]\n",
      "2601: [discriminator loss: 0.582301, acc: 0.718750] [adversarial loss: 0.916916, acc: 0.421875]\n",
      "2602: [discriminator loss: 0.618824, acc: 0.632812] [adversarial loss: 1.921996, acc: 0.062500]\n",
      "2603: [discriminator loss: 0.517055, acc: 0.734375] [adversarial loss: 0.886110, acc: 0.515625]\n",
      "2604: [discriminator loss: 0.483553, acc: 0.804688] [adversarial loss: 1.495858, acc: 0.125000]\n",
      "2605: [discriminator loss: 0.477980, acc: 0.765625] [adversarial loss: 1.290456, acc: 0.140625]\n",
      "2606: [discriminator loss: 0.526099, acc: 0.718750] [adversarial loss: 1.451606, acc: 0.093750]\n",
      "2607: [discriminator loss: 0.575897, acc: 0.687500] [adversarial loss: 1.515024, acc: 0.140625]\n",
      "2608: [discriminator loss: 0.506893, acc: 0.726562] [adversarial loss: 1.556946, acc: 0.125000]\n",
      "2609: [discriminator loss: 0.508110, acc: 0.742188] [adversarial loss: 0.913228, acc: 0.375000]\n",
      "2610: [discriminator loss: 0.511848, acc: 0.718750] [adversarial loss: 2.034510, acc: 0.093750]\n",
      "2611: [discriminator loss: 0.469417, acc: 0.757812] [adversarial loss: 0.968657, acc: 0.328125]\n",
      "2612: [discriminator loss: 0.556278, acc: 0.710938] [adversarial loss: 1.541221, acc: 0.062500]\n",
      "2613: [discriminator loss: 0.433174, acc: 0.796875] [adversarial loss: 1.235968, acc: 0.187500]\n",
      "2614: [discriminator loss: 0.454596, acc: 0.796875] [adversarial loss: 1.343253, acc: 0.156250]\n",
      "2615: [discriminator loss: 0.435197, acc: 0.835938] [adversarial loss: 1.163436, acc: 0.234375]\n",
      "2616: [discriminator loss: 0.482304, acc: 0.773438] [adversarial loss: 1.464790, acc: 0.156250]\n",
      "2617: [discriminator loss: 0.543722, acc: 0.703125] [adversarial loss: 1.040007, acc: 0.375000]\n",
      "2618: [discriminator loss: 0.506516, acc: 0.765625] [adversarial loss: 1.616790, acc: 0.078125]\n",
      "2619: [discriminator loss: 0.463984, acc: 0.828125] [adversarial loss: 1.303396, acc: 0.250000]\n",
      "2620: [discriminator loss: 0.546288, acc: 0.742188] [adversarial loss: 1.507559, acc: 0.171875]\n",
      "2621: [discriminator loss: 0.494833, acc: 0.757812] [adversarial loss: 0.800181, acc: 0.484375]\n",
      "2622: [discriminator loss: 0.551678, acc: 0.726562] [adversarial loss: 1.864184, acc: 0.078125]\n",
      "2623: [discriminator loss: 0.559924, acc: 0.726562] [adversarial loss: 0.867069, acc: 0.484375]\n",
      "2624: [discriminator loss: 0.523745, acc: 0.750000] [adversarial loss: 1.625018, acc: 0.140625]\n",
      "2625: [discriminator loss: 0.523421, acc: 0.781250] [adversarial loss: 1.050407, acc: 0.375000]\n",
      "2626: [discriminator loss: 0.501075, acc: 0.734375] [adversarial loss: 1.648273, acc: 0.062500]\n",
      "2627: [discriminator loss: 0.503276, acc: 0.757812] [adversarial loss: 1.180048, acc: 0.281250]\n",
      "2628: [discriminator loss: 0.494630, acc: 0.773438] [adversarial loss: 1.598782, acc: 0.140625]\n",
      "2629: [discriminator loss: 0.497935, acc: 0.742188] [adversarial loss: 1.453701, acc: 0.171875]\n",
      "2630: [discriminator loss: 0.434931, acc: 0.812500] [adversarial loss: 1.235617, acc: 0.234375]\n",
      "2631: [discriminator loss: 0.455354, acc: 0.773438] [adversarial loss: 1.100146, acc: 0.281250]\n",
      "2632: [discriminator loss: 0.524892, acc: 0.734375] [adversarial loss: 2.018646, acc: 0.015625]\n",
      "2633: [discriminator loss: 0.475022, acc: 0.765625] [adversarial loss: 0.884417, acc: 0.421875]\n",
      "2634: [discriminator loss: 0.481395, acc: 0.750000] [adversarial loss: 1.873098, acc: 0.140625]\n",
      "2635: [discriminator loss: 0.511489, acc: 0.734375] [adversarial loss: 1.147315, acc: 0.265625]\n",
      "2636: [discriminator loss: 0.504917, acc: 0.718750] [adversarial loss: 1.791302, acc: 0.093750]\n",
      "2637: [discriminator loss: 0.557951, acc: 0.718750] [adversarial loss: 1.207938, acc: 0.250000]\n",
      "2638: [discriminator loss: 0.452307, acc: 0.796875] [adversarial loss: 1.611947, acc: 0.187500]\n",
      "2639: [discriminator loss: 0.466761, acc: 0.804688] [adversarial loss: 1.031913, acc: 0.312500]\n",
      "2640: [discriminator loss: 0.482198, acc: 0.742188] [adversarial loss: 1.841448, acc: 0.031250]\n",
      "2641: [discriminator loss: 0.436917, acc: 0.789062] [adversarial loss: 1.247164, acc: 0.265625]\n",
      "2642: [discriminator loss: 0.445602, acc: 0.781250] [adversarial loss: 1.841365, acc: 0.109375]\n",
      "2643: [discriminator loss: 0.535117, acc: 0.703125] [adversarial loss: 1.314608, acc: 0.265625]\n",
      "2644: [discriminator loss: 0.507480, acc: 0.757812] [adversarial loss: 1.491010, acc: 0.140625]\n",
      "2645: [discriminator loss: 0.489635, acc: 0.757812] [adversarial loss: 1.260810, acc: 0.140625]\n",
      "2646: [discriminator loss: 0.456551, acc: 0.789062] [adversarial loss: 1.254053, acc: 0.296875]\n",
      "2647: [discriminator loss: 0.642394, acc: 0.648438] [adversarial loss: 1.470030, acc: 0.234375]\n",
      "2648: [discriminator loss: 0.480122, acc: 0.796875] [adversarial loss: 1.330734, acc: 0.234375]\n",
      "2649: [discriminator loss: 0.542659, acc: 0.742188] [adversarial loss: 1.465112, acc: 0.156250]\n",
      "2650: [discriminator loss: 0.430046, acc: 0.812500] [adversarial loss: 1.437146, acc: 0.140625]\n",
      "2651: [discriminator loss: 0.440345, acc: 0.781250] [adversarial loss: 1.743139, acc: 0.078125]\n",
      "2652: [discriminator loss: 0.435650, acc: 0.781250] [adversarial loss: 1.394284, acc: 0.109375]\n",
      "2653: [discriminator loss: 0.483443, acc: 0.757812] [adversarial loss: 1.443058, acc: 0.140625]\n",
      "2654: [discriminator loss: 0.528783, acc: 0.703125] [adversarial loss: 1.193658, acc: 0.390625]\n",
      "2655: [discriminator loss: 0.546992, acc: 0.750000] [adversarial loss: 1.671384, acc: 0.109375]\n",
      "2656: [discriminator loss: 0.483955, acc: 0.757812] [adversarial loss: 1.137544, acc: 0.281250]\n",
      "2657: [discriminator loss: 0.514925, acc: 0.757812] [adversarial loss: 1.399257, acc: 0.125000]\n",
      "2658: [discriminator loss: 0.474479, acc: 0.812500] [adversarial loss: 1.279711, acc: 0.171875]\n",
      "2659: [discriminator loss: 0.511630, acc: 0.742188] [adversarial loss: 1.186356, acc: 0.187500]\n",
      "2660: [discriminator loss: 0.507089, acc: 0.789062] [adversarial loss: 1.193536, acc: 0.234375]\n",
      "2661: [discriminator loss: 0.396428, acc: 0.789062] [adversarial loss: 1.549026, acc: 0.171875]\n",
      "2662: [discriminator loss: 0.551150, acc: 0.726562] [adversarial loss: 1.949592, acc: 0.140625]\n",
      "2663: [discriminator loss: 0.511209, acc: 0.757812] [adversarial loss: 0.804813, acc: 0.531250]\n",
      "2664: [discriminator loss: 0.560550, acc: 0.710938] [adversarial loss: 2.173945, acc: 0.031250]\n",
      "2665: [discriminator loss: 0.605126, acc: 0.710938] [adversarial loss: 0.945412, acc: 0.406250]\n",
      "2666: [discriminator loss: 0.586907, acc: 0.695312] [adversarial loss: 2.073049, acc: 0.109375]\n",
      "2667: [discriminator loss: 0.542359, acc: 0.695312] [adversarial loss: 1.005309, acc: 0.390625]\n",
      "2668: [discriminator loss: 0.495440, acc: 0.750000] [adversarial loss: 1.354944, acc: 0.156250]\n",
      "2669: [discriminator loss: 0.457233, acc: 0.796875] [adversarial loss: 1.104082, acc: 0.281250]\n",
      "2670: [discriminator loss: 0.459094, acc: 0.750000] [adversarial loss: 1.672557, acc: 0.109375]\n",
      "2671: [discriminator loss: 0.468671, acc: 0.796875] [adversarial loss: 1.121707, acc: 0.265625]\n",
      "2672: [discriminator loss: 0.490279, acc: 0.765625] [adversarial loss: 1.808153, acc: 0.062500]\n",
      "2673: [discriminator loss: 0.483781, acc: 0.742188] [adversarial loss: 1.087382, acc: 0.203125]\n",
      "2674: [discriminator loss: 0.533859, acc: 0.718750] [adversarial loss: 1.942498, acc: 0.078125]\n",
      "2675: [discriminator loss: 0.547903, acc: 0.734375] [adversarial loss: 0.792209, acc: 0.578125]\n",
      "2676: [discriminator loss: 0.663753, acc: 0.648438] [adversarial loss: 2.064914, acc: 0.046875]\n",
      "2677: [discriminator loss: 0.510165, acc: 0.734375] [adversarial loss: 1.163569, acc: 0.265625]\n",
      "2678: [discriminator loss: 0.492856, acc: 0.757812] [adversarial loss: 1.261507, acc: 0.250000]\n",
      "2679: [discriminator loss: 0.460925, acc: 0.789062] [adversarial loss: 1.240608, acc: 0.171875]\n",
      "2680: [discriminator loss: 0.514358, acc: 0.734375] [adversarial loss: 1.373611, acc: 0.218750]\n",
      "2681: [discriminator loss: 0.393881, acc: 0.828125] [adversarial loss: 1.509619, acc: 0.156250]\n",
      "2682: [discriminator loss: 0.515329, acc: 0.710938] [adversarial loss: 1.104555, acc: 0.359375]\n",
      "2683: [discriminator loss: 0.512201, acc: 0.750000] [adversarial loss: 1.774858, acc: 0.078125]\n",
      "2684: [discriminator loss: 0.487003, acc: 0.757812] [adversarial loss: 1.156256, acc: 0.312500]\n",
      "2685: [discriminator loss: 0.547477, acc: 0.734375] [adversarial loss: 1.472795, acc: 0.125000]\n",
      "2686: [discriminator loss: 0.466005, acc: 0.789062] [adversarial loss: 1.315212, acc: 0.171875]\n",
      "2687: [discriminator loss: 0.455318, acc: 0.828125] [adversarial loss: 1.371236, acc: 0.187500]\n",
      "2688: [discriminator loss: 0.448748, acc: 0.820312] [adversarial loss: 1.047894, acc: 0.343750]\n",
      "2689: [discriminator loss: 0.456405, acc: 0.789062] [adversarial loss: 1.671524, acc: 0.062500]\n",
      "2690: [discriminator loss: 0.449996, acc: 0.804688] [adversarial loss: 1.332938, acc: 0.234375]\n",
      "2691: [discriminator loss: 0.527845, acc: 0.742188] [adversarial loss: 1.512192, acc: 0.250000]\n",
      "2692: [discriminator loss: 0.483361, acc: 0.796875] [adversarial loss: 1.275507, acc: 0.281250]\n",
      "2693: [discriminator loss: 0.507283, acc: 0.773438] [adversarial loss: 1.829917, acc: 0.093750]\n",
      "2694: [discriminator loss: 0.543925, acc: 0.742188] [adversarial loss: 1.183331, acc: 0.265625]\n",
      "2695: [discriminator loss: 0.508767, acc: 0.765625] [adversarial loss: 1.830703, acc: 0.125000]\n",
      "2696: [discriminator loss: 0.608773, acc: 0.656250] [adversarial loss: 1.070526, acc: 0.328125]\n",
      "2697: [discriminator loss: 0.533342, acc: 0.734375] [adversarial loss: 1.707677, acc: 0.140625]\n",
      "2698: [discriminator loss: 0.506576, acc: 0.773438] [adversarial loss: 1.232641, acc: 0.218750]\n",
      "2699: [discriminator loss: 0.471253, acc: 0.781250] [adversarial loss: 1.574787, acc: 0.093750]\n",
      "2700: [discriminator loss: 0.494466, acc: 0.796875] [adversarial loss: 1.213954, acc: 0.218750]\n",
      "2701: [discriminator loss: 0.453416, acc: 0.765625] [adversarial loss: 1.843399, acc: 0.140625]\n",
      "2702: [discriminator loss: 0.487934, acc: 0.773438] [adversarial loss: 1.159676, acc: 0.265625]\n",
      "2703: [discriminator loss: 0.523444, acc: 0.734375] [adversarial loss: 1.699717, acc: 0.171875]\n",
      "2704: [discriminator loss: 0.461105, acc: 0.781250] [adversarial loss: 1.095174, acc: 0.296875]\n",
      "2705: [discriminator loss: 0.488697, acc: 0.726562] [adversarial loss: 1.636629, acc: 0.125000]\n",
      "2706: [discriminator loss: 0.485481, acc: 0.757812] [adversarial loss: 1.148437, acc: 0.296875]\n",
      "2707: [discriminator loss: 0.598361, acc: 0.765625] [adversarial loss: 2.079392, acc: 0.046875]\n",
      "2708: [discriminator loss: 0.580593, acc: 0.742188] [adversarial loss: 0.928252, acc: 0.421875]\n",
      "2709: [discriminator loss: 0.632614, acc: 0.585938] [adversarial loss: 1.896164, acc: 0.093750]\n",
      "2710: [discriminator loss: 0.539437, acc: 0.726562] [adversarial loss: 0.987608, acc: 0.406250]\n",
      "2711: [discriminator loss: 0.436837, acc: 0.812500] [adversarial loss: 1.634641, acc: 0.125000]\n",
      "2712: [discriminator loss: 0.459236, acc: 0.773438] [adversarial loss: 0.944156, acc: 0.406250]\n",
      "2713: [discriminator loss: 0.456564, acc: 0.789062] [adversarial loss: 1.633263, acc: 0.109375]\n",
      "2714: [discriminator loss: 0.494516, acc: 0.734375] [adversarial loss: 1.126387, acc: 0.281250]\n",
      "2715: [discriminator loss: 0.443971, acc: 0.812500] [adversarial loss: 1.754421, acc: 0.140625]\n",
      "2716: [discriminator loss: 0.507084, acc: 0.734375] [adversarial loss: 1.283180, acc: 0.218750]\n",
      "2717: [discriminator loss: 0.444281, acc: 0.781250] [adversarial loss: 1.514196, acc: 0.140625]\n",
      "2718: [discriminator loss: 0.465576, acc: 0.734375] [adversarial loss: 1.204782, acc: 0.203125]\n",
      "2719: [discriminator loss: 0.394129, acc: 0.843750] [adversarial loss: 1.566722, acc: 0.140625]\n",
      "2720: [discriminator loss: 0.392377, acc: 0.843750] [adversarial loss: 1.406367, acc: 0.156250]\n",
      "2721: [discriminator loss: 0.515599, acc: 0.718750] [adversarial loss: 1.239121, acc: 0.171875]\n",
      "2722: [discriminator loss: 0.459859, acc: 0.781250] [adversarial loss: 1.054140, acc: 0.343750]\n",
      "2723: [discriminator loss: 0.542145, acc: 0.703125] [adversarial loss: 1.618661, acc: 0.171875]\n",
      "2724: [discriminator loss: 0.491818, acc: 0.757812] [adversarial loss: 0.943555, acc: 0.375000]\n",
      "2725: [discriminator loss: 0.597232, acc: 0.679688] [adversarial loss: 2.215022, acc: 0.046875]\n",
      "2726: [discriminator loss: 0.679002, acc: 0.632812] [adversarial loss: 0.878383, acc: 0.484375]\n",
      "2727: [discriminator loss: 0.613171, acc: 0.640625] [adversarial loss: 2.211725, acc: 0.062500]\n",
      "2728: [discriminator loss: 0.524893, acc: 0.710938] [adversarial loss: 1.334087, acc: 0.218750]\n",
      "2729: [discriminator loss: 0.408195, acc: 0.820312] [adversarial loss: 1.469511, acc: 0.093750]\n",
      "2730: [discriminator loss: 0.429908, acc: 0.796875] [adversarial loss: 1.255716, acc: 0.250000]\n",
      "2731: [discriminator loss: 0.454385, acc: 0.812500] [adversarial loss: 1.372997, acc: 0.218750]\n",
      "2732: [discriminator loss: 0.487989, acc: 0.734375] [adversarial loss: 0.965986, acc: 0.421875]\n",
      "2733: [discriminator loss: 0.509684, acc: 0.757812] [adversarial loss: 1.717462, acc: 0.125000]\n",
      "2734: [discriminator loss: 0.533206, acc: 0.726562] [adversarial loss: 1.216557, acc: 0.203125]\n",
      "2735: [discriminator loss: 0.575667, acc: 0.726562] [adversarial loss: 1.422591, acc: 0.109375]\n",
      "2736: [discriminator loss: 0.474778, acc: 0.750000] [adversarial loss: 1.286419, acc: 0.218750]\n",
      "2737: [discriminator loss: 0.454783, acc: 0.796875] [adversarial loss: 1.619077, acc: 0.093750]\n",
      "2738: [discriminator loss: 0.443204, acc: 0.804688] [adversarial loss: 1.225886, acc: 0.250000]\n",
      "2739: [discriminator loss: 0.538422, acc: 0.695312] [adversarial loss: 1.845649, acc: 0.062500]\n",
      "2740: [discriminator loss: 0.474469, acc: 0.750000] [adversarial loss: 1.309307, acc: 0.234375]\n",
      "2741: [discriminator loss: 0.455233, acc: 0.750000] [adversarial loss: 1.546091, acc: 0.203125]\n",
      "2742: [discriminator loss: 0.429400, acc: 0.789062] [adversarial loss: 1.377738, acc: 0.234375]\n",
      "2743: [discriminator loss: 0.409656, acc: 0.843750] [adversarial loss: 1.419432, acc: 0.156250]\n",
      "2744: [discriminator loss: 0.433475, acc: 0.789062] [adversarial loss: 1.588097, acc: 0.171875]\n",
      "2745: [discriminator loss: 0.462826, acc: 0.773438] [adversarial loss: 1.562230, acc: 0.171875]\n",
      "2746: [discriminator loss: 0.532193, acc: 0.734375] [adversarial loss: 2.072372, acc: 0.062500]\n",
      "2747: [discriminator loss: 0.503370, acc: 0.742188] [adversarial loss: 1.271188, acc: 0.218750]\n",
      "2748: [discriminator loss: 0.459354, acc: 0.734375] [adversarial loss: 1.862309, acc: 0.062500]\n",
      "2749: [discriminator loss: 0.555217, acc: 0.742188] [adversarial loss: 1.294496, acc: 0.281250]\n",
      "2750: [discriminator loss: 0.446133, acc: 0.789062] [adversarial loss: 1.679010, acc: 0.093750]\n",
      "2751: [discriminator loss: 0.458802, acc: 0.750000] [adversarial loss: 1.140270, acc: 0.312500]\n",
      "2752: [discriminator loss: 0.475758, acc: 0.757812] [adversarial loss: 1.885219, acc: 0.031250]\n",
      "2753: [discriminator loss: 0.482385, acc: 0.750000] [adversarial loss: 0.714047, acc: 0.546875]\n",
      "2754: [discriminator loss: 0.725684, acc: 0.687500] [adversarial loss: 1.989746, acc: 0.046875]\n",
      "2755: [discriminator loss: 0.553883, acc: 0.687500] [adversarial loss: 1.326295, acc: 0.234375]\n",
      "2756: [discriminator loss: 0.522476, acc: 0.757812] [adversarial loss: 1.300423, acc: 0.281250]\n",
      "2757: [discriminator loss: 0.466594, acc: 0.710938] [adversarial loss: 1.294537, acc: 0.234375]\n",
      "2758: [discriminator loss: 0.441219, acc: 0.804688] [adversarial loss: 1.857968, acc: 0.046875]\n",
      "2759: [discriminator loss: 0.431853, acc: 0.812500] [adversarial loss: 0.924883, acc: 0.515625]\n",
      "2760: [discriminator loss: 0.401945, acc: 0.843750] [adversarial loss: 1.605780, acc: 0.171875]\n",
      "2761: [discriminator loss: 0.482315, acc: 0.750000] [adversarial loss: 1.302933, acc: 0.234375]\n",
      "2762: [discriminator loss: 0.511577, acc: 0.765625] [adversarial loss: 1.866749, acc: 0.093750]\n",
      "2763: [discriminator loss: 0.577510, acc: 0.726562] [adversarial loss: 0.906714, acc: 0.406250]\n",
      "2764: [discriminator loss: 0.536769, acc: 0.710938] [adversarial loss: 1.669667, acc: 0.125000]\n",
      "2765: [discriminator loss: 0.483653, acc: 0.757812] [adversarial loss: 1.189511, acc: 0.281250]\n",
      "2766: [discriminator loss: 0.441425, acc: 0.796875] [adversarial loss: 1.833822, acc: 0.078125]\n",
      "2767: [discriminator loss: 0.585154, acc: 0.687500] [adversarial loss: 1.160221, acc: 0.250000]\n",
      "2768: [discriminator loss: 0.486362, acc: 0.796875] [adversarial loss: 1.679238, acc: 0.109375]\n",
      "2769: [discriminator loss: 0.528916, acc: 0.781250] [adversarial loss: 1.149352, acc: 0.312500]\n",
      "2770: [discriminator loss: 0.468577, acc: 0.804688] [adversarial loss: 1.556763, acc: 0.125000]\n",
      "2771: [discriminator loss: 0.540751, acc: 0.710938] [adversarial loss: 1.579649, acc: 0.171875]\n",
      "2772: [discriminator loss: 0.521619, acc: 0.718750] [adversarial loss: 1.137550, acc: 0.296875]\n",
      "2773: [discriminator loss: 0.488689, acc: 0.781250] [adversarial loss: 1.457989, acc: 0.203125]\n",
      "2774: [discriminator loss: 0.517586, acc: 0.757812] [adversarial loss: 0.836608, acc: 0.468750]\n",
      "2775: [discriminator loss: 0.502253, acc: 0.742188] [adversarial loss: 1.890039, acc: 0.062500]\n",
      "2776: [discriminator loss: 0.583012, acc: 0.679688] [adversarial loss: 1.053175, acc: 0.390625]\n",
      "2777: [discriminator loss: 0.543433, acc: 0.718750] [adversarial loss: 1.774068, acc: 0.093750]\n",
      "2778: [discriminator loss: 0.491749, acc: 0.750000] [adversarial loss: 1.215052, acc: 0.218750]\n",
      "2779: [discriminator loss: 0.474389, acc: 0.781250] [adversarial loss: 1.547680, acc: 0.125000]\n",
      "2780: [discriminator loss: 0.488011, acc: 0.742188] [adversarial loss: 1.416595, acc: 0.140625]\n",
      "2781: [discriminator loss: 0.555820, acc: 0.765625] [adversarial loss: 1.454785, acc: 0.171875]\n",
      "2782: [discriminator loss: 0.475205, acc: 0.781250] [adversarial loss: 2.059921, acc: 0.031250]\n",
      "2783: [discriminator loss: 0.483938, acc: 0.781250] [adversarial loss: 0.781075, acc: 0.468750]\n",
      "2784: [discriminator loss: 0.635467, acc: 0.671875] [adversarial loss: 2.211798, acc: 0.031250]\n",
      "2785: [discriminator loss: 0.574427, acc: 0.710938] [adversarial loss: 1.136952, acc: 0.296875]\n",
      "2786: [discriminator loss: 0.527541, acc: 0.757812] [adversarial loss: 1.893106, acc: 0.046875]\n",
      "2787: [discriminator loss: 0.560823, acc: 0.718750] [adversarial loss: 1.101810, acc: 0.312500]\n",
      "2788: [discriminator loss: 0.449398, acc: 0.789062] [adversarial loss: 1.391668, acc: 0.187500]\n",
      "2789: [discriminator loss: 0.488197, acc: 0.796875] [adversarial loss: 1.171501, acc: 0.234375]\n",
      "2790: [discriminator loss: 0.477463, acc: 0.781250] [adversarial loss: 1.462181, acc: 0.187500]\n",
      "2791: [discriminator loss: 0.487082, acc: 0.750000] [adversarial loss: 1.310534, acc: 0.265625]\n",
      "2792: [discriminator loss: 0.438505, acc: 0.781250] [adversarial loss: 1.832038, acc: 0.093750]\n",
      "2793: [discriminator loss: 0.581736, acc: 0.703125] [adversarial loss: 0.947455, acc: 0.343750]\n",
      "2794: [discriminator loss: 0.534058, acc: 0.679688] [adversarial loss: 1.781936, acc: 0.078125]\n",
      "2795: [discriminator loss: 0.536401, acc: 0.750000] [adversarial loss: 1.281736, acc: 0.234375]\n",
      "2796: [discriminator loss: 0.531381, acc: 0.750000] [adversarial loss: 1.468934, acc: 0.125000]\n",
      "2797: [discriminator loss: 0.541820, acc: 0.726562] [adversarial loss: 1.289326, acc: 0.171875]\n",
      "2798: [discriminator loss: 0.486246, acc: 0.820312] [adversarial loss: 1.595477, acc: 0.156250]\n",
      "2799: [discriminator loss: 0.568366, acc: 0.679688] [adversarial loss: 1.848671, acc: 0.187500]\n",
      "2800: [discriminator loss: 0.539887, acc: 0.742188] [adversarial loss: 0.996711, acc: 0.296875]\n",
      "2801: [discriminator loss: 0.532390, acc: 0.718750] [adversarial loss: 1.791969, acc: 0.062500]\n",
      "2802: [discriminator loss: 0.569134, acc: 0.726562] [adversarial loss: 0.883290, acc: 0.406250]\n",
      "2803: [discriminator loss: 0.543833, acc: 0.773438] [adversarial loss: 1.742213, acc: 0.078125]\n",
      "2804: [discriminator loss: 0.543420, acc: 0.710938] [adversarial loss: 1.350941, acc: 0.250000]\n",
      "2805: [discriminator loss: 0.460124, acc: 0.773438] [adversarial loss: 1.236129, acc: 0.265625]\n",
      "2806: [discriminator loss: 0.406470, acc: 0.820312] [adversarial loss: 1.176455, acc: 0.281250]\n",
      "2807: [discriminator loss: 0.470661, acc: 0.789062] [adversarial loss: 1.443074, acc: 0.140625]\n",
      "2808: [discriminator loss: 0.500127, acc: 0.765625] [adversarial loss: 1.400636, acc: 0.125000]\n",
      "2809: [discriminator loss: 0.549056, acc: 0.718750] [adversarial loss: 1.554429, acc: 0.171875]\n",
      "2810: [discriminator loss: 0.499562, acc: 0.750000] [adversarial loss: 0.863837, acc: 0.390625]\n",
      "2811: [discriminator loss: 0.490777, acc: 0.750000] [adversarial loss: 2.073851, acc: 0.031250]\n",
      "2812: [discriminator loss: 0.457924, acc: 0.789062] [adversarial loss: 1.022473, acc: 0.359375]\n",
      "2813: [discriminator loss: 0.621287, acc: 0.617188] [adversarial loss: 1.988112, acc: 0.031250]\n",
      "2814: [discriminator loss: 0.530635, acc: 0.710938] [adversarial loss: 1.212031, acc: 0.296875]\n",
      "2815: [discriminator loss: 0.528604, acc: 0.726562] [adversarial loss: 1.517049, acc: 0.140625]\n",
      "2816: [discriminator loss: 0.510634, acc: 0.734375] [adversarial loss: 1.336873, acc: 0.203125]\n",
      "2817: [discriminator loss: 0.459223, acc: 0.773438] [adversarial loss: 1.106055, acc: 0.281250]\n",
      "2818: [discriminator loss: 0.442699, acc: 0.804688] [adversarial loss: 1.265291, acc: 0.171875]\n",
      "2819: [discriminator loss: 0.493392, acc: 0.750000] [adversarial loss: 1.630993, acc: 0.093750]\n",
      "2820: [discriminator loss: 0.509694, acc: 0.765625] [adversarial loss: 1.143682, acc: 0.218750]\n",
      "2821: [discriminator loss: 0.490310, acc: 0.781250] [adversarial loss: 1.834817, acc: 0.093750]\n",
      "2822: [discriminator loss: 0.529291, acc: 0.750000] [adversarial loss: 0.918208, acc: 0.390625]\n",
      "2823: [discriminator loss: 0.578178, acc: 0.710938] [adversarial loss: 1.788188, acc: 0.046875]\n",
      "2824: [discriminator loss: 0.542966, acc: 0.726562] [adversarial loss: 1.150279, acc: 0.296875]\n",
      "2825: [discriminator loss: 0.477464, acc: 0.757812] [adversarial loss: 1.421681, acc: 0.171875]\n",
      "2826: [discriminator loss: 0.510666, acc: 0.781250] [adversarial loss: 1.105171, acc: 0.312500]\n",
      "2827: [discriminator loss: 0.512855, acc: 0.757812] [adversarial loss: 1.668262, acc: 0.140625]\n",
      "2828: [discriminator loss: 0.584220, acc: 0.664062] [adversarial loss: 1.084930, acc: 0.250000]\n",
      "2829: [discriminator loss: 0.446494, acc: 0.828125] [adversarial loss: 1.891734, acc: 0.031250]\n",
      "2830: [discriminator loss: 0.478549, acc: 0.750000] [adversarial loss: 1.261897, acc: 0.328125]\n",
      "2831: [discriminator loss: 0.511753, acc: 0.765625] [adversarial loss: 1.775920, acc: 0.140625]\n",
      "2832: [discriminator loss: 0.524174, acc: 0.750000] [adversarial loss: 0.943991, acc: 0.500000]\n",
      "2833: [discriminator loss: 0.488114, acc: 0.773438] [adversarial loss: 1.500165, acc: 0.171875]\n",
      "2834: [discriminator loss: 0.437844, acc: 0.804688] [adversarial loss: 1.529198, acc: 0.125000]\n",
      "2835: [discriminator loss: 0.462304, acc: 0.750000] [adversarial loss: 1.244547, acc: 0.265625]\n",
      "2836: [discriminator loss: 0.544737, acc: 0.718750] [adversarial loss: 1.631205, acc: 0.156250]\n",
      "2837: [discriminator loss: 0.517938, acc: 0.757812] [adversarial loss: 1.343014, acc: 0.250000]\n",
      "2838: [discriminator loss: 0.463059, acc: 0.789062] [adversarial loss: 1.681404, acc: 0.093750]\n",
      "2839: [discriminator loss: 0.521375, acc: 0.734375] [adversarial loss: 1.040336, acc: 0.343750]\n",
      "2840: [discriminator loss: 0.417108, acc: 0.796875] [adversarial loss: 1.488064, acc: 0.156250]\n",
      "2841: [discriminator loss: 0.505780, acc: 0.773438] [adversarial loss: 1.131973, acc: 0.312500]\n",
      "2842: [discriminator loss: 0.477401, acc: 0.781250] [adversarial loss: 1.579715, acc: 0.125000]\n",
      "2843: [discriminator loss: 0.575909, acc: 0.718750] [adversarial loss: 1.064734, acc: 0.296875]\n",
      "2844: [discriminator loss: 0.560645, acc: 0.710938] [adversarial loss: 1.782366, acc: 0.078125]\n",
      "2845: [discriminator loss: 0.528484, acc: 0.773438] [adversarial loss: 0.873248, acc: 0.500000]\n",
      "2846: [discriminator loss: 0.489211, acc: 0.804688] [adversarial loss: 1.758231, acc: 0.093750]\n",
      "2847: [discriminator loss: 0.514387, acc: 0.742188] [adversarial loss: 1.240721, acc: 0.281250]\n",
      "2848: [discriminator loss: 0.522839, acc: 0.765625] [adversarial loss: 1.552882, acc: 0.078125]\n",
      "2849: [discriminator loss: 0.428786, acc: 0.789062] [adversarial loss: 1.305658, acc: 0.171875]\n",
      "2850: [discriminator loss: 0.435420, acc: 0.820312] [adversarial loss: 1.302306, acc: 0.187500]\n",
      "2851: [discriminator loss: 0.526596, acc: 0.742188] [adversarial loss: 1.426594, acc: 0.093750]\n",
      "2852: [discriminator loss: 0.450578, acc: 0.828125] [adversarial loss: 1.307316, acc: 0.265625]\n",
      "2853: [discriminator loss: 0.434147, acc: 0.765625] [adversarial loss: 1.528368, acc: 0.093750]\n",
      "2854: [discriminator loss: 0.443221, acc: 0.796875] [adversarial loss: 1.469663, acc: 0.187500]\n",
      "2855: [discriminator loss: 0.421977, acc: 0.781250] [adversarial loss: 1.375127, acc: 0.187500]\n",
      "2856: [discriminator loss: 0.476744, acc: 0.773438] [adversarial loss: 0.993320, acc: 0.375000]\n",
      "2857: [discriminator loss: 0.553438, acc: 0.757812] [adversarial loss: 2.051206, acc: 0.109375]\n",
      "2858: [discriminator loss: 0.540738, acc: 0.781250] [adversarial loss: 0.900953, acc: 0.468750]\n",
      "2859: [discriminator loss: 0.560124, acc: 0.695312] [adversarial loss: 1.880816, acc: 0.046875]\n",
      "2860: [discriminator loss: 0.381394, acc: 0.843750] [adversarial loss: 1.114402, acc: 0.390625]\n",
      "2861: [discriminator loss: 0.484409, acc: 0.773438] [adversarial loss: 1.278216, acc: 0.218750]\n",
      "2862: [discriminator loss: 0.399850, acc: 0.812500] [adversarial loss: 1.788464, acc: 0.109375]\n",
      "2863: [discriminator loss: 0.581631, acc: 0.695312] [adversarial loss: 1.069282, acc: 0.312500]\n",
      "2864: [discriminator loss: 0.540376, acc: 0.695312] [adversarial loss: 2.051032, acc: 0.015625]\n",
      "2865: [discriminator loss: 0.496627, acc: 0.773438] [adversarial loss: 0.991826, acc: 0.328125]\n",
      "2866: [discriminator loss: 0.490073, acc: 0.765625] [adversarial loss: 1.607271, acc: 0.125000]\n",
      "2867: [discriminator loss: 0.528352, acc: 0.742188] [adversarial loss: 1.193046, acc: 0.328125]\n",
      "2868: [discriminator loss: 0.466223, acc: 0.789062] [adversarial loss: 1.239533, acc: 0.250000]\n",
      "2869: [discriminator loss: 0.535422, acc: 0.718750] [adversarial loss: 1.475155, acc: 0.125000]\n",
      "2870: [discriminator loss: 0.456652, acc: 0.820312] [adversarial loss: 1.785890, acc: 0.109375]\n",
      "2871: [discriminator loss: 0.519649, acc: 0.757812] [adversarial loss: 1.437480, acc: 0.203125]\n",
      "2872: [discriminator loss: 0.485923, acc: 0.750000] [adversarial loss: 1.279630, acc: 0.250000]\n",
      "2873: [discriminator loss: 0.478257, acc: 0.773438] [adversarial loss: 1.391792, acc: 0.265625]\n",
      "2874: [discriminator loss: 0.487077, acc: 0.734375] [adversarial loss: 1.479291, acc: 0.140625]\n",
      "2875: [discriminator loss: 0.519934, acc: 0.726562] [adversarial loss: 1.411483, acc: 0.109375]\n",
      "2876: [discriminator loss: 0.493713, acc: 0.750000] [adversarial loss: 1.640721, acc: 0.078125]\n",
      "2877: [discriminator loss: 0.490641, acc: 0.765625] [adversarial loss: 0.846362, acc: 0.500000]\n",
      "2878: [discriminator loss: 0.502694, acc: 0.718750] [adversarial loss: 1.781553, acc: 0.140625]\n",
      "2879: [discriminator loss: 0.602315, acc: 0.695312] [adversarial loss: 1.067899, acc: 0.359375]\n",
      "2880: [discriminator loss: 0.498814, acc: 0.734375] [adversarial loss: 1.796251, acc: 0.062500]\n",
      "2881: [discriminator loss: 0.541984, acc: 0.757812] [adversarial loss: 1.203345, acc: 0.250000]\n",
      "2882: [discriminator loss: 0.549610, acc: 0.734375] [adversarial loss: 1.828334, acc: 0.140625]\n",
      "2883: [discriminator loss: 0.563075, acc: 0.710938] [adversarial loss: 1.025260, acc: 0.328125]\n",
      "2884: [discriminator loss: 0.520645, acc: 0.726562] [adversarial loss: 1.788325, acc: 0.062500]\n",
      "2885: [discriminator loss: 0.478556, acc: 0.726562] [adversarial loss: 1.008071, acc: 0.328125]\n",
      "2886: [discriminator loss: 0.471579, acc: 0.828125] [adversarial loss: 1.790993, acc: 0.156250]\n",
      "2887: [discriminator loss: 0.458271, acc: 0.820312] [adversarial loss: 1.338847, acc: 0.203125]\n",
      "2888: [discriminator loss: 0.491015, acc: 0.726562] [adversarial loss: 1.385286, acc: 0.250000]\n",
      "2889: [discriminator loss: 0.355750, acc: 0.843750] [adversarial loss: 1.404696, acc: 0.171875]\n",
      "2890: [discriminator loss: 0.445421, acc: 0.789062] [adversarial loss: 1.395286, acc: 0.203125]\n",
      "2891: [discriminator loss: 0.525087, acc: 0.742188] [adversarial loss: 1.063119, acc: 0.375000]\n",
      "2892: [discriminator loss: 0.456762, acc: 0.828125] [adversarial loss: 1.261879, acc: 0.203125]\n",
      "2893: [discriminator loss: 0.457473, acc: 0.812500] [adversarial loss: 1.290182, acc: 0.234375]\n",
      "2894: [discriminator loss: 0.453410, acc: 0.789062] [adversarial loss: 1.313409, acc: 0.140625]\n",
      "2895: [discriminator loss: 0.602947, acc: 0.687500] [adversarial loss: 1.599925, acc: 0.078125]\n",
      "2896: [discriminator loss: 0.449981, acc: 0.828125] [adversarial loss: 1.142814, acc: 0.265625]\n",
      "2897: [discriminator loss: 0.455945, acc: 0.781250] [adversarial loss: 1.616951, acc: 0.156250]\n",
      "2898: [discriminator loss: 0.513152, acc: 0.742188] [adversarial loss: 1.185470, acc: 0.296875]\n",
      "2899: [discriminator loss: 0.504317, acc: 0.742188] [adversarial loss: 1.945762, acc: 0.093750]\n",
      "2900: [discriminator loss: 0.533977, acc: 0.695312] [adversarial loss: 0.954466, acc: 0.437500]\n",
      "2901: [discriminator loss: 0.491990, acc: 0.765625] [adversarial loss: 1.700019, acc: 0.140625]\n",
      "2902: [discriminator loss: 0.444985, acc: 0.742188] [adversarial loss: 0.811259, acc: 0.437500]\n",
      "2903: [discriminator loss: 0.494098, acc: 0.765625] [adversarial loss: 1.887856, acc: 0.062500]\n",
      "2904: [discriminator loss: 0.516135, acc: 0.718750] [adversarial loss: 1.005550, acc: 0.312500]\n",
      "2905: [discriminator loss: 0.489010, acc: 0.781250] [adversarial loss: 2.095719, acc: 0.062500]\n",
      "2906: [discriminator loss: 0.612253, acc: 0.679688] [adversarial loss: 0.813097, acc: 0.468750]\n",
      "2907: [discriminator loss: 0.511819, acc: 0.773438] [adversarial loss: 1.910255, acc: 0.078125]\n",
      "2908: [discriminator loss: 0.475819, acc: 0.773438] [adversarial loss: 1.102536, acc: 0.343750]\n",
      "2909: [discriminator loss: 0.464268, acc: 0.804688] [adversarial loss: 1.420029, acc: 0.187500]\n",
      "2910: [discriminator loss: 0.469187, acc: 0.773438] [adversarial loss: 1.289795, acc: 0.281250]\n",
      "2911: [discriminator loss: 0.418490, acc: 0.843750] [adversarial loss: 1.399275, acc: 0.187500]\n",
      "2912: [discriminator loss: 0.486882, acc: 0.789062] [adversarial loss: 1.524035, acc: 0.187500]\n",
      "2913: [discriminator loss: 0.428406, acc: 0.796875] [adversarial loss: 1.326034, acc: 0.265625]\n",
      "2914: [discriminator loss: 0.465478, acc: 0.789062] [adversarial loss: 1.736393, acc: 0.093750]\n",
      "2915: [discriminator loss: 0.544287, acc: 0.710938] [adversarial loss: 1.159714, acc: 0.265625]\n",
      "2916: [discriminator loss: 0.404318, acc: 0.804688] [adversarial loss: 1.393538, acc: 0.203125]\n",
      "2917: [discriminator loss: 0.524095, acc: 0.695312] [adversarial loss: 1.299778, acc: 0.156250]\n",
      "2918: [discriminator loss: 0.490392, acc: 0.781250] [adversarial loss: 1.465520, acc: 0.156250]\n",
      "2919: [discriminator loss: 0.460612, acc: 0.781250] [adversarial loss: 1.017218, acc: 0.437500]\n",
      "2920: [discriminator loss: 0.487522, acc: 0.750000] [adversarial loss: 1.784335, acc: 0.093750]\n",
      "2921: [discriminator loss: 0.423020, acc: 0.773438] [adversarial loss: 1.516966, acc: 0.203125]\n",
      "2922: [discriminator loss: 0.540270, acc: 0.687500] [adversarial loss: 1.288083, acc: 0.296875]\n",
      "2923: [discriminator loss: 0.468221, acc: 0.781250] [adversarial loss: 1.532215, acc: 0.140625]\n",
      "2924: [discriminator loss: 0.542637, acc: 0.726562] [adversarial loss: 1.696845, acc: 0.218750]\n",
      "2925: [discriminator loss: 0.539381, acc: 0.710938] [adversarial loss: 1.085843, acc: 0.265625]\n",
      "2926: [discriminator loss: 0.480166, acc: 0.773438] [adversarial loss: 1.441843, acc: 0.250000]\n",
      "2927: [discriminator loss: 0.517340, acc: 0.750000] [adversarial loss: 1.218324, acc: 0.312500]\n",
      "2928: [discriminator loss: 0.563405, acc: 0.718750] [adversarial loss: 2.060213, acc: 0.031250]\n",
      "2929: [discriminator loss: 0.535206, acc: 0.695312] [adversarial loss: 1.040421, acc: 0.421875]\n",
      "2930: [discriminator loss: 0.567410, acc: 0.750000] [adversarial loss: 1.999169, acc: 0.109375]\n",
      "2931: [discriminator loss: 0.564756, acc: 0.718750] [adversarial loss: 1.082062, acc: 0.343750]\n",
      "2932: [discriminator loss: 0.507271, acc: 0.726562] [adversarial loss: 1.730432, acc: 0.062500]\n",
      "2933: [discriminator loss: 0.450527, acc: 0.820312] [adversarial loss: 1.249315, acc: 0.265625]\n",
      "2934: [discriminator loss: 0.431210, acc: 0.812500] [adversarial loss: 1.375976, acc: 0.218750]\n",
      "2935: [discriminator loss: 0.511272, acc: 0.718750] [adversarial loss: 1.526675, acc: 0.156250]\n",
      "2936: [discriminator loss: 0.409823, acc: 0.859375] [adversarial loss: 1.499468, acc: 0.203125]\n",
      "2937: [discriminator loss: 0.501433, acc: 0.789062] [adversarial loss: 1.678869, acc: 0.062500]\n",
      "2938: [discriminator loss: 0.477330, acc: 0.765625] [adversarial loss: 1.237385, acc: 0.250000]\n",
      "2939: [discriminator loss: 0.516092, acc: 0.718750] [adversarial loss: 1.779358, acc: 0.125000]\n",
      "2940: [discriminator loss: 0.480828, acc: 0.812500] [adversarial loss: 1.137724, acc: 0.375000]\n",
      "2941: [discriminator loss: 0.504333, acc: 0.750000] [adversarial loss: 1.394385, acc: 0.218750]\n",
      "2942: [discriminator loss: 0.419569, acc: 0.789062] [adversarial loss: 1.406824, acc: 0.250000]\n",
      "2943: [discriminator loss: 0.520330, acc: 0.757812] [adversarial loss: 1.657753, acc: 0.109375]\n",
      "2944: [discriminator loss: 0.486508, acc: 0.773438] [adversarial loss: 1.009543, acc: 0.375000]\n",
      "2945: [discriminator loss: 0.506846, acc: 0.718750] [adversarial loss: 1.982392, acc: 0.140625]\n",
      "2946: [discriminator loss: 0.453852, acc: 0.757812] [adversarial loss: 1.145824, acc: 0.343750]\n",
      "2947: [discriminator loss: 0.654329, acc: 0.632812] [adversarial loss: 2.117872, acc: 0.062500]\n",
      "2948: [discriminator loss: 0.611170, acc: 0.687500] [adversarial loss: 0.844503, acc: 0.515625]\n",
      "2949: [discriminator loss: 0.492413, acc: 0.742188] [adversarial loss: 1.970070, acc: 0.109375]\n",
      "2950: [discriminator loss: 0.584783, acc: 0.703125] [adversarial loss: 1.168047, acc: 0.328125]\n",
      "2951: [discriminator loss: 0.485289, acc: 0.750000] [adversarial loss: 1.497619, acc: 0.156250]\n",
      "2952: [discriminator loss: 0.548429, acc: 0.742188] [adversarial loss: 1.651065, acc: 0.140625]\n",
      "2953: [discriminator loss: 0.550487, acc: 0.726562] [adversarial loss: 0.992046, acc: 0.406250]\n",
      "2954: [discriminator loss: 0.508367, acc: 0.742188] [adversarial loss: 1.731475, acc: 0.156250]\n",
      "2955: [discriminator loss: 0.504945, acc: 0.789062] [adversarial loss: 1.180390, acc: 0.265625]\n",
      "2956: [discriminator loss: 0.547596, acc: 0.679688] [adversarial loss: 1.817234, acc: 0.062500]\n",
      "2957: [discriminator loss: 0.570731, acc: 0.687500] [adversarial loss: 0.859939, acc: 0.437500]\n",
      "2958: [discriminator loss: 0.496474, acc: 0.750000] [adversarial loss: 1.607507, acc: 0.093750]\n",
      "2959: [discriminator loss: 0.429833, acc: 0.789062] [adversarial loss: 1.206043, acc: 0.250000]\n",
      "2960: [discriminator loss: 0.589780, acc: 0.648438] [adversarial loss: 1.870498, acc: 0.046875]\n",
      "2961: [discriminator loss: 0.432734, acc: 0.804688] [adversarial loss: 1.302472, acc: 0.281250]\n",
      "2962: [discriminator loss: 0.478022, acc: 0.804688] [adversarial loss: 1.208969, acc: 0.203125]\n",
      "2963: [discriminator loss: 0.444073, acc: 0.781250] [adversarial loss: 1.386686, acc: 0.187500]\n",
      "2964: [discriminator loss: 0.538989, acc: 0.710938] [adversarial loss: 1.297860, acc: 0.203125]\n",
      "2965: [discriminator loss: 0.528727, acc: 0.757812] [adversarial loss: 1.279760, acc: 0.250000]\n",
      "2966: [discriminator loss: 0.488344, acc: 0.750000] [adversarial loss: 1.625061, acc: 0.140625]\n",
      "2967: [discriminator loss: 0.446533, acc: 0.765625] [adversarial loss: 1.405689, acc: 0.171875]\n",
      "2968: [discriminator loss: 0.524682, acc: 0.750000] [adversarial loss: 1.339567, acc: 0.312500]\n",
      "2969: [discriminator loss: 0.474338, acc: 0.765625] [adversarial loss: 1.406373, acc: 0.171875]\n",
      "2970: [discriminator loss: 0.470867, acc: 0.796875] [adversarial loss: 1.469791, acc: 0.171875]\n",
      "2971: [discriminator loss: 0.527210, acc: 0.757812] [adversarial loss: 1.519114, acc: 0.125000]\n",
      "2972: [discriminator loss: 0.481296, acc: 0.812500] [adversarial loss: 1.834902, acc: 0.125000]\n",
      "2973: [discriminator loss: 0.545663, acc: 0.734375] [adversarial loss: 0.852398, acc: 0.515625]\n",
      "2974: [discriminator loss: 0.528390, acc: 0.742188] [adversarial loss: 1.852582, acc: 0.125000]\n",
      "2975: [discriminator loss: 0.507313, acc: 0.789062] [adversarial loss: 0.859268, acc: 0.406250]\n",
      "2976: [discriminator loss: 0.518208, acc: 0.726562] [adversarial loss: 1.830376, acc: 0.093750]\n",
      "2977: [discriminator loss: 0.628692, acc: 0.703125] [adversarial loss: 0.752749, acc: 0.484375]\n",
      "2978: [discriminator loss: 0.553456, acc: 0.742188] [adversarial loss: 1.812271, acc: 0.062500]\n",
      "2979: [discriminator loss: 0.524361, acc: 0.718750] [adversarial loss: 1.299257, acc: 0.250000]\n",
      "2980: [discriminator loss: 0.464288, acc: 0.804688] [adversarial loss: 1.994429, acc: 0.140625]\n",
      "2981: [discriminator loss: 0.600457, acc: 0.750000] [adversarial loss: 0.931667, acc: 0.296875]\n",
      "2982: [discriminator loss: 0.500898, acc: 0.710938] [adversarial loss: 1.743712, acc: 0.031250]\n",
      "2983: [discriminator loss: 0.484008, acc: 0.757812] [adversarial loss: 1.326807, acc: 0.250000]\n",
      "2984: [discriminator loss: 0.468205, acc: 0.796875] [adversarial loss: 1.450110, acc: 0.218750]\n",
      "2985: [discriminator loss: 0.480038, acc: 0.781250] [adversarial loss: 1.129341, acc: 0.343750]\n",
      "2986: [discriminator loss: 0.571165, acc: 0.742188] [adversarial loss: 1.892899, acc: 0.093750]\n",
      "2987: [discriminator loss: 0.517233, acc: 0.734375] [adversarial loss: 1.133468, acc: 0.312500]\n",
      "2988: [discriminator loss: 0.568543, acc: 0.703125] [adversarial loss: 1.950173, acc: 0.093750]\n",
      "2989: [discriminator loss: 0.492419, acc: 0.765625] [adversarial loss: 0.887064, acc: 0.484375]\n",
      "2990: [discriminator loss: 0.488338, acc: 0.789062] [adversarial loss: 2.017632, acc: 0.140625]\n",
      "2991: [discriminator loss: 0.597243, acc: 0.656250] [adversarial loss: 1.296879, acc: 0.171875]\n",
      "2992: [discriminator loss: 0.472410, acc: 0.742188] [adversarial loss: 1.414201, acc: 0.203125]\n",
      "2993: [discriminator loss: 0.592763, acc: 0.718750] [adversarial loss: 1.103562, acc: 0.359375]\n",
      "2994: [discriminator loss: 0.492051, acc: 0.781250] [adversarial loss: 1.581307, acc: 0.171875]\n",
      "2995: [discriminator loss: 0.497305, acc: 0.789062] [adversarial loss: 1.372466, acc: 0.250000]\n",
      "2996: [discriminator loss: 0.448943, acc: 0.773438] [adversarial loss: 1.387422, acc: 0.187500]\n",
      "2997: [discriminator loss: 0.519373, acc: 0.750000] [adversarial loss: 1.419119, acc: 0.156250]\n",
      "2998: [discriminator loss: 0.396527, acc: 0.820312] [adversarial loss: 1.028021, acc: 0.296875]\n",
      "2999: [discriminator loss: 0.394192, acc: 0.859375] [adversarial loss: 1.509311, acc: 0.187500]\n",
      "3000: [discriminator loss: 0.602079, acc: 0.710938] [adversarial loss: 1.910406, acc: 0.140625]\n",
      "3001: [discriminator loss: 0.534555, acc: 0.750000] [adversarial loss: 1.077258, acc: 0.343750]\n",
      "3002: [discriminator loss: 0.528099, acc: 0.734375] [adversarial loss: 1.839163, acc: 0.125000]\n",
      "3003: [discriminator loss: 0.511213, acc: 0.765625] [adversarial loss: 1.157877, acc: 0.312500]\n",
      "3004: [discriminator loss: 0.424464, acc: 0.796875] [adversarial loss: 1.396622, acc: 0.140625]\n",
      "3005: [discriminator loss: 0.456624, acc: 0.796875] [adversarial loss: 1.274181, acc: 0.187500]\n",
      "3006: [discriminator loss: 0.447570, acc: 0.789062] [adversarial loss: 1.433622, acc: 0.156250]\n",
      "3007: [discriminator loss: 0.473570, acc: 0.750000] [adversarial loss: 1.383186, acc: 0.187500]\n",
      "3008: [discriminator loss: 0.504982, acc: 0.750000] [adversarial loss: 1.583955, acc: 0.203125]\n",
      "3009: [discriminator loss: 0.526066, acc: 0.757812] [adversarial loss: 1.137460, acc: 0.250000]\n",
      "3010: [discriminator loss: 0.434628, acc: 0.828125] [adversarial loss: 1.833598, acc: 0.062500]\n",
      "3011: [discriminator loss: 0.423889, acc: 0.781250] [adversarial loss: 0.965139, acc: 0.406250]\n",
      "3012: [discriminator loss: 0.540378, acc: 0.757812] [adversarial loss: 1.917007, acc: 0.031250]\n",
      "3013: [discriminator loss: 0.600528, acc: 0.710938] [adversarial loss: 0.859288, acc: 0.531250]\n",
      "3014: [discriminator loss: 0.641141, acc: 0.648438] [adversarial loss: 2.014200, acc: 0.078125]\n",
      "3015: [discriminator loss: 0.580260, acc: 0.710938] [adversarial loss: 1.067677, acc: 0.437500]\n",
      "3016: [discriminator loss: 0.470607, acc: 0.789062] [adversarial loss: 1.766848, acc: 0.109375]\n",
      "3017: [discriminator loss: 0.503498, acc: 0.757812] [adversarial loss: 1.150692, acc: 0.203125]\n",
      "3018: [discriminator loss: 0.463276, acc: 0.781250] [adversarial loss: 1.704539, acc: 0.156250]\n",
      "3019: [discriminator loss: 0.517345, acc: 0.781250] [adversarial loss: 1.181114, acc: 0.281250]\n",
      "3020: [discriminator loss: 0.426833, acc: 0.757812] [adversarial loss: 1.152624, acc: 0.312500]\n",
      "3021: [discriminator loss: 0.507936, acc: 0.718750] [adversarial loss: 1.526776, acc: 0.140625]\n",
      "3022: [discriminator loss: 0.509467, acc: 0.781250] [adversarial loss: 0.998450, acc: 0.359375]\n",
      "3023: [discriminator loss: 0.499929, acc: 0.765625] [adversarial loss: 1.805062, acc: 0.109375]\n",
      "3024: [discriminator loss: 0.457745, acc: 0.773438] [adversarial loss: 1.177542, acc: 0.281250]\n",
      "3025: [discriminator loss: 0.431135, acc: 0.757812] [adversarial loss: 1.552578, acc: 0.187500]\n",
      "3026: [discriminator loss: 0.472929, acc: 0.812500] [adversarial loss: 1.413144, acc: 0.234375]\n",
      "3027: [discriminator loss: 0.445946, acc: 0.789062] [adversarial loss: 1.463193, acc: 0.171875]\n",
      "3028: [discriminator loss: 0.465502, acc: 0.789062] [adversarial loss: 1.498724, acc: 0.171875]\n",
      "3029: [discriminator loss: 0.492818, acc: 0.789062] [adversarial loss: 1.641676, acc: 0.156250]\n",
      "3030: [discriminator loss: 0.494376, acc: 0.765625] [adversarial loss: 1.060536, acc: 0.296875]\n",
      "3031: [discriminator loss: 0.635531, acc: 0.703125] [adversarial loss: 1.840662, acc: 0.078125]\n",
      "3032: [discriminator loss: 0.523686, acc: 0.726562] [adversarial loss: 0.858122, acc: 0.375000]\n",
      "3033: [discriminator loss: 0.587000, acc: 0.664062] [adversarial loss: 2.208692, acc: 0.062500]\n",
      "3034: [discriminator loss: 0.517285, acc: 0.726562] [adversarial loss: 0.963195, acc: 0.265625]\n",
      "3035: [discriminator loss: 0.500417, acc: 0.765625] [adversarial loss: 1.811431, acc: 0.125000]\n",
      "3036: [discriminator loss: 0.482133, acc: 0.726562] [adversarial loss: 1.231109, acc: 0.156250]\n",
      "3037: [discriminator loss: 0.457179, acc: 0.812500] [adversarial loss: 1.451908, acc: 0.125000]\n",
      "3038: [discriminator loss: 0.518447, acc: 0.750000] [adversarial loss: 1.292911, acc: 0.250000]\n",
      "3039: [discriminator loss: 0.558232, acc: 0.664062] [adversarial loss: 1.203156, acc: 0.328125]\n",
      "3040: [discriminator loss: 0.456549, acc: 0.796875] [adversarial loss: 1.521305, acc: 0.187500]\n",
      "3041: [discriminator loss: 0.442881, acc: 0.765625] [adversarial loss: 1.592879, acc: 0.125000]\n",
      "3042: [discriminator loss: 0.496104, acc: 0.734375] [adversarial loss: 1.353069, acc: 0.171875]\n",
      "3043: [discriminator loss: 0.499347, acc: 0.781250] [adversarial loss: 1.290074, acc: 0.218750]\n",
      "3044: [discriminator loss: 0.468105, acc: 0.796875] [adversarial loss: 1.461531, acc: 0.156250]\n",
      "3045: [discriminator loss: 0.405853, acc: 0.851562] [adversarial loss: 1.086078, acc: 0.328125]\n",
      "3046: [discriminator loss: 0.476275, acc: 0.796875] [adversarial loss: 1.679636, acc: 0.171875]\n",
      "3047: [discriminator loss: 0.477758, acc: 0.750000] [adversarial loss: 1.863627, acc: 0.093750]\n",
      "3048: [discriminator loss: 0.527216, acc: 0.710938] [adversarial loss: 1.313309, acc: 0.281250]\n",
      "3049: [discriminator loss: 0.457527, acc: 0.796875] [adversarial loss: 1.730678, acc: 0.078125]\n",
      "3050: [discriminator loss: 0.536757, acc: 0.750000] [adversarial loss: 1.120628, acc: 0.359375]\n",
      "3051: [discriminator loss: 0.481893, acc: 0.773438] [adversarial loss: 1.839046, acc: 0.187500]\n",
      "3052: [discriminator loss: 0.430610, acc: 0.789062] [adversarial loss: 1.191211, acc: 0.296875]\n",
      "3053: [discriminator loss: 0.409780, acc: 0.835938] [adversarial loss: 1.895618, acc: 0.109375]\n",
      "3054: [discriminator loss: 0.504793, acc: 0.757812] [adversarial loss: 1.148167, acc: 0.312500]\n",
      "3055: [discriminator loss: 0.505740, acc: 0.757812] [adversarial loss: 1.926156, acc: 0.156250]\n",
      "3056: [discriminator loss: 0.628441, acc: 0.664062] [adversarial loss: 0.859121, acc: 0.421875]\n",
      "3057: [discriminator loss: 0.583990, acc: 0.671875] [adversarial loss: 1.848567, acc: 0.093750]\n",
      "3058: [discriminator loss: 0.558340, acc: 0.679688] [adversarial loss: 1.101718, acc: 0.250000]\n",
      "3059: [discriminator loss: 0.533555, acc: 0.765625] [adversarial loss: 1.946664, acc: 0.062500]\n",
      "3060: [discriminator loss: 0.505900, acc: 0.757812] [adversarial loss: 1.181557, acc: 0.328125]\n",
      "3061: [discriminator loss: 0.494245, acc: 0.742188] [adversarial loss: 1.687506, acc: 0.171875]\n",
      "3062: [discriminator loss: 0.483492, acc: 0.781250] [adversarial loss: 1.244891, acc: 0.218750]\n",
      "3063: [discriminator loss: 0.508878, acc: 0.781250] [adversarial loss: 1.620186, acc: 0.062500]\n",
      "3064: [discriminator loss: 0.517511, acc: 0.734375] [adversarial loss: 1.568989, acc: 0.187500]\n",
      "3065: [discriminator loss: 0.481108, acc: 0.718750] [adversarial loss: 1.475584, acc: 0.140625]\n",
      "3066: [discriminator loss: 0.455392, acc: 0.812500] [adversarial loss: 1.209004, acc: 0.296875]\n",
      "3067: [discriminator loss: 0.489900, acc: 0.757812] [adversarial loss: 1.663360, acc: 0.093750]\n",
      "3068: [discriminator loss: 0.531173, acc: 0.718750] [adversarial loss: 0.867802, acc: 0.375000]\n",
      "3069: [discriminator loss: 0.509284, acc: 0.718750] [adversarial loss: 1.686436, acc: 0.109375]\n",
      "3070: [discriminator loss: 0.472207, acc: 0.773438] [adversarial loss: 1.045244, acc: 0.343750]\n",
      "3071: [discriminator loss: 0.478997, acc: 0.742188] [adversarial loss: 1.252168, acc: 0.296875]\n",
      "3072: [discriminator loss: 0.510524, acc: 0.726562] [adversarial loss: 1.528341, acc: 0.109375]\n",
      "3073: [discriminator loss: 0.445518, acc: 0.781250] [adversarial loss: 1.305993, acc: 0.265625]\n",
      "3074: [discriminator loss: 0.474614, acc: 0.726562] [adversarial loss: 1.455564, acc: 0.171875]\n",
      "3075: [discriminator loss: 0.498764, acc: 0.773438] [adversarial loss: 1.208520, acc: 0.250000]\n",
      "3076: [discriminator loss: 0.537094, acc: 0.750000] [adversarial loss: 1.781020, acc: 0.109375]\n",
      "3077: [discriminator loss: 0.435243, acc: 0.796875] [adversarial loss: 1.359375, acc: 0.265625]\n",
      "3078: [discriminator loss: 0.467665, acc: 0.812500] [adversarial loss: 1.326977, acc: 0.234375]\n",
      "3079: [discriminator loss: 0.463755, acc: 0.812500] [adversarial loss: 1.262349, acc: 0.187500]\n",
      "3080: [discriminator loss: 0.459003, acc: 0.765625] [adversarial loss: 1.305400, acc: 0.265625]\n",
      "3081: [discriminator loss: 0.544982, acc: 0.703125] [adversarial loss: 1.393473, acc: 0.265625]\n",
      "3082: [discriminator loss: 0.523321, acc: 0.718750] [adversarial loss: 1.401520, acc: 0.203125]\n",
      "3083: [discriminator loss: 0.464456, acc: 0.812500] [adversarial loss: 1.606806, acc: 0.125000]\n",
      "3084: [discriminator loss: 0.473757, acc: 0.796875] [adversarial loss: 0.939605, acc: 0.421875]\n",
      "3085: [discriminator loss: 0.485383, acc: 0.765625] [adversarial loss: 1.634070, acc: 0.109375]\n",
      "3086: [discriminator loss: 0.392022, acc: 0.851562] [adversarial loss: 1.041087, acc: 0.281250]\n",
      "3087: [discriminator loss: 0.476427, acc: 0.726562] [adversarial loss: 2.463130, acc: 0.031250]\n",
      "3088: [discriminator loss: 0.756842, acc: 0.640625] [adversarial loss: 0.862586, acc: 0.406250]\n",
      "3089: [discriminator loss: 0.595804, acc: 0.710938] [adversarial loss: 2.019933, acc: 0.031250]\n",
      "3090: [discriminator loss: 0.467051, acc: 0.750000] [adversarial loss: 1.299446, acc: 0.171875]\n",
      "3091: [discriminator loss: 0.503143, acc: 0.757812] [adversarial loss: 1.345293, acc: 0.265625]\n",
      "3092: [discriminator loss: 0.557362, acc: 0.726562] [adversarial loss: 1.024744, acc: 0.359375]\n",
      "3093: [discriminator loss: 0.515284, acc: 0.734375] [adversarial loss: 1.523260, acc: 0.140625]\n",
      "3094: [discriminator loss: 0.489386, acc: 0.765625] [adversarial loss: 1.078641, acc: 0.343750]\n",
      "3095: [discriminator loss: 0.481286, acc: 0.812500] [adversarial loss: 1.215738, acc: 0.281250]\n",
      "3096: [discriminator loss: 0.578489, acc: 0.687500] [adversarial loss: 1.462148, acc: 0.171875]\n",
      "3097: [discriminator loss: 0.501551, acc: 0.750000] [adversarial loss: 1.049138, acc: 0.421875]\n",
      "3098: [discriminator loss: 0.419752, acc: 0.820312] [adversarial loss: 1.933817, acc: 0.062500]\n",
      "3099: [discriminator loss: 0.527981, acc: 0.734375] [adversarial loss: 0.983933, acc: 0.359375]\n",
      "3100: [discriminator loss: 0.612531, acc: 0.726562] [adversarial loss: 1.961717, acc: 0.078125]\n",
      "3101: [discriminator loss: 0.516260, acc: 0.710938] [adversarial loss: 1.209015, acc: 0.218750]\n",
      "3102: [discriminator loss: 0.596159, acc: 0.679688] [adversarial loss: 1.758793, acc: 0.109375]\n",
      "3103: [discriminator loss: 0.526006, acc: 0.695312] [adversarial loss: 1.009292, acc: 0.375000]\n",
      "3104: [discriminator loss: 0.495262, acc: 0.750000] [adversarial loss: 1.819344, acc: 0.140625]\n",
      "3105: [discriminator loss: 0.481898, acc: 0.757812] [adversarial loss: 1.188359, acc: 0.281250]\n",
      "3106: [discriminator loss: 0.413083, acc: 0.843750] [adversarial loss: 1.740755, acc: 0.062500]\n",
      "3107: [discriminator loss: 0.513337, acc: 0.765625] [adversarial loss: 1.101931, acc: 0.281250]\n",
      "3108: [discriminator loss: 0.547558, acc: 0.765625] [adversarial loss: 2.099295, acc: 0.000000]\n",
      "3109: [discriminator loss: 0.508017, acc: 0.750000] [adversarial loss: 1.068012, acc: 0.328125]\n",
      "3110: [discriminator loss: 0.491415, acc: 0.773438] [adversarial loss: 1.662669, acc: 0.125000]\n",
      "3111: [discriminator loss: 0.476750, acc: 0.757812] [adversarial loss: 1.437155, acc: 0.218750]\n",
      "3112: [discriminator loss: 0.483932, acc: 0.750000] [adversarial loss: 1.213333, acc: 0.250000]\n",
      "3113: [discriminator loss: 0.512505, acc: 0.718750] [adversarial loss: 1.647011, acc: 0.125000]\n",
      "3114: [discriminator loss: 0.505210, acc: 0.734375] [adversarial loss: 1.085171, acc: 0.296875]\n",
      "3115: [discriminator loss: 0.457491, acc: 0.789062] [adversarial loss: 1.544889, acc: 0.109375]\n",
      "3116: [discriminator loss: 0.524286, acc: 0.773438] [adversarial loss: 1.357620, acc: 0.218750]\n",
      "3117: [discriminator loss: 0.407020, acc: 0.835938] [adversarial loss: 1.426820, acc: 0.171875]\n",
      "3118: [discriminator loss: 0.413028, acc: 0.835938] [adversarial loss: 1.305470, acc: 0.265625]\n",
      "3119: [discriminator loss: 0.467068, acc: 0.781250] [adversarial loss: 1.428226, acc: 0.171875]\n",
      "3120: [discriminator loss: 0.424527, acc: 0.789062] [adversarial loss: 1.538462, acc: 0.156250]\n",
      "3121: [discriminator loss: 0.469925, acc: 0.804688] [adversarial loss: 1.397540, acc: 0.265625]\n",
      "3122: [discriminator loss: 0.435566, acc: 0.765625] [adversarial loss: 1.631531, acc: 0.109375]\n",
      "3123: [discriminator loss: 0.545814, acc: 0.742188] [adversarial loss: 1.156471, acc: 0.234375]\n",
      "3124: [discriminator loss: 0.640625, acc: 0.671875] [adversarial loss: 1.745862, acc: 0.078125]\n",
      "3125: [discriminator loss: 0.485909, acc: 0.765625] [adversarial loss: 0.729270, acc: 0.562500]\n",
      "3126: [discriminator loss: 0.519210, acc: 0.710938] [adversarial loss: 1.628703, acc: 0.156250]\n",
      "3127: [discriminator loss: 0.534384, acc: 0.765625] [adversarial loss: 0.951990, acc: 0.390625]\n",
      "3128: [discriminator loss: 0.678460, acc: 0.632812] [adversarial loss: 2.077055, acc: 0.062500]\n",
      "3129: [discriminator loss: 0.562674, acc: 0.734375] [adversarial loss: 1.056078, acc: 0.390625]\n",
      "3130: [discriminator loss: 0.438455, acc: 0.773438] [adversarial loss: 1.689217, acc: 0.140625]\n",
      "3131: [discriminator loss: 0.605768, acc: 0.718750] [adversarial loss: 0.924239, acc: 0.406250]\n",
      "3132: [discriminator loss: 0.514246, acc: 0.765625] [adversarial loss: 1.640196, acc: 0.062500]\n",
      "3133: [discriminator loss: 0.440807, acc: 0.781250] [adversarial loss: 1.211720, acc: 0.187500]\n",
      "3134: [discriminator loss: 0.405753, acc: 0.804688] [adversarial loss: 1.360014, acc: 0.125000]\n",
      "3135: [discriminator loss: 0.517828, acc: 0.687500] [adversarial loss: 1.020769, acc: 0.406250]\n",
      "3136: [discriminator loss: 0.456868, acc: 0.812500] [adversarial loss: 1.482691, acc: 0.140625]\n",
      "3137: [discriminator loss: 0.574645, acc: 0.687500] [adversarial loss: 1.210499, acc: 0.203125]\n",
      "3138: [discriminator loss: 0.501714, acc: 0.718750] [adversarial loss: 1.863478, acc: 0.078125]\n",
      "3139: [discriminator loss: 0.501259, acc: 0.750000] [adversarial loss: 0.947291, acc: 0.390625]\n",
      "3140: [discriminator loss: 0.504419, acc: 0.710938] [adversarial loss: 1.707778, acc: 0.093750]\n",
      "3141: [discriminator loss: 0.447227, acc: 0.796875] [adversarial loss: 1.261658, acc: 0.359375]\n",
      "3142: [discriminator loss: 0.579022, acc: 0.734375] [adversarial loss: 1.287333, acc: 0.203125]\n",
      "3143: [discriminator loss: 0.491712, acc: 0.804688] [adversarial loss: 1.423848, acc: 0.250000]\n",
      "3144: [discriminator loss: 0.490147, acc: 0.789062] [adversarial loss: 0.925647, acc: 0.437500]\n",
      "3145: [discriminator loss: 0.459740, acc: 0.742188] [adversarial loss: 1.816423, acc: 0.078125]\n",
      "3146: [discriminator loss: 0.516315, acc: 0.726562] [adversarial loss: 1.219701, acc: 0.265625]\n",
      "3147: [discriminator loss: 0.439534, acc: 0.804688] [adversarial loss: 1.267662, acc: 0.296875]\n",
      "3148: [discriminator loss: 0.490729, acc: 0.773438] [adversarial loss: 1.787704, acc: 0.125000]\n",
      "3149: [discriminator loss: 0.504919, acc: 0.718750] [adversarial loss: 1.034104, acc: 0.390625]\n",
      "3150: [discriminator loss: 0.526137, acc: 0.726562] [adversarial loss: 2.153072, acc: 0.015625]\n",
      "3151: [discriminator loss: 0.508472, acc: 0.726562] [adversarial loss: 1.056581, acc: 0.343750]\n",
      "3152: [discriminator loss: 0.465589, acc: 0.789062] [adversarial loss: 1.668758, acc: 0.140625]\n",
      "3153: [discriminator loss: 0.470419, acc: 0.742188] [adversarial loss: 1.188146, acc: 0.296875]\n",
      "3154: [discriminator loss: 0.510632, acc: 0.750000] [adversarial loss: 1.435939, acc: 0.203125]\n",
      "3155: [discriminator loss: 0.523203, acc: 0.679688] [adversarial loss: 0.845169, acc: 0.453125]\n",
      "3156: [discriminator loss: 0.573163, acc: 0.656250] [adversarial loss: 2.122240, acc: 0.093750]\n",
      "3157: [discriminator loss: 0.467203, acc: 0.804688] [adversarial loss: 1.021845, acc: 0.359375]\n",
      "3158: [discriminator loss: 0.537219, acc: 0.726562] [adversarial loss: 1.600844, acc: 0.156250]\n",
      "3159: [discriminator loss: 0.490040, acc: 0.773438] [adversarial loss: 0.997686, acc: 0.328125]\n",
      "3160: [discriminator loss: 0.465250, acc: 0.773438] [adversarial loss: 1.493453, acc: 0.156250]\n",
      "3161: [discriminator loss: 0.477030, acc: 0.781250] [adversarial loss: 1.153357, acc: 0.250000]\n",
      "3162: [discriminator loss: 0.520050, acc: 0.750000] [adversarial loss: 1.597908, acc: 0.109375]\n",
      "3163: [discriminator loss: 0.496429, acc: 0.734375] [adversarial loss: 1.203246, acc: 0.218750]\n",
      "3164: [discriminator loss: 0.471449, acc: 0.765625] [adversarial loss: 1.889054, acc: 0.125000]\n",
      "3165: [discriminator loss: 0.526895, acc: 0.718750] [adversarial loss: 1.016483, acc: 0.265625]\n",
      "3166: [discriminator loss: 0.485458, acc: 0.750000] [adversarial loss: 1.820310, acc: 0.078125]\n",
      "3167: [discriminator loss: 0.456700, acc: 0.804688] [adversarial loss: 1.076293, acc: 0.281250]\n",
      "3168: [discriminator loss: 0.487347, acc: 0.765625] [adversarial loss: 1.756955, acc: 0.031250]\n",
      "3169: [discriminator loss: 0.428929, acc: 0.796875] [adversarial loss: 1.132748, acc: 0.281250]\n",
      "3170: [discriminator loss: 0.465284, acc: 0.765625] [adversarial loss: 1.658175, acc: 0.093750]\n",
      "3171: [discriminator loss: 0.547608, acc: 0.726562] [adversarial loss: 1.072902, acc: 0.296875]\n",
      "3172: [discriminator loss: 0.544292, acc: 0.695312] [adversarial loss: 1.839042, acc: 0.078125]\n",
      "3173: [discriminator loss: 0.582450, acc: 0.679688] [adversarial loss: 1.047593, acc: 0.328125]\n",
      "3174: [discriminator loss: 0.477871, acc: 0.804688] [adversarial loss: 1.270277, acc: 0.265625]\n",
      "3175: [discriminator loss: 0.465664, acc: 0.781250] [adversarial loss: 1.371481, acc: 0.171875]\n",
      "3176: [discriminator loss: 0.530536, acc: 0.726562] [adversarial loss: 1.455962, acc: 0.109375]\n",
      "3177: [discriminator loss: 0.489519, acc: 0.757812] [adversarial loss: 1.395513, acc: 0.234375]\n",
      "3178: [discriminator loss: 0.472823, acc: 0.773438] [adversarial loss: 1.376514, acc: 0.125000]\n",
      "3179: [discriminator loss: 0.449792, acc: 0.820312] [adversarial loss: 0.992892, acc: 0.343750]\n",
      "3180: [discriminator loss: 0.548057, acc: 0.710938] [adversarial loss: 1.952907, acc: 0.046875]\n",
      "3181: [discriminator loss: 0.692007, acc: 0.648438] [adversarial loss: 1.008889, acc: 0.406250]\n",
      "3182: [discriminator loss: 0.570072, acc: 0.734375] [adversarial loss: 1.792306, acc: 0.187500]\n",
      "3183: [discriminator loss: 0.617250, acc: 0.695312] [adversarial loss: 1.018449, acc: 0.375000]\n",
      "3184: [discriminator loss: 0.504190, acc: 0.796875] [adversarial loss: 1.681548, acc: 0.140625]\n",
      "3185: [discriminator loss: 0.459045, acc: 0.789062] [adversarial loss: 1.457895, acc: 0.093750]\n",
      "3186: [discriminator loss: 0.426207, acc: 0.781250] [adversarial loss: 1.362695, acc: 0.171875]\n",
      "3187: [discriminator loss: 0.460899, acc: 0.789062] [adversarial loss: 1.428890, acc: 0.234375]\n",
      "3188: [discriminator loss: 0.543102, acc: 0.703125] [adversarial loss: 1.529887, acc: 0.203125]\n",
      "3189: [discriminator loss: 0.441837, acc: 0.773438] [adversarial loss: 1.718229, acc: 0.093750]\n",
      "3190: [discriminator loss: 0.492175, acc: 0.734375] [adversarial loss: 1.251172, acc: 0.203125]\n",
      "3191: [discriminator loss: 0.482105, acc: 0.765625] [adversarial loss: 1.542802, acc: 0.187500]\n",
      "3192: [discriminator loss: 0.430669, acc: 0.789062] [adversarial loss: 1.206237, acc: 0.312500]\n",
      "3193: [discriminator loss: 0.501519, acc: 0.789062] [adversarial loss: 1.840655, acc: 0.093750]\n",
      "3194: [discriminator loss: 0.574731, acc: 0.664062] [adversarial loss: 0.806739, acc: 0.437500]\n",
      "3195: [discriminator loss: 0.475173, acc: 0.773438] [adversarial loss: 1.239490, acc: 0.328125]\n",
      "3196: [discriminator loss: 0.453017, acc: 0.804688] [adversarial loss: 1.355601, acc: 0.250000]\n",
      "3197: [discriminator loss: 0.473462, acc: 0.820312] [adversarial loss: 1.500208, acc: 0.140625]\n",
      "3198: [discriminator loss: 0.439962, acc: 0.796875] [adversarial loss: 1.639080, acc: 0.156250]\n",
      "3199: [discriminator loss: 0.523955, acc: 0.765625] [adversarial loss: 1.047626, acc: 0.343750]\n",
      "3200: [discriminator loss: 0.405475, acc: 0.835938] [adversarial loss: 1.537669, acc: 0.125000]\n",
      "3201: [discriminator loss: 0.492530, acc: 0.757812] [adversarial loss: 1.171137, acc: 0.312500]\n",
      "3202: [discriminator loss: 0.518164, acc: 0.742188] [adversarial loss: 1.137304, acc: 0.203125]\n",
      "3203: [discriminator loss: 0.502131, acc: 0.812500] [adversarial loss: 1.675409, acc: 0.109375]\n",
      "3204: [discriminator loss: 0.525286, acc: 0.710938] [adversarial loss: 1.242998, acc: 0.296875]\n",
      "3205: [discriminator loss: 0.434122, acc: 0.812500] [adversarial loss: 1.498035, acc: 0.171875]\n",
      "3206: [discriminator loss: 0.429894, acc: 0.796875] [adversarial loss: 1.234464, acc: 0.359375]\n",
      "3207: [discriminator loss: 0.523031, acc: 0.750000] [adversarial loss: 1.474625, acc: 0.156250]\n",
      "3208: [discriminator loss: 0.544202, acc: 0.750000] [adversarial loss: 1.097866, acc: 0.281250]\n",
      "3209: [discriminator loss: 0.536389, acc: 0.734375] [adversarial loss: 1.594575, acc: 0.093750]\n",
      "3210: [discriminator loss: 0.500002, acc: 0.757812] [adversarial loss: 0.990700, acc: 0.265625]\n",
      "3211: [discriminator loss: 0.445783, acc: 0.804688] [adversarial loss: 2.149969, acc: 0.093750]\n",
      "3212: [discriminator loss: 0.458683, acc: 0.804688] [adversarial loss: 1.250163, acc: 0.203125]\n",
      "3213: [discriminator loss: 0.608289, acc: 0.695312] [adversarial loss: 1.927524, acc: 0.046875]\n",
      "3214: [discriminator loss: 0.585750, acc: 0.710938] [adversarial loss: 0.834187, acc: 0.468750]\n",
      "3215: [discriminator loss: 0.613740, acc: 0.640625] [adversarial loss: 1.853921, acc: 0.078125]\n",
      "3216: [discriminator loss: 0.506815, acc: 0.742188] [adversarial loss: 1.301904, acc: 0.250000]\n",
      "3217: [discriminator loss: 0.448547, acc: 0.789062] [adversarial loss: 1.964512, acc: 0.078125]\n",
      "3218: [discriminator loss: 0.566355, acc: 0.718750] [adversarial loss: 1.194178, acc: 0.328125]\n",
      "3219: [discriminator loss: 0.500260, acc: 0.750000] [adversarial loss: 1.533373, acc: 0.140625]\n",
      "3220: [discriminator loss: 0.384022, acc: 0.867188] [adversarial loss: 1.363026, acc: 0.156250]\n",
      "3221: [discriminator loss: 0.553108, acc: 0.718750] [adversarial loss: 1.142244, acc: 0.281250]\n",
      "3222: [discriminator loss: 0.469053, acc: 0.804688] [adversarial loss: 1.297026, acc: 0.156250]\n",
      "3223: [discriminator loss: 0.457799, acc: 0.757812] [adversarial loss: 1.045887, acc: 0.312500]\n",
      "3224: [discriminator loss: 0.522235, acc: 0.742188] [adversarial loss: 1.707772, acc: 0.078125]\n",
      "3225: [discriminator loss: 0.508036, acc: 0.695312] [adversarial loss: 0.907030, acc: 0.531250]\n",
      "3226: [discriminator loss: 0.588770, acc: 0.718750] [adversarial loss: 1.812675, acc: 0.015625]\n",
      "3227: [discriminator loss: 0.571859, acc: 0.718750] [adversarial loss: 1.160881, acc: 0.281250]\n",
      "3228: [discriminator loss: 0.448014, acc: 0.789062] [adversarial loss: 1.269195, acc: 0.203125]\n",
      "3229: [discriminator loss: 0.455031, acc: 0.789062] [adversarial loss: 1.626837, acc: 0.187500]\n",
      "3230: [discriminator loss: 0.574043, acc: 0.750000] [adversarial loss: 1.223132, acc: 0.281250]\n",
      "3231: [discriminator loss: 0.485914, acc: 0.765625] [adversarial loss: 1.301394, acc: 0.187500]\n",
      "3232: [discriminator loss: 0.463058, acc: 0.781250] [adversarial loss: 1.192754, acc: 0.281250]\n",
      "3233: [discriminator loss: 0.376806, acc: 0.867188] [adversarial loss: 1.743572, acc: 0.125000]\n",
      "3234: [discriminator loss: 0.605009, acc: 0.703125] [adversarial loss: 0.931664, acc: 0.453125]\n",
      "3235: [discriminator loss: 0.557710, acc: 0.671875] [adversarial loss: 1.849419, acc: 0.062500]\n",
      "3236: [discriminator loss: 0.599176, acc: 0.734375] [adversarial loss: 0.986349, acc: 0.375000]\n",
      "3237: [discriminator loss: 0.520000, acc: 0.750000] [adversarial loss: 1.741891, acc: 0.140625]\n",
      "3238: [discriminator loss: 0.492005, acc: 0.765625] [adversarial loss: 1.299649, acc: 0.203125]\n",
      "3239: [discriminator loss: 0.518596, acc: 0.773438] [adversarial loss: 1.524910, acc: 0.109375]\n",
      "3240: [discriminator loss: 0.454247, acc: 0.789062] [adversarial loss: 1.192229, acc: 0.156250]\n",
      "3241: [discriminator loss: 0.469456, acc: 0.789062] [adversarial loss: 1.340453, acc: 0.234375]\n",
      "3242: [discriminator loss: 0.439722, acc: 0.796875] [adversarial loss: 1.476762, acc: 0.062500]\n",
      "3243: [discriminator loss: 0.510508, acc: 0.757812] [adversarial loss: 1.010705, acc: 0.421875]\n",
      "3244: [discriminator loss: 0.473901, acc: 0.765625] [adversarial loss: 1.813621, acc: 0.046875]\n",
      "3245: [discriminator loss: 0.448849, acc: 0.781250] [adversarial loss: 1.109184, acc: 0.250000]\n",
      "3246: [discriminator loss: 0.514188, acc: 0.734375] [adversarial loss: 1.651630, acc: 0.062500]\n",
      "3247: [discriminator loss: 0.461968, acc: 0.796875] [adversarial loss: 1.189506, acc: 0.312500]\n",
      "3248: [discriminator loss: 0.452159, acc: 0.789062] [adversarial loss: 1.622294, acc: 0.171875]\n",
      "3249: [discriminator loss: 0.484154, acc: 0.781250] [adversarial loss: 1.122072, acc: 0.281250]\n",
      "3250: [discriminator loss: 0.454060, acc: 0.820312] [adversarial loss: 1.366170, acc: 0.218750]\n",
      "3251: [discriminator loss: 0.457069, acc: 0.789062] [adversarial loss: 1.541155, acc: 0.156250]\n",
      "3252: [discriminator loss: 0.476358, acc: 0.781250] [adversarial loss: 1.288768, acc: 0.171875]\n",
      "3253: [discriminator loss: 0.481316, acc: 0.757812] [adversarial loss: 1.155250, acc: 0.328125]\n",
      "3254: [discriminator loss: 0.465056, acc: 0.765625] [adversarial loss: 2.227407, acc: 0.046875]\n",
      "3255: [discriminator loss: 0.578494, acc: 0.726562] [adversarial loss: 0.985694, acc: 0.359375]\n",
      "3256: [discriminator loss: 0.478593, acc: 0.757812] [adversarial loss: 1.742003, acc: 0.109375]\n",
      "3257: [discriminator loss: 0.575328, acc: 0.734375] [adversarial loss: 0.923114, acc: 0.453125]\n",
      "3258: [discriminator loss: 0.555895, acc: 0.718750] [adversarial loss: 1.987907, acc: 0.062500]\n",
      "3259: [discriminator loss: 0.510605, acc: 0.718750] [adversarial loss: 1.063577, acc: 0.328125]\n",
      "3260: [discriminator loss: 0.629957, acc: 0.679688] [adversarial loss: 1.786175, acc: 0.078125]\n",
      "3261: [discriminator loss: 0.559926, acc: 0.726562] [adversarial loss: 1.201524, acc: 0.234375]\n",
      "3262: [discriminator loss: 0.551077, acc: 0.695312] [adversarial loss: 1.374822, acc: 0.203125]\n",
      "3263: [discriminator loss: 0.456011, acc: 0.765625] [adversarial loss: 1.087505, acc: 0.312500]\n",
      "3264: [discriminator loss: 0.482364, acc: 0.789062] [adversarial loss: 1.447808, acc: 0.156250]\n",
      "3265: [discriminator loss: 0.379637, acc: 0.867188] [adversarial loss: 1.147312, acc: 0.265625]\n",
      "3266: [discriminator loss: 0.472105, acc: 0.773438] [adversarial loss: 1.471285, acc: 0.156250]\n",
      "3267: [discriminator loss: 0.469551, acc: 0.726562] [adversarial loss: 1.459912, acc: 0.187500]\n",
      "3268: [discriminator loss: 0.523977, acc: 0.757812] [adversarial loss: 1.501477, acc: 0.156250]\n",
      "3269: [discriminator loss: 0.576978, acc: 0.695312] [adversarial loss: 1.002533, acc: 0.343750]\n",
      "3270: [discriminator loss: 0.515593, acc: 0.742188] [adversarial loss: 1.718346, acc: 0.140625]\n",
      "3271: [discriminator loss: 0.510387, acc: 0.726562] [adversarial loss: 1.038305, acc: 0.328125]\n",
      "3272: [discriminator loss: 0.550640, acc: 0.742188] [adversarial loss: 1.952134, acc: 0.062500]\n",
      "3273: [discriminator loss: 0.550063, acc: 0.710938] [adversarial loss: 1.105491, acc: 0.406250]\n",
      "3274: [discriminator loss: 0.497531, acc: 0.765625] [adversarial loss: 1.668664, acc: 0.078125]\n",
      "3275: [discriminator loss: 0.455914, acc: 0.773438] [adversarial loss: 1.342417, acc: 0.203125]\n",
      "3276: [discriminator loss: 0.513250, acc: 0.710938] [adversarial loss: 1.493485, acc: 0.093750]\n",
      "3277: [discriminator loss: 0.474418, acc: 0.765625] [adversarial loss: 1.116907, acc: 0.281250]\n",
      "3278: [discriminator loss: 0.480816, acc: 0.734375] [adversarial loss: 1.573294, acc: 0.093750]\n",
      "3279: [discriminator loss: 0.495577, acc: 0.742188] [adversarial loss: 1.122360, acc: 0.218750]\n",
      "3280: [discriminator loss: 0.478852, acc: 0.789062] [adversarial loss: 1.785292, acc: 0.078125]\n",
      "3281: [discriminator loss: 0.572214, acc: 0.718750] [adversarial loss: 1.404262, acc: 0.156250]\n",
      "3282: [discriminator loss: 0.463607, acc: 0.773438] [adversarial loss: 1.618034, acc: 0.125000]\n",
      "3283: [discriminator loss: 0.502312, acc: 0.750000] [adversarial loss: 1.264257, acc: 0.296875]\n",
      "3284: [discriminator loss: 0.492207, acc: 0.742188] [adversarial loss: 1.677450, acc: 0.078125]\n",
      "3285: [discriminator loss: 0.531030, acc: 0.742188] [adversarial loss: 1.204240, acc: 0.203125]\n",
      "3286: [discriminator loss: 0.560962, acc: 0.718750] [adversarial loss: 2.172669, acc: 0.031250]\n",
      "3287: [discriminator loss: 0.487853, acc: 0.757812] [adversarial loss: 1.156393, acc: 0.343750]\n",
      "3288: [discriminator loss: 0.553851, acc: 0.742188] [adversarial loss: 1.755786, acc: 0.078125]\n",
      "3289: [discriminator loss: 0.520904, acc: 0.726562] [adversarial loss: 1.137242, acc: 0.312500]\n",
      "3290: [discriminator loss: 0.516467, acc: 0.781250] [adversarial loss: 1.616140, acc: 0.109375]\n",
      "3291: [discriminator loss: 0.480507, acc: 0.789062] [adversarial loss: 1.082034, acc: 0.312500]\n",
      "3292: [discriminator loss: 0.528080, acc: 0.757812] [adversarial loss: 1.912099, acc: 0.109375]\n",
      "3293: [discriminator loss: 0.482110, acc: 0.750000] [adversarial loss: 1.319721, acc: 0.156250]\n",
      "3294: [discriminator loss: 0.468448, acc: 0.750000] [adversarial loss: 1.429037, acc: 0.171875]\n",
      "3295: [discriminator loss: 0.415549, acc: 0.812500] [adversarial loss: 1.325361, acc: 0.187500]\n",
      "3296: [discriminator loss: 0.461042, acc: 0.796875] [adversarial loss: 1.285442, acc: 0.234375]\n",
      "3297: [discriminator loss: 0.519420, acc: 0.789062] [adversarial loss: 1.747537, acc: 0.125000]\n",
      "3298: [discriminator loss: 0.519418, acc: 0.710938] [adversarial loss: 0.996949, acc: 0.375000]\n",
      "3299: [discriminator loss: 0.405376, acc: 0.812500] [adversarial loss: 1.683860, acc: 0.156250]\n",
      "3300: [discriminator loss: 0.438285, acc: 0.765625] [adversarial loss: 0.844586, acc: 0.468750]\n",
      "3301: [discriminator loss: 0.616075, acc: 0.640625] [adversarial loss: 2.064802, acc: 0.078125]\n",
      "3302: [discriminator loss: 0.560351, acc: 0.648438] [adversarial loss: 1.122127, acc: 0.328125]\n",
      "3303: [discriminator loss: 0.475964, acc: 0.757812] [adversarial loss: 1.278501, acc: 0.250000]\n",
      "3304: [discriminator loss: 0.614048, acc: 0.664062] [adversarial loss: 1.676853, acc: 0.078125]\n",
      "3305: [discriminator loss: 0.548637, acc: 0.734375] [adversarial loss: 1.292531, acc: 0.171875]\n",
      "3306: [discriminator loss: 0.472896, acc: 0.742188] [adversarial loss: 1.354275, acc: 0.203125]\n",
      "3307: [discriminator loss: 0.435120, acc: 0.812500] [adversarial loss: 1.634987, acc: 0.140625]\n",
      "3308: [discriminator loss: 0.447820, acc: 0.804688] [adversarial loss: 1.366253, acc: 0.218750]\n",
      "3309: [discriminator loss: 0.480254, acc: 0.804688] [adversarial loss: 1.231574, acc: 0.250000]\n",
      "3310: [discriminator loss: 0.520343, acc: 0.710938] [adversarial loss: 1.470784, acc: 0.187500]\n",
      "3311: [discriminator loss: 0.573598, acc: 0.695312] [adversarial loss: 1.052646, acc: 0.359375]\n",
      "3312: [discriminator loss: 0.452441, acc: 0.828125] [adversarial loss: 1.627117, acc: 0.140625]\n",
      "3313: [discriminator loss: 0.525743, acc: 0.765625] [adversarial loss: 1.490056, acc: 0.203125]\n",
      "3314: [discriminator loss: 0.425873, acc: 0.796875] [adversarial loss: 1.496593, acc: 0.140625]\n",
      "3315: [discriminator loss: 0.457812, acc: 0.789062] [adversarial loss: 1.427587, acc: 0.171875]\n",
      "3316: [discriminator loss: 0.478310, acc: 0.781250] [adversarial loss: 1.440628, acc: 0.171875]\n",
      "3317: [discriminator loss: 0.433602, acc: 0.796875] [adversarial loss: 1.298393, acc: 0.250000]\n",
      "3318: [discriminator loss: 0.453761, acc: 0.804688] [adversarial loss: 1.109957, acc: 0.312500]\n",
      "3319: [discriminator loss: 0.468369, acc: 0.789062] [adversarial loss: 1.850869, acc: 0.093750]\n",
      "3320: [discriminator loss: 0.618622, acc: 0.687500] [adversarial loss: 0.968911, acc: 0.406250]\n",
      "3321: [discriminator loss: 0.557294, acc: 0.718750] [adversarial loss: 2.172396, acc: 0.031250]\n",
      "3322: [discriminator loss: 0.647579, acc: 0.656250] [adversarial loss: 0.844194, acc: 0.515625]\n",
      "3323: [discriminator loss: 0.630292, acc: 0.671875] [adversarial loss: 1.850165, acc: 0.093750]\n",
      "3324: [discriminator loss: 0.516936, acc: 0.742188] [adversarial loss: 0.860768, acc: 0.421875]\n",
      "3325: [discriminator loss: 0.546498, acc: 0.734375] [adversarial loss: 1.824522, acc: 0.125000]\n",
      "3326: [discriminator loss: 0.531134, acc: 0.726562] [adversarial loss: 1.050614, acc: 0.359375]\n",
      "3327: [discriminator loss: 0.519261, acc: 0.750000] [adversarial loss: 1.762982, acc: 0.125000]\n",
      "3328: [discriminator loss: 0.454115, acc: 0.781250] [adversarial loss: 1.474838, acc: 0.203125]\n",
      "3329: [discriminator loss: 0.499937, acc: 0.804688] [adversarial loss: 1.323788, acc: 0.171875]\n",
      "3330: [discriminator loss: 0.456578, acc: 0.820312] [adversarial loss: 1.396396, acc: 0.218750]\n",
      "3331: [discriminator loss: 0.359961, acc: 0.875000] [adversarial loss: 1.551217, acc: 0.140625]\n",
      "3332: [discriminator loss: 0.453419, acc: 0.781250] [adversarial loss: 1.041029, acc: 0.296875]\n",
      "3333: [discriminator loss: 0.550959, acc: 0.773438] [adversarial loss: 1.354777, acc: 0.281250]\n",
      "3334: [discriminator loss: 0.519607, acc: 0.718750] [adversarial loss: 1.329053, acc: 0.265625]\n",
      "3335: [discriminator loss: 0.430116, acc: 0.804688] [adversarial loss: 1.474365, acc: 0.187500]\n",
      "3336: [discriminator loss: 0.468894, acc: 0.804688] [adversarial loss: 1.188129, acc: 0.296875]\n",
      "3337: [discriminator loss: 0.510531, acc: 0.742188] [adversarial loss: 1.207486, acc: 0.234375]\n",
      "3338: [discriminator loss: 0.516821, acc: 0.726562] [adversarial loss: 1.164743, acc: 0.328125]\n",
      "3339: [discriminator loss: 0.444284, acc: 0.781250] [adversarial loss: 1.457886, acc: 0.203125]\n",
      "3340: [discriminator loss: 0.433441, acc: 0.789062] [adversarial loss: 1.344807, acc: 0.281250]\n",
      "3341: [discriminator loss: 0.429481, acc: 0.781250] [adversarial loss: 1.384949, acc: 0.171875]\n",
      "3342: [discriminator loss: 0.532779, acc: 0.718750] [adversarial loss: 1.535252, acc: 0.203125]\n",
      "3343: [discriminator loss: 0.425007, acc: 0.820312] [adversarial loss: 1.346991, acc: 0.281250]\n",
      "3344: [discriminator loss: 0.521342, acc: 0.710938] [adversarial loss: 1.840281, acc: 0.046875]\n",
      "3345: [discriminator loss: 0.505132, acc: 0.726562] [adversarial loss: 0.834153, acc: 0.437500]\n",
      "3346: [discriminator loss: 0.482819, acc: 0.757812] [adversarial loss: 1.816554, acc: 0.046875]\n",
      "3347: [discriminator loss: 0.533393, acc: 0.710938] [adversarial loss: 0.813785, acc: 0.500000]\n",
      "3348: [discriminator loss: 0.457560, acc: 0.789062] [adversarial loss: 1.481187, acc: 0.250000]\n",
      "3349: [discriminator loss: 0.523516, acc: 0.726562] [adversarial loss: 1.190218, acc: 0.218750]\n",
      "3350: [discriminator loss: 0.443675, acc: 0.773438] [adversarial loss: 1.678006, acc: 0.078125]\n",
      "3351: [discriminator loss: 0.422989, acc: 0.789062] [adversarial loss: 1.380582, acc: 0.234375]\n",
      "3352: [discriminator loss: 0.550653, acc: 0.710938] [adversarial loss: 1.150976, acc: 0.328125]\n",
      "3353: [discriminator loss: 0.466582, acc: 0.750000] [adversarial loss: 1.640650, acc: 0.140625]\n",
      "3354: [discriminator loss: 0.539291, acc: 0.757812] [adversarial loss: 1.098805, acc: 0.312500]\n",
      "3355: [discriminator loss: 0.448690, acc: 0.812500] [adversarial loss: 1.581372, acc: 0.171875]\n",
      "3356: [discriminator loss: 0.421524, acc: 0.820312] [adversarial loss: 0.981659, acc: 0.421875]\n",
      "3357: [discriminator loss: 0.512174, acc: 0.726562] [adversarial loss: 2.254665, acc: 0.062500]\n",
      "3358: [discriminator loss: 0.584095, acc: 0.718750] [adversarial loss: 0.887241, acc: 0.468750]\n",
      "3359: [discriminator loss: 0.566255, acc: 0.718750] [adversarial loss: 1.905313, acc: 0.125000]\n",
      "3360: [discriminator loss: 0.559455, acc: 0.750000] [adversarial loss: 1.093855, acc: 0.234375]\n",
      "3361: [discriminator loss: 0.477624, acc: 0.742188] [adversarial loss: 1.624749, acc: 0.109375]\n",
      "3362: [discriminator loss: 0.499086, acc: 0.765625] [adversarial loss: 1.084637, acc: 0.359375]\n",
      "3363: [discriminator loss: 0.595864, acc: 0.687500] [adversarial loss: 1.954936, acc: 0.171875]\n",
      "3364: [discriminator loss: 0.503533, acc: 0.765625] [adversarial loss: 1.146353, acc: 0.343750]\n",
      "3365: [discriminator loss: 0.483167, acc: 0.765625] [adversarial loss: 1.759914, acc: 0.093750]\n",
      "3366: [discriminator loss: 0.525626, acc: 0.765625] [adversarial loss: 1.157221, acc: 0.250000]\n",
      "3367: [discriminator loss: 0.528198, acc: 0.687500] [adversarial loss: 1.783886, acc: 0.109375]\n",
      "3368: [discriminator loss: 0.619034, acc: 0.656250] [adversarial loss: 1.092427, acc: 0.328125]\n",
      "3369: [discriminator loss: 0.496527, acc: 0.757812] [adversarial loss: 1.402529, acc: 0.093750]\n",
      "3370: [discriminator loss: 0.437415, acc: 0.773438] [adversarial loss: 1.127888, acc: 0.343750]\n",
      "3371: [discriminator loss: 0.546816, acc: 0.695312] [adversarial loss: 1.288355, acc: 0.312500]\n",
      "3372: [discriminator loss: 0.570315, acc: 0.695312] [adversarial loss: 1.520906, acc: 0.250000]\n",
      "3373: [discriminator loss: 0.469044, acc: 0.757812] [adversarial loss: 1.461057, acc: 0.125000]\n",
      "3374: [discriminator loss: 0.507293, acc: 0.718750] [adversarial loss: 1.399280, acc: 0.140625]\n",
      "3375: [discriminator loss: 0.505775, acc: 0.750000] [adversarial loss: 1.763126, acc: 0.062500]\n",
      "3376: [discriminator loss: 0.462383, acc: 0.773438] [adversarial loss: 0.968418, acc: 0.375000]\n",
      "3377: [discriminator loss: 0.489311, acc: 0.781250] [adversarial loss: 1.508495, acc: 0.203125]\n",
      "3378: [discriminator loss: 0.414848, acc: 0.820312] [adversarial loss: 1.373445, acc: 0.187500]\n",
      "3379: [discriminator loss: 0.524117, acc: 0.757812] [adversarial loss: 1.144486, acc: 0.265625]\n",
      "3380: [discriminator loss: 0.464601, acc: 0.796875] [adversarial loss: 1.893567, acc: 0.093750]\n",
      "3381: [discriminator loss: 0.560017, acc: 0.710938] [adversarial loss: 0.753107, acc: 0.484375]\n",
      "3382: [discriminator loss: 0.661959, acc: 0.640625] [adversarial loss: 2.299484, acc: 0.046875]\n",
      "3383: [discriminator loss: 0.562480, acc: 0.703125] [adversarial loss: 0.930038, acc: 0.421875]\n",
      "3384: [discriminator loss: 0.577587, acc: 0.718750] [adversarial loss: 1.726268, acc: 0.171875]\n",
      "3385: [discriminator loss: 0.521838, acc: 0.781250] [adversarial loss: 1.204086, acc: 0.265625]\n",
      "3386: [discriminator loss: 0.414749, acc: 0.820312] [adversarial loss: 1.459257, acc: 0.171875]\n",
      "3387: [discriminator loss: 0.428921, acc: 0.828125] [adversarial loss: 1.741804, acc: 0.093750]\n",
      "3388: [discriminator loss: 0.480195, acc: 0.765625] [adversarial loss: 1.565795, acc: 0.125000]\n",
      "3389: [discriminator loss: 0.441768, acc: 0.750000] [adversarial loss: 1.470775, acc: 0.171875]\n",
      "3390: [discriminator loss: 0.516556, acc: 0.703125] [adversarial loss: 1.504708, acc: 0.156250]\n",
      "3391: [discriminator loss: 0.447386, acc: 0.796875] [adversarial loss: 1.276515, acc: 0.203125]\n",
      "3392: [discriminator loss: 0.470341, acc: 0.765625] [adversarial loss: 1.549008, acc: 0.187500]\n",
      "3393: [discriminator loss: 0.426427, acc: 0.820312] [adversarial loss: 1.251468, acc: 0.328125]\n",
      "3394: [discriminator loss: 0.469216, acc: 0.726562] [adversarial loss: 1.230111, acc: 0.234375]\n",
      "3395: [discriminator loss: 0.578860, acc: 0.718750] [adversarial loss: 1.804284, acc: 0.078125]\n",
      "3396: [discriminator loss: 0.507263, acc: 0.710938] [adversarial loss: 0.971094, acc: 0.296875]\n",
      "3397: [discriminator loss: 0.515683, acc: 0.726562] [adversarial loss: 1.876940, acc: 0.062500]\n",
      "3398: [discriminator loss: 0.480791, acc: 0.757812] [adversarial loss: 1.235932, acc: 0.281250]\n",
      "3399: [discriminator loss: 0.502345, acc: 0.757812] [adversarial loss: 1.410857, acc: 0.250000]\n",
      "3400: [discriminator loss: 0.512764, acc: 0.726562] [adversarial loss: 1.279627, acc: 0.234375]\n",
      "3401: [discriminator loss: 0.471710, acc: 0.781250] [adversarial loss: 1.146946, acc: 0.375000]\n",
      "3402: [discriminator loss: 0.485657, acc: 0.765625] [adversarial loss: 1.312129, acc: 0.250000]\n",
      "3403: [discriminator loss: 0.482334, acc: 0.742188] [adversarial loss: 1.675664, acc: 0.109375]\n",
      "3404: [discriminator loss: 0.549679, acc: 0.742188] [adversarial loss: 1.142155, acc: 0.265625]\n",
      "3405: [discriminator loss: 0.601379, acc: 0.695312] [adversarial loss: 2.104355, acc: 0.046875]\n",
      "3406: [discriminator loss: 0.467492, acc: 0.812500] [adversarial loss: 0.930218, acc: 0.421875]\n",
      "3407: [discriminator loss: 0.465602, acc: 0.765625] [adversarial loss: 1.908504, acc: 0.093750]\n",
      "3408: [discriminator loss: 0.450502, acc: 0.734375] [adversarial loss: 0.917045, acc: 0.406250]\n",
      "3409: [discriminator loss: 0.573859, acc: 0.703125] [adversarial loss: 1.922499, acc: 0.093750]\n",
      "3410: [discriminator loss: 0.610894, acc: 0.679688] [adversarial loss: 0.986249, acc: 0.468750]\n",
      "3411: [discriminator loss: 0.592516, acc: 0.695312] [adversarial loss: 1.469653, acc: 0.125000]\n",
      "3412: [discriminator loss: 0.506886, acc: 0.703125] [adversarial loss: 1.031544, acc: 0.359375]\n",
      "3413: [discriminator loss: 0.574308, acc: 0.695312] [adversarial loss: 1.610357, acc: 0.078125]\n",
      "3414: [discriminator loss: 0.554530, acc: 0.734375] [adversarial loss: 1.185884, acc: 0.203125]\n",
      "3415: [discriminator loss: 0.429232, acc: 0.820312] [adversarial loss: 1.896949, acc: 0.140625]\n",
      "3416: [discriminator loss: 0.481256, acc: 0.781250] [adversarial loss: 1.425338, acc: 0.125000]\n",
      "3417: [discriminator loss: 0.461975, acc: 0.765625] [adversarial loss: 1.474707, acc: 0.140625]\n",
      "3418: [discriminator loss: 0.476737, acc: 0.757812] [adversarial loss: 1.194023, acc: 0.281250]\n",
      "3419: [discriminator loss: 0.606826, acc: 0.664062] [adversarial loss: 2.050005, acc: 0.046875]\n",
      "3420: [discriminator loss: 0.565280, acc: 0.671875] [adversarial loss: 0.958591, acc: 0.406250]\n",
      "3421: [discriminator loss: 0.577936, acc: 0.640625] [adversarial loss: 1.737203, acc: 0.093750]\n",
      "3422: [discriminator loss: 0.603819, acc: 0.710938] [adversarial loss: 1.004426, acc: 0.328125]\n",
      "3423: [discriminator loss: 0.536008, acc: 0.695312] [adversarial loss: 1.380178, acc: 0.171875]\n",
      "3424: [discriminator loss: 0.497260, acc: 0.773438] [adversarial loss: 1.325450, acc: 0.281250]\n",
      "3425: [discriminator loss: 0.447461, acc: 0.757812] [adversarial loss: 1.175112, acc: 0.234375]\n",
      "3426: [discriminator loss: 0.508934, acc: 0.750000] [adversarial loss: 1.468859, acc: 0.171875]\n",
      "3427: [discriminator loss: 0.505117, acc: 0.765625] [adversarial loss: 1.175601, acc: 0.312500]\n",
      "3428: [discriminator loss: 0.488922, acc: 0.765625] [adversarial loss: 1.709342, acc: 0.125000]\n",
      "3429: [discriminator loss: 0.452398, acc: 0.765625] [adversarial loss: 0.981736, acc: 0.375000]\n",
      "3430: [discriminator loss: 0.466309, acc: 0.750000] [adversarial loss: 1.976634, acc: 0.062500]\n",
      "3431: [discriminator loss: 0.561824, acc: 0.710938] [adversarial loss: 0.867328, acc: 0.406250]\n",
      "3432: [discriminator loss: 0.560661, acc: 0.734375] [adversarial loss: 2.167386, acc: 0.031250]\n",
      "3433: [discriminator loss: 0.589790, acc: 0.703125] [adversarial loss: 1.008129, acc: 0.296875]\n",
      "3434: [discriminator loss: 0.609408, acc: 0.679688] [adversarial loss: 1.685824, acc: 0.156250]\n",
      "3435: [discriminator loss: 0.453215, acc: 0.812500] [adversarial loss: 1.312773, acc: 0.281250]\n",
      "3436: [discriminator loss: 0.458074, acc: 0.804688] [adversarial loss: 1.474357, acc: 0.187500]\n",
      "3437: [discriminator loss: 0.513526, acc: 0.757812] [adversarial loss: 1.361597, acc: 0.218750]\n",
      "3438: [discriminator loss: 0.480660, acc: 0.789062] [adversarial loss: 1.244740, acc: 0.296875]\n",
      "3439: [discriminator loss: 0.451756, acc: 0.796875] [adversarial loss: 1.393156, acc: 0.250000]\n",
      "3440: [discriminator loss: 0.455435, acc: 0.796875] [adversarial loss: 1.308825, acc: 0.250000]\n",
      "3441: [discriminator loss: 0.496317, acc: 0.750000] [adversarial loss: 1.507528, acc: 0.062500]\n",
      "3442: [discriminator loss: 0.559261, acc: 0.703125] [adversarial loss: 1.403301, acc: 0.156250]\n",
      "3443: [discriminator loss: 0.483180, acc: 0.765625] [adversarial loss: 1.143879, acc: 0.312500]\n",
      "3444: [discriminator loss: 0.484039, acc: 0.812500] [adversarial loss: 1.356857, acc: 0.250000]\n",
      "3445: [discriminator loss: 0.506617, acc: 0.750000] [adversarial loss: 1.635207, acc: 0.171875]\n",
      "3446: [discriminator loss: 0.573416, acc: 0.695312] [adversarial loss: 0.746063, acc: 0.562500]\n",
      "3447: [discriminator loss: 0.428734, acc: 0.804688] [adversarial loss: 1.403187, acc: 0.125000]\n",
      "3448: [discriminator loss: 0.500504, acc: 0.750000] [adversarial loss: 1.371272, acc: 0.140625]\n",
      "3449: [discriminator loss: 0.515621, acc: 0.718750] [adversarial loss: 1.543026, acc: 0.093750]\n",
      "3450: [discriminator loss: 0.467751, acc: 0.757812] [adversarial loss: 1.287135, acc: 0.203125]\n",
      "3451: [discriminator loss: 0.468519, acc: 0.765625] [adversarial loss: 1.311845, acc: 0.203125]\n",
      "3452: [discriminator loss: 0.483438, acc: 0.781250] [adversarial loss: 1.221771, acc: 0.265625]\n",
      "3453: [discriminator loss: 0.516244, acc: 0.734375] [adversarial loss: 1.187115, acc: 0.375000]\n",
      "3454: [discriminator loss: 0.533525, acc: 0.750000] [adversarial loss: 1.550630, acc: 0.109375]\n",
      "3455: [discriminator loss: 0.550985, acc: 0.703125] [adversarial loss: 1.243794, acc: 0.281250]\n",
      "3456: [discriminator loss: 0.455968, acc: 0.796875] [adversarial loss: 1.597630, acc: 0.156250]\n",
      "3457: [discriminator loss: 0.622625, acc: 0.687500] [adversarial loss: 1.398467, acc: 0.265625]\n",
      "3458: [discriminator loss: 0.555601, acc: 0.695312] [adversarial loss: 0.865246, acc: 0.453125]\n",
      "3459: [discriminator loss: 0.545908, acc: 0.718750] [adversarial loss: 2.331316, acc: 0.000000]\n",
      "3460: [discriminator loss: 0.616657, acc: 0.679688] [adversarial loss: 0.857280, acc: 0.437500]\n",
      "3461: [discriminator loss: 0.639567, acc: 0.648438] [adversarial loss: 1.869622, acc: 0.093750]\n",
      "3462: [discriminator loss: 0.533140, acc: 0.765625] [adversarial loss: 1.225981, acc: 0.218750]\n",
      "3463: [discriminator loss: 0.586474, acc: 0.671875] [adversarial loss: 1.355307, acc: 0.140625]\n",
      "3464: [discriminator loss: 0.436955, acc: 0.765625] [adversarial loss: 1.286089, acc: 0.218750]\n",
      "3465: [discriminator loss: 0.472955, acc: 0.789062] [adversarial loss: 1.362609, acc: 0.203125]\n",
      "3466: [discriminator loss: 0.463777, acc: 0.789062] [adversarial loss: 1.466907, acc: 0.171875]\n",
      "3467: [discriminator loss: 0.437115, acc: 0.820312] [adversarial loss: 1.256014, acc: 0.218750]\n",
      "3468: [discriminator loss: 0.518150, acc: 0.742188] [adversarial loss: 1.390594, acc: 0.187500]\n",
      "3469: [discriminator loss: 0.489475, acc: 0.750000] [adversarial loss: 1.726653, acc: 0.140625]\n",
      "3470: [discriminator loss: 0.423757, acc: 0.820312] [adversarial loss: 0.995398, acc: 0.468750]\n",
      "3471: [discriminator loss: 0.515978, acc: 0.710938] [adversarial loss: 1.660784, acc: 0.140625]\n",
      "3472: [discriminator loss: 0.521871, acc: 0.757812] [adversarial loss: 1.059228, acc: 0.359375]\n",
      "3473: [discriminator loss: 0.512746, acc: 0.765625] [adversarial loss: 1.640682, acc: 0.109375]\n",
      "3474: [discriminator loss: 0.517518, acc: 0.789062] [adversarial loss: 0.885567, acc: 0.359375]\n",
      "3475: [discriminator loss: 0.547673, acc: 0.750000] [adversarial loss: 2.281281, acc: 0.046875]\n",
      "3476: [discriminator loss: 0.565257, acc: 0.765625] [adversarial loss: 0.923255, acc: 0.437500]\n",
      "3477: [discriminator loss: 0.526415, acc: 0.695312] [adversarial loss: 1.646017, acc: 0.156250]\n",
      "3478: [discriminator loss: 0.434164, acc: 0.812500] [adversarial loss: 1.198734, acc: 0.281250]\n",
      "3479: [discriminator loss: 0.527413, acc: 0.703125] [adversarial loss: 1.723489, acc: 0.078125]\n",
      "3480: [discriminator loss: 0.626161, acc: 0.671875] [adversarial loss: 0.848379, acc: 0.421875]\n",
      "3481: [discriminator loss: 0.531130, acc: 0.734375] [adversarial loss: 1.770245, acc: 0.093750]\n",
      "3482: [discriminator loss: 0.528324, acc: 0.757812] [adversarial loss: 0.915093, acc: 0.390625]\n",
      "3483: [discriminator loss: 0.525972, acc: 0.734375] [adversarial loss: 1.702556, acc: 0.109375]\n",
      "3484: [discriminator loss: 0.493079, acc: 0.750000] [adversarial loss: 1.208712, acc: 0.234375]\n",
      "3485: [discriminator loss: 0.515516, acc: 0.765625] [adversarial loss: 1.372720, acc: 0.156250]\n",
      "3486: [discriminator loss: 0.504267, acc: 0.742188] [adversarial loss: 0.929574, acc: 0.375000]\n",
      "3487: [discriminator loss: 0.592225, acc: 0.703125] [adversarial loss: 1.836504, acc: 0.093750]\n",
      "3488: [discriminator loss: 0.466752, acc: 0.789062] [adversarial loss: 1.314062, acc: 0.265625]\n",
      "3489: [discriminator loss: 0.501425, acc: 0.703125] [adversarial loss: 1.252456, acc: 0.234375]\n",
      "3490: [discriminator loss: 0.507821, acc: 0.734375] [adversarial loss: 1.437285, acc: 0.218750]\n",
      "3491: [discriminator loss: 0.542917, acc: 0.742188] [adversarial loss: 1.265208, acc: 0.171875]\n",
      "3492: [discriminator loss: 0.518968, acc: 0.750000] [adversarial loss: 1.061090, acc: 0.406250]\n",
      "3493: [discriminator loss: 0.521564, acc: 0.726562] [adversarial loss: 1.269969, acc: 0.187500]\n",
      "3494: [discriminator loss: 0.475217, acc: 0.765625] [adversarial loss: 1.455619, acc: 0.125000]\n",
      "3495: [discriminator loss: 0.453391, acc: 0.781250] [adversarial loss: 1.409505, acc: 0.171875]\n",
      "3496: [discriminator loss: 0.523568, acc: 0.742188] [adversarial loss: 0.769148, acc: 0.484375]\n",
      "3497: [discriminator loss: 0.456595, acc: 0.781250] [adversarial loss: 1.697635, acc: 0.046875]\n",
      "3498: [discriminator loss: 0.532732, acc: 0.664062] [adversarial loss: 0.932143, acc: 0.390625]\n",
      "3499: [discriminator loss: 0.515106, acc: 0.789062] [adversarial loss: 1.652999, acc: 0.140625]\n",
      "3500: [discriminator loss: 0.449713, acc: 0.812500] [adversarial loss: 1.169674, acc: 0.187500]\n",
      "3501: [discriminator loss: 0.539016, acc: 0.687500] [adversarial loss: 2.053166, acc: 0.031250]\n",
      "3502: [discriminator loss: 0.523993, acc: 0.710938] [adversarial loss: 1.022511, acc: 0.328125]\n",
      "3503: [discriminator loss: 0.534968, acc: 0.671875] [adversarial loss: 1.713719, acc: 0.062500]\n",
      "3504: [discriminator loss: 0.494393, acc: 0.750000] [adversarial loss: 1.437297, acc: 0.203125]\n",
      "3505: [discriminator loss: 0.477955, acc: 0.750000] [adversarial loss: 1.292619, acc: 0.250000]\n",
      "3506: [discriminator loss: 0.505405, acc: 0.734375] [adversarial loss: 1.701568, acc: 0.171875]\n",
      "3507: [discriminator loss: 0.456737, acc: 0.773438] [adversarial loss: 1.124741, acc: 0.312500]\n",
      "3508: [discriminator loss: 0.577402, acc: 0.718750] [adversarial loss: 1.856477, acc: 0.093750]\n",
      "3509: [discriminator loss: 0.643629, acc: 0.656250] [adversarial loss: 0.953369, acc: 0.421875]\n",
      "3510: [discriminator loss: 0.469902, acc: 0.781250] [adversarial loss: 1.713493, acc: 0.078125]\n",
      "3511: [discriminator loss: 0.497623, acc: 0.742188] [adversarial loss: 1.075515, acc: 0.281250]\n",
      "3512: [discriminator loss: 0.433886, acc: 0.781250] [adversarial loss: 1.234416, acc: 0.312500]\n",
      "3513: [discriminator loss: 0.550290, acc: 0.750000] [adversarial loss: 1.398043, acc: 0.171875]\n",
      "3514: [discriminator loss: 0.535207, acc: 0.742188] [adversarial loss: 1.088831, acc: 0.296875]\n",
      "3515: [discriminator loss: 0.488779, acc: 0.781250] [adversarial loss: 1.350386, acc: 0.187500]\n",
      "3516: [discriminator loss: 0.516539, acc: 0.750000] [adversarial loss: 0.989887, acc: 0.296875]\n",
      "3517: [discriminator loss: 0.515215, acc: 0.726562] [adversarial loss: 1.874813, acc: 0.156250]\n",
      "3518: [discriminator loss: 0.477059, acc: 0.750000] [adversarial loss: 0.964497, acc: 0.359375]\n",
      "3519: [discriminator loss: 0.440827, acc: 0.812500] [adversarial loss: 1.871066, acc: 0.062500]\n",
      "3520: [discriminator loss: 0.474557, acc: 0.757812] [adversarial loss: 1.039391, acc: 0.312500]\n",
      "3521: [discriminator loss: 0.599523, acc: 0.710938] [adversarial loss: 2.000141, acc: 0.062500]\n",
      "3522: [discriminator loss: 0.610724, acc: 0.718750] [adversarial loss: 1.139460, acc: 0.375000]\n",
      "3523: [discriminator loss: 0.520491, acc: 0.773438] [adversarial loss: 1.294448, acc: 0.250000]\n",
      "3524: [discriminator loss: 0.492111, acc: 0.773438] [adversarial loss: 1.070994, acc: 0.406250]\n",
      "3525: [discriminator loss: 0.503289, acc: 0.726562] [adversarial loss: 1.431767, acc: 0.140625]\n",
      "3526: [discriminator loss: 0.527306, acc: 0.765625] [adversarial loss: 1.305911, acc: 0.187500]\n",
      "3527: [discriminator loss: 0.438679, acc: 0.781250] [adversarial loss: 1.280087, acc: 0.234375]\n",
      "3528: [discriminator loss: 0.486064, acc: 0.789062] [adversarial loss: 1.217940, acc: 0.218750]\n",
      "3529: [discriminator loss: 0.556162, acc: 0.718750] [adversarial loss: 1.476637, acc: 0.187500]\n",
      "3530: [discriminator loss: 0.545965, acc: 0.757812] [adversarial loss: 1.212259, acc: 0.234375]\n",
      "3531: [discriminator loss: 0.466998, acc: 0.773438] [adversarial loss: 1.144919, acc: 0.265625]\n",
      "3532: [discriminator loss: 0.457530, acc: 0.789062] [adversarial loss: 1.613795, acc: 0.093750]\n",
      "3533: [discriminator loss: 0.486605, acc: 0.796875] [adversarial loss: 1.229007, acc: 0.296875]\n",
      "3534: [discriminator loss: 0.477630, acc: 0.789062] [adversarial loss: 1.431086, acc: 0.156250]\n",
      "3535: [discriminator loss: 0.447349, acc: 0.773438] [adversarial loss: 0.991817, acc: 0.375000]\n",
      "3536: [discriminator loss: 0.446428, acc: 0.789062] [adversarial loss: 1.559452, acc: 0.140625]\n",
      "3537: [discriminator loss: 0.538546, acc: 0.718750] [adversarial loss: 0.952867, acc: 0.375000]\n",
      "3538: [discriminator loss: 0.545902, acc: 0.734375] [adversarial loss: 1.767826, acc: 0.062500]\n",
      "3539: [discriminator loss: 0.490433, acc: 0.742188] [adversarial loss: 1.002869, acc: 0.359375]\n",
      "3540: [discriminator loss: 0.472899, acc: 0.750000] [adversarial loss: 1.532272, acc: 0.156250]\n",
      "3541: [discriminator loss: 0.508521, acc: 0.734375] [adversarial loss: 0.903537, acc: 0.453125]\n",
      "3542: [discriminator loss: 0.558074, acc: 0.695312] [adversarial loss: 1.772623, acc: 0.078125]\n",
      "3543: [discriminator loss: 0.478536, acc: 0.773438] [adversarial loss: 1.026315, acc: 0.390625]\n",
      "3544: [discriminator loss: 0.503061, acc: 0.765625] [adversarial loss: 1.701125, acc: 0.140625]\n",
      "3545: [discriminator loss: 0.480856, acc: 0.773438] [adversarial loss: 1.192850, acc: 0.312500]\n",
      "3546: [discriminator loss: 0.494411, acc: 0.773438] [adversarial loss: 1.720345, acc: 0.109375]\n",
      "3547: [discriminator loss: 0.619666, acc: 0.640625] [adversarial loss: 1.075919, acc: 0.250000]\n",
      "3548: [discriminator loss: 0.521760, acc: 0.742188] [adversarial loss: 1.711596, acc: 0.156250]\n",
      "3549: [discriminator loss: 0.485598, acc: 0.773438] [adversarial loss: 1.653997, acc: 0.093750]\n",
      "3550: [discriminator loss: 0.445191, acc: 0.781250] [adversarial loss: 1.152864, acc: 0.234375]\n",
      "3551: [discriminator loss: 0.422998, acc: 0.812500] [adversarial loss: 1.708169, acc: 0.093750]\n",
      "3552: [discriminator loss: 0.545081, acc: 0.734375] [adversarial loss: 1.135786, acc: 0.250000]\n",
      "3553: [discriminator loss: 0.540714, acc: 0.703125] [adversarial loss: 1.580960, acc: 0.078125]\n",
      "3554: [discriminator loss: 0.553711, acc: 0.671875] [adversarial loss: 0.869917, acc: 0.375000]\n",
      "3555: [discriminator loss: 0.505445, acc: 0.757812] [adversarial loss: 1.534066, acc: 0.187500]\n",
      "3556: [discriminator loss: 0.532470, acc: 0.718750] [adversarial loss: 0.899943, acc: 0.406250]\n",
      "3557: [discriminator loss: 0.490684, acc: 0.757812] [adversarial loss: 1.853710, acc: 0.125000]\n",
      "3558: [discriminator loss: 0.428766, acc: 0.812500] [adversarial loss: 0.944338, acc: 0.406250]\n",
      "3559: [discriminator loss: 0.552533, acc: 0.757812] [adversarial loss: 1.653103, acc: 0.093750]\n",
      "3560: [discriminator loss: 0.582370, acc: 0.726562] [adversarial loss: 0.789917, acc: 0.531250]\n",
      "3561: [discriminator loss: 0.464570, acc: 0.781250] [adversarial loss: 1.600661, acc: 0.140625]\n",
      "3562: [discriminator loss: 0.532627, acc: 0.718750] [adversarial loss: 1.401393, acc: 0.140625]\n",
      "3563: [discriminator loss: 0.538575, acc: 0.734375] [adversarial loss: 1.507238, acc: 0.140625]\n",
      "3564: [discriminator loss: 0.494575, acc: 0.773438] [adversarial loss: 1.517009, acc: 0.125000]\n",
      "3565: [discriminator loss: 0.489349, acc: 0.796875] [adversarial loss: 1.472213, acc: 0.156250]\n",
      "3566: [discriminator loss: 0.367324, acc: 0.898438] [adversarial loss: 0.973869, acc: 0.375000]\n",
      "3567: [discriminator loss: 0.492058, acc: 0.710938] [adversarial loss: 1.874789, acc: 0.109375]\n",
      "3568: [discriminator loss: 0.499452, acc: 0.773438] [adversarial loss: 1.087217, acc: 0.359375]\n",
      "3569: [discriminator loss: 0.563699, acc: 0.664062] [adversarial loss: 1.865537, acc: 0.109375]\n",
      "3570: [discriminator loss: 0.486832, acc: 0.757812] [adversarial loss: 0.921131, acc: 0.453125]\n",
      "3571: [discriminator loss: 0.560842, acc: 0.710938] [adversarial loss: 2.028233, acc: 0.046875]\n",
      "3572: [discriminator loss: 0.728573, acc: 0.601562] [adversarial loss: 1.078610, acc: 0.312500]\n",
      "3573: [discriminator loss: 0.460325, acc: 0.804688] [adversarial loss: 1.491546, acc: 0.140625]\n",
      "3574: [discriminator loss: 0.528012, acc: 0.750000] [adversarial loss: 0.976440, acc: 0.390625]\n",
      "3575: [discriminator loss: 0.485401, acc: 0.789062] [adversarial loss: 1.547901, acc: 0.125000]\n",
      "3576: [discriminator loss: 0.596904, acc: 0.695312] [adversarial loss: 1.068211, acc: 0.343750]\n",
      "3577: [discriminator loss: 0.508615, acc: 0.773438] [adversarial loss: 1.555814, acc: 0.171875]\n",
      "3578: [discriminator loss: 0.452797, acc: 0.796875] [adversarial loss: 1.191314, acc: 0.250000]\n",
      "3579: [discriminator loss: 0.464444, acc: 0.750000] [adversarial loss: 1.257716, acc: 0.296875]\n",
      "3580: [discriminator loss: 0.452142, acc: 0.812500] [adversarial loss: 1.092544, acc: 0.328125]\n",
      "3581: [discriminator loss: 0.479849, acc: 0.742188] [adversarial loss: 1.652892, acc: 0.078125]\n",
      "3582: [discriminator loss: 0.484617, acc: 0.765625] [adversarial loss: 1.169675, acc: 0.218750]\n",
      "3583: [discriminator loss: 0.409467, acc: 0.867188] [adversarial loss: 1.477318, acc: 0.140625]\n",
      "3584: [discriminator loss: 0.395660, acc: 0.796875] [adversarial loss: 1.273326, acc: 0.187500]\n",
      "3585: [discriminator loss: 0.424742, acc: 0.820312] [adversarial loss: 1.716903, acc: 0.125000]\n",
      "3586: [discriminator loss: 0.563302, acc: 0.687500] [adversarial loss: 0.842166, acc: 0.484375]\n",
      "3587: [discriminator loss: 0.659728, acc: 0.625000] [adversarial loss: 1.844640, acc: 0.109375]\n",
      "3588: [discriminator loss: 0.469811, acc: 0.750000] [adversarial loss: 1.421130, acc: 0.171875]\n",
      "3589: [discriminator loss: 0.547357, acc: 0.718750] [adversarial loss: 1.414412, acc: 0.187500]\n",
      "3590: [discriminator loss: 0.474669, acc: 0.796875] [adversarial loss: 1.343740, acc: 0.234375]\n",
      "3591: [discriminator loss: 0.472320, acc: 0.757812] [adversarial loss: 1.533453, acc: 0.156250]\n",
      "3592: [discriminator loss: 0.458298, acc: 0.789062] [adversarial loss: 1.270899, acc: 0.234375]\n",
      "3593: [discriminator loss: 0.414459, acc: 0.843750] [adversarial loss: 1.735525, acc: 0.093750]\n",
      "3594: [discriminator loss: 0.424871, acc: 0.812500] [adversarial loss: 0.979966, acc: 0.421875]\n",
      "3595: [discriminator loss: 0.579086, acc: 0.703125] [adversarial loss: 1.921991, acc: 0.062500]\n",
      "3596: [discriminator loss: 0.605803, acc: 0.695312] [adversarial loss: 0.768030, acc: 0.468750]\n",
      "3597: [discriminator loss: 0.584879, acc: 0.679688] [adversarial loss: 1.975714, acc: 0.093750]\n",
      "3598: [discriminator loss: 0.540170, acc: 0.726562] [adversarial loss: 1.260291, acc: 0.203125]\n",
      "3599: [discriminator loss: 0.467398, acc: 0.757812] [adversarial loss: 1.760156, acc: 0.046875]\n",
      "3600: [discriminator loss: 0.542253, acc: 0.734375] [adversarial loss: 1.346013, acc: 0.234375]\n",
      "3601: [discriminator loss: 0.541344, acc: 0.742188] [adversarial loss: 1.352580, acc: 0.203125]\n",
      "3602: [discriminator loss: 0.495873, acc: 0.742188] [adversarial loss: 1.044631, acc: 0.312500]\n",
      "3603: [discriminator loss: 0.463690, acc: 0.820312] [adversarial loss: 1.704545, acc: 0.171875]\n",
      "3604: [discriminator loss: 0.445845, acc: 0.835938] [adversarial loss: 1.191441, acc: 0.265625]\n",
      "3605: [discriminator loss: 0.405544, acc: 0.773438] [adversarial loss: 1.775108, acc: 0.125000]\n",
      "3606: [discriminator loss: 0.584557, acc: 0.703125] [adversarial loss: 1.123363, acc: 0.312500]\n",
      "3607: [discriminator loss: 0.529377, acc: 0.742188] [adversarial loss: 1.567971, acc: 0.125000]\n",
      "3608: [discriminator loss: 0.417429, acc: 0.789062] [adversarial loss: 1.218991, acc: 0.187500]\n",
      "3609: [discriminator loss: 0.458878, acc: 0.820312] [adversarial loss: 1.471635, acc: 0.203125]\n",
      "3610: [discriminator loss: 0.468254, acc: 0.789062] [adversarial loss: 0.832975, acc: 0.531250]\n",
      "3611: [discriminator loss: 0.567517, acc: 0.710938] [adversarial loss: 2.327962, acc: 0.046875]\n",
      "3612: [discriminator loss: 0.536016, acc: 0.679688] [adversarial loss: 0.910240, acc: 0.406250]\n",
      "3613: [discriminator loss: 0.536896, acc: 0.750000] [adversarial loss: 1.630066, acc: 0.140625]\n",
      "3614: [discriminator loss: 0.529780, acc: 0.742188] [adversarial loss: 1.050984, acc: 0.375000]\n",
      "3615: [discriminator loss: 0.534994, acc: 0.742188] [adversarial loss: 2.052336, acc: 0.015625]\n",
      "3616: [discriminator loss: 0.510461, acc: 0.742188] [adversarial loss: 0.932005, acc: 0.437500]\n",
      "3617: [discriminator loss: 0.519723, acc: 0.718750] [adversarial loss: 1.660615, acc: 0.046875]\n",
      "3618: [discriminator loss: 0.496599, acc: 0.742188] [adversarial loss: 1.025095, acc: 0.343750]\n",
      "3619: [discriminator loss: 0.440423, acc: 0.812500] [adversarial loss: 1.293739, acc: 0.250000]\n",
      "3620: [discriminator loss: 0.517368, acc: 0.695312] [adversarial loss: 1.530766, acc: 0.171875]\n",
      "3621: [discriminator loss: 0.535475, acc: 0.742188] [adversarial loss: 1.237725, acc: 0.218750]\n",
      "3622: [discriminator loss: 0.528241, acc: 0.757812] [adversarial loss: 1.876503, acc: 0.046875]\n",
      "3623: [discriminator loss: 0.476105, acc: 0.812500] [adversarial loss: 1.274920, acc: 0.218750]\n",
      "3624: [discriminator loss: 0.491546, acc: 0.773438] [adversarial loss: 1.396891, acc: 0.187500]\n",
      "3625: [discriminator loss: 0.437482, acc: 0.789062] [adversarial loss: 1.775720, acc: 0.062500]\n",
      "3626: [discriminator loss: 0.462019, acc: 0.789062] [adversarial loss: 0.952356, acc: 0.437500]\n",
      "3627: [discriminator loss: 0.413490, acc: 0.812500] [adversarial loss: 1.540163, acc: 0.203125]\n",
      "3628: [discriminator loss: 0.447450, acc: 0.765625] [adversarial loss: 1.598880, acc: 0.156250]\n",
      "3629: [discriminator loss: 0.401763, acc: 0.843750] [adversarial loss: 1.297477, acc: 0.218750]\n",
      "3630: [discriminator loss: 0.536714, acc: 0.695312] [adversarial loss: 1.401278, acc: 0.156250]\n",
      "3631: [discriminator loss: 0.459987, acc: 0.750000] [adversarial loss: 1.460524, acc: 0.250000]\n",
      "3632: [discriminator loss: 0.538623, acc: 0.789062] [adversarial loss: 1.326526, acc: 0.234375]\n",
      "3633: [discriminator loss: 0.526976, acc: 0.718750] [adversarial loss: 2.173134, acc: 0.031250]\n",
      "3634: [discriminator loss: 0.529941, acc: 0.734375] [adversarial loss: 0.816151, acc: 0.531250]\n",
      "3635: [discriminator loss: 0.525567, acc: 0.710938] [adversarial loss: 2.044108, acc: 0.046875]\n",
      "3636: [discriminator loss: 0.558119, acc: 0.687500] [adversarial loss: 0.793332, acc: 0.531250]\n",
      "3637: [discriminator loss: 0.539904, acc: 0.695312] [adversarial loss: 1.673033, acc: 0.125000]\n",
      "3638: [discriminator loss: 0.548061, acc: 0.687500] [adversarial loss: 0.948002, acc: 0.359375]\n",
      "3639: [discriminator loss: 0.590401, acc: 0.703125] [adversarial loss: 1.561665, acc: 0.140625]\n",
      "3640: [discriminator loss: 0.404727, acc: 0.804688] [adversarial loss: 1.275962, acc: 0.234375]\n",
      "3641: [discriminator loss: 0.463510, acc: 0.796875] [adversarial loss: 1.394688, acc: 0.187500]\n",
      "3642: [discriminator loss: 0.491313, acc: 0.750000] [adversarial loss: 1.258949, acc: 0.234375]\n",
      "3643: [discriminator loss: 0.517235, acc: 0.695312] [adversarial loss: 1.398680, acc: 0.187500]\n",
      "3644: [discriminator loss: 0.465457, acc: 0.781250] [adversarial loss: 1.509654, acc: 0.140625]\n",
      "3645: [discriminator loss: 0.518769, acc: 0.734375] [adversarial loss: 1.278027, acc: 0.234375]\n",
      "3646: [discriminator loss: 0.409429, acc: 0.851562] [adversarial loss: 1.398814, acc: 0.187500]\n",
      "3647: [discriminator loss: 0.478108, acc: 0.781250] [adversarial loss: 0.980689, acc: 0.421875]\n",
      "3648: [discriminator loss: 0.486897, acc: 0.750000] [adversarial loss: 1.345281, acc: 0.234375]\n",
      "3649: [discriminator loss: 0.552517, acc: 0.750000] [adversarial loss: 1.044273, acc: 0.296875]\n",
      "3650: [discriminator loss: 0.456442, acc: 0.757812] [adversarial loss: 1.846115, acc: 0.078125]\n",
      "3651: [discriminator loss: 0.481912, acc: 0.773438] [adversarial loss: 1.239614, acc: 0.312500]\n",
      "3652: [discriminator loss: 0.516678, acc: 0.750000] [adversarial loss: 1.837201, acc: 0.125000]\n",
      "3653: [discriminator loss: 0.482173, acc: 0.773438] [adversarial loss: 1.155851, acc: 0.265625]\n",
      "3654: [discriminator loss: 0.498632, acc: 0.757812] [adversarial loss: 1.312526, acc: 0.218750]\n",
      "3655: [discriminator loss: 0.534304, acc: 0.750000] [adversarial loss: 1.748453, acc: 0.109375]\n",
      "3656: [discriminator loss: 0.575923, acc: 0.726562] [adversarial loss: 1.227272, acc: 0.343750]\n",
      "3657: [discriminator loss: 0.471360, acc: 0.804688] [adversarial loss: 1.507473, acc: 0.093750]\n",
      "3658: [discriminator loss: 0.453840, acc: 0.796875] [adversarial loss: 1.196622, acc: 0.375000]\n",
      "3659: [discriminator loss: 0.489491, acc: 0.757812] [adversarial loss: 2.079195, acc: 0.062500]\n",
      "3660: [discriminator loss: 0.433966, acc: 0.789062] [adversarial loss: 1.111060, acc: 0.281250]\n",
      "3661: [discriminator loss: 0.531546, acc: 0.710938] [adversarial loss: 1.561329, acc: 0.140625]\n",
      "3662: [discriminator loss: 0.442902, acc: 0.820312] [adversarial loss: 1.166474, acc: 0.250000]\n",
      "3663: [discriminator loss: 0.578915, acc: 0.703125] [adversarial loss: 1.024933, acc: 0.390625]\n",
      "3664: [discriminator loss: 0.508802, acc: 0.750000] [adversarial loss: 1.693120, acc: 0.078125]\n",
      "3665: [discriminator loss: 0.598071, acc: 0.679688] [adversarial loss: 1.059053, acc: 0.296875]\n",
      "3666: [discriminator loss: 0.593182, acc: 0.718750] [adversarial loss: 1.576899, acc: 0.140625]\n",
      "3667: [discriminator loss: 0.567730, acc: 0.695312] [adversarial loss: 0.828472, acc: 0.500000]\n",
      "3668: [discriminator loss: 0.475529, acc: 0.742188] [adversarial loss: 1.925546, acc: 0.062500]\n",
      "3669: [discriminator loss: 0.511476, acc: 0.734375] [adversarial loss: 0.862459, acc: 0.500000]\n",
      "3670: [discriminator loss: 0.549467, acc: 0.734375] [adversarial loss: 1.810677, acc: 0.078125]\n",
      "3671: [discriminator loss: 0.545166, acc: 0.718750] [adversarial loss: 0.976772, acc: 0.390625]\n",
      "3672: [discriminator loss: 0.504294, acc: 0.757812] [adversarial loss: 1.996528, acc: 0.062500]\n",
      "3673: [discriminator loss: 0.528474, acc: 0.687500] [adversarial loss: 0.997260, acc: 0.328125]\n",
      "3674: [discriminator loss: 0.553472, acc: 0.695312] [adversarial loss: 1.857039, acc: 0.093750]\n",
      "3675: [discriminator loss: 0.465097, acc: 0.804688] [adversarial loss: 1.220374, acc: 0.250000]\n",
      "3676: [discriminator loss: 0.489596, acc: 0.765625] [adversarial loss: 1.424285, acc: 0.218750]\n",
      "3677: [discriminator loss: 0.517497, acc: 0.742188] [adversarial loss: 1.104752, acc: 0.281250]\n",
      "3678: [discriminator loss: 0.451522, acc: 0.789062] [adversarial loss: 1.694535, acc: 0.109375]\n",
      "3679: [discriminator loss: 0.503206, acc: 0.726562] [adversarial loss: 0.963237, acc: 0.375000]\n",
      "3680: [discriminator loss: 0.561530, acc: 0.750000] [adversarial loss: 1.765559, acc: 0.125000]\n",
      "3681: [discriminator loss: 0.520959, acc: 0.742188] [adversarial loss: 1.060207, acc: 0.250000]\n",
      "3682: [discriminator loss: 0.564940, acc: 0.695312] [adversarial loss: 1.617803, acc: 0.140625]\n",
      "3683: [discriminator loss: 0.406126, acc: 0.843750] [adversarial loss: 0.996901, acc: 0.343750]\n",
      "3684: [discriminator loss: 0.524117, acc: 0.734375] [adversarial loss: 1.330284, acc: 0.250000]\n",
      "3685: [discriminator loss: 0.554839, acc: 0.671875] [adversarial loss: 1.036775, acc: 0.328125]\n",
      "3686: [discriminator loss: 0.485643, acc: 0.750000] [adversarial loss: 1.602123, acc: 0.109375]\n",
      "3687: [discriminator loss: 0.541565, acc: 0.734375] [adversarial loss: 0.894058, acc: 0.421875]\n",
      "3688: [discriminator loss: 0.525570, acc: 0.687500] [adversarial loss: 1.793644, acc: 0.109375]\n",
      "3689: [discriminator loss: 0.543329, acc: 0.718750] [adversarial loss: 0.976846, acc: 0.421875]\n",
      "3690: [discriminator loss: 0.543461, acc: 0.695312] [adversarial loss: 2.070642, acc: 0.031250]\n",
      "3691: [discriminator loss: 0.532754, acc: 0.726562] [adversarial loss: 1.045251, acc: 0.312500]\n",
      "3692: [discriminator loss: 0.443048, acc: 0.804688] [adversarial loss: 1.766762, acc: 0.125000]\n",
      "3693: [discriminator loss: 0.470001, acc: 0.773438] [adversarial loss: 1.489295, acc: 0.187500]\n",
      "3694: [discriminator loss: 0.471945, acc: 0.789062] [adversarial loss: 1.197456, acc: 0.296875]\n",
      "3695: [discriminator loss: 0.475510, acc: 0.750000] [adversarial loss: 1.303731, acc: 0.250000]\n",
      "3696: [discriminator loss: 0.453786, acc: 0.781250] [adversarial loss: 1.395770, acc: 0.125000]\n",
      "3697: [discriminator loss: 0.448796, acc: 0.781250] [adversarial loss: 1.239186, acc: 0.250000]\n",
      "3698: [discriminator loss: 0.510291, acc: 0.726562] [adversarial loss: 1.340333, acc: 0.187500]\n",
      "3699: [discriminator loss: 0.496037, acc: 0.742188] [adversarial loss: 1.037279, acc: 0.359375]\n",
      "3700: [discriminator loss: 0.465131, acc: 0.765625] [adversarial loss: 1.702571, acc: 0.125000]\n",
      "3701: [discriminator loss: 0.539806, acc: 0.734375] [adversarial loss: 1.061601, acc: 0.281250]\n",
      "3702: [discriminator loss: 0.454445, acc: 0.789062] [adversarial loss: 1.565152, acc: 0.156250]\n",
      "3703: [discriminator loss: 0.525707, acc: 0.757812] [adversarial loss: 0.855522, acc: 0.421875]\n",
      "3704: [discriminator loss: 0.480649, acc: 0.757812] [adversarial loss: 1.720842, acc: 0.078125]\n",
      "3705: [discriminator loss: 0.464356, acc: 0.765625] [adversarial loss: 0.999046, acc: 0.328125]\n",
      "3706: [discriminator loss: 0.414797, acc: 0.820312] [adversarial loss: 1.741623, acc: 0.140625]\n",
      "3707: [discriminator loss: 0.530497, acc: 0.695312] [adversarial loss: 1.212455, acc: 0.203125]\n",
      "3708: [discriminator loss: 0.521873, acc: 0.734375] [adversarial loss: 1.249625, acc: 0.281250]\n",
      "3709: [discriminator loss: 0.478245, acc: 0.843750] [adversarial loss: 1.321436, acc: 0.218750]\n",
      "3710: [discriminator loss: 0.533699, acc: 0.726562] [adversarial loss: 1.475239, acc: 0.171875]\n",
      "3711: [discriminator loss: 0.467487, acc: 0.757812] [adversarial loss: 1.160606, acc: 0.171875]\n",
      "3712: [discriminator loss: 0.496939, acc: 0.765625] [adversarial loss: 1.464644, acc: 0.234375]\n",
      "3713: [discriminator loss: 0.478963, acc: 0.773438] [adversarial loss: 0.938445, acc: 0.359375]\n",
      "3714: [discriminator loss: 0.526786, acc: 0.726562] [adversarial loss: 1.882569, acc: 0.062500]\n",
      "3715: [discriminator loss: 0.423940, acc: 0.828125] [adversarial loss: 1.260000, acc: 0.265625]\n",
      "3716: [discriminator loss: 0.485190, acc: 0.734375] [adversarial loss: 1.392366, acc: 0.218750]\n",
      "3717: [discriminator loss: 0.439173, acc: 0.804688] [adversarial loss: 1.404872, acc: 0.171875]\n",
      "3718: [discriminator loss: 0.440118, acc: 0.781250] [adversarial loss: 1.448019, acc: 0.156250]\n",
      "3719: [discriminator loss: 0.599161, acc: 0.695312] [adversarial loss: 0.823352, acc: 0.531250]\n",
      "3720: [discriminator loss: 0.590917, acc: 0.703125] [adversarial loss: 2.242697, acc: 0.031250]\n",
      "3721: [discriminator loss: 0.667949, acc: 0.640625] [adversarial loss: 0.771109, acc: 0.531250]\n",
      "3722: [discriminator loss: 0.621085, acc: 0.671875] [adversarial loss: 2.052226, acc: 0.062500]\n",
      "3723: [discriminator loss: 0.466520, acc: 0.781250] [adversarial loss: 1.453244, acc: 0.156250]\n",
      "3724: [discriminator loss: 0.460570, acc: 0.765625] [adversarial loss: 1.064486, acc: 0.328125]\n",
      "3725: [discriminator loss: 0.492440, acc: 0.734375] [adversarial loss: 1.366728, acc: 0.203125]\n",
      "3726: [discriminator loss: 0.486076, acc: 0.742188] [adversarial loss: 1.280060, acc: 0.296875]\n",
      "3727: [discriminator loss: 0.441402, acc: 0.789062] [adversarial loss: 1.512241, acc: 0.187500]\n",
      "3728: [discriminator loss: 0.447538, acc: 0.804688] [adversarial loss: 1.252739, acc: 0.250000]\n",
      "3729: [discriminator loss: 0.442614, acc: 0.789062] [adversarial loss: 1.601527, acc: 0.109375]\n",
      "3730: [discriminator loss: 0.406058, acc: 0.820312] [adversarial loss: 0.970150, acc: 0.375000]\n",
      "3731: [discriminator loss: 0.552780, acc: 0.710938] [adversarial loss: 1.842502, acc: 0.140625]\n",
      "3732: [discriminator loss: 0.535645, acc: 0.734375] [adversarial loss: 1.058862, acc: 0.312500]\n",
      "3733: [discriminator loss: 0.566769, acc: 0.664062] [adversarial loss: 1.906584, acc: 0.078125]\n",
      "3734: [discriminator loss: 0.559295, acc: 0.710938] [adversarial loss: 0.852990, acc: 0.500000]\n",
      "3735: [discriminator loss: 0.632142, acc: 0.632812] [adversarial loss: 2.146021, acc: 0.031250]\n",
      "3736: [discriminator loss: 0.584922, acc: 0.664062] [adversarial loss: 0.856375, acc: 0.500000]\n",
      "3737: [discriminator loss: 0.571368, acc: 0.703125] [adversarial loss: 1.859015, acc: 0.046875]\n",
      "3738: [discriminator loss: 0.467105, acc: 0.742188] [adversarial loss: 1.170892, acc: 0.203125]\n",
      "3739: [discriminator loss: 0.521992, acc: 0.695312] [adversarial loss: 1.206939, acc: 0.171875]\n",
      "3740: [discriminator loss: 0.525164, acc: 0.742188] [adversarial loss: 1.241774, acc: 0.265625]\n",
      "3741: [discriminator loss: 0.438013, acc: 0.765625] [adversarial loss: 1.392233, acc: 0.171875]\n",
      "3742: [discriminator loss: 0.461280, acc: 0.796875] [adversarial loss: 1.367195, acc: 0.156250]\n",
      "3743: [discriminator loss: 0.429386, acc: 0.804688] [adversarial loss: 1.303292, acc: 0.281250]\n",
      "3744: [discriminator loss: 0.484344, acc: 0.757812] [adversarial loss: 1.571061, acc: 0.203125]\n",
      "3745: [discriminator loss: 0.486143, acc: 0.781250] [adversarial loss: 1.716321, acc: 0.109375]\n",
      "3746: [discriminator loss: 0.529755, acc: 0.750000] [adversarial loss: 1.216625, acc: 0.328125]\n",
      "3747: [discriminator loss: 0.466174, acc: 0.757812] [adversarial loss: 1.680087, acc: 0.156250]\n",
      "3748: [discriminator loss: 0.429799, acc: 0.804688] [adversarial loss: 1.123280, acc: 0.265625]\n",
      "3749: [discriminator loss: 0.464871, acc: 0.804688] [adversarial loss: 1.632623, acc: 0.171875]\n",
      "3750: [discriminator loss: 0.482505, acc: 0.757812] [adversarial loss: 1.084368, acc: 0.328125]\n",
      "3751: [discriminator loss: 0.463561, acc: 0.789062] [adversarial loss: 1.658709, acc: 0.125000]\n",
      "3752: [discriminator loss: 0.429513, acc: 0.796875] [adversarial loss: 0.850551, acc: 0.437500]\n",
      "3753: [discriminator loss: 0.536216, acc: 0.679688] [adversarial loss: 1.848989, acc: 0.093750]\n",
      "3754: [discriminator loss: 0.471977, acc: 0.757812] [adversarial loss: 0.872403, acc: 0.484375]\n",
      "3755: [discriminator loss: 0.651141, acc: 0.640625] [adversarial loss: 2.072101, acc: 0.031250]\n",
      "3756: [discriminator loss: 0.576871, acc: 0.710938] [adversarial loss: 0.949764, acc: 0.359375]\n",
      "3757: [discriminator loss: 0.486536, acc: 0.789062] [adversarial loss: 1.509889, acc: 0.125000]\n",
      "3758: [discriminator loss: 0.468755, acc: 0.804688] [adversarial loss: 1.138923, acc: 0.343750]\n",
      "3759: [discriminator loss: 0.481044, acc: 0.773438] [adversarial loss: 1.186674, acc: 0.234375]\n",
      "3760: [discriminator loss: 0.501869, acc: 0.718750] [adversarial loss: 1.377732, acc: 0.187500]\n",
      "3761: [discriminator loss: 0.489743, acc: 0.773438] [adversarial loss: 1.810789, acc: 0.046875]\n",
      "3762: [discriminator loss: 0.452490, acc: 0.789062] [adversarial loss: 1.195190, acc: 0.281250]\n",
      "3763: [discriminator loss: 0.447556, acc: 0.789062] [adversarial loss: 1.504893, acc: 0.218750]\n",
      "3764: [discriminator loss: 0.419695, acc: 0.835938] [adversarial loss: 1.150236, acc: 0.265625]\n",
      "3765: [discriminator loss: 0.454331, acc: 0.789062] [adversarial loss: 1.568211, acc: 0.187500]\n",
      "3766: [discriminator loss: 0.415284, acc: 0.828125] [adversarial loss: 1.124368, acc: 0.343750]\n",
      "3767: [discriminator loss: 0.474401, acc: 0.726562] [adversarial loss: 2.010045, acc: 0.046875]\n",
      "3768: [discriminator loss: 0.535627, acc: 0.703125] [adversarial loss: 1.040493, acc: 0.296875]\n",
      "3769: [discriminator loss: 0.511035, acc: 0.687500] [adversarial loss: 1.384847, acc: 0.156250]\n",
      "3770: [discriminator loss: 0.451800, acc: 0.781250] [adversarial loss: 1.386838, acc: 0.187500]\n",
      "3771: [discriminator loss: 0.488503, acc: 0.781250] [adversarial loss: 1.439867, acc: 0.171875]\n",
      "3772: [discriminator loss: 0.503288, acc: 0.796875] [adversarial loss: 1.061578, acc: 0.265625]\n",
      "3773: [discriminator loss: 0.485421, acc: 0.765625] [adversarial loss: 1.623106, acc: 0.046875]\n",
      "3774: [discriminator loss: 0.382271, acc: 0.835938] [adversarial loss: 1.610332, acc: 0.125000]\n",
      "3775: [discriminator loss: 0.466830, acc: 0.781250] [adversarial loss: 1.054000, acc: 0.453125]\n",
      "3776: [discriminator loss: 0.517187, acc: 0.757812] [adversarial loss: 1.907429, acc: 0.078125]\n",
      "3777: [discriminator loss: 0.522813, acc: 0.734375] [adversarial loss: 1.075149, acc: 0.328125]\n",
      "3778: [discriminator loss: 0.453126, acc: 0.703125] [adversarial loss: 2.199439, acc: 0.046875]\n",
      "3779: [discriminator loss: 0.476496, acc: 0.828125] [adversarial loss: 1.045441, acc: 0.343750]\n",
      "3780: [discriminator loss: 0.556587, acc: 0.734375] [adversarial loss: 1.761149, acc: 0.046875]\n",
      "3781: [discriminator loss: 0.528451, acc: 0.734375] [adversarial loss: 0.810430, acc: 0.500000]\n",
      "3782: [discriminator loss: 0.557291, acc: 0.679688] [adversarial loss: 1.718579, acc: 0.109375]\n",
      "3783: [discriminator loss: 0.479178, acc: 0.757812] [adversarial loss: 1.062330, acc: 0.328125]\n",
      "3784: [discriminator loss: 0.483911, acc: 0.804688] [adversarial loss: 1.346666, acc: 0.265625]\n",
      "3785: [discriminator loss: 0.437908, acc: 0.796875] [adversarial loss: 1.394674, acc: 0.265625]\n",
      "3786: [discriminator loss: 0.573147, acc: 0.664062] [adversarial loss: 1.496612, acc: 0.109375]\n",
      "3787: [discriminator loss: 0.532271, acc: 0.703125] [adversarial loss: 0.930198, acc: 0.390625]\n",
      "3788: [discriminator loss: 0.526369, acc: 0.742188] [adversarial loss: 1.678950, acc: 0.125000]\n",
      "3789: [discriminator loss: 0.526143, acc: 0.742188] [adversarial loss: 1.215274, acc: 0.312500]\n",
      "3790: [discriminator loss: 0.544095, acc: 0.703125] [adversarial loss: 1.356983, acc: 0.250000]\n",
      "3791: [discriminator loss: 0.469283, acc: 0.734375] [adversarial loss: 1.756928, acc: 0.109375]\n",
      "3792: [discriminator loss: 0.507301, acc: 0.734375] [adversarial loss: 1.067999, acc: 0.343750]\n",
      "3793: [discriminator loss: 0.488430, acc: 0.757812] [adversarial loss: 1.533963, acc: 0.187500]\n",
      "3794: [discriminator loss: 0.471939, acc: 0.765625] [adversarial loss: 1.101807, acc: 0.312500]\n",
      "3795: [discriminator loss: 0.505248, acc: 0.773438] [adversarial loss: 1.653605, acc: 0.140625]\n",
      "3796: [discriminator loss: 0.446625, acc: 0.804688] [adversarial loss: 1.131338, acc: 0.328125]\n",
      "3797: [discriminator loss: 0.523464, acc: 0.765625] [adversarial loss: 1.591695, acc: 0.109375]\n",
      "3798: [discriminator loss: 0.474347, acc: 0.750000] [adversarial loss: 0.970387, acc: 0.421875]\n",
      "3799: [discriminator loss: 0.473208, acc: 0.789062] [adversarial loss: 1.543267, acc: 0.140625]\n",
      "3800: [discriminator loss: 0.487532, acc: 0.781250] [adversarial loss: 1.277904, acc: 0.265625]\n",
      "3801: [discriminator loss: 0.527925, acc: 0.734375] [adversarial loss: 1.479674, acc: 0.187500]\n",
      "3802: [discriminator loss: 0.461886, acc: 0.773438] [adversarial loss: 1.676661, acc: 0.093750]\n",
      "3803: [discriminator loss: 0.481262, acc: 0.765625] [adversarial loss: 1.260698, acc: 0.218750]\n",
      "3804: [discriminator loss: 0.552819, acc: 0.757812] [adversarial loss: 2.097979, acc: 0.062500]\n",
      "3805: [discriminator loss: 0.515492, acc: 0.742188] [adversarial loss: 1.021667, acc: 0.359375]\n",
      "3806: [discriminator loss: 0.454899, acc: 0.765625] [adversarial loss: 1.488788, acc: 0.109375]\n",
      "3807: [discriminator loss: 0.566259, acc: 0.695312] [adversarial loss: 1.014880, acc: 0.281250]\n",
      "3808: [discriminator loss: 0.590994, acc: 0.648438] [adversarial loss: 2.059399, acc: 0.062500]\n",
      "3809: [discriminator loss: 0.610338, acc: 0.703125] [adversarial loss: 0.975050, acc: 0.468750]\n",
      "3810: [discriminator loss: 0.585726, acc: 0.734375] [adversarial loss: 2.027992, acc: 0.078125]\n",
      "3811: [discriminator loss: 0.522431, acc: 0.773438] [adversarial loss: 1.226712, acc: 0.296875]\n",
      "3812: [discriminator loss: 0.476675, acc: 0.781250] [adversarial loss: 1.555947, acc: 0.078125]\n",
      "3813: [discriminator loss: 0.493637, acc: 0.765625] [adversarial loss: 1.177010, acc: 0.171875]\n",
      "3814: [discriminator loss: 0.494395, acc: 0.773438] [adversarial loss: 1.525892, acc: 0.140625]\n",
      "3815: [discriminator loss: 0.529891, acc: 0.773438] [adversarial loss: 1.303923, acc: 0.312500]\n",
      "3816: [discriminator loss: 0.556666, acc: 0.765625] [adversarial loss: 1.729302, acc: 0.125000]\n",
      "3817: [discriminator loss: 0.502847, acc: 0.781250] [adversarial loss: 1.179282, acc: 0.203125]\n",
      "3818: [discriminator loss: 0.434664, acc: 0.820312] [adversarial loss: 1.509808, acc: 0.109375]\n",
      "3819: [discriminator loss: 0.447400, acc: 0.765625] [adversarial loss: 1.180137, acc: 0.296875]\n",
      "3820: [discriminator loss: 0.500270, acc: 0.773438] [adversarial loss: 1.643724, acc: 0.156250]\n",
      "3821: [discriminator loss: 0.520879, acc: 0.765625] [adversarial loss: 1.818776, acc: 0.109375]\n",
      "3822: [discriminator loss: 0.501173, acc: 0.734375] [adversarial loss: 0.806225, acc: 0.562500]\n",
      "3823: [discriminator loss: 0.600950, acc: 0.710938] [adversarial loss: 1.646274, acc: 0.140625]\n",
      "3824: [discriminator loss: 0.537723, acc: 0.710938] [adversarial loss: 0.869455, acc: 0.468750]\n",
      "3825: [discriminator loss: 0.509747, acc: 0.726562] [adversarial loss: 1.771136, acc: 0.093750]\n",
      "3826: [discriminator loss: 0.553143, acc: 0.687500] [adversarial loss: 0.939529, acc: 0.468750]\n",
      "3827: [discriminator loss: 0.523392, acc: 0.734375] [adversarial loss: 2.116335, acc: 0.046875]\n",
      "3828: [discriminator loss: 0.588795, acc: 0.664062] [adversarial loss: 0.846119, acc: 0.453125]\n",
      "3829: [discriminator loss: 0.469685, acc: 0.757812] [adversarial loss: 1.659269, acc: 0.140625]\n",
      "3830: [discriminator loss: 0.464140, acc: 0.765625] [adversarial loss: 0.824226, acc: 0.546875]\n",
      "3831: [discriminator loss: 0.476547, acc: 0.765625] [adversarial loss: 1.142477, acc: 0.328125]\n",
      "3832: [discriminator loss: 0.432797, acc: 0.796875] [adversarial loss: 0.528418, acc: 0.750000]\n",
      "3833: [discriminator loss: 0.492801, acc: 0.757812] [adversarial loss: 1.709526, acc: 0.125000]\n",
      "3834: [discriminator loss: 0.596574, acc: 0.734375] [adversarial loss: 0.916212, acc: 0.437500]\n",
      "3835: [discriminator loss: 0.627745, acc: 0.710938] [adversarial loss: 1.999530, acc: 0.062500]\n",
      "3836: [discriminator loss: 0.502772, acc: 0.750000] [adversarial loss: 1.160630, acc: 0.250000]\n",
      "3837: [discriminator loss: 0.444128, acc: 0.781250] [adversarial loss: 1.768280, acc: 0.156250]\n",
      "3838: [discriminator loss: 0.433432, acc: 0.820312] [adversarial loss: 1.369835, acc: 0.203125]\n",
      "3839: [discriminator loss: 0.407125, acc: 0.820312] [adversarial loss: 1.543608, acc: 0.140625]\n",
      "3840: [discriminator loss: 0.516030, acc: 0.710938] [adversarial loss: 1.194506, acc: 0.296875]\n",
      "3841: [discriminator loss: 0.508959, acc: 0.757812] [adversarial loss: 1.259811, acc: 0.218750]\n",
      "3842: [discriminator loss: 0.498429, acc: 0.789062] [adversarial loss: 1.694935, acc: 0.062500]\n",
      "3843: [discriminator loss: 0.468419, acc: 0.781250] [adversarial loss: 1.057969, acc: 0.312500]\n",
      "3844: [discriminator loss: 0.470277, acc: 0.773438] [adversarial loss: 1.358451, acc: 0.203125]\n",
      "3845: [discriminator loss: 0.514828, acc: 0.750000] [adversarial loss: 1.524993, acc: 0.125000]\n",
      "3846: [discriminator loss: 0.517271, acc: 0.742188] [adversarial loss: 1.048291, acc: 0.343750]\n",
      "3847: [discriminator loss: 0.504978, acc: 0.695312] [adversarial loss: 1.483516, acc: 0.187500]\n",
      "3848: [discriminator loss: 0.510215, acc: 0.734375] [adversarial loss: 1.479000, acc: 0.156250]\n",
      "3849: [discriminator loss: 0.471345, acc: 0.789062] [adversarial loss: 1.178551, acc: 0.156250]\n",
      "3850: [discriminator loss: 0.533491, acc: 0.757812] [adversarial loss: 1.237924, acc: 0.218750]\n",
      "3851: [discriminator loss: 0.584534, acc: 0.695312] [adversarial loss: 1.721395, acc: 0.062500]\n",
      "3852: [discriminator loss: 0.569591, acc: 0.695312] [adversarial loss: 0.910892, acc: 0.390625]\n",
      "3853: [discriminator loss: 0.506371, acc: 0.773438] [adversarial loss: 1.670422, acc: 0.078125]\n",
      "3854: [discriminator loss: 0.513351, acc: 0.765625] [adversarial loss: 1.013192, acc: 0.250000]\n",
      "3855: [discriminator loss: 0.451560, acc: 0.796875] [adversarial loss: 1.400947, acc: 0.218750]\n",
      "3856: [discriminator loss: 0.545494, acc: 0.695312] [adversarial loss: 1.358437, acc: 0.281250]\n",
      "3857: [discriminator loss: 0.501869, acc: 0.757812] [adversarial loss: 1.462881, acc: 0.140625]\n",
      "3858: [discriminator loss: 0.466794, acc: 0.742188] [adversarial loss: 1.236708, acc: 0.234375]\n",
      "3859: [discriminator loss: 0.445118, acc: 0.820312] [adversarial loss: 1.153366, acc: 0.281250]\n",
      "3860: [discriminator loss: 0.577374, acc: 0.664062] [adversarial loss: 0.967395, acc: 0.406250]\n",
      "3861: [discriminator loss: 0.458676, acc: 0.773438] [adversarial loss: 1.757354, acc: 0.109375]\n",
      "3862: [discriminator loss: 0.519728, acc: 0.718750] [adversarial loss: 0.741909, acc: 0.562500]\n",
      "3863: [discriminator loss: 0.662078, acc: 0.648438] [adversarial loss: 2.258399, acc: 0.015625]\n",
      "3864: [discriminator loss: 0.728746, acc: 0.609375] [adversarial loss: 0.985536, acc: 0.359375]\n",
      "3865: [discriminator loss: 0.491339, acc: 0.742188] [adversarial loss: 1.487264, acc: 0.109375]\n",
      "3866: [discriminator loss: 0.501930, acc: 0.734375] [adversarial loss: 1.157914, acc: 0.281250]\n",
      "3867: [discriminator loss: 0.492566, acc: 0.742188] [adversarial loss: 1.285649, acc: 0.187500]\n",
      "3868: [discriminator loss: 0.459798, acc: 0.781250] [adversarial loss: 1.290392, acc: 0.218750]\n",
      "3869: [discriminator loss: 0.483779, acc: 0.757812] [adversarial loss: 1.328089, acc: 0.250000]\n",
      "3870: [discriminator loss: 0.499482, acc: 0.726562] [adversarial loss: 1.448741, acc: 0.125000]\n",
      "3871: [discriminator loss: 0.449793, acc: 0.796875] [adversarial loss: 1.444051, acc: 0.234375]\n",
      "3872: [discriminator loss: 0.523661, acc: 0.687500] [adversarial loss: 1.378423, acc: 0.156250]\n",
      "3873: [discriminator loss: 0.431060, acc: 0.820312] [adversarial loss: 1.213193, acc: 0.312500]\n",
      "3874: [discriminator loss: 0.464474, acc: 0.820312] [adversarial loss: 1.527819, acc: 0.156250]\n",
      "3875: [discriminator loss: 0.482287, acc: 0.781250] [adversarial loss: 0.958651, acc: 0.375000]\n",
      "3876: [discriminator loss: 0.546104, acc: 0.742188] [adversarial loss: 1.591501, acc: 0.171875]\n",
      "3877: [discriminator loss: 0.572455, acc: 0.703125] [adversarial loss: 1.201457, acc: 0.156250]\n",
      "3878: [discriminator loss: 0.517561, acc: 0.710938] [adversarial loss: 1.617360, acc: 0.093750]\n",
      "3879: [discriminator loss: 0.466758, acc: 0.773438] [adversarial loss: 1.128559, acc: 0.296875]\n",
      "3880: [discriminator loss: 0.516170, acc: 0.757812] [adversarial loss: 1.644412, acc: 0.125000]\n",
      "3881: [discriminator loss: 0.470194, acc: 0.765625] [adversarial loss: 1.217491, acc: 0.203125]\n",
      "3882: [discriminator loss: 0.478201, acc: 0.757812] [adversarial loss: 1.496404, acc: 0.203125]\n",
      "3883: [discriminator loss: 0.522431, acc: 0.757812] [adversarial loss: 1.376248, acc: 0.156250]\n",
      "3884: [discriminator loss: 0.417819, acc: 0.820312] [adversarial loss: 1.239949, acc: 0.203125]\n",
      "3885: [discriminator loss: 0.503073, acc: 0.773438] [adversarial loss: 1.061733, acc: 0.250000]\n",
      "3886: [discriminator loss: 0.503267, acc: 0.687500] [adversarial loss: 1.845206, acc: 0.046875]\n",
      "3887: [discriminator loss: 0.536999, acc: 0.726562] [adversarial loss: 0.752045, acc: 0.578125]\n",
      "3888: [discriminator loss: 0.533381, acc: 0.726562] [adversarial loss: 1.799846, acc: 0.093750]\n",
      "3889: [discriminator loss: 0.515121, acc: 0.765625] [adversarial loss: 1.155746, acc: 0.328125]\n",
      "3890: [discriminator loss: 0.521192, acc: 0.703125] [adversarial loss: 1.788619, acc: 0.046875]\n",
      "3891: [discriminator loss: 0.525752, acc: 0.687500] [adversarial loss: 0.941406, acc: 0.375000]\n",
      "3892: [discriminator loss: 0.531984, acc: 0.734375] [adversarial loss: 1.700932, acc: 0.125000]\n",
      "3893: [discriminator loss: 0.496688, acc: 0.734375] [adversarial loss: 1.166716, acc: 0.296875]\n",
      "3894: [discriminator loss: 0.567491, acc: 0.757812] [adversarial loss: 1.441728, acc: 0.203125]\n",
      "3895: [discriminator loss: 0.480796, acc: 0.757812] [adversarial loss: 1.025833, acc: 0.343750]\n",
      "3896: [discriminator loss: 0.492092, acc: 0.765625] [adversarial loss: 1.731334, acc: 0.140625]\n",
      "3897: [discriminator loss: 0.509095, acc: 0.734375] [adversarial loss: 0.943652, acc: 0.359375]\n",
      "3898: [discriminator loss: 0.469213, acc: 0.765625] [adversarial loss: 1.801584, acc: 0.046875]\n",
      "3899: [discriminator loss: 0.476205, acc: 0.765625] [adversarial loss: 1.326619, acc: 0.218750]\n",
      "3900: [discriminator loss: 0.470429, acc: 0.773438] [adversarial loss: 1.428925, acc: 0.187500]\n",
      "3901: [discriminator loss: 0.486508, acc: 0.742188] [adversarial loss: 1.556966, acc: 0.125000]\n",
      "3902: [discriminator loss: 0.545133, acc: 0.734375] [adversarial loss: 1.055503, acc: 0.312500]\n",
      "3903: [discriminator loss: 0.422944, acc: 0.804688] [adversarial loss: 1.499497, acc: 0.156250]\n",
      "3904: [discriminator loss: 0.467119, acc: 0.796875] [adversarial loss: 1.468838, acc: 0.093750]\n",
      "3905: [discriminator loss: 0.504552, acc: 0.742188] [adversarial loss: 1.200487, acc: 0.234375]\n",
      "3906: [discriminator loss: 0.477616, acc: 0.773438] [adversarial loss: 1.437309, acc: 0.125000]\n",
      "3907: [discriminator loss: 0.470795, acc: 0.750000] [adversarial loss: 1.355609, acc: 0.125000]\n",
      "3908: [discriminator loss: 0.583091, acc: 0.703125] [adversarial loss: 1.453327, acc: 0.203125]\n",
      "3909: [discriminator loss: 0.406381, acc: 0.796875] [adversarial loss: 1.202688, acc: 0.203125]\n",
      "3910: [discriminator loss: 0.580110, acc: 0.718750] [adversarial loss: 1.567478, acc: 0.156250]\n",
      "3911: [discriminator loss: 0.512267, acc: 0.718750] [adversarial loss: 1.052018, acc: 0.281250]\n",
      "3912: [discriminator loss: 0.503713, acc: 0.734375] [adversarial loss: 1.537790, acc: 0.203125]\n",
      "3913: [discriminator loss: 0.516968, acc: 0.726562] [adversarial loss: 1.485260, acc: 0.125000]\n",
      "3914: [discriminator loss: 0.476055, acc: 0.750000] [adversarial loss: 1.189513, acc: 0.250000]\n",
      "3915: [discriminator loss: 0.536116, acc: 0.765625] [adversarial loss: 1.710068, acc: 0.109375]\n",
      "3916: [discriminator loss: 0.456969, acc: 0.765625] [adversarial loss: 1.192147, acc: 0.250000]\n",
      "3917: [discriminator loss: 0.558184, acc: 0.726562] [adversarial loss: 1.458593, acc: 0.125000]\n",
      "3918: [discriminator loss: 0.486737, acc: 0.789062] [adversarial loss: 0.950871, acc: 0.406250]\n",
      "3919: [discriminator loss: 0.452416, acc: 0.804688] [adversarial loss: 2.241511, acc: 0.046875]\n",
      "3920: [discriminator loss: 0.548816, acc: 0.726562] [adversarial loss: 0.703306, acc: 0.578125]\n",
      "3921: [discriminator loss: 0.647592, acc: 0.617188] [adversarial loss: 2.129528, acc: 0.062500]\n",
      "3922: [discriminator loss: 0.518607, acc: 0.734375] [adversarial loss: 1.151184, acc: 0.375000]\n",
      "3923: [discriminator loss: 0.464489, acc: 0.804688] [adversarial loss: 1.650313, acc: 0.125000]\n",
      "3924: [discriminator loss: 0.470993, acc: 0.765625] [adversarial loss: 1.113400, acc: 0.234375]\n",
      "3925: [discriminator loss: 0.469199, acc: 0.773438] [adversarial loss: 1.682635, acc: 0.093750]\n",
      "3926: [discriminator loss: 0.473659, acc: 0.757812] [adversarial loss: 1.226230, acc: 0.218750]\n",
      "3927: [discriminator loss: 0.482850, acc: 0.734375] [adversarial loss: 1.179148, acc: 0.218750]\n",
      "3928: [discriminator loss: 0.438367, acc: 0.812500] [adversarial loss: 1.307539, acc: 0.171875]\n",
      "3929: [discriminator loss: 0.452376, acc: 0.781250] [adversarial loss: 1.214599, acc: 0.187500]\n",
      "3930: [discriminator loss: 0.526350, acc: 0.757812] [adversarial loss: 1.227320, acc: 0.218750]\n",
      "3931: [discriminator loss: 0.485052, acc: 0.765625] [adversarial loss: 1.194763, acc: 0.218750]\n",
      "3932: [discriminator loss: 0.464626, acc: 0.773438] [adversarial loss: 1.591616, acc: 0.125000]\n",
      "3933: [discriminator loss: 0.429076, acc: 0.812500] [adversarial loss: 1.194265, acc: 0.265625]\n",
      "3934: [discriminator loss: 0.493946, acc: 0.773438] [adversarial loss: 1.454954, acc: 0.187500]\n",
      "3935: [discriminator loss: 0.461391, acc: 0.765625] [adversarial loss: 1.468837, acc: 0.171875]\n",
      "3936: [discriminator loss: 0.550444, acc: 0.718750] [adversarial loss: 1.080936, acc: 0.296875]\n",
      "3937: [discriminator loss: 0.584439, acc: 0.734375] [adversarial loss: 2.030347, acc: 0.062500]\n",
      "3938: [discriminator loss: 0.608028, acc: 0.695312] [adversarial loss: 0.746883, acc: 0.484375]\n",
      "3939: [discriminator loss: 0.547477, acc: 0.742188] [adversarial loss: 1.753284, acc: 0.062500]\n",
      "3940: [discriminator loss: 0.469566, acc: 0.765625] [adversarial loss: 1.031196, acc: 0.312500]\n",
      "3941: [discriminator loss: 0.522948, acc: 0.742188] [adversarial loss: 1.274658, acc: 0.203125]\n",
      "3942: [discriminator loss: 0.485044, acc: 0.789062] [adversarial loss: 1.626065, acc: 0.093750]\n",
      "3943: [discriminator loss: 0.551585, acc: 0.726562] [adversarial loss: 0.941978, acc: 0.390625]\n",
      "3944: [discriminator loss: 0.578215, acc: 0.742188] [adversarial loss: 1.871551, acc: 0.078125]\n",
      "3945: [discriminator loss: 0.488440, acc: 0.773438] [adversarial loss: 1.200989, acc: 0.265625]\n",
      "3946: [discriminator loss: 0.508458, acc: 0.742188] [adversarial loss: 1.848792, acc: 0.093750]\n",
      "3947: [discriminator loss: 0.510931, acc: 0.765625] [adversarial loss: 1.114031, acc: 0.281250]\n",
      "3948: [discriminator loss: 0.593882, acc: 0.679688] [adversarial loss: 1.714539, acc: 0.140625]\n",
      "3949: [discriminator loss: 0.465217, acc: 0.812500] [adversarial loss: 1.106941, acc: 0.343750]\n",
      "3950: [discriminator loss: 0.504332, acc: 0.703125] [adversarial loss: 1.837392, acc: 0.109375]\n",
      "3951: [discriminator loss: 0.553859, acc: 0.703125] [adversarial loss: 0.841810, acc: 0.453125]\n",
      "3952: [discriminator loss: 0.555055, acc: 0.679688] [adversarial loss: 1.838016, acc: 0.078125]\n",
      "3953: [discriminator loss: 0.513006, acc: 0.726562] [adversarial loss: 1.316045, acc: 0.218750]\n",
      "3954: [discriminator loss: 0.502260, acc: 0.726562] [adversarial loss: 1.539664, acc: 0.203125]\n",
      "3955: [discriminator loss: 0.434063, acc: 0.820312] [adversarial loss: 1.440272, acc: 0.234375]\n",
      "3956: [discriminator loss: 0.448932, acc: 0.765625] [adversarial loss: 1.481011, acc: 0.218750]\n",
      "3957: [discriminator loss: 0.493866, acc: 0.757812] [adversarial loss: 1.557565, acc: 0.156250]\n",
      "3958: [discriminator loss: 0.432466, acc: 0.796875] [adversarial loss: 1.158247, acc: 0.250000]\n",
      "3959: [discriminator loss: 0.443795, acc: 0.812500] [adversarial loss: 1.616923, acc: 0.093750]\n",
      "3960: [discriminator loss: 0.492714, acc: 0.773438] [adversarial loss: 1.176990, acc: 0.218750]\n",
      "3961: [discriminator loss: 0.547295, acc: 0.742188] [adversarial loss: 1.656474, acc: 0.171875]\n",
      "3962: [discriminator loss: 0.485765, acc: 0.765625] [adversarial loss: 1.323961, acc: 0.250000]\n",
      "3963: [discriminator loss: 0.486163, acc: 0.773438] [adversarial loss: 1.382489, acc: 0.281250]\n",
      "3964: [discriminator loss: 0.420730, acc: 0.828125] [adversarial loss: 1.733851, acc: 0.140625]\n",
      "3965: [discriminator loss: 0.465984, acc: 0.765625] [adversarial loss: 1.248866, acc: 0.312500]\n",
      "3966: [discriminator loss: 0.467817, acc: 0.750000] [adversarial loss: 1.480304, acc: 0.187500]\n",
      "3967: [discriminator loss: 0.457853, acc: 0.804688] [adversarial loss: 0.982530, acc: 0.359375]\n",
      "3968: [discriminator loss: 0.478958, acc: 0.789062] [adversarial loss: 1.585382, acc: 0.140625]\n",
      "3969: [discriminator loss: 0.528388, acc: 0.757812] [adversarial loss: 1.440322, acc: 0.171875]\n",
      "3970: [discriminator loss: 0.514094, acc: 0.773438] [adversarial loss: 1.088004, acc: 0.265625]\n",
      "3971: [discriminator loss: 0.470489, acc: 0.781250] [adversarial loss: 1.791775, acc: 0.093750]\n",
      "3972: [discriminator loss: 0.467473, acc: 0.773438] [adversarial loss: 1.188055, acc: 0.359375]\n",
      "3973: [discriminator loss: 0.451914, acc: 0.781250] [adversarial loss: 1.893048, acc: 0.046875]\n",
      "3974: [discriminator loss: 0.493059, acc: 0.734375] [adversarial loss: 1.106229, acc: 0.281250]\n",
      "3975: [discriminator loss: 0.465309, acc: 0.796875] [adversarial loss: 1.700894, acc: 0.093750]\n",
      "3976: [discriminator loss: 0.485503, acc: 0.773438] [adversarial loss: 0.864313, acc: 0.500000]\n",
      "3977: [discriminator loss: 0.530618, acc: 0.710938] [adversarial loss: 1.770079, acc: 0.125000]\n",
      "3978: [discriminator loss: 0.572452, acc: 0.648438] [adversarial loss: 1.106376, acc: 0.281250]\n",
      "3979: [discriminator loss: 0.403063, acc: 0.789062] [adversarial loss: 1.559542, acc: 0.125000]\n",
      "3980: [discriminator loss: 0.531436, acc: 0.687500] [adversarial loss: 1.200292, acc: 0.312500]\n",
      "3981: [discriminator loss: 0.501965, acc: 0.757812] [adversarial loss: 1.786395, acc: 0.062500]\n",
      "3982: [discriminator loss: 0.498325, acc: 0.742188] [adversarial loss: 1.013954, acc: 0.359375]\n",
      "3983: [discriminator loss: 0.511332, acc: 0.726562] [adversarial loss: 1.622805, acc: 0.171875]\n",
      "3984: [discriminator loss: 0.480340, acc: 0.750000] [adversarial loss: 1.319197, acc: 0.312500]\n",
      "3985: [discriminator loss: 0.545808, acc: 0.718750] [adversarial loss: 1.710919, acc: 0.109375]\n",
      "3986: [discriminator loss: 0.476225, acc: 0.765625] [adversarial loss: 1.148025, acc: 0.296875]\n",
      "3987: [discriminator loss: 0.480325, acc: 0.789062] [adversarial loss: 1.322129, acc: 0.203125]\n",
      "3988: [discriminator loss: 0.477609, acc: 0.812500] [adversarial loss: 1.504359, acc: 0.171875]\n",
      "3989: [discriminator loss: 0.501219, acc: 0.757812] [adversarial loss: 1.184578, acc: 0.296875]\n",
      "3990: [discriminator loss: 0.514024, acc: 0.781250] [adversarial loss: 1.273746, acc: 0.265625]\n",
      "3991: [discriminator loss: 0.588122, acc: 0.750000] [adversarial loss: 1.482718, acc: 0.156250]\n",
      "3992: [discriminator loss: 0.411783, acc: 0.796875] [adversarial loss: 1.311528, acc: 0.265625]\n",
      "3993: [discriminator loss: 0.497417, acc: 0.742188] [adversarial loss: 1.570942, acc: 0.171875]\n",
      "3994: [discriminator loss: 0.483323, acc: 0.757812] [adversarial loss: 1.267538, acc: 0.234375]\n",
      "3995: [discriminator loss: 0.544579, acc: 0.695312] [adversarial loss: 1.637681, acc: 0.187500]\n",
      "3996: [discriminator loss: 0.485156, acc: 0.750000] [adversarial loss: 1.116436, acc: 0.265625]\n",
      "3997: [discriminator loss: 0.546337, acc: 0.718750] [adversarial loss: 2.320624, acc: 0.031250]\n",
      "3998: [discriminator loss: 0.710539, acc: 0.656250] [adversarial loss: 0.687085, acc: 0.625000]\n",
      "3999: [discriminator loss: 0.585936, acc: 0.687500] [adversarial loss: 1.835155, acc: 0.093750]\n",
      "4000: [discriminator loss: 0.428241, acc: 0.804688] [adversarial loss: 1.213937, acc: 0.218750]\n",
      "4001: [discriminator loss: 0.526610, acc: 0.742188] [adversarial loss: 1.183873, acc: 0.296875]\n",
      "4002: [discriminator loss: 0.539294, acc: 0.726562] [adversarial loss: 1.863244, acc: 0.046875]\n",
      "4003: [discriminator loss: 0.516538, acc: 0.765625] [adversarial loss: 1.224105, acc: 0.265625]\n",
      "4004: [discriminator loss: 0.465170, acc: 0.757812] [adversarial loss: 1.552054, acc: 0.093750]\n",
      "4005: [discriminator loss: 0.412333, acc: 0.789062] [adversarial loss: 1.309443, acc: 0.265625]\n",
      "4006: [discriminator loss: 0.525255, acc: 0.765625] [adversarial loss: 1.282172, acc: 0.187500]\n",
      "4007: [discriminator loss: 0.437722, acc: 0.789062] [adversarial loss: 1.519188, acc: 0.171875]\n",
      "4008: [discriminator loss: 0.510576, acc: 0.726562] [adversarial loss: 1.126038, acc: 0.296875]\n",
      "4009: [discriminator loss: 0.558144, acc: 0.656250] [adversarial loss: 2.050058, acc: 0.031250]\n",
      "4010: [discriminator loss: 0.466303, acc: 0.765625] [adversarial loss: 1.252490, acc: 0.281250]\n",
      "4011: [discriminator loss: 0.568919, acc: 0.710938] [adversarial loss: 1.666637, acc: 0.109375]\n",
      "4012: [discriminator loss: 0.546876, acc: 0.710938] [adversarial loss: 1.342643, acc: 0.171875]\n",
      "4013: [discriminator loss: 0.486724, acc: 0.765625] [adversarial loss: 1.346041, acc: 0.218750]\n",
      "4014: [discriminator loss: 0.446824, acc: 0.804688] [adversarial loss: 1.227067, acc: 0.171875]\n",
      "4015: [discriminator loss: 0.456507, acc: 0.789062] [adversarial loss: 1.498445, acc: 0.093750]\n",
      "4016: [discriminator loss: 0.526185, acc: 0.718750] [adversarial loss: 1.336781, acc: 0.203125]\n",
      "4017: [discriminator loss: 0.488264, acc: 0.765625] [adversarial loss: 1.420602, acc: 0.171875]\n",
      "4018: [discriminator loss: 0.502851, acc: 0.765625] [adversarial loss: 1.208760, acc: 0.343750]\n",
      "4019: [discriminator loss: 0.520494, acc: 0.742188] [adversarial loss: 1.845221, acc: 0.125000]\n",
      "4020: [discriminator loss: 0.534523, acc: 0.710938] [adversarial loss: 1.088049, acc: 0.296875]\n",
      "4021: [discriminator loss: 0.536049, acc: 0.703125] [adversarial loss: 1.606182, acc: 0.109375]\n",
      "4022: [discriminator loss: 0.470470, acc: 0.781250] [adversarial loss: 1.266399, acc: 0.281250]\n",
      "4023: [discriminator loss: 0.541065, acc: 0.757812] [adversarial loss: 1.310204, acc: 0.250000]\n",
      "4024: [discriminator loss: 0.511743, acc: 0.710938] [adversarial loss: 1.134803, acc: 0.312500]\n",
      "4025: [discriminator loss: 0.519659, acc: 0.718750] [adversarial loss: 1.300749, acc: 0.140625]\n",
      "4026: [discriminator loss: 0.529701, acc: 0.765625] [adversarial loss: 1.408519, acc: 0.109375]\n",
      "4027: [discriminator loss: 0.449425, acc: 0.765625] [adversarial loss: 1.267872, acc: 0.234375]\n",
      "4028: [discriminator loss: 0.564824, acc: 0.703125] [adversarial loss: 1.864644, acc: 0.109375]\n",
      "4029: [discriminator loss: 0.563848, acc: 0.703125] [adversarial loss: 0.893553, acc: 0.421875]\n",
      "4030: [discriminator loss: 0.546422, acc: 0.726562] [adversarial loss: 2.082214, acc: 0.031250]\n",
      "4031: [discriminator loss: 0.487494, acc: 0.726562] [adversarial loss: 1.063007, acc: 0.359375]\n",
      "4032: [discriminator loss: 0.480019, acc: 0.773438] [adversarial loss: 1.733639, acc: 0.093750]\n",
      "4033: [discriminator loss: 0.531512, acc: 0.765625] [adversarial loss: 1.242138, acc: 0.265625]\n",
      "4034: [discriminator loss: 0.475729, acc: 0.710938] [adversarial loss: 1.289929, acc: 0.187500]\n",
      "4035: [discriminator loss: 0.516897, acc: 0.734375] [adversarial loss: 1.244667, acc: 0.250000]\n",
      "4036: [discriminator loss: 0.493125, acc: 0.742188] [adversarial loss: 0.968732, acc: 0.437500]\n",
      "4037: [discriminator loss: 0.536026, acc: 0.710938] [adversarial loss: 1.820390, acc: 0.078125]\n",
      "4038: [discriminator loss: 0.494815, acc: 0.742188] [adversarial loss: 1.099329, acc: 0.328125]\n",
      "4039: [discriminator loss: 0.466595, acc: 0.781250] [adversarial loss: 1.150134, acc: 0.281250]\n",
      "4040: [discriminator loss: 0.499245, acc: 0.789062] [adversarial loss: 1.500284, acc: 0.187500]\n",
      "4041: [discriminator loss: 0.509919, acc: 0.734375] [adversarial loss: 1.149747, acc: 0.296875]\n",
      "4042: [discriminator loss: 0.545808, acc: 0.734375] [adversarial loss: 1.092779, acc: 0.296875]\n",
      "4043: [discriminator loss: 0.450751, acc: 0.781250] [adversarial loss: 1.234207, acc: 0.218750]\n",
      "4044: [discriminator loss: 0.505075, acc: 0.757812] [adversarial loss: 1.815312, acc: 0.062500]\n",
      "4045: [discriminator loss: 0.506182, acc: 0.757812] [adversarial loss: 1.097076, acc: 0.265625]\n",
      "4046: [discriminator loss: 0.537912, acc: 0.703125] [adversarial loss: 1.817771, acc: 0.093750]\n",
      "4047: [discriminator loss: 0.614628, acc: 0.695312] [adversarial loss: 0.750106, acc: 0.562500]\n",
      "4048: [discriminator loss: 0.541010, acc: 0.656250] [adversarial loss: 1.874802, acc: 0.078125]\n",
      "4049: [discriminator loss: 0.450246, acc: 0.773438] [adversarial loss: 1.183261, acc: 0.281250]\n",
      "4050: [discriminator loss: 0.574425, acc: 0.671875] [adversarial loss: 1.304027, acc: 0.187500]\n",
      "4051: [discriminator loss: 0.482225, acc: 0.757812] [adversarial loss: 1.223466, acc: 0.234375]\n",
      "4052: [discriminator loss: 0.567342, acc: 0.734375] [adversarial loss: 1.147876, acc: 0.328125]\n",
      "4053: [discriminator loss: 0.527973, acc: 0.734375] [adversarial loss: 1.513223, acc: 0.187500]\n",
      "4054: [discriminator loss: 0.528941, acc: 0.734375] [adversarial loss: 1.161129, acc: 0.234375]\n",
      "4055: [discriminator loss: 0.455034, acc: 0.804688] [adversarial loss: 1.430294, acc: 0.203125]\n",
      "4056: [discriminator loss: 0.465866, acc: 0.757812] [adversarial loss: 0.988290, acc: 0.375000]\n",
      "4057: [discriminator loss: 0.527905, acc: 0.726562] [adversarial loss: 1.932406, acc: 0.093750]\n",
      "4058: [discriminator loss: 0.457369, acc: 0.742188] [adversarial loss: 1.152810, acc: 0.250000]\n",
      "4059: [discriminator loss: 0.471845, acc: 0.796875] [adversarial loss: 1.715084, acc: 0.140625]\n",
      "4060: [discriminator loss: 0.532171, acc: 0.703125] [adversarial loss: 1.074893, acc: 0.296875]\n",
      "4061: [discriminator loss: 0.503135, acc: 0.742188] [adversarial loss: 1.657927, acc: 0.093750]\n",
      "4062: [discriminator loss: 0.562825, acc: 0.734375] [adversarial loss: 1.004799, acc: 0.375000]\n",
      "4063: [discriminator loss: 0.527823, acc: 0.703125] [adversarial loss: 1.728786, acc: 0.078125]\n",
      "4064: [discriminator loss: 0.475490, acc: 0.773438] [adversarial loss: 1.170916, acc: 0.296875]\n",
      "4065: [discriminator loss: 0.464975, acc: 0.789062] [adversarial loss: 1.547736, acc: 0.109375]\n",
      "4066: [discriminator loss: 0.535695, acc: 0.718750] [adversarial loss: 1.171086, acc: 0.281250]\n",
      "4067: [discriminator loss: 0.500358, acc: 0.734375] [adversarial loss: 1.866329, acc: 0.093750]\n",
      "4068: [discriminator loss: 0.436756, acc: 0.812500] [adversarial loss: 1.032277, acc: 0.312500]\n",
      "4069: [discriminator loss: 0.527505, acc: 0.703125] [adversarial loss: 1.580010, acc: 0.093750]\n",
      "4070: [discriminator loss: 0.500171, acc: 0.750000] [adversarial loss: 1.067463, acc: 0.281250]\n",
      "4071: [discriminator loss: 0.545696, acc: 0.773438] [adversarial loss: 1.575484, acc: 0.156250]\n",
      "4072: [discriminator loss: 0.477123, acc: 0.789062] [adversarial loss: 1.164641, acc: 0.343750]\n",
      "4073: [discriminator loss: 0.547436, acc: 0.726562] [adversarial loss: 1.468341, acc: 0.187500]\n",
      "4074: [discriminator loss: 0.525533, acc: 0.703125] [adversarial loss: 0.956321, acc: 0.390625]\n",
      "4075: [discriminator loss: 0.558762, acc: 0.695312] [adversarial loss: 1.785204, acc: 0.125000]\n",
      "4076: [discriminator loss: 0.562161, acc: 0.664062] [adversarial loss: 0.875049, acc: 0.500000]\n",
      "4077: [discriminator loss: 0.526131, acc: 0.742188] [adversarial loss: 1.734413, acc: 0.109375]\n",
      "4078: [discriminator loss: 0.527090, acc: 0.710938] [adversarial loss: 1.281807, acc: 0.203125]\n",
      "4079: [discriminator loss: 0.551774, acc: 0.718750] [adversarial loss: 1.817071, acc: 0.078125]\n",
      "4080: [discriminator loss: 0.468030, acc: 0.773438] [adversarial loss: 1.098450, acc: 0.312500]\n",
      "4081: [discriminator loss: 0.461741, acc: 0.796875] [adversarial loss: 1.999677, acc: 0.062500]\n",
      "4082: [discriminator loss: 0.505417, acc: 0.750000] [adversarial loss: 1.389799, acc: 0.203125]\n",
      "4083: [discriminator loss: 0.597232, acc: 0.734375] [adversarial loss: 1.375664, acc: 0.203125]\n",
      "4084: [discriminator loss: 0.461967, acc: 0.781250] [adversarial loss: 1.391039, acc: 0.187500]\n",
      "4085: [discriminator loss: 0.538881, acc: 0.703125] [adversarial loss: 1.520042, acc: 0.140625]\n",
      "4086: [discriminator loss: 0.569073, acc: 0.726562] [adversarial loss: 1.128191, acc: 0.312500]\n",
      "4087: [discriminator loss: 0.447554, acc: 0.757812] [adversarial loss: 1.429694, acc: 0.234375]\n",
      "4088: [discriminator loss: 0.546716, acc: 0.718750] [adversarial loss: 1.117726, acc: 0.312500]\n",
      "4089: [discriminator loss: 0.419578, acc: 0.828125] [adversarial loss: 1.580850, acc: 0.109375]\n",
      "4090: [discriminator loss: 0.486808, acc: 0.742188] [adversarial loss: 1.046107, acc: 0.296875]\n",
      "4091: [discriminator loss: 0.472903, acc: 0.773438] [adversarial loss: 1.432346, acc: 0.093750]\n",
      "4092: [discriminator loss: 0.457607, acc: 0.796875] [adversarial loss: 1.017764, acc: 0.328125]\n",
      "4093: [discriminator loss: 0.623454, acc: 0.703125] [adversarial loss: 1.794161, acc: 0.156250]\n",
      "4094: [discriminator loss: 0.501141, acc: 0.750000] [adversarial loss: 0.882571, acc: 0.390625]\n",
      "4095: [discriminator loss: 0.633892, acc: 0.648438] [adversarial loss: 1.911193, acc: 0.078125]\n",
      "4096: [discriminator loss: 0.530396, acc: 0.750000] [adversarial loss: 1.097348, acc: 0.281250]\n",
      "4097: [discriminator loss: 0.509460, acc: 0.789062] [adversarial loss: 1.478611, acc: 0.171875]\n",
      "4098: [discriminator loss: 0.497464, acc: 0.781250] [adversarial loss: 1.245848, acc: 0.296875]\n",
      "4099: [discriminator loss: 0.507632, acc: 0.757812] [adversarial loss: 0.820322, acc: 0.453125]\n",
      "4100: [discriminator loss: 0.507031, acc: 0.710938] [adversarial loss: 1.359682, acc: 0.250000]\n",
      "4101: [discriminator loss: 0.457027, acc: 0.773438] [adversarial loss: 1.145582, acc: 0.265625]\n",
      "4102: [discriminator loss: 0.565459, acc: 0.687500] [adversarial loss: 1.893684, acc: 0.109375]\n",
      "4103: [discriminator loss: 0.564090, acc: 0.687500] [adversarial loss: 0.913088, acc: 0.406250]\n",
      "4104: [discriminator loss: 0.506518, acc: 0.734375] [adversarial loss: 1.189943, acc: 0.265625]\n",
      "4105: [discriminator loss: 0.514248, acc: 0.726562] [adversarial loss: 1.262151, acc: 0.203125]\n",
      "4106: [discriminator loss: 0.486363, acc: 0.750000] [adversarial loss: 1.488762, acc: 0.171875]\n",
      "4107: [discriminator loss: 0.595913, acc: 0.687500] [adversarial loss: 1.186473, acc: 0.218750]\n",
      "4108: [discriminator loss: 0.457153, acc: 0.773438] [adversarial loss: 1.476579, acc: 0.171875]\n",
      "4109: [discriminator loss: 0.415488, acc: 0.851562] [adversarial loss: 1.292278, acc: 0.234375]\n",
      "4110: [discriminator loss: 0.470721, acc: 0.804688] [adversarial loss: 1.223241, acc: 0.250000]\n",
      "4111: [discriminator loss: 0.502098, acc: 0.765625] [adversarial loss: 1.235682, acc: 0.171875]\n",
      "4112: [discriminator loss: 0.372106, acc: 0.859375] [adversarial loss: 1.621951, acc: 0.156250]\n",
      "4113: [discriminator loss: 0.498813, acc: 0.750000] [adversarial loss: 1.190750, acc: 0.281250]\n",
      "4114: [discriminator loss: 0.504594, acc: 0.742188] [adversarial loss: 1.485946, acc: 0.171875]\n",
      "4115: [discriminator loss: 0.536963, acc: 0.710938] [adversarial loss: 1.146048, acc: 0.375000]\n",
      "4116: [discriminator loss: 0.513496, acc: 0.695312] [adversarial loss: 1.823604, acc: 0.078125]\n",
      "4117: [discriminator loss: 0.555642, acc: 0.742188] [adversarial loss: 0.832992, acc: 0.500000]\n",
      "4118: [discriminator loss: 0.537442, acc: 0.710938] [adversarial loss: 1.933416, acc: 0.031250]\n",
      "4119: [discriminator loss: 0.578519, acc: 0.687500] [adversarial loss: 0.932400, acc: 0.406250]\n",
      "4120: [discriminator loss: 0.600217, acc: 0.679688] [adversarial loss: 1.946660, acc: 0.062500]\n",
      "4121: [discriminator loss: 0.547905, acc: 0.726562] [adversarial loss: 1.021713, acc: 0.312500]\n",
      "4122: [discriminator loss: 0.490688, acc: 0.781250] [adversarial loss: 1.649179, acc: 0.171875]\n",
      "4123: [discriminator loss: 0.517468, acc: 0.757812] [adversarial loss: 1.284955, acc: 0.250000]\n",
      "4124: [discriminator loss: 0.471896, acc: 0.812500] [adversarial loss: 1.362419, acc: 0.156250]\n",
      "4125: [discriminator loss: 0.462920, acc: 0.757812] [adversarial loss: 0.953422, acc: 0.390625]\n",
      "4126: [discriminator loss: 0.449539, acc: 0.796875] [adversarial loss: 1.564742, acc: 0.125000]\n",
      "4127: [discriminator loss: 0.539517, acc: 0.726562] [adversarial loss: 1.446041, acc: 0.125000]\n",
      "4128: [discriminator loss: 0.517549, acc: 0.750000] [adversarial loss: 1.560967, acc: 0.140625]\n",
      "4129: [discriminator loss: 0.550888, acc: 0.734375] [adversarial loss: 0.932987, acc: 0.375000]\n",
      "4130: [discriminator loss: 0.483794, acc: 0.710938] [adversarial loss: 1.490268, acc: 0.140625]\n",
      "4131: [discriminator loss: 0.546203, acc: 0.695312] [adversarial loss: 1.045261, acc: 0.359375]\n",
      "4132: [discriminator loss: 0.544399, acc: 0.765625] [adversarial loss: 1.673002, acc: 0.078125]\n",
      "4133: [discriminator loss: 0.551049, acc: 0.710938] [adversarial loss: 1.233538, acc: 0.234375]\n",
      "4134: [discriminator loss: 0.450639, acc: 0.812500] [adversarial loss: 1.428642, acc: 0.203125]\n",
      "4135: [discriminator loss: 0.465255, acc: 0.734375] [adversarial loss: 1.229765, acc: 0.156250]\n",
      "4136: [discriminator loss: 0.509661, acc: 0.734375] [adversarial loss: 1.546708, acc: 0.046875]\n",
      "4137: [discriminator loss: 0.468621, acc: 0.820312] [adversarial loss: 1.302464, acc: 0.218750]\n",
      "4138: [discriminator loss: 0.417690, acc: 0.835938] [adversarial loss: 1.397687, acc: 0.234375]\n",
      "4139: [discriminator loss: 0.482431, acc: 0.742188] [adversarial loss: 0.971002, acc: 0.312500]\n",
      "4140: [discriminator loss: 0.514426, acc: 0.734375] [adversarial loss: 1.864483, acc: 0.062500]\n",
      "4141: [discriminator loss: 0.627103, acc: 0.671875] [adversarial loss: 0.842082, acc: 0.500000]\n",
      "4142: [discriminator loss: 0.577520, acc: 0.695312] [adversarial loss: 1.892099, acc: 0.062500]\n",
      "4143: [discriminator loss: 0.529889, acc: 0.750000] [adversarial loss: 1.237423, acc: 0.234375]\n",
      "4144: [discriminator loss: 0.538117, acc: 0.671875] [adversarial loss: 1.852913, acc: 0.031250]\n",
      "4145: [discriminator loss: 0.485628, acc: 0.750000] [adversarial loss: 1.131878, acc: 0.281250]\n",
      "4146: [discriminator loss: 0.449408, acc: 0.789062] [adversarial loss: 1.743892, acc: 0.078125]\n",
      "4147: [discriminator loss: 0.563841, acc: 0.695312] [adversarial loss: 1.203864, acc: 0.328125]\n",
      "4148: [discriminator loss: 0.455036, acc: 0.742188] [adversarial loss: 1.697520, acc: 0.062500]\n",
      "4149: [discriminator loss: 0.423776, acc: 0.789062] [adversarial loss: 1.258950, acc: 0.140625]\n",
      "4150: [discriminator loss: 0.469061, acc: 0.781250] [adversarial loss: 1.461679, acc: 0.156250]\n",
      "4151: [discriminator loss: 0.592141, acc: 0.703125] [adversarial loss: 1.104057, acc: 0.343750]\n",
      "4152: [discriminator loss: 0.508362, acc: 0.742188] [adversarial loss: 1.621537, acc: 0.078125]\n",
      "4153: [discriminator loss: 0.481238, acc: 0.750000] [adversarial loss: 1.042377, acc: 0.359375]\n",
      "4154: [discriminator loss: 0.521742, acc: 0.734375] [adversarial loss: 1.687553, acc: 0.093750]\n",
      "4155: [discriminator loss: 0.534580, acc: 0.742188] [adversarial loss: 1.033981, acc: 0.343750]\n",
      "4156: [discriminator loss: 0.474522, acc: 0.757812] [adversarial loss: 1.634819, acc: 0.156250]\n",
      "4157: [discriminator loss: 0.561251, acc: 0.703125] [adversarial loss: 1.087765, acc: 0.328125]\n",
      "4158: [discriminator loss: 0.556088, acc: 0.726562] [adversarial loss: 1.374208, acc: 0.218750]\n",
      "4159: [discriminator loss: 0.542682, acc: 0.734375] [adversarial loss: 1.563736, acc: 0.078125]\n",
      "4160: [discriminator loss: 0.517334, acc: 0.734375] [adversarial loss: 0.942146, acc: 0.375000]\n",
      "4161: [discriminator loss: 0.517846, acc: 0.750000] [adversarial loss: 1.312454, acc: 0.296875]\n",
      "4162: [discriminator loss: 0.520050, acc: 0.726562] [adversarial loss: 1.279572, acc: 0.218750]\n",
      "4163: [discriminator loss: 0.469507, acc: 0.796875] [adversarial loss: 1.518860, acc: 0.109375]\n",
      "4164: [discriminator loss: 0.424548, acc: 0.820312] [adversarial loss: 1.226265, acc: 0.234375]\n",
      "4165: [discriminator loss: 0.520427, acc: 0.742188] [adversarial loss: 1.380574, acc: 0.125000]\n",
      "4166: [discriminator loss: 0.544636, acc: 0.734375] [adversarial loss: 1.261883, acc: 0.234375]\n",
      "4167: [discriminator loss: 0.457342, acc: 0.781250] [adversarial loss: 1.677528, acc: 0.156250]\n",
      "4168: [discriminator loss: 0.453621, acc: 0.812500] [adversarial loss: 1.399945, acc: 0.281250]\n",
      "4169: [discriminator loss: 0.522292, acc: 0.726562] [adversarial loss: 1.000366, acc: 0.390625]\n",
      "4170: [discriminator loss: 0.591266, acc: 0.664062] [adversarial loss: 2.075577, acc: 0.015625]\n",
      "4171: [discriminator loss: 0.542308, acc: 0.703125] [adversarial loss: 0.969013, acc: 0.421875]\n",
      "4172: [discriminator loss: 0.547421, acc: 0.718750] [adversarial loss: 1.695942, acc: 0.046875]\n",
      "4173: [discriminator loss: 0.517605, acc: 0.789062] [adversarial loss: 0.912887, acc: 0.343750]\n",
      "4174: [discriminator loss: 0.585253, acc: 0.695312] [adversarial loss: 1.367954, acc: 0.109375]\n",
      "4175: [discriminator loss: 0.527161, acc: 0.718750] [adversarial loss: 1.332274, acc: 0.156250]\n",
      "4176: [discriminator loss: 0.544751, acc: 0.742188] [adversarial loss: 1.252846, acc: 0.250000]\n",
      "4177: [discriminator loss: 0.458276, acc: 0.796875] [adversarial loss: 1.339976, acc: 0.171875]\n",
      "4178: [discriminator loss: 0.516157, acc: 0.789062] [adversarial loss: 1.443387, acc: 0.140625]\n",
      "4179: [discriminator loss: 0.452533, acc: 0.804688] [adversarial loss: 1.271259, acc: 0.156250]\n",
      "4180: [discriminator loss: 0.483499, acc: 0.718750] [adversarial loss: 1.427017, acc: 0.203125]\n",
      "4181: [discriminator loss: 0.533418, acc: 0.742188] [adversarial loss: 0.973213, acc: 0.312500]\n",
      "4182: [discriminator loss: 0.602515, acc: 0.687500] [adversarial loss: 1.770339, acc: 0.109375]\n",
      "4183: [discriminator loss: 0.590523, acc: 0.710938] [adversarial loss: 1.300646, acc: 0.218750]\n",
      "4184: [discriminator loss: 0.522210, acc: 0.718750] [adversarial loss: 1.936050, acc: 0.015625]\n",
      "4185: [discriminator loss: 0.547084, acc: 0.710938] [adversarial loss: 0.896722, acc: 0.421875]\n",
      "4186: [discriminator loss: 0.549937, acc: 0.703125] [adversarial loss: 1.725554, acc: 0.125000]\n",
      "4187: [discriminator loss: 0.573235, acc: 0.703125] [adversarial loss: 1.034289, acc: 0.343750]\n",
      "4188: [discriminator loss: 0.518568, acc: 0.726562] [adversarial loss: 1.373076, acc: 0.093750]\n",
      "4189: [discriminator loss: 0.496950, acc: 0.742188] [adversarial loss: 1.242962, acc: 0.203125]\n",
      "4190: [discriminator loss: 0.386544, acc: 0.812500] [adversarial loss: 1.454779, acc: 0.187500]\n",
      "4191: [discriminator loss: 0.443661, acc: 0.812500] [adversarial loss: 1.101992, acc: 0.375000]\n",
      "4192: [discriminator loss: 0.497347, acc: 0.804688] [adversarial loss: 1.470261, acc: 0.125000]\n",
      "4193: [discriminator loss: 0.475697, acc: 0.750000] [adversarial loss: 1.180756, acc: 0.281250]\n",
      "4194: [discriminator loss: 0.526678, acc: 0.726562] [adversarial loss: 1.095186, acc: 0.328125]\n",
      "4195: [discriminator loss: 0.561132, acc: 0.710938] [adversarial loss: 1.352389, acc: 0.156250]\n",
      "4196: [discriminator loss: 0.527018, acc: 0.710938] [adversarial loss: 1.230164, acc: 0.359375]\n",
      "4197: [discriminator loss: 0.592703, acc: 0.671875] [adversarial loss: 1.380990, acc: 0.140625]\n",
      "4198: [discriminator loss: 0.475167, acc: 0.796875] [adversarial loss: 1.025999, acc: 0.328125]\n",
      "4199: [discriminator loss: 0.553238, acc: 0.726562] [adversarial loss: 1.448148, acc: 0.125000]\n",
      "4200: [discriminator loss: 0.569827, acc: 0.679688] [adversarial loss: 1.293904, acc: 0.218750]\n",
      "4201: [discriminator loss: 0.509878, acc: 0.765625] [adversarial loss: 1.204112, acc: 0.281250]\n",
      "4202: [discriminator loss: 0.424351, acc: 0.812500] [adversarial loss: 1.464403, acc: 0.156250]\n",
      "4203: [discriminator loss: 0.534368, acc: 0.718750] [adversarial loss: 0.987900, acc: 0.343750]\n",
      "4204: [discriminator loss: 0.527120, acc: 0.710938] [adversarial loss: 1.423593, acc: 0.171875]\n",
      "4205: [discriminator loss: 0.533480, acc: 0.750000] [adversarial loss: 1.371471, acc: 0.156250]\n",
      "4206: [discriminator loss: 0.545887, acc: 0.742188] [adversarial loss: 1.147135, acc: 0.312500]\n",
      "4207: [discriminator loss: 0.467641, acc: 0.781250] [adversarial loss: 1.455436, acc: 0.203125]\n",
      "4208: [discriminator loss: 0.515808, acc: 0.757812] [adversarial loss: 1.330517, acc: 0.187500]\n",
      "4209: [discriminator loss: 0.535444, acc: 0.750000] [adversarial loss: 1.192992, acc: 0.250000]\n",
      "4210: [discriminator loss: 0.531580, acc: 0.734375] [adversarial loss: 1.581849, acc: 0.156250]\n",
      "4211: [discriminator loss: 0.451568, acc: 0.781250] [adversarial loss: 1.071848, acc: 0.312500]\n",
      "4212: [discriminator loss: 0.556452, acc: 0.757812] [adversarial loss: 1.341952, acc: 0.281250]\n",
      "4213: [discriminator loss: 0.442180, acc: 0.789062] [adversarial loss: 1.515255, acc: 0.140625]\n",
      "4214: [discriminator loss: 0.546183, acc: 0.703125] [adversarial loss: 1.270378, acc: 0.312500]\n",
      "4215: [discriminator loss: 0.417691, acc: 0.812500] [adversarial loss: 1.561350, acc: 0.171875]\n",
      "4216: [discriminator loss: 0.487398, acc: 0.796875] [adversarial loss: 0.750341, acc: 0.531250]\n",
      "4217: [discriminator loss: 0.443386, acc: 0.765625] [adversarial loss: 1.996586, acc: 0.062500]\n",
      "4218: [discriminator loss: 0.520278, acc: 0.726562] [adversarial loss: 1.185407, acc: 0.312500]\n",
      "4219: [discriminator loss: 0.501875, acc: 0.742188] [adversarial loss: 1.844635, acc: 0.062500]\n",
      "4220: [discriminator loss: 0.489908, acc: 0.734375] [adversarial loss: 0.930649, acc: 0.406250]\n",
      "4221: [discriminator loss: 0.582806, acc: 0.664062] [adversarial loss: 1.989863, acc: 0.078125]\n",
      "4222: [discriminator loss: 0.539909, acc: 0.734375] [adversarial loss: 1.035668, acc: 0.359375]\n",
      "4223: [discriminator loss: 0.579603, acc: 0.710938] [adversarial loss: 1.573378, acc: 0.125000]\n",
      "4224: [discriminator loss: 0.483412, acc: 0.765625] [adversarial loss: 1.417697, acc: 0.218750]\n",
      "4225: [discriminator loss: 0.535452, acc: 0.734375] [adversarial loss: 1.206908, acc: 0.234375]\n",
      "4226: [discriminator loss: 0.531315, acc: 0.703125] [adversarial loss: 1.189802, acc: 0.328125]\n",
      "4227: [discriminator loss: 0.486091, acc: 0.757812] [adversarial loss: 1.192500, acc: 0.296875]\n",
      "4228: [discriminator loss: 0.433956, acc: 0.820312] [adversarial loss: 1.288886, acc: 0.234375]\n",
      "4229: [discriminator loss: 0.489042, acc: 0.742188] [adversarial loss: 1.407905, acc: 0.140625]\n",
      "4230: [discriminator loss: 0.543955, acc: 0.710938] [adversarial loss: 1.046220, acc: 0.265625]\n",
      "4231: [discriminator loss: 0.503055, acc: 0.789062] [adversarial loss: 1.856176, acc: 0.062500]\n",
      "4232: [discriminator loss: 0.474405, acc: 0.773438] [adversarial loss: 1.253428, acc: 0.250000]\n",
      "4233: [discriminator loss: 0.485416, acc: 0.757812] [adversarial loss: 1.810711, acc: 0.093750]\n",
      "4234: [discriminator loss: 0.536865, acc: 0.757812] [adversarial loss: 1.144180, acc: 0.234375]\n",
      "4235: [discriminator loss: 0.475341, acc: 0.750000] [adversarial loss: 1.581410, acc: 0.156250]\n",
      "4236: [discriminator loss: 0.456577, acc: 0.757812] [adversarial loss: 1.246170, acc: 0.296875]\n",
      "4237: [discriminator loss: 0.534169, acc: 0.726562] [adversarial loss: 1.718515, acc: 0.078125]\n",
      "4238: [discriminator loss: 0.517382, acc: 0.757812] [adversarial loss: 1.399141, acc: 0.203125]\n",
      "4239: [discriminator loss: 0.567639, acc: 0.718750] [adversarial loss: 1.166680, acc: 0.250000]\n",
      "4240: [discriminator loss: 0.445096, acc: 0.789062] [adversarial loss: 1.402337, acc: 0.187500]\n",
      "4241: [discriminator loss: 0.479762, acc: 0.750000] [adversarial loss: 0.993820, acc: 0.343750]\n",
      "4242: [discriminator loss: 0.493092, acc: 0.804688] [adversarial loss: 1.835466, acc: 0.046875]\n",
      "4243: [discriminator loss: 0.564375, acc: 0.726562] [adversarial loss: 0.987089, acc: 0.359375]\n",
      "4244: [discriminator loss: 0.501528, acc: 0.757812] [adversarial loss: 1.777253, acc: 0.109375]\n",
      "4245: [discriminator loss: 0.584487, acc: 0.695312] [adversarial loss: 1.042126, acc: 0.375000]\n",
      "4246: [discriminator loss: 0.573402, acc: 0.734375] [adversarial loss: 1.758661, acc: 0.046875]\n",
      "4247: [discriminator loss: 0.566577, acc: 0.718750] [adversarial loss: 0.910274, acc: 0.437500]\n",
      "4248: [discriminator loss: 0.507918, acc: 0.734375] [adversarial loss: 1.991786, acc: 0.093750]\n",
      "4249: [discriminator loss: 0.508490, acc: 0.750000] [adversarial loss: 1.111778, acc: 0.343750]\n",
      "4250: [discriminator loss: 0.456225, acc: 0.750000] [adversarial loss: 1.106284, acc: 0.312500]\n",
      "4251: [discriminator loss: 0.532210, acc: 0.765625] [adversarial loss: 2.159224, acc: 0.015625]\n",
      "4252: [discriminator loss: 0.638032, acc: 0.679688] [adversarial loss: 0.997361, acc: 0.390625]\n",
      "4253: [discriminator loss: 0.567261, acc: 0.687500] [adversarial loss: 1.776298, acc: 0.093750]\n",
      "4254: [discriminator loss: 0.549240, acc: 0.750000] [adversarial loss: 1.150038, acc: 0.375000]\n",
      "4255: [discriminator loss: 0.523518, acc: 0.781250] [adversarial loss: 1.323380, acc: 0.218750]\n",
      "4256: [discriminator loss: 0.491907, acc: 0.765625] [adversarial loss: 1.354968, acc: 0.156250]\n",
      "4257: [discriminator loss: 0.486324, acc: 0.726562] [adversarial loss: 1.180939, acc: 0.234375]\n",
      "4258: [discriminator loss: 0.443847, acc: 0.820312] [adversarial loss: 1.265795, acc: 0.296875]\n",
      "4259: [discriminator loss: 0.481909, acc: 0.765625] [adversarial loss: 1.071258, acc: 0.296875]\n",
      "4260: [discriminator loss: 0.468027, acc: 0.773438] [adversarial loss: 1.909984, acc: 0.062500]\n",
      "4261: [discriminator loss: 0.496170, acc: 0.757812] [adversarial loss: 1.112518, acc: 0.218750]\n",
      "4262: [discriminator loss: 0.545937, acc: 0.757812] [adversarial loss: 1.782578, acc: 0.125000]\n",
      "4263: [discriminator loss: 0.494244, acc: 0.757812] [adversarial loss: 0.971164, acc: 0.390625]\n",
      "4264: [discriminator loss: 0.493977, acc: 0.765625] [adversarial loss: 1.446179, acc: 0.125000]\n",
      "4265: [discriminator loss: 0.496255, acc: 0.750000] [adversarial loss: 1.156793, acc: 0.234375]\n",
      "4266: [discriminator loss: 0.548007, acc: 0.742188] [adversarial loss: 1.788652, acc: 0.062500]\n",
      "4267: [discriminator loss: 0.476730, acc: 0.750000] [adversarial loss: 1.255781, acc: 0.234375]\n",
      "4268: [discriminator loss: 0.438634, acc: 0.765625] [adversarial loss: 1.205506, acc: 0.296875]\n",
      "4269: [discriminator loss: 0.440729, acc: 0.781250] [adversarial loss: 1.396655, acc: 0.265625]\n",
      "4270: [discriminator loss: 0.549164, acc: 0.710938] [adversarial loss: 1.099896, acc: 0.375000]\n",
      "4271: [discriminator loss: 0.515391, acc: 0.757812] [adversarial loss: 1.381247, acc: 0.250000]\n",
      "4272: [discriminator loss: 0.457035, acc: 0.796875] [adversarial loss: 1.052178, acc: 0.312500]\n",
      "4273: [discriminator loss: 0.533006, acc: 0.703125] [adversarial loss: 1.533795, acc: 0.093750]\n",
      "4274: [discriminator loss: 0.499100, acc: 0.734375] [adversarial loss: 1.079154, acc: 0.328125]\n",
      "4275: [discriminator loss: 0.493851, acc: 0.757812] [adversarial loss: 1.557358, acc: 0.156250]\n",
      "4276: [discriminator loss: 0.528374, acc: 0.718750] [adversarial loss: 1.010952, acc: 0.390625]\n",
      "4277: [discriminator loss: 0.418565, acc: 0.789062] [adversarial loss: 2.029636, acc: 0.093750]\n",
      "4278: [discriminator loss: 0.573668, acc: 0.734375] [adversarial loss: 0.906042, acc: 0.437500]\n",
      "4279: [discriminator loss: 0.501495, acc: 0.726562] [adversarial loss: 1.661017, acc: 0.140625]\n",
      "4280: [discriminator loss: 0.435661, acc: 0.812500] [adversarial loss: 1.495584, acc: 0.187500]\n",
      "4281: [discriminator loss: 0.405372, acc: 0.820312] [adversarial loss: 1.258583, acc: 0.375000]\n",
      "4282: [discriminator loss: 0.543259, acc: 0.734375] [adversarial loss: 1.506016, acc: 0.171875]\n",
      "4283: [discriminator loss: 0.634942, acc: 0.679688] [adversarial loss: 1.285829, acc: 0.203125]\n",
      "4284: [discriminator loss: 0.496598, acc: 0.765625] [adversarial loss: 1.341802, acc: 0.218750]\n",
      "4285: [discriminator loss: 0.531263, acc: 0.726562] [adversarial loss: 1.323391, acc: 0.250000]\n",
      "4286: [discriminator loss: 0.501810, acc: 0.750000] [adversarial loss: 1.617348, acc: 0.171875]\n",
      "4287: [discriminator loss: 0.522417, acc: 0.703125] [adversarial loss: 1.008877, acc: 0.343750]\n",
      "4288: [discriminator loss: 0.520401, acc: 0.734375] [adversarial loss: 1.699625, acc: 0.093750]\n",
      "4289: [discriminator loss: 0.589196, acc: 0.742188] [adversarial loss: 1.157693, acc: 0.375000]\n",
      "4290: [discriminator loss: 0.563333, acc: 0.671875] [adversarial loss: 1.547992, acc: 0.140625]\n",
      "4291: [discriminator loss: 0.536807, acc: 0.718750] [adversarial loss: 1.440695, acc: 0.156250]\n",
      "4292: [discriminator loss: 0.508214, acc: 0.750000] [adversarial loss: 1.227340, acc: 0.265625]\n",
      "4293: [discriminator loss: 0.464769, acc: 0.812500] [adversarial loss: 1.900841, acc: 0.046875]\n",
      "4294: [discriminator loss: 0.544165, acc: 0.695312] [adversarial loss: 0.948946, acc: 0.453125]\n",
      "4295: [discriminator loss: 0.549347, acc: 0.734375] [adversarial loss: 1.555626, acc: 0.156250]\n",
      "4296: [discriminator loss: 0.487112, acc: 0.781250] [adversarial loss: 1.473436, acc: 0.156250]\n",
      "4297: [discriminator loss: 0.517499, acc: 0.750000] [adversarial loss: 1.254242, acc: 0.265625]\n",
      "4298: [discriminator loss: 0.423860, acc: 0.812500] [adversarial loss: 1.313018, acc: 0.171875]\n",
      "4299: [discriminator loss: 0.449788, acc: 0.789062] [adversarial loss: 1.518662, acc: 0.156250]\n",
      "4300: [discriminator loss: 0.562396, acc: 0.695312] [adversarial loss: 1.625270, acc: 0.125000]\n",
      "4301: [discriminator loss: 0.548752, acc: 0.734375] [adversarial loss: 1.210464, acc: 0.328125]\n",
      "4302: [discriminator loss: 0.527103, acc: 0.726562] [adversarial loss: 1.653126, acc: 0.093750]\n",
      "4303: [discriminator loss: 0.432342, acc: 0.804688] [adversarial loss: 1.086048, acc: 0.265625]\n",
      "4304: [discriminator loss: 0.526606, acc: 0.726562] [adversarial loss: 1.469221, acc: 0.156250]\n",
      "4305: [discriminator loss: 0.587963, acc: 0.710938] [adversarial loss: 1.028577, acc: 0.359375]\n",
      "4306: [discriminator loss: 0.513164, acc: 0.765625] [adversarial loss: 1.759313, acc: 0.078125]\n",
      "4307: [discriminator loss: 0.453797, acc: 0.765625] [adversarial loss: 1.303067, acc: 0.140625]\n",
      "4308: [discriminator loss: 0.517922, acc: 0.718750] [adversarial loss: 1.278817, acc: 0.218750]\n",
      "4309: [discriminator loss: 0.495531, acc: 0.750000] [adversarial loss: 1.028149, acc: 0.390625]\n",
      "4310: [discriminator loss: 0.558106, acc: 0.695312] [adversarial loss: 1.481209, acc: 0.171875]\n",
      "4311: [discriminator loss: 0.548061, acc: 0.703125] [adversarial loss: 0.874645, acc: 0.437500]\n",
      "4312: [discriminator loss: 0.595660, acc: 0.656250] [adversarial loss: 2.199897, acc: 0.062500]\n",
      "4313: [discriminator loss: 0.502480, acc: 0.718750] [adversarial loss: 1.040563, acc: 0.406250]\n",
      "4314: [discriminator loss: 0.532022, acc: 0.718750] [adversarial loss: 1.521625, acc: 0.109375]\n",
      "4315: [discriminator loss: 0.538108, acc: 0.710938] [adversarial loss: 1.197224, acc: 0.265625]\n",
      "4316: [discriminator loss: 0.457714, acc: 0.773438] [adversarial loss: 1.163125, acc: 0.312500]\n",
      "4317: [discriminator loss: 0.617739, acc: 0.664062] [adversarial loss: 1.428090, acc: 0.093750]\n",
      "4318: [discriminator loss: 0.563047, acc: 0.750000] [adversarial loss: 0.989942, acc: 0.312500]\n",
      "4319: [discriminator loss: 0.505521, acc: 0.718750] [adversarial loss: 1.782121, acc: 0.078125]\n",
      "4320: [discriminator loss: 0.449427, acc: 0.812500] [adversarial loss: 1.169906, acc: 0.234375]\n",
      "4321: [discriminator loss: 0.489844, acc: 0.796875] [adversarial loss: 1.196189, acc: 0.296875]\n",
      "4322: [discriminator loss: 0.548450, acc: 0.703125] [adversarial loss: 1.477172, acc: 0.125000]\n",
      "4323: [discriminator loss: 0.452002, acc: 0.812500] [adversarial loss: 1.188659, acc: 0.343750]\n",
      "4324: [discriminator loss: 0.512143, acc: 0.765625] [adversarial loss: 1.466542, acc: 0.187500]\n",
      "4325: [discriminator loss: 0.501267, acc: 0.773438] [adversarial loss: 1.049952, acc: 0.296875]\n",
      "4326: [discriminator loss: 0.593695, acc: 0.703125] [adversarial loss: 1.603708, acc: 0.171875]\n",
      "4327: [discriminator loss: 0.434837, acc: 0.796875] [adversarial loss: 1.220648, acc: 0.265625]\n",
      "4328: [discriminator loss: 0.509743, acc: 0.742188] [adversarial loss: 1.443597, acc: 0.187500]\n",
      "4329: [discriminator loss: 0.499820, acc: 0.757812] [adversarial loss: 1.133191, acc: 0.312500]\n",
      "4330: [discriminator loss: 0.486799, acc: 0.773438] [adversarial loss: 1.534387, acc: 0.171875]\n",
      "4331: [discriminator loss: 0.507211, acc: 0.757812] [adversarial loss: 1.007794, acc: 0.343750]\n",
      "4332: [discriminator loss: 0.461520, acc: 0.820312] [adversarial loss: 1.808006, acc: 0.078125]\n",
      "4333: [discriminator loss: 0.513363, acc: 0.726562] [adversarial loss: 0.994260, acc: 0.406250]\n",
      "4334: [discriminator loss: 0.514274, acc: 0.734375] [adversarial loss: 1.627554, acc: 0.140625]\n",
      "4335: [discriminator loss: 0.565934, acc: 0.703125] [adversarial loss: 1.003698, acc: 0.359375]\n",
      "4336: [discriminator loss: 0.534488, acc: 0.765625] [adversarial loss: 1.786365, acc: 0.140625]\n",
      "4337: [discriminator loss: 0.543815, acc: 0.695312] [adversarial loss: 1.173019, acc: 0.281250]\n",
      "4338: [discriminator loss: 0.521443, acc: 0.710938] [adversarial loss: 1.685159, acc: 0.046875]\n",
      "4339: [discriminator loss: 0.466034, acc: 0.773438] [adversarial loss: 1.079642, acc: 0.312500]\n",
      "4340: [discriminator loss: 0.465516, acc: 0.765625] [adversarial loss: 1.454138, acc: 0.203125]\n",
      "4341: [discriminator loss: 0.535398, acc: 0.726562] [adversarial loss: 1.305762, acc: 0.171875]\n",
      "4342: [discriminator loss: 0.475422, acc: 0.773438] [adversarial loss: 1.099995, acc: 0.312500]\n",
      "4343: [discriminator loss: 0.478944, acc: 0.781250] [adversarial loss: 1.516005, acc: 0.140625]\n",
      "4344: [discriminator loss: 0.438266, acc: 0.789062] [adversarial loss: 1.062095, acc: 0.218750]\n",
      "4345: [discriminator loss: 0.453826, acc: 0.796875] [adversarial loss: 1.745789, acc: 0.031250]\n",
      "4346: [discriminator loss: 0.624413, acc: 0.671875] [adversarial loss: 0.869887, acc: 0.437500]\n",
      "4347: [discriminator loss: 0.536989, acc: 0.718750] [adversarial loss: 1.709127, acc: 0.093750]\n",
      "4348: [discriminator loss: 0.519507, acc: 0.734375] [adversarial loss: 0.870863, acc: 0.546875]\n",
      "4349: [discriminator loss: 0.561045, acc: 0.687500] [adversarial loss: 2.191453, acc: 0.031250]\n",
      "4350: [discriminator loss: 0.568778, acc: 0.718750] [adversarial loss: 0.920434, acc: 0.468750]\n",
      "4351: [discriminator loss: 0.448808, acc: 0.773438] [adversarial loss: 1.409614, acc: 0.234375]\n",
      "4352: [discriminator loss: 0.546078, acc: 0.742188] [adversarial loss: 1.433459, acc: 0.187500]\n",
      "4353: [discriminator loss: 0.462471, acc: 0.765625] [adversarial loss: 1.105451, acc: 0.234375]\n",
      "4354: [discriminator loss: 0.529835, acc: 0.718750] [adversarial loss: 1.569283, acc: 0.187500]\n",
      "4355: [discriminator loss: 0.495554, acc: 0.789062] [adversarial loss: 1.212047, acc: 0.312500]\n",
      "4356: [discriminator loss: 0.529327, acc: 0.710938] [adversarial loss: 1.255689, acc: 0.218750]\n",
      "4357: [discriminator loss: 0.518001, acc: 0.718750] [adversarial loss: 1.372299, acc: 0.250000]\n",
      "4358: [discriminator loss: 0.517986, acc: 0.742188] [adversarial loss: 0.821835, acc: 0.500000]\n",
      "4359: [discriminator loss: 0.485772, acc: 0.710938] [adversarial loss: 1.669500, acc: 0.109375]\n",
      "4360: [discriminator loss: 0.478987, acc: 0.789062] [adversarial loss: 1.235252, acc: 0.328125]\n",
      "4361: [discriminator loss: 0.552865, acc: 0.710938] [adversarial loss: 1.600392, acc: 0.078125]\n",
      "4362: [discriminator loss: 0.491412, acc: 0.804688] [adversarial loss: 1.494565, acc: 0.140625]\n",
      "4363: [discriminator loss: 0.475671, acc: 0.765625] [adversarial loss: 1.387198, acc: 0.156250]\n",
      "4364: [discriminator loss: 0.473670, acc: 0.757812] [adversarial loss: 1.227647, acc: 0.250000]\n",
      "4365: [discriminator loss: 0.485304, acc: 0.781250] [adversarial loss: 1.678002, acc: 0.062500]\n",
      "4366: [discriminator loss: 0.449865, acc: 0.789062] [adversarial loss: 1.313655, acc: 0.218750]\n",
      "4367: [discriminator loss: 0.529646, acc: 0.703125] [adversarial loss: 1.691783, acc: 0.187500]\n",
      "4368: [discriminator loss: 0.563617, acc: 0.734375] [adversarial loss: 0.935204, acc: 0.421875]\n",
      "4369: [discriminator loss: 0.546381, acc: 0.718750] [adversarial loss: 1.964424, acc: 0.046875]\n",
      "4370: [discriminator loss: 0.592016, acc: 0.734375] [adversarial loss: 0.782482, acc: 0.531250]\n",
      "4371: [discriminator loss: 0.553066, acc: 0.710938] [adversarial loss: 1.565078, acc: 0.093750]\n",
      "4372: [discriminator loss: 0.466258, acc: 0.796875] [adversarial loss: 1.320944, acc: 0.156250]\n",
      "4373: [discriminator loss: 0.461349, acc: 0.835938] [adversarial loss: 1.335774, acc: 0.140625]\n",
      "4374: [discriminator loss: 0.466036, acc: 0.765625] [adversarial loss: 1.439620, acc: 0.156250]\n",
      "4375: [discriminator loss: 0.525401, acc: 0.726562] [adversarial loss: 1.419302, acc: 0.171875]\n",
      "4376: [discriminator loss: 0.525064, acc: 0.695312] [adversarial loss: 1.405903, acc: 0.140625]\n",
      "4377: [discriminator loss: 0.586870, acc: 0.687500] [adversarial loss: 1.638919, acc: 0.078125]\n",
      "4378: [discriminator loss: 0.490456, acc: 0.804688] [adversarial loss: 1.028970, acc: 0.375000]\n",
      "4379: [discriminator loss: 0.451487, acc: 0.835938] [adversarial loss: 1.563161, acc: 0.078125]\n",
      "4380: [discriminator loss: 0.495157, acc: 0.765625] [adversarial loss: 1.281979, acc: 0.328125]\n",
      "4381: [discriminator loss: 0.529989, acc: 0.687500] [adversarial loss: 1.832323, acc: 0.125000]\n",
      "4382: [discriminator loss: 0.505776, acc: 0.757812] [adversarial loss: 0.881260, acc: 0.375000]\n",
      "4383: [discriminator loss: 0.448610, acc: 0.804688] [adversarial loss: 1.772254, acc: 0.125000]\n",
      "4384: [discriminator loss: 0.536738, acc: 0.718750] [adversarial loss: 1.130229, acc: 0.281250]\n",
      "4385: [discriminator loss: 0.493100, acc: 0.781250] [adversarial loss: 1.621855, acc: 0.109375]\n",
      "4386: [discriminator loss: 0.503377, acc: 0.757812] [adversarial loss: 1.103115, acc: 0.265625]\n",
      "4387: [discriminator loss: 0.518253, acc: 0.765625] [adversarial loss: 1.434442, acc: 0.140625]\n",
      "4388: [discriminator loss: 0.476765, acc: 0.781250] [adversarial loss: 1.308783, acc: 0.218750]\n",
      "4389: [discriminator loss: 0.399611, acc: 0.859375] [adversarial loss: 1.440664, acc: 0.171875]\n",
      "4390: [discriminator loss: 0.579397, acc: 0.695312] [adversarial loss: 1.402500, acc: 0.171875]\n",
      "4391: [discriminator loss: 0.450623, acc: 0.804688] [adversarial loss: 1.174958, acc: 0.281250]\n",
      "4392: [discriminator loss: 0.547093, acc: 0.687500] [adversarial loss: 1.474090, acc: 0.171875]\n",
      "4393: [discriminator loss: 0.462950, acc: 0.781250] [adversarial loss: 1.397592, acc: 0.156250]\n",
      "4394: [discriminator loss: 0.499696, acc: 0.726562] [adversarial loss: 1.260845, acc: 0.265625]\n",
      "4395: [discriminator loss: 0.437889, acc: 0.828125] [adversarial loss: 1.433547, acc: 0.156250]\n",
      "4396: [discriminator loss: 0.575939, acc: 0.687500] [adversarial loss: 1.131212, acc: 0.296875]\n",
      "4397: [discriminator loss: 0.524950, acc: 0.726562] [adversarial loss: 1.258638, acc: 0.203125]\n",
      "4398: [discriminator loss: 0.619257, acc: 0.679688] [adversarial loss: 1.183342, acc: 0.234375]\n",
      "4399: [discriminator loss: 0.518234, acc: 0.726562] [adversarial loss: 1.235088, acc: 0.281250]\n",
      "4400: [discriminator loss: 0.577198, acc: 0.734375] [adversarial loss: 1.572839, acc: 0.093750]\n",
      "4401: [discriminator loss: 0.508212, acc: 0.742188] [adversarial loss: 1.080937, acc: 0.281250]\n",
      "4402: [discriminator loss: 0.571493, acc: 0.695312] [adversarial loss: 1.607496, acc: 0.093750]\n",
      "4403: [discriminator loss: 0.564215, acc: 0.695312] [adversarial loss: 1.078673, acc: 0.312500]\n",
      "4404: [discriminator loss: 0.511871, acc: 0.765625] [adversarial loss: 1.432961, acc: 0.187500]\n",
      "4405: [discriminator loss: 0.460884, acc: 0.781250] [adversarial loss: 1.376582, acc: 0.187500]\n",
      "4406: [discriminator loss: 0.481197, acc: 0.765625] [adversarial loss: 1.257545, acc: 0.187500]\n",
      "4407: [discriminator loss: 0.448984, acc: 0.843750] [adversarial loss: 1.337266, acc: 0.281250]\n",
      "4408: [discriminator loss: 0.512669, acc: 0.726562] [adversarial loss: 1.361518, acc: 0.250000]\n",
      "4409: [discriminator loss: 0.535082, acc: 0.703125] [adversarial loss: 0.983999, acc: 0.312500]\n",
      "4410: [discriminator loss: 0.520909, acc: 0.750000] [adversarial loss: 1.824259, acc: 0.140625]\n",
      "4411: [discriminator loss: 0.621608, acc: 0.664062] [adversarial loss: 0.973607, acc: 0.375000]\n",
      "4412: [discriminator loss: 0.543381, acc: 0.679688] [adversarial loss: 1.835994, acc: 0.156250]\n",
      "4413: [discriminator loss: 0.535461, acc: 0.734375] [adversarial loss: 1.091522, acc: 0.250000]\n",
      "4414: [discriminator loss: 0.503593, acc: 0.757812] [adversarial loss: 1.605177, acc: 0.203125]\n",
      "4415: [discriminator loss: 0.570678, acc: 0.679688] [adversarial loss: 1.224710, acc: 0.265625]\n",
      "4416: [discriminator loss: 0.503033, acc: 0.710938] [adversarial loss: 1.421058, acc: 0.109375]\n",
      "4417: [discriminator loss: 0.454616, acc: 0.781250] [adversarial loss: 1.098171, acc: 0.375000]\n",
      "4418: [discriminator loss: 0.579336, acc: 0.656250] [adversarial loss: 1.806157, acc: 0.031250]\n",
      "4419: [discriminator loss: 0.510282, acc: 0.773438] [adversarial loss: 1.114720, acc: 0.281250]\n",
      "4420: [discriminator loss: 0.575273, acc: 0.718750] [adversarial loss: 1.609337, acc: 0.062500]\n",
      "4421: [discriminator loss: 0.566105, acc: 0.703125] [adversarial loss: 1.092970, acc: 0.281250]\n",
      "4422: [discriminator loss: 0.444448, acc: 0.820312] [adversarial loss: 1.673729, acc: 0.093750]\n",
      "4423: [discriminator loss: 0.496424, acc: 0.757812] [adversarial loss: 1.226404, acc: 0.281250]\n",
      "4424: [discriminator loss: 0.546888, acc: 0.703125] [adversarial loss: 1.810369, acc: 0.140625]\n",
      "4425: [discriminator loss: 0.537228, acc: 0.703125] [adversarial loss: 1.192746, acc: 0.281250]\n",
      "4426: [discriminator loss: 0.509046, acc: 0.781250] [adversarial loss: 1.504717, acc: 0.125000]\n",
      "4427: [discriminator loss: 0.487973, acc: 0.734375] [adversarial loss: 1.126153, acc: 0.312500]\n",
      "4428: [discriminator loss: 0.552272, acc: 0.734375] [adversarial loss: 1.336459, acc: 0.171875]\n",
      "4429: [discriminator loss: 0.540715, acc: 0.765625] [adversarial loss: 1.314358, acc: 0.234375]\n",
      "4430: [discriminator loss: 0.488897, acc: 0.757812] [adversarial loss: 1.221604, acc: 0.265625]\n",
      "4431: [discriminator loss: 0.489006, acc: 0.781250] [adversarial loss: 1.438486, acc: 0.187500]\n",
      "4432: [discriminator loss: 0.491721, acc: 0.789062] [adversarial loss: 1.507922, acc: 0.140625]\n",
      "4433: [discriminator loss: 0.518369, acc: 0.742188] [adversarial loss: 1.123048, acc: 0.390625]\n",
      "4434: [discriminator loss: 0.562343, acc: 0.703125] [adversarial loss: 1.281528, acc: 0.234375]\n",
      "4435: [discriminator loss: 0.431762, acc: 0.796875] [adversarial loss: 1.421371, acc: 0.234375]\n",
      "4436: [discriminator loss: 0.560301, acc: 0.703125] [adversarial loss: 1.130960, acc: 0.265625]\n",
      "4437: [discriminator loss: 0.513894, acc: 0.765625] [adversarial loss: 1.825791, acc: 0.078125]\n",
      "4438: [discriminator loss: 0.514228, acc: 0.757812] [adversarial loss: 1.007903, acc: 0.281250]\n",
      "4439: [discriminator loss: 0.532291, acc: 0.750000] [adversarial loss: 1.866915, acc: 0.078125]\n",
      "4440: [discriminator loss: 0.511663, acc: 0.757812] [adversarial loss: 1.009362, acc: 0.312500]\n",
      "4441: [discriminator loss: 0.469459, acc: 0.781250] [adversarial loss: 1.686799, acc: 0.093750]\n",
      "4442: [discriminator loss: 0.440948, acc: 0.789062] [adversarial loss: 1.094087, acc: 0.390625]\n",
      "4443: [discriminator loss: 0.548550, acc: 0.742188] [adversarial loss: 1.623040, acc: 0.062500]\n",
      "4444: [discriminator loss: 0.464335, acc: 0.789062] [adversarial loss: 1.202797, acc: 0.296875]\n",
      "4445: [discriminator loss: 0.500402, acc: 0.750000] [adversarial loss: 1.869847, acc: 0.093750]\n",
      "4446: [discriminator loss: 0.559178, acc: 0.703125] [adversarial loss: 1.002917, acc: 0.390625]\n",
      "4447: [discriminator loss: 0.573985, acc: 0.734375] [adversarial loss: 1.318940, acc: 0.218750]\n",
      "4448: [discriminator loss: 0.537906, acc: 0.718750] [adversarial loss: 1.358695, acc: 0.171875]\n",
      "4449: [discriminator loss: 0.557877, acc: 0.703125] [adversarial loss: 1.290729, acc: 0.265625]\n",
      "4450: [discriminator loss: 0.489541, acc: 0.812500] [adversarial loss: 1.254932, acc: 0.250000]\n",
      "4451: [discriminator loss: 0.595939, acc: 0.664062] [adversarial loss: 1.533142, acc: 0.171875]\n",
      "4452: [discriminator loss: 0.467293, acc: 0.789062] [adversarial loss: 1.118352, acc: 0.328125]\n",
      "4453: [discriminator loss: 0.594723, acc: 0.625000] [adversarial loss: 1.546007, acc: 0.125000]\n",
      "4454: [discriminator loss: 0.459006, acc: 0.750000] [adversarial loss: 1.483265, acc: 0.171875]\n",
      "4455: [discriminator loss: 0.480127, acc: 0.781250] [adversarial loss: 1.125721, acc: 0.343750]\n",
      "4456: [discriminator loss: 0.555122, acc: 0.742188] [adversarial loss: 2.122393, acc: 0.046875]\n",
      "4457: [discriminator loss: 0.538678, acc: 0.710938] [adversarial loss: 1.105870, acc: 0.375000]\n",
      "4458: [discriminator loss: 0.587974, acc: 0.656250] [adversarial loss: 2.034296, acc: 0.046875]\n",
      "4459: [discriminator loss: 0.608414, acc: 0.679688] [adversarial loss: 0.832566, acc: 0.531250]\n",
      "4460: [discriminator loss: 0.507457, acc: 0.679688] [adversarial loss: 1.665759, acc: 0.078125]\n",
      "4461: [discriminator loss: 0.543506, acc: 0.757812] [adversarial loss: 0.972794, acc: 0.375000]\n",
      "4462: [discriminator loss: 0.573640, acc: 0.710938] [adversarial loss: 1.650801, acc: 0.109375]\n",
      "4463: [discriminator loss: 0.558381, acc: 0.726562] [adversarial loss: 1.207260, acc: 0.234375]\n",
      "4464: [discriminator loss: 0.560924, acc: 0.750000] [adversarial loss: 1.573619, acc: 0.062500]\n",
      "4465: [discriminator loss: 0.514575, acc: 0.726562] [adversarial loss: 1.176760, acc: 0.265625]\n",
      "4466: [discriminator loss: 0.482852, acc: 0.781250] [adversarial loss: 1.310273, acc: 0.281250]\n",
      "4467: [discriminator loss: 0.466728, acc: 0.820312] [adversarial loss: 1.136968, acc: 0.312500]\n",
      "4468: [discriminator loss: 0.512659, acc: 0.726562] [adversarial loss: 1.189809, acc: 0.265625]\n",
      "4469: [discriminator loss: 0.497805, acc: 0.781250] [adversarial loss: 1.697588, acc: 0.156250]\n",
      "4470: [discriminator loss: 0.514505, acc: 0.804688] [adversarial loss: 1.266980, acc: 0.234375]\n",
      "4471: [discriminator loss: 0.558021, acc: 0.734375] [adversarial loss: 1.250731, acc: 0.250000]\n",
      "4472: [discriminator loss: 0.547619, acc: 0.710938] [adversarial loss: 1.214402, acc: 0.234375]\n",
      "4473: [discriminator loss: 0.511084, acc: 0.726562] [adversarial loss: 1.123884, acc: 0.328125]\n",
      "4474: [discriminator loss: 0.492013, acc: 0.734375] [adversarial loss: 1.611319, acc: 0.125000]\n",
      "4475: [discriminator loss: 0.515144, acc: 0.750000] [adversarial loss: 1.122287, acc: 0.328125]\n",
      "4476: [discriminator loss: 0.525363, acc: 0.734375] [adversarial loss: 1.549180, acc: 0.078125]\n",
      "4477: [discriminator loss: 0.501506, acc: 0.726562] [adversarial loss: 1.229507, acc: 0.234375]\n",
      "4478: [discriminator loss: 0.509249, acc: 0.726562] [adversarial loss: 1.459972, acc: 0.093750]\n",
      "4479: [discriminator loss: 0.506111, acc: 0.789062] [adversarial loss: 1.035830, acc: 0.328125]\n",
      "4480: [discriminator loss: 0.543160, acc: 0.710938] [adversarial loss: 1.723224, acc: 0.140625]\n",
      "4481: [discriminator loss: 0.569207, acc: 0.726562] [adversarial loss: 0.961302, acc: 0.359375]\n",
      "4482: [discriminator loss: 0.509021, acc: 0.742188] [adversarial loss: 1.571209, acc: 0.062500]\n",
      "4483: [discriminator loss: 0.493368, acc: 0.765625] [adversarial loss: 0.939034, acc: 0.328125]\n",
      "4484: [discriminator loss: 0.491448, acc: 0.742188] [adversarial loss: 1.256193, acc: 0.250000]\n",
      "4485: [discriminator loss: 0.538785, acc: 0.703125] [adversarial loss: 0.877130, acc: 0.406250]\n",
      "4486: [discriminator loss: 0.538178, acc: 0.750000] [adversarial loss: 1.829760, acc: 0.078125]\n",
      "4487: [discriminator loss: 0.521424, acc: 0.718750] [adversarial loss: 0.913767, acc: 0.437500]\n",
      "4488: [discriminator loss: 0.557374, acc: 0.757812] [adversarial loss: 1.315300, acc: 0.171875]\n",
      "4489: [discriminator loss: 0.485859, acc: 0.765625] [adversarial loss: 1.239311, acc: 0.234375]\n",
      "4490: [discriminator loss: 0.572419, acc: 0.734375] [adversarial loss: 1.482033, acc: 0.187500]\n",
      "4491: [discriminator loss: 0.521866, acc: 0.742188] [adversarial loss: 1.441613, acc: 0.218750]\n",
      "4492: [discriminator loss: 0.522511, acc: 0.726562] [adversarial loss: 1.334940, acc: 0.250000]\n",
      "4493: [discriminator loss: 0.486818, acc: 0.820312] [adversarial loss: 1.275444, acc: 0.218750]\n",
      "4494: [discriminator loss: 0.508222, acc: 0.765625] [adversarial loss: 1.250426, acc: 0.250000]\n",
      "4495: [discriminator loss: 0.606137, acc: 0.710938] [adversarial loss: 1.331587, acc: 0.187500]\n",
      "4496: [discriminator loss: 0.472297, acc: 0.750000] [adversarial loss: 1.283061, acc: 0.281250]\n",
      "4497: [discriminator loss: 0.495110, acc: 0.757812] [adversarial loss: 1.622979, acc: 0.046875]\n",
      "4498: [discriminator loss: 0.460776, acc: 0.765625] [adversarial loss: 1.286885, acc: 0.265625]\n",
      "4499: [discriminator loss: 0.526154, acc: 0.757812] [adversarial loss: 1.504103, acc: 0.171875]\n",
      "4500: [discriminator loss: 0.510077, acc: 0.695312] [adversarial loss: 1.247437, acc: 0.250000]\n",
      "4501: [discriminator loss: 0.514149, acc: 0.742188] [adversarial loss: 1.296284, acc: 0.234375]\n",
      "4502: [discriminator loss: 0.484921, acc: 0.765625] [adversarial loss: 1.300055, acc: 0.312500]\n",
      "4503: [discriminator loss: 0.471815, acc: 0.765625] [adversarial loss: 1.369747, acc: 0.250000]\n",
      "4504: [discriminator loss: 0.524597, acc: 0.765625] [adversarial loss: 1.136506, acc: 0.312500]\n",
      "4505: [discriminator loss: 0.545983, acc: 0.718750] [adversarial loss: 1.269204, acc: 0.203125]\n",
      "4506: [discriminator loss: 0.463297, acc: 0.781250] [adversarial loss: 1.886240, acc: 0.093750]\n",
      "4507: [discriminator loss: 0.507560, acc: 0.718750] [adversarial loss: 0.969525, acc: 0.484375]\n",
      "4508: [discriminator loss: 0.577256, acc: 0.687500] [adversarial loss: 1.917392, acc: 0.062500]\n",
      "4509: [discriminator loss: 0.483971, acc: 0.750000] [adversarial loss: 1.003185, acc: 0.375000]\n",
      "4510: [discriminator loss: 0.543828, acc: 0.687500] [adversarial loss: 1.876649, acc: 0.156250]\n",
      "4511: [discriminator loss: 0.566463, acc: 0.718750] [adversarial loss: 1.051954, acc: 0.359375]\n",
      "4512: [discriminator loss: 0.532765, acc: 0.718750] [adversarial loss: 1.949386, acc: 0.015625]\n",
      "4513: [discriminator loss: 0.563859, acc: 0.734375] [adversarial loss: 1.099637, acc: 0.312500]\n",
      "4514: [discriminator loss: 0.491239, acc: 0.796875] [adversarial loss: 1.428203, acc: 0.187500]\n",
      "4515: [discriminator loss: 0.523466, acc: 0.750000] [adversarial loss: 1.060173, acc: 0.296875]\n",
      "4516: [discriminator loss: 0.480627, acc: 0.726562] [adversarial loss: 1.327588, acc: 0.234375]\n",
      "4517: [discriminator loss: 0.507299, acc: 0.757812] [adversarial loss: 1.602212, acc: 0.093750]\n",
      "4518: [discriminator loss: 0.530561, acc: 0.742188] [adversarial loss: 1.063023, acc: 0.343750]\n",
      "4519: [discriminator loss: 0.493461, acc: 0.773438] [adversarial loss: 1.179284, acc: 0.265625]\n",
      "4520: [discriminator loss: 0.543657, acc: 0.742188] [adversarial loss: 1.241347, acc: 0.109375]\n",
      "4521: [discriminator loss: 0.515058, acc: 0.757812] [adversarial loss: 1.332171, acc: 0.046875]\n",
      "4522: [discriminator loss: 0.450445, acc: 0.757812] [adversarial loss: 1.473707, acc: 0.218750]\n",
      "4523: [discriminator loss: 0.490723, acc: 0.750000] [adversarial loss: 1.148525, acc: 0.203125]\n",
      "4524: [discriminator loss: 0.446735, acc: 0.812500] [adversarial loss: 1.906090, acc: 0.109375]\n",
      "4525: [discriminator loss: 0.589706, acc: 0.718750] [adversarial loss: 1.108735, acc: 0.296875]\n",
      "4526: [discriminator loss: 0.648187, acc: 0.679688] [adversarial loss: 1.556754, acc: 0.109375]\n",
      "4527: [discriminator loss: 0.527020, acc: 0.765625] [adversarial loss: 0.875498, acc: 0.531250]\n",
      "4528: [discriminator loss: 0.522619, acc: 0.718750] [adversarial loss: 1.723136, acc: 0.093750]\n",
      "4529: [discriminator loss: 0.538612, acc: 0.703125] [adversarial loss: 0.926064, acc: 0.406250]\n",
      "4530: [discriminator loss: 0.509081, acc: 0.750000] [adversarial loss: 1.564881, acc: 0.171875]\n",
      "4531: [discriminator loss: 0.479263, acc: 0.742188] [adversarial loss: 1.235811, acc: 0.234375]\n",
      "4532: [discriminator loss: 0.490705, acc: 0.757812] [adversarial loss: 1.440603, acc: 0.125000]\n",
      "4533: [discriminator loss: 0.443350, acc: 0.789062] [adversarial loss: 1.508933, acc: 0.187500]\n",
      "4534: [discriminator loss: 0.458686, acc: 0.781250] [adversarial loss: 1.506137, acc: 0.140625]\n",
      "4535: [discriminator loss: 0.490728, acc: 0.789062] [adversarial loss: 1.292281, acc: 0.140625]\n",
      "4536: [discriminator loss: 0.481541, acc: 0.781250] [adversarial loss: 1.391504, acc: 0.171875]\n",
      "4537: [discriminator loss: 0.522135, acc: 0.781250] [adversarial loss: 1.731272, acc: 0.062500]\n",
      "4538: [discriminator loss: 0.595970, acc: 0.679688] [adversarial loss: 0.881006, acc: 0.484375]\n",
      "4539: [discriminator loss: 0.597856, acc: 0.679688] [adversarial loss: 1.837610, acc: 0.109375]\n",
      "4540: [discriminator loss: 0.510026, acc: 0.757812] [adversarial loss: 1.049387, acc: 0.421875]\n",
      "4541: [discriminator loss: 0.481293, acc: 0.781250] [adversarial loss: 1.691302, acc: 0.156250]\n",
      "4542: [discriminator loss: 0.564954, acc: 0.726562] [adversarial loss: 0.991516, acc: 0.406250]\n",
      "4543: [discriminator loss: 0.535586, acc: 0.757812] [adversarial loss: 1.290397, acc: 0.250000]\n",
      "4544: [discriminator loss: 0.526729, acc: 0.726562] [adversarial loss: 1.131540, acc: 0.312500]\n",
      "4545: [discriminator loss: 0.580651, acc: 0.718750] [adversarial loss: 1.412403, acc: 0.093750]\n",
      "4546: [discriminator loss: 0.558389, acc: 0.718750] [adversarial loss: 1.203120, acc: 0.281250]\n",
      "4547: [discriminator loss: 0.517798, acc: 0.726562] [adversarial loss: 1.331100, acc: 0.140625]\n",
      "4548: [discriminator loss: 0.573732, acc: 0.679688] [adversarial loss: 0.972003, acc: 0.375000]\n",
      "4549: [discriminator loss: 0.521117, acc: 0.742188] [adversarial loss: 1.909497, acc: 0.109375]\n",
      "4550: [discriminator loss: 0.703191, acc: 0.625000] [adversarial loss: 0.829120, acc: 0.500000]\n",
      "4551: [discriminator loss: 0.546696, acc: 0.703125] [adversarial loss: 1.441057, acc: 0.140625]\n",
      "4552: [discriminator loss: 0.577732, acc: 0.703125] [adversarial loss: 1.016276, acc: 0.406250]\n",
      "4553: [discriminator loss: 0.475455, acc: 0.773438] [adversarial loss: 1.427135, acc: 0.187500]\n",
      "4554: [discriminator loss: 0.552833, acc: 0.687500] [adversarial loss: 1.258827, acc: 0.218750]\n",
      "4555: [discriminator loss: 0.441609, acc: 0.765625] [adversarial loss: 1.162785, acc: 0.343750]\n",
      "4556: [discriminator loss: 0.552748, acc: 0.710938] [adversarial loss: 1.583159, acc: 0.125000]\n",
      "4557: [discriminator loss: 0.454938, acc: 0.804688] [adversarial loss: 1.315179, acc: 0.156250]\n",
      "4558: [discriminator loss: 0.545440, acc: 0.734375] [adversarial loss: 1.539381, acc: 0.093750]\n",
      "4559: [discriminator loss: 0.447387, acc: 0.742188] [adversarial loss: 1.045892, acc: 0.312500]\n",
      "4560: [discriminator loss: 0.471624, acc: 0.804688] [adversarial loss: 1.288373, acc: 0.156250]\n",
      "4561: [discriminator loss: 0.545546, acc: 0.757812] [adversarial loss: 1.280035, acc: 0.156250]\n",
      "4562: [discriminator loss: 0.533402, acc: 0.703125] [adversarial loss: 1.166722, acc: 0.265625]\n",
      "4563: [discriminator loss: 0.499214, acc: 0.796875] [adversarial loss: 1.411531, acc: 0.140625]\n",
      "4564: [discriminator loss: 0.532523, acc: 0.703125] [adversarial loss: 1.131615, acc: 0.265625]\n",
      "4565: [discriminator loss: 0.483845, acc: 0.765625] [adversarial loss: 1.505166, acc: 0.125000]\n",
      "4566: [discriminator loss: 0.522368, acc: 0.726562] [adversarial loss: 0.909948, acc: 0.406250]\n",
      "4567: [discriminator loss: 0.493416, acc: 0.750000] [adversarial loss: 1.278598, acc: 0.234375]\n",
      "4568: [discriminator loss: 0.544103, acc: 0.687500] [adversarial loss: 1.139375, acc: 0.265625]\n",
      "4569: [discriminator loss: 0.549822, acc: 0.703125] [adversarial loss: 1.610164, acc: 0.140625]\n",
      "4570: [discriminator loss: 0.527164, acc: 0.742188] [adversarial loss: 0.910957, acc: 0.421875]\n",
      "4571: [discriminator loss: 0.573636, acc: 0.710938] [adversarial loss: 1.742294, acc: 0.109375]\n",
      "4572: [discriminator loss: 0.566825, acc: 0.718750] [adversarial loss: 1.097419, acc: 0.343750]\n",
      "4573: [discriminator loss: 0.510099, acc: 0.734375] [adversarial loss: 1.353674, acc: 0.125000]\n",
      "4574: [discriminator loss: 0.454987, acc: 0.789062] [adversarial loss: 1.162619, acc: 0.250000]\n",
      "4575: [discriminator loss: 0.511855, acc: 0.742188] [adversarial loss: 1.301567, acc: 0.140625]\n",
      "4576: [discriminator loss: 0.539212, acc: 0.750000] [adversarial loss: 1.128941, acc: 0.312500]\n",
      "4577: [discriminator loss: 0.510366, acc: 0.726562] [adversarial loss: 1.583853, acc: 0.140625]\n",
      "4578: [discriminator loss: 0.500625, acc: 0.781250] [adversarial loss: 1.237617, acc: 0.265625]\n",
      "4579: [discriminator loss: 0.447023, acc: 0.789062] [adversarial loss: 1.365834, acc: 0.187500]\n",
      "4580: [discriminator loss: 0.590058, acc: 0.710938] [adversarial loss: 0.828496, acc: 0.406250]\n",
      "4581: [discriminator loss: 0.499863, acc: 0.671875] [adversarial loss: 1.635861, acc: 0.125000]\n",
      "4582: [discriminator loss: 0.548979, acc: 0.703125] [adversarial loss: 1.089506, acc: 0.296875]\n",
      "4583: [discriminator loss: 0.442989, acc: 0.812500] [adversarial loss: 1.661052, acc: 0.171875]\n",
      "4584: [discriminator loss: 0.532453, acc: 0.734375] [adversarial loss: 0.827412, acc: 0.437500]\n",
      "4585: [discriminator loss: 0.535898, acc: 0.710938] [adversarial loss: 1.476780, acc: 0.203125]\n",
      "4586: [discriminator loss: 0.543783, acc: 0.757812] [adversarial loss: 0.885344, acc: 0.406250]\n",
      "4587: [discriminator loss: 0.457487, acc: 0.828125] [adversarial loss: 1.655764, acc: 0.125000]\n",
      "4588: [discriminator loss: 0.545944, acc: 0.734375] [adversarial loss: 0.949649, acc: 0.390625]\n",
      "4589: [discriminator loss: 0.473814, acc: 0.781250] [adversarial loss: 1.521242, acc: 0.109375]\n",
      "4590: [discriminator loss: 0.501797, acc: 0.804688] [adversarial loss: 1.189404, acc: 0.250000]\n",
      "4591: [discriminator loss: 0.556898, acc: 0.726562] [adversarial loss: 1.309270, acc: 0.203125]\n",
      "4592: [discriminator loss: 0.538566, acc: 0.750000] [adversarial loss: 0.853292, acc: 0.484375]\n",
      "4593: [discriminator loss: 0.501711, acc: 0.773438] [adversarial loss: 1.656736, acc: 0.062500]\n",
      "4594: [discriminator loss: 0.533529, acc: 0.734375] [adversarial loss: 1.237755, acc: 0.203125]\n",
      "4595: [discriminator loss: 0.608808, acc: 0.695312] [adversarial loss: 1.390898, acc: 0.171875]\n",
      "4596: [discriminator loss: 0.543031, acc: 0.726562] [adversarial loss: 0.907843, acc: 0.390625]\n",
      "4597: [discriminator loss: 0.590887, acc: 0.710938] [adversarial loss: 1.692259, acc: 0.093750]\n",
      "4598: [discriminator loss: 0.495720, acc: 0.726562] [adversarial loss: 1.287489, acc: 0.156250]\n",
      "4599: [discriminator loss: 0.473724, acc: 0.789062] [adversarial loss: 1.555427, acc: 0.187500]\n",
      "4600: [discriminator loss: 0.521074, acc: 0.742188] [adversarial loss: 1.096168, acc: 0.296875]\n",
      "4601: [discriminator loss: 0.596117, acc: 0.710938] [adversarial loss: 1.504152, acc: 0.156250]\n",
      "4602: [discriminator loss: 0.479180, acc: 0.781250] [adversarial loss: 0.944531, acc: 0.359375]\n",
      "4603: [discriminator loss: 0.649870, acc: 0.656250] [adversarial loss: 1.869338, acc: 0.078125]\n",
      "4604: [discriminator loss: 0.528698, acc: 0.718750] [adversarial loss: 0.979947, acc: 0.359375]\n",
      "4605: [discriminator loss: 0.461215, acc: 0.781250] [adversarial loss: 1.343891, acc: 0.187500]\n",
      "4606: [discriminator loss: 0.460931, acc: 0.812500] [adversarial loss: 1.149413, acc: 0.281250]\n",
      "4607: [discriminator loss: 0.497748, acc: 0.773438] [adversarial loss: 1.407331, acc: 0.156250]\n",
      "4608: [discriminator loss: 0.454970, acc: 0.789062] [adversarial loss: 1.276903, acc: 0.187500]\n",
      "4609: [discriminator loss: 0.501065, acc: 0.718750] [adversarial loss: 1.578816, acc: 0.156250]\n",
      "4610: [discriminator loss: 0.553857, acc: 0.710938] [adversarial loss: 1.269360, acc: 0.250000]\n",
      "4611: [discriminator loss: 0.549779, acc: 0.757812] [adversarial loss: 1.215411, acc: 0.203125]\n",
      "4612: [discriminator loss: 0.490535, acc: 0.734375] [adversarial loss: 1.488235, acc: 0.156250]\n",
      "4613: [discriminator loss: 0.492399, acc: 0.773438] [adversarial loss: 1.198686, acc: 0.234375]\n",
      "4614: [discriminator loss: 0.528985, acc: 0.734375] [adversarial loss: 1.846578, acc: 0.093750]\n",
      "4615: [discriminator loss: 0.613096, acc: 0.695312] [adversarial loss: 0.930278, acc: 0.390625]\n",
      "4616: [discriminator loss: 0.520579, acc: 0.734375] [adversarial loss: 2.106352, acc: 0.078125]\n",
      "4617: [discriminator loss: 0.549290, acc: 0.726562] [adversarial loss: 0.910913, acc: 0.406250]\n",
      "4618: [discriminator loss: 0.486696, acc: 0.765625] [adversarial loss: 1.710247, acc: 0.109375]\n",
      "4619: [discriminator loss: 0.601432, acc: 0.687500] [adversarial loss: 0.910446, acc: 0.375000]\n",
      "4620: [discriminator loss: 0.544819, acc: 0.664062] [adversarial loss: 1.681695, acc: 0.156250]\n",
      "4621: [discriminator loss: 0.498211, acc: 0.765625] [adversarial loss: 1.020859, acc: 0.328125]\n",
      "4622: [discriminator loss: 0.493241, acc: 0.765625] [adversarial loss: 1.627328, acc: 0.078125]\n",
      "4623: [discriminator loss: 0.606394, acc: 0.664062] [adversarial loss: 1.015850, acc: 0.312500]\n",
      "4624: [discriminator loss: 0.562889, acc: 0.710938] [adversarial loss: 1.496808, acc: 0.171875]\n",
      "4625: [discriminator loss: 0.454658, acc: 0.765625] [adversarial loss: 1.442895, acc: 0.140625]\n",
      "4626: [discriminator loss: 0.484396, acc: 0.781250] [adversarial loss: 1.144667, acc: 0.234375]\n",
      "4627: [discriminator loss: 0.504493, acc: 0.757812] [adversarial loss: 1.693242, acc: 0.078125]\n",
      "4628: [discriminator loss: 0.488734, acc: 0.742188] [adversarial loss: 1.222909, acc: 0.250000]\n",
      "4629: [discriminator loss: 0.448654, acc: 0.765625] [adversarial loss: 1.162467, acc: 0.265625]\n",
      "4630: [discriminator loss: 0.491286, acc: 0.750000] [adversarial loss: 1.418249, acc: 0.328125]\n",
      "4631: [discriminator loss: 0.624047, acc: 0.632812] [adversarial loss: 1.383225, acc: 0.171875]\n",
      "4632: [discriminator loss: 0.465679, acc: 0.781250] [adversarial loss: 1.155257, acc: 0.265625]\n",
      "4633: [discriminator loss: 0.490392, acc: 0.734375] [adversarial loss: 1.159489, acc: 0.328125]\n",
      "4634: [discriminator loss: 0.509279, acc: 0.710938] [adversarial loss: 1.645128, acc: 0.046875]\n",
      "4635: [discriminator loss: 0.490407, acc: 0.765625] [adversarial loss: 1.117395, acc: 0.359375]\n",
      "4636: [discriminator loss: 0.474774, acc: 0.796875] [adversarial loss: 1.644791, acc: 0.156250]\n",
      "4637: [discriminator loss: 0.531051, acc: 0.781250] [adversarial loss: 1.186832, acc: 0.281250]\n",
      "4638: [discriminator loss: 0.563991, acc: 0.718750] [adversarial loss: 1.652979, acc: 0.109375]\n",
      "4639: [discriminator loss: 0.499881, acc: 0.773438] [adversarial loss: 1.082709, acc: 0.265625]\n",
      "4640: [discriminator loss: 0.540665, acc: 0.734375] [adversarial loss: 1.646194, acc: 0.109375]\n",
      "4641: [discriminator loss: 0.547041, acc: 0.726562] [adversarial loss: 0.905270, acc: 0.406250]\n",
      "4642: [discriminator loss: 0.611939, acc: 0.656250] [adversarial loss: 1.914894, acc: 0.093750]\n",
      "4643: [discriminator loss: 0.584664, acc: 0.703125] [adversarial loss: 1.000553, acc: 0.484375]\n",
      "4644: [discriminator loss: 0.496598, acc: 0.757812] [adversarial loss: 1.156614, acc: 0.265625]\n",
      "4645: [discriminator loss: 0.488787, acc: 0.742188] [adversarial loss: 1.600211, acc: 0.140625]\n",
      "4646: [discriminator loss: 0.512905, acc: 0.757812] [adversarial loss: 1.477084, acc: 0.218750]\n",
      "4647: [discriminator loss: 0.483815, acc: 0.773438] [adversarial loss: 1.643419, acc: 0.125000]\n",
      "4648: [discriminator loss: 0.588324, acc: 0.695312] [adversarial loss: 0.971672, acc: 0.390625]\n",
      "4649: [discriminator loss: 0.576428, acc: 0.718750] [adversarial loss: 1.816263, acc: 0.078125]\n",
      "4650: [discriminator loss: 0.529761, acc: 0.734375] [adversarial loss: 0.856626, acc: 0.484375]\n",
      "4651: [discriminator loss: 0.563746, acc: 0.687500] [adversarial loss: 1.588393, acc: 0.109375]\n",
      "4652: [discriminator loss: 0.556387, acc: 0.687500] [adversarial loss: 1.057668, acc: 0.343750]\n",
      "4653: [discriminator loss: 0.508495, acc: 0.781250] [adversarial loss: 1.726919, acc: 0.125000]\n",
      "4654: [discriminator loss: 0.516114, acc: 0.765625] [adversarial loss: 0.900784, acc: 0.453125]\n",
      "4655: [discriminator loss: 0.516200, acc: 0.773438] [adversarial loss: 1.525576, acc: 0.171875]\n",
      "4656: [discriminator loss: 0.455942, acc: 0.781250] [adversarial loss: 1.105839, acc: 0.203125]\n",
      "4657: [discriminator loss: 0.517518, acc: 0.734375] [adversarial loss: 1.371048, acc: 0.140625]\n",
      "4658: [discriminator loss: 0.528678, acc: 0.773438] [adversarial loss: 1.288007, acc: 0.187500]\n",
      "4659: [discriminator loss: 0.540061, acc: 0.750000] [adversarial loss: 1.271579, acc: 0.250000]\n",
      "4660: [discriminator loss: 0.504058, acc: 0.757812] [adversarial loss: 1.746163, acc: 0.125000]\n",
      "4661: [discriminator loss: 0.606613, acc: 0.742188] [adversarial loss: 0.760164, acc: 0.500000]\n",
      "4662: [discriminator loss: 0.609751, acc: 0.718750] [adversarial loss: 1.597579, acc: 0.062500]\n",
      "4663: [discriminator loss: 0.492156, acc: 0.773438] [adversarial loss: 1.016568, acc: 0.375000]\n",
      "4664: [discriminator loss: 0.563704, acc: 0.710938] [adversarial loss: 1.242438, acc: 0.250000]\n",
      "4665: [discriminator loss: 0.435365, acc: 0.804688] [adversarial loss: 1.242286, acc: 0.203125]\n",
      "4666: [discriminator loss: 0.480632, acc: 0.789062] [adversarial loss: 1.407343, acc: 0.218750]\n",
      "4667: [discriminator loss: 0.580033, acc: 0.664062] [adversarial loss: 1.322493, acc: 0.171875]\n",
      "4668: [discriminator loss: 0.600351, acc: 0.656250] [adversarial loss: 1.099889, acc: 0.265625]\n",
      "4669: [discriminator loss: 0.534531, acc: 0.773438] [adversarial loss: 1.237953, acc: 0.234375]\n",
      "4670: [discriminator loss: 0.462619, acc: 0.765625] [adversarial loss: 1.800064, acc: 0.109375]\n",
      "4671: [discriminator loss: 0.553480, acc: 0.718750] [adversarial loss: 1.131758, acc: 0.250000]\n",
      "4672: [discriminator loss: 0.481629, acc: 0.781250] [adversarial loss: 1.557427, acc: 0.171875]\n",
      "4673: [discriminator loss: 0.534105, acc: 0.695312] [adversarial loss: 1.075411, acc: 0.359375]\n",
      "4674: [discriminator loss: 0.496406, acc: 0.835938] [adversarial loss: 1.171443, acc: 0.281250]\n",
      "4675: [discriminator loss: 0.590352, acc: 0.726562] [adversarial loss: 1.455375, acc: 0.156250]\n",
      "4676: [discriminator loss: 0.491350, acc: 0.773438] [adversarial loss: 0.998738, acc: 0.406250]\n",
      "4677: [discriminator loss: 0.499680, acc: 0.757812] [adversarial loss: 1.478943, acc: 0.156250]\n",
      "4678: [discriminator loss: 0.538390, acc: 0.742188] [adversarial loss: 0.960142, acc: 0.343750]\n",
      "4679: [discriminator loss: 0.484440, acc: 0.781250] [adversarial loss: 1.756485, acc: 0.109375]\n",
      "4680: [discriminator loss: 0.594134, acc: 0.695312] [adversarial loss: 0.949958, acc: 0.406250]\n",
      "4681: [discriminator loss: 0.442437, acc: 0.828125] [adversarial loss: 1.636182, acc: 0.109375]\n",
      "4682: [discriminator loss: 0.558134, acc: 0.718750] [adversarial loss: 1.110281, acc: 0.375000]\n",
      "4683: [discriminator loss: 0.472167, acc: 0.726562] [adversarial loss: 1.494627, acc: 0.218750]\n",
      "4684: [discriminator loss: 0.543413, acc: 0.710938] [adversarial loss: 0.831770, acc: 0.390625]\n",
      "4685: [discriminator loss: 0.570500, acc: 0.734375] [adversarial loss: 1.599692, acc: 0.109375]\n",
      "4686: [discriminator loss: 0.528499, acc: 0.726562] [adversarial loss: 0.896252, acc: 0.390625]\n",
      "4687: [discriminator loss: 0.588123, acc: 0.664062] [adversarial loss: 1.463436, acc: 0.187500]\n",
      "4688: [discriminator loss: 0.534608, acc: 0.742188] [adversarial loss: 0.935781, acc: 0.421875]\n",
      "4689: [discriminator loss: 0.537769, acc: 0.718750] [adversarial loss: 1.617615, acc: 0.125000]\n",
      "4690: [discriminator loss: 0.482080, acc: 0.765625] [adversarial loss: 1.219683, acc: 0.250000]\n",
      "4691: [discriminator loss: 0.527767, acc: 0.742188] [adversarial loss: 1.331998, acc: 0.171875]\n",
      "4692: [discriminator loss: 0.506184, acc: 0.734375] [adversarial loss: 1.226247, acc: 0.328125]\n",
      "4693: [discriminator loss: 0.512623, acc: 0.812500] [adversarial loss: 1.769036, acc: 0.062500]\n",
      "4694: [discriminator loss: 0.534514, acc: 0.687500] [adversarial loss: 0.919628, acc: 0.296875]\n",
      "4695: [discriminator loss: 0.544622, acc: 0.742188] [adversarial loss: 1.621963, acc: 0.046875]\n",
      "4696: [discriminator loss: 0.507952, acc: 0.789062] [adversarial loss: 1.063984, acc: 0.328125]\n",
      "4697: [discriminator loss: 0.522892, acc: 0.750000] [adversarial loss: 1.865496, acc: 0.062500]\n",
      "4698: [discriminator loss: 0.480953, acc: 0.789062] [adversarial loss: 1.038901, acc: 0.359375]\n",
      "4699: [discriminator loss: 0.565492, acc: 0.710938] [adversarial loss: 1.271425, acc: 0.156250]\n",
      "4700: [discriminator loss: 0.463368, acc: 0.781250] [adversarial loss: 1.235974, acc: 0.281250]\n",
      "4701: [discriminator loss: 0.539523, acc: 0.695312] [adversarial loss: 1.099425, acc: 0.328125]\n",
      "4702: [discriminator loss: 0.527756, acc: 0.742188] [adversarial loss: 1.363580, acc: 0.156250]\n",
      "4703: [discriminator loss: 0.463306, acc: 0.757812] [adversarial loss: 1.121999, acc: 0.234375]\n",
      "4704: [discriminator loss: 0.411063, acc: 0.820312] [adversarial loss: 1.708198, acc: 0.093750]\n",
      "4705: [discriminator loss: 0.500356, acc: 0.734375] [adversarial loss: 0.933050, acc: 0.343750]\n",
      "4706: [discriminator loss: 0.601059, acc: 0.664062] [adversarial loss: 1.450263, acc: 0.187500]\n",
      "4707: [discriminator loss: 0.531532, acc: 0.750000] [adversarial loss: 1.202946, acc: 0.250000]\n",
      "4708: [discriminator loss: 0.560314, acc: 0.656250] [adversarial loss: 1.235510, acc: 0.218750]\n",
      "4709: [discriminator loss: 0.495757, acc: 0.750000] [adversarial loss: 0.974108, acc: 0.359375]\n",
      "4710: [discriminator loss: 0.488587, acc: 0.789062] [adversarial loss: 1.797515, acc: 0.093750]\n",
      "4711: [discriminator loss: 0.558640, acc: 0.710938] [adversarial loss: 0.995357, acc: 0.296875]\n",
      "4712: [discriminator loss: 0.506661, acc: 0.789062] [adversarial loss: 1.791352, acc: 0.046875]\n",
      "4713: [discriminator loss: 0.575342, acc: 0.710938] [adversarial loss: 0.852456, acc: 0.437500]\n",
      "4714: [discriminator loss: 0.599171, acc: 0.640625] [adversarial loss: 1.580081, acc: 0.125000]\n",
      "4715: [discriminator loss: 0.553851, acc: 0.742188] [adversarial loss: 1.049931, acc: 0.296875]\n",
      "4716: [discriminator loss: 0.495744, acc: 0.750000] [adversarial loss: 1.361010, acc: 0.187500]\n",
      "4717: [discriminator loss: 0.553529, acc: 0.695312] [adversarial loss: 1.032229, acc: 0.343750]\n",
      "4718: [discriminator loss: 0.520231, acc: 0.750000] [adversarial loss: 1.532724, acc: 0.125000]\n",
      "4719: [discriminator loss: 0.484313, acc: 0.742188] [adversarial loss: 0.921578, acc: 0.390625]\n",
      "4720: [discriminator loss: 0.504106, acc: 0.718750] [adversarial loss: 1.425262, acc: 0.187500]\n",
      "4721: [discriminator loss: 0.466845, acc: 0.789062] [adversarial loss: 1.215091, acc: 0.296875]\n",
      "4722: [discriminator loss: 0.573735, acc: 0.687500] [adversarial loss: 1.488354, acc: 0.171875]\n",
      "4723: [discriminator loss: 0.462105, acc: 0.796875] [adversarial loss: 1.392537, acc: 0.109375]\n",
      "4724: [discriminator loss: 0.544204, acc: 0.726562] [adversarial loss: 1.276900, acc: 0.218750]\n",
      "4725: [discriminator loss: 0.469949, acc: 0.773438] [adversarial loss: 1.081830, acc: 0.281250]\n",
      "4726: [discriminator loss: 0.488854, acc: 0.726562] [adversarial loss: 1.652268, acc: 0.078125]\n",
      "4727: [discriminator loss: 0.497798, acc: 0.757812] [adversarial loss: 1.020813, acc: 0.328125]\n",
      "4728: [discriminator loss: 0.505818, acc: 0.781250] [adversarial loss: 1.634784, acc: 0.140625]\n",
      "4729: [discriminator loss: 0.578223, acc: 0.679688] [adversarial loss: 1.092300, acc: 0.312500]\n",
      "4730: [discriminator loss: 0.531522, acc: 0.710938] [adversarial loss: 0.927351, acc: 0.375000]\n",
      "4731: [discriminator loss: 0.534282, acc: 0.750000] [adversarial loss: 1.709832, acc: 0.078125]\n",
      "4732: [discriminator loss: 0.484312, acc: 0.757812] [adversarial loss: 1.115729, acc: 0.296875]\n",
      "4733: [discriminator loss: 0.439850, acc: 0.765625] [adversarial loss: 1.485623, acc: 0.187500]\n",
      "4734: [discriminator loss: 0.554380, acc: 0.726562] [adversarial loss: 1.168388, acc: 0.218750]\n",
      "4735: [discriminator loss: 0.513536, acc: 0.703125] [adversarial loss: 1.576887, acc: 0.125000]\n",
      "4736: [discriminator loss: 0.470207, acc: 0.796875] [adversarial loss: 1.325942, acc: 0.171875]\n",
      "4737: [discriminator loss: 0.574216, acc: 0.679688] [adversarial loss: 1.263185, acc: 0.156250]\n",
      "4738: [discriminator loss: 0.542758, acc: 0.734375] [adversarial loss: 1.013437, acc: 0.328125]\n",
      "4739: [discriminator loss: 0.512359, acc: 0.750000] [adversarial loss: 1.521821, acc: 0.078125]\n",
      "4740: [discriminator loss: 0.582945, acc: 0.695312] [adversarial loss: 0.856078, acc: 0.468750]\n",
      "4741: [discriminator loss: 0.589868, acc: 0.695312] [adversarial loss: 1.778264, acc: 0.078125]\n",
      "4742: [discriminator loss: 0.571180, acc: 0.703125] [adversarial loss: 0.828494, acc: 0.421875]\n",
      "4743: [discriminator loss: 0.460723, acc: 0.812500] [adversarial loss: 1.717959, acc: 0.093750]\n",
      "4744: [discriminator loss: 0.555075, acc: 0.765625] [adversarial loss: 1.076066, acc: 0.281250]\n",
      "4745: [discriminator loss: 0.529925, acc: 0.726562] [adversarial loss: 1.334664, acc: 0.203125]\n",
      "4746: [discriminator loss: 0.570437, acc: 0.695312] [adversarial loss: 1.149195, acc: 0.187500]\n",
      "4747: [discriminator loss: 0.496940, acc: 0.781250] [adversarial loss: 1.613871, acc: 0.109375]\n",
      "4748: [discriminator loss: 0.521283, acc: 0.695312] [adversarial loss: 1.099845, acc: 0.250000]\n",
      "4749: [discriminator loss: 0.578014, acc: 0.687500] [adversarial loss: 1.776204, acc: 0.046875]\n",
      "4750: [discriminator loss: 0.553010, acc: 0.718750] [adversarial loss: 0.815092, acc: 0.500000]\n",
      "4751: [discriminator loss: 0.618603, acc: 0.648438] [adversarial loss: 1.371886, acc: 0.125000]\n",
      "4752: [discriminator loss: 0.515872, acc: 0.718750] [adversarial loss: 0.899976, acc: 0.437500]\n",
      "4753: [discriminator loss: 0.513955, acc: 0.734375] [adversarial loss: 1.185622, acc: 0.187500]\n",
      "4754: [discriminator loss: 0.516924, acc: 0.734375] [adversarial loss: 1.104147, acc: 0.312500]\n",
      "4755: [discriminator loss: 0.546356, acc: 0.703125] [adversarial loss: 1.543024, acc: 0.109375]\n",
      "4756: [discriminator loss: 0.509836, acc: 0.718750] [adversarial loss: 0.974459, acc: 0.406250]\n",
      "4757: [discriminator loss: 0.531017, acc: 0.742188] [adversarial loss: 1.347797, acc: 0.218750]\n",
      "4758: [discriminator loss: 0.532737, acc: 0.734375] [adversarial loss: 1.167481, acc: 0.359375]\n",
      "4759: [discriminator loss: 0.418358, acc: 0.820312] [adversarial loss: 1.276298, acc: 0.218750]\n",
      "4760: [discriminator loss: 0.476796, acc: 0.804688] [adversarial loss: 1.563154, acc: 0.093750]\n",
      "4761: [discriminator loss: 0.649605, acc: 0.648438] [adversarial loss: 1.009272, acc: 0.328125]\n",
      "4762: [discriminator loss: 0.543019, acc: 0.718750] [adversarial loss: 1.952994, acc: 0.109375]\n",
      "4763: [discriminator loss: 0.536712, acc: 0.664062] [adversarial loss: 0.923175, acc: 0.375000]\n",
      "4764: [discriminator loss: 0.463789, acc: 0.828125] [adversarial loss: 1.427659, acc: 0.125000]\n",
      "4765: [discriminator loss: 0.548860, acc: 0.734375] [adversarial loss: 1.242271, acc: 0.250000]\n",
      "4766: [discriminator loss: 0.536529, acc: 0.765625] [adversarial loss: 1.165331, acc: 0.343750]\n",
      "4767: [discriminator loss: 0.412001, acc: 0.812500] [adversarial loss: 1.287571, acc: 0.218750]\n",
      "4768: [discriminator loss: 0.485681, acc: 0.796875] [adversarial loss: 1.031601, acc: 0.328125]\n",
      "4769: [discriminator loss: 0.562418, acc: 0.695312] [adversarial loss: 1.349566, acc: 0.265625]\n",
      "4770: [discriminator loss: 0.561424, acc: 0.726562] [adversarial loss: 1.053343, acc: 0.406250]\n",
      "4771: [discriminator loss: 0.433949, acc: 0.828125] [adversarial loss: 1.636752, acc: 0.078125]\n",
      "4772: [discriminator loss: 0.480236, acc: 0.750000] [adversarial loss: 1.379148, acc: 0.171875]\n",
      "4773: [discriminator loss: 0.510384, acc: 0.750000] [adversarial loss: 1.100535, acc: 0.265625]\n",
      "4774: [discriminator loss: 0.467377, acc: 0.750000] [adversarial loss: 1.518938, acc: 0.093750]\n",
      "4775: [discriminator loss: 0.611762, acc: 0.632812] [adversarial loss: 1.244906, acc: 0.203125]\n",
      "4776: [discriminator loss: 0.509628, acc: 0.710938] [adversarial loss: 1.444676, acc: 0.109375]\n",
      "4777: [discriminator loss: 0.539198, acc: 0.734375] [adversarial loss: 0.736521, acc: 0.531250]\n",
      "4778: [discriminator loss: 0.493580, acc: 0.726562] [adversarial loss: 1.608020, acc: 0.125000]\n",
      "4779: [discriminator loss: 0.504450, acc: 0.687500] [adversarial loss: 1.197745, acc: 0.296875]\n",
      "4780: [discriminator loss: 0.556327, acc: 0.718750] [adversarial loss: 1.190691, acc: 0.265625]\n",
      "4781: [discriminator loss: 0.493504, acc: 0.757812] [adversarial loss: 1.490317, acc: 0.093750]\n",
      "4782: [discriminator loss: 0.452064, acc: 0.781250] [adversarial loss: 1.176973, acc: 0.218750]\n",
      "4783: [discriminator loss: 0.481759, acc: 0.734375] [adversarial loss: 1.309241, acc: 0.218750]\n",
      "4784: [discriminator loss: 0.476215, acc: 0.750000] [adversarial loss: 1.269968, acc: 0.250000]\n",
      "4785: [discriminator loss: 0.576803, acc: 0.687500] [adversarial loss: 1.458356, acc: 0.156250]\n",
      "4786: [discriminator loss: 0.558432, acc: 0.687500] [adversarial loss: 1.016303, acc: 0.328125]\n",
      "4787: [discriminator loss: 0.512021, acc: 0.703125] [adversarial loss: 1.587949, acc: 0.187500]\n",
      "4788: [discriminator loss: 0.546686, acc: 0.750000] [adversarial loss: 1.133890, acc: 0.265625]\n",
      "4789: [discriminator loss: 0.486552, acc: 0.750000] [adversarial loss: 1.145290, acc: 0.250000]\n",
      "4790: [discriminator loss: 0.458132, acc: 0.765625] [adversarial loss: 1.784440, acc: 0.078125]\n",
      "4791: [discriminator loss: 0.545691, acc: 0.710938] [adversarial loss: 0.884578, acc: 0.453125]\n",
      "4792: [discriminator loss: 0.541492, acc: 0.703125] [adversarial loss: 1.873774, acc: 0.078125]\n",
      "4793: [discriminator loss: 0.558487, acc: 0.679688] [adversarial loss: 0.801004, acc: 0.484375]\n",
      "4794: [discriminator loss: 0.522051, acc: 0.726562] [adversarial loss: 1.812991, acc: 0.078125]\n",
      "4795: [discriminator loss: 0.641142, acc: 0.718750] [adversarial loss: 0.978304, acc: 0.343750]\n",
      "4796: [discriminator loss: 0.573190, acc: 0.671875] [adversarial loss: 1.541396, acc: 0.156250]\n",
      "4797: [discriminator loss: 0.527972, acc: 0.750000] [adversarial loss: 1.218263, acc: 0.203125]\n",
      "4798: [discriminator loss: 0.519088, acc: 0.718750] [adversarial loss: 1.237505, acc: 0.234375]\n",
      "4799: [discriminator loss: 0.533210, acc: 0.765625] [adversarial loss: 1.060107, acc: 0.328125]\n",
      "4800: [discriminator loss: 0.524541, acc: 0.773438] [adversarial loss: 1.516858, acc: 0.125000]\n",
      "4801: [discriminator loss: 0.523285, acc: 0.742188] [adversarial loss: 1.203298, acc: 0.234375]\n",
      "4802: [discriminator loss: 0.462660, acc: 0.796875] [adversarial loss: 1.417673, acc: 0.156250]\n",
      "4803: [discriminator loss: 0.523977, acc: 0.750000] [adversarial loss: 1.263842, acc: 0.203125]\n",
      "4804: [discriminator loss: 0.482661, acc: 0.726562] [adversarial loss: 1.227355, acc: 0.250000]\n",
      "4805: [discriminator loss: 0.556327, acc: 0.710938] [adversarial loss: 1.615064, acc: 0.093750]\n",
      "4806: [discriminator loss: 0.539729, acc: 0.710938] [adversarial loss: 1.046923, acc: 0.328125]\n",
      "4807: [discriminator loss: 0.535809, acc: 0.710938] [adversarial loss: 1.704916, acc: 0.109375]\n",
      "4808: [discriminator loss: 0.537614, acc: 0.703125] [adversarial loss: 1.050132, acc: 0.312500]\n",
      "4809: [discriminator loss: 0.593781, acc: 0.656250] [adversarial loss: 1.571166, acc: 0.156250]\n",
      "4810: [discriminator loss: 0.519367, acc: 0.718750] [adversarial loss: 1.326361, acc: 0.156250]\n",
      "4811: [discriminator loss: 0.525503, acc: 0.710938] [adversarial loss: 1.254996, acc: 0.171875]\n",
      "4812: [discriminator loss: 0.453151, acc: 0.781250] [adversarial loss: 1.126262, acc: 0.187500]\n",
      "4813: [discriminator loss: 0.489786, acc: 0.734375] [adversarial loss: 1.504298, acc: 0.171875]\n",
      "4814: [discriminator loss: 0.451342, acc: 0.773438] [adversarial loss: 1.012594, acc: 0.281250]\n",
      "4815: [discriminator loss: 0.464601, acc: 0.781250] [adversarial loss: 1.358941, acc: 0.171875]\n",
      "4816: [discriminator loss: 0.572119, acc: 0.718750] [adversarial loss: 1.193849, acc: 0.265625]\n",
      "4817: [discriminator loss: 0.494403, acc: 0.812500] [adversarial loss: 1.542736, acc: 0.125000]\n",
      "4818: [discriminator loss: 0.498814, acc: 0.757812] [adversarial loss: 1.198149, acc: 0.250000]\n",
      "4819: [discriminator loss: 0.518851, acc: 0.734375] [adversarial loss: 1.493094, acc: 0.109375]\n",
      "4820: [discriminator loss: 0.578909, acc: 0.710938] [adversarial loss: 1.345246, acc: 0.171875]\n",
      "4821: [discriminator loss: 0.445804, acc: 0.757812] [adversarial loss: 1.186374, acc: 0.312500]\n",
      "4822: [discriminator loss: 0.490737, acc: 0.742188] [adversarial loss: 1.653573, acc: 0.156250]\n",
      "4823: [discriminator loss: 0.549822, acc: 0.726562] [adversarial loss: 1.087653, acc: 0.296875]\n",
      "4824: [discriminator loss: 0.551592, acc: 0.726562] [adversarial loss: 1.578408, acc: 0.125000]\n",
      "4825: [discriminator loss: 0.597668, acc: 0.703125] [adversarial loss: 0.722778, acc: 0.562500]\n",
      "4826: [discriminator loss: 0.631745, acc: 0.664062] [adversarial loss: 1.791043, acc: 0.093750]\n",
      "4827: [discriminator loss: 0.536511, acc: 0.726562] [adversarial loss: 0.923280, acc: 0.453125]\n",
      "4828: [discriminator loss: 0.644396, acc: 0.593750] [adversarial loss: 1.545835, acc: 0.140625]\n",
      "4829: [discriminator loss: 0.503165, acc: 0.765625] [adversarial loss: 1.117974, acc: 0.296875]\n",
      "4830: [discriminator loss: 0.582544, acc: 0.718750] [adversarial loss: 1.343881, acc: 0.218750]\n",
      "4831: [discriminator loss: 0.547813, acc: 0.679688] [adversarial loss: 1.052168, acc: 0.343750]\n",
      "4832: [discriminator loss: 0.541297, acc: 0.710938] [adversarial loss: 1.431334, acc: 0.109375]\n",
      "4833: [discriminator loss: 0.472029, acc: 0.765625] [adversarial loss: 1.091911, acc: 0.343750]\n",
      "4834: [discriminator loss: 0.577632, acc: 0.656250] [adversarial loss: 1.542055, acc: 0.109375]\n",
      "4835: [discriminator loss: 0.500712, acc: 0.726562] [adversarial loss: 1.025476, acc: 0.328125]\n",
      "4836: [discriminator loss: 0.489215, acc: 0.765625] [adversarial loss: 1.411422, acc: 0.187500]\n",
      "4837: [discriminator loss: 0.530866, acc: 0.765625] [adversarial loss: 0.981552, acc: 0.328125]\n",
      "4838: [discriminator loss: 0.547904, acc: 0.695312] [adversarial loss: 1.632200, acc: 0.109375]\n",
      "4839: [discriminator loss: 0.489802, acc: 0.757812] [adversarial loss: 1.046341, acc: 0.250000]\n",
      "4840: [discriminator loss: 0.521875, acc: 0.726562] [adversarial loss: 1.477721, acc: 0.171875]\n",
      "4841: [discriminator loss: 0.495849, acc: 0.757812] [adversarial loss: 1.183724, acc: 0.171875]\n",
      "4842: [discriminator loss: 0.489412, acc: 0.789062] [adversarial loss: 1.365101, acc: 0.171875]\n",
      "4843: [discriminator loss: 0.545624, acc: 0.757812] [adversarial loss: 1.832998, acc: 0.125000]\n",
      "4844: [discriminator loss: 0.572265, acc: 0.710938] [adversarial loss: 0.829891, acc: 0.531250]\n",
      "4845: [discriminator loss: 0.517714, acc: 0.773438] [adversarial loss: 1.489491, acc: 0.156250]\n",
      "4846: [discriminator loss: 0.537318, acc: 0.734375] [adversarial loss: 0.922726, acc: 0.437500]\n",
      "4847: [discriminator loss: 0.538046, acc: 0.718750] [adversarial loss: 1.614197, acc: 0.125000]\n",
      "4848: [discriminator loss: 0.542962, acc: 0.726562] [adversarial loss: 1.239743, acc: 0.171875]\n",
      "4849: [discriminator loss: 0.489449, acc: 0.773438] [adversarial loss: 1.507797, acc: 0.140625]\n",
      "4850: [discriminator loss: 0.543531, acc: 0.734375] [adversarial loss: 1.096193, acc: 0.296875]\n",
      "4851: [discriminator loss: 0.499385, acc: 0.750000] [adversarial loss: 1.391752, acc: 0.156250]\n",
      "4852: [discriminator loss: 0.609171, acc: 0.664062] [adversarial loss: 1.000047, acc: 0.390625]\n",
      "4853: [discriminator loss: 0.549554, acc: 0.718750] [adversarial loss: 1.262774, acc: 0.250000]\n",
      "4854: [discriminator loss: 0.595175, acc: 0.671875] [adversarial loss: 1.414878, acc: 0.140625]\n",
      "4855: [discriminator loss: 0.524766, acc: 0.750000] [adversarial loss: 1.139921, acc: 0.265625]\n",
      "4856: [discriminator loss: 0.446187, acc: 0.796875] [adversarial loss: 1.295986, acc: 0.156250]\n",
      "4857: [discriminator loss: 0.463851, acc: 0.804688] [adversarial loss: 1.469481, acc: 0.156250]\n",
      "4858: [discriminator loss: 0.525878, acc: 0.742188] [adversarial loss: 1.193070, acc: 0.250000]\n",
      "4859: [discriminator loss: 0.470967, acc: 0.757812] [adversarial loss: 1.269192, acc: 0.218750]\n",
      "4860: [discriminator loss: 0.528408, acc: 0.726562] [adversarial loss: 1.158494, acc: 0.312500]\n",
      "4861: [discriminator loss: 0.386667, acc: 0.875000] [adversarial loss: 1.196041, acc: 0.328125]\n",
      "4862: [discriminator loss: 0.514246, acc: 0.734375] [adversarial loss: 1.189067, acc: 0.203125]\n",
      "4863: [discriminator loss: 0.509159, acc: 0.757812] [adversarial loss: 1.119942, acc: 0.343750]\n",
      "4864: [discriminator loss: 0.437444, acc: 0.789062] [adversarial loss: 1.430273, acc: 0.156250]\n",
      "4865: [discriminator loss: 0.484710, acc: 0.757812] [adversarial loss: 1.046227, acc: 0.296875]\n",
      "4866: [discriminator loss: 0.600293, acc: 0.671875] [adversarial loss: 2.102728, acc: 0.062500]\n",
      "4867: [discriminator loss: 0.619988, acc: 0.664062] [adversarial loss: 0.902045, acc: 0.375000]\n",
      "4868: [discriminator loss: 0.567182, acc: 0.710938] [adversarial loss: 1.654631, acc: 0.062500]\n",
      "4869: [discriminator loss: 0.604932, acc: 0.695312] [adversarial loss: 0.998122, acc: 0.421875]\n",
      "4870: [discriminator loss: 0.512948, acc: 0.710938] [adversarial loss: 1.812279, acc: 0.078125]\n",
      "4871: [discriminator loss: 0.482790, acc: 0.765625] [adversarial loss: 1.119689, acc: 0.250000]\n",
      "4872: [discriminator loss: 0.523354, acc: 0.742188] [adversarial loss: 1.475163, acc: 0.125000]\n",
      "4873: [discriminator loss: 0.481525, acc: 0.750000] [adversarial loss: 1.150864, acc: 0.218750]\n",
      "4874: [discriminator loss: 0.509435, acc: 0.695312] [adversarial loss: 1.499072, acc: 0.125000]\n",
      "4875: [discriminator loss: 0.485067, acc: 0.820312] [adversarial loss: 1.389732, acc: 0.156250]\n",
      "4876: [discriminator loss: 0.493895, acc: 0.765625] [adversarial loss: 1.032681, acc: 0.312500]\n",
      "4877: [discriminator loss: 0.553277, acc: 0.695312] [adversarial loss: 1.314510, acc: 0.203125]\n",
      "4878: [discriminator loss: 0.516670, acc: 0.773438] [adversarial loss: 1.293400, acc: 0.171875]\n",
      "4879: [discriminator loss: 0.438852, acc: 0.812500] [adversarial loss: 1.310113, acc: 0.203125]\n",
      "4880: [discriminator loss: 0.541929, acc: 0.703125] [adversarial loss: 0.989789, acc: 0.296875]\n",
      "4881: [discriminator loss: 0.457809, acc: 0.773438] [adversarial loss: 1.514028, acc: 0.093750]\n",
      "4882: [discriminator loss: 0.495037, acc: 0.750000] [adversarial loss: 0.840557, acc: 0.468750]\n",
      "4883: [discriminator loss: 0.531036, acc: 0.765625] [adversarial loss: 1.567710, acc: 0.187500]\n",
      "4884: [discriminator loss: 0.667318, acc: 0.625000] [adversarial loss: 0.703844, acc: 0.546875]\n",
      "4885: [discriminator loss: 0.536391, acc: 0.734375] [adversarial loss: 1.587494, acc: 0.125000]\n",
      "4886: [discriminator loss: 0.495227, acc: 0.742188] [adversarial loss: 1.113023, acc: 0.375000]\n",
      "4887: [discriminator loss: 0.489713, acc: 0.734375] [adversarial loss: 1.347572, acc: 0.187500]\n",
      "4888: [discriminator loss: 0.542345, acc: 0.710938] [adversarial loss: 1.271003, acc: 0.203125]\n",
      "4889: [discriminator loss: 0.496004, acc: 0.734375] [adversarial loss: 1.188991, acc: 0.203125]\n",
      "4890: [discriminator loss: 0.522089, acc: 0.718750] [adversarial loss: 1.508904, acc: 0.015625]\n",
      "4891: [discriminator loss: 0.541355, acc: 0.726562] [adversarial loss: 0.858425, acc: 0.390625]\n",
      "4892: [discriminator loss: 0.526049, acc: 0.742188] [adversarial loss: 1.183693, acc: 0.218750]\n",
      "4893: [discriminator loss: 0.498347, acc: 0.750000] [adversarial loss: 1.529048, acc: 0.062500]\n",
      "4894: [discriminator loss: 0.447986, acc: 0.781250] [adversarial loss: 0.988576, acc: 0.343750]\n",
      "4895: [discriminator loss: 0.520140, acc: 0.687500] [adversarial loss: 1.673683, acc: 0.109375]\n",
      "4896: [discriminator loss: 0.475214, acc: 0.781250] [adversarial loss: 1.191824, acc: 0.281250]\n",
      "4897: [discriminator loss: 0.548885, acc: 0.718750] [adversarial loss: 1.352099, acc: 0.203125]\n",
      "4898: [discriminator loss: 0.418869, acc: 0.828125] [adversarial loss: 1.040199, acc: 0.390625]\n",
      "4899: [discriminator loss: 0.516135, acc: 0.726562] [adversarial loss: 1.402984, acc: 0.234375]\n",
      "4900: [discriminator loss: 0.520573, acc: 0.750000] [adversarial loss: 1.407409, acc: 0.187500]\n",
      "4901: [discriminator loss: 0.625732, acc: 0.640625] [adversarial loss: 1.636418, acc: 0.109375]\n",
      "4902: [discriminator loss: 0.557783, acc: 0.695312] [adversarial loss: 1.080256, acc: 0.390625]\n",
      "4903: [discriminator loss: 0.590631, acc: 0.710938] [adversarial loss: 1.512795, acc: 0.218750]\n",
      "4904: [discriminator loss: 0.633640, acc: 0.703125] [adversarial loss: 0.817608, acc: 0.484375]\n",
      "4905: [discriminator loss: 0.573104, acc: 0.687500] [adversarial loss: 1.592003, acc: 0.046875]\n",
      "4906: [discriminator loss: 0.571432, acc: 0.710938] [adversarial loss: 0.882530, acc: 0.406250]\n",
      "4907: [discriminator loss: 0.493216, acc: 0.765625] [adversarial loss: 1.394304, acc: 0.125000]\n",
      "4908: [discriminator loss: 0.506936, acc: 0.773438] [adversarial loss: 1.160652, acc: 0.312500]\n",
      "4909: [discriminator loss: 0.533661, acc: 0.703125] [adversarial loss: 1.420231, acc: 0.156250]\n",
      "4910: [discriminator loss: 0.445409, acc: 0.796875] [adversarial loss: 0.978847, acc: 0.328125]\n",
      "4911: [discriminator loss: 0.522591, acc: 0.750000] [adversarial loss: 1.563720, acc: 0.062500]\n",
      "4912: [discriminator loss: 0.567756, acc: 0.687500] [adversarial loss: 0.980570, acc: 0.484375]\n",
      "4913: [discriminator loss: 0.469956, acc: 0.726562] [adversarial loss: 1.423188, acc: 0.140625]\n",
      "4914: [discriminator loss: 0.621485, acc: 0.687500] [adversarial loss: 1.145659, acc: 0.296875]\n",
      "4915: [discriminator loss: 0.485904, acc: 0.734375] [adversarial loss: 1.472597, acc: 0.109375]\n",
      "4916: [discriminator loss: 0.518089, acc: 0.742188] [adversarial loss: 1.055650, acc: 0.328125]\n",
      "4917: [discriminator loss: 0.506386, acc: 0.750000] [adversarial loss: 1.066622, acc: 0.296875]\n",
      "4918: [discriminator loss: 0.438742, acc: 0.812500] [adversarial loss: 1.418788, acc: 0.140625]\n",
      "4919: [discriminator loss: 0.517136, acc: 0.757812] [adversarial loss: 1.294621, acc: 0.234375]\n",
      "4920: [discriminator loss: 0.545803, acc: 0.687500] [adversarial loss: 1.575267, acc: 0.140625]\n",
      "4921: [discriminator loss: 0.559656, acc: 0.703125] [adversarial loss: 1.049799, acc: 0.421875]\n",
      "4922: [discriminator loss: 0.512880, acc: 0.750000] [adversarial loss: 1.763926, acc: 0.062500]\n",
      "4923: [discriminator loss: 0.585401, acc: 0.632812] [adversarial loss: 0.842041, acc: 0.468750]\n",
      "4924: [discriminator loss: 0.530478, acc: 0.687500] [adversarial loss: 1.798068, acc: 0.046875]\n",
      "4925: [discriminator loss: 0.547526, acc: 0.726562] [adversarial loss: 1.240021, acc: 0.218750]\n",
      "4926: [discriminator loss: 0.451626, acc: 0.828125] [adversarial loss: 1.457280, acc: 0.109375]\n",
      "4927: [discriminator loss: 0.468596, acc: 0.765625] [adversarial loss: 1.137543, acc: 0.296875]\n",
      "4928: [discriminator loss: 0.395539, acc: 0.867188] [adversarial loss: 1.319737, acc: 0.140625]\n",
      "4929: [discriminator loss: 0.593105, acc: 0.648438] [adversarial loss: 1.248921, acc: 0.171875]\n",
      "4930: [discriminator loss: 0.561746, acc: 0.703125] [adversarial loss: 1.300414, acc: 0.234375]\n",
      "4931: [discriminator loss: 0.517129, acc: 0.718750] [adversarial loss: 1.300431, acc: 0.203125]\n",
      "4932: [discriminator loss: 0.516100, acc: 0.750000] [adversarial loss: 1.328895, acc: 0.203125]\n",
      "4933: [discriminator loss: 0.523483, acc: 0.734375] [adversarial loss: 1.207590, acc: 0.218750]\n",
      "4934: [discriminator loss: 0.450599, acc: 0.789062] [adversarial loss: 1.122230, acc: 0.359375]\n",
      "4935: [discriminator loss: 0.542593, acc: 0.742188] [adversarial loss: 1.346902, acc: 0.203125]\n",
      "4936: [discriminator loss: 0.527858, acc: 0.710938] [adversarial loss: 1.364839, acc: 0.218750]\n",
      "4937: [discriminator loss: 0.539118, acc: 0.734375] [adversarial loss: 1.409193, acc: 0.171875]\n",
      "4938: [discriminator loss: 0.510449, acc: 0.757812] [adversarial loss: 1.396668, acc: 0.187500]\n",
      "4939: [discriminator loss: 0.500683, acc: 0.742188] [adversarial loss: 1.422341, acc: 0.125000]\n",
      "4940: [discriminator loss: 0.548355, acc: 0.710938] [adversarial loss: 1.006100, acc: 0.296875]\n",
      "4941: [discriminator loss: 0.486214, acc: 0.726562] [adversarial loss: 1.708055, acc: 0.093750]\n",
      "4942: [discriminator loss: 0.478639, acc: 0.750000] [adversarial loss: 0.976357, acc: 0.375000]\n",
      "4943: [discriminator loss: 0.576285, acc: 0.734375] [adversarial loss: 1.750190, acc: 0.046875]\n",
      "4944: [discriminator loss: 0.495159, acc: 0.781250] [adversarial loss: 1.111996, acc: 0.390625]\n",
      "4945: [discriminator loss: 0.508702, acc: 0.726562] [adversarial loss: 1.680147, acc: 0.140625]\n",
      "4946: [discriminator loss: 0.561085, acc: 0.695312] [adversarial loss: 1.226123, acc: 0.203125]\n",
      "4947: [discriminator loss: 0.537034, acc: 0.718750] [adversarial loss: 1.041003, acc: 0.328125]\n",
      "4948: [discriminator loss: 0.514613, acc: 0.710938] [adversarial loss: 1.640895, acc: 0.093750]\n",
      "4949: [discriminator loss: 0.445641, acc: 0.812500] [adversarial loss: 1.092419, acc: 0.343750]\n",
      "4950: [discriminator loss: 0.569658, acc: 0.679688] [adversarial loss: 1.259964, acc: 0.203125]\n",
      "4951: [discriminator loss: 0.474600, acc: 0.789062] [adversarial loss: 1.019983, acc: 0.328125]\n",
      "4952: [discriminator loss: 0.487867, acc: 0.742188] [adversarial loss: 1.698537, acc: 0.140625]\n",
      "4953: [discriminator loss: 0.614200, acc: 0.671875] [adversarial loss: 1.041989, acc: 0.375000]\n",
      "4954: [discriminator loss: 0.572452, acc: 0.718750] [adversarial loss: 1.376423, acc: 0.187500]\n",
      "4955: [discriminator loss: 0.584482, acc: 0.664062] [adversarial loss: 1.134281, acc: 0.203125]\n",
      "4956: [discriminator loss: 0.480680, acc: 0.757812] [adversarial loss: 1.555535, acc: 0.062500]\n",
      "4957: [discriminator loss: 0.517292, acc: 0.757812] [adversarial loss: 1.097993, acc: 0.375000]\n",
      "4958: [discriminator loss: 0.476705, acc: 0.757812] [adversarial loss: 1.626469, acc: 0.078125]\n",
      "4959: [discriminator loss: 0.441287, acc: 0.765625] [adversarial loss: 1.189755, acc: 0.218750]\n",
      "4960: [discriminator loss: 0.511557, acc: 0.773438] [adversarial loss: 1.694752, acc: 0.062500]\n",
      "4961: [discriminator loss: 0.508809, acc: 0.757812] [adversarial loss: 1.430060, acc: 0.140625]\n",
      "4962: [discriminator loss: 0.476543, acc: 0.796875] [adversarial loss: 1.147885, acc: 0.281250]\n",
      "4963: [discriminator loss: 0.509836, acc: 0.734375] [adversarial loss: 1.410742, acc: 0.140625]\n",
      "4964: [discriminator loss: 0.583024, acc: 0.695312] [adversarial loss: 1.562508, acc: 0.109375]\n",
      "4965: [discriminator loss: 0.479085, acc: 0.765625] [adversarial loss: 1.321252, acc: 0.203125]\n",
      "4966: [discriminator loss: 0.482368, acc: 0.765625] [adversarial loss: 1.566807, acc: 0.156250]\n",
      "4967: [discriminator loss: 0.491274, acc: 0.781250] [adversarial loss: 0.800491, acc: 0.546875]\n",
      "4968: [discriminator loss: 0.550041, acc: 0.718750] [adversarial loss: 2.009368, acc: 0.093750]\n",
      "4969: [discriminator loss: 0.628916, acc: 0.679688] [adversarial loss: 0.857779, acc: 0.453125]\n",
      "4970: [discriminator loss: 0.572242, acc: 0.671875] [adversarial loss: 1.570695, acc: 0.093750]\n",
      "4971: [discriminator loss: 0.498309, acc: 0.781250] [adversarial loss: 1.186034, acc: 0.250000]\n",
      "4972: [discriminator loss: 0.508918, acc: 0.773438] [adversarial loss: 1.116372, acc: 0.328125]\n",
      "4973: [discriminator loss: 0.494301, acc: 0.773438] [adversarial loss: 1.670906, acc: 0.109375]\n",
      "4974: [discriminator loss: 0.510406, acc: 0.781250] [adversarial loss: 1.069161, acc: 0.265625]\n",
      "4975: [discriminator loss: 0.435070, acc: 0.765625] [adversarial loss: 1.279119, acc: 0.218750]\n",
      "4976: [discriminator loss: 0.488601, acc: 0.796875] [adversarial loss: 1.407465, acc: 0.171875]\n",
      "4977: [discriminator loss: 0.476264, acc: 0.789062] [adversarial loss: 1.102212, acc: 0.250000]\n",
      "4978: [discriminator loss: 0.518989, acc: 0.734375] [adversarial loss: 1.120589, acc: 0.265625]\n",
      "4979: [discriminator loss: 0.550233, acc: 0.679688] [adversarial loss: 1.214717, acc: 0.250000]\n",
      "4980: [discriminator loss: 0.574401, acc: 0.726562] [adversarial loss: 1.459813, acc: 0.234375]\n",
      "4981: [discriminator loss: 0.515799, acc: 0.757812] [adversarial loss: 1.325642, acc: 0.265625]\n",
      "4982: [discriminator loss: 0.484519, acc: 0.734375] [adversarial loss: 0.953311, acc: 0.406250]\n",
      "4983: [discriminator loss: 0.541708, acc: 0.742188] [adversarial loss: 1.700456, acc: 0.062500]\n",
      "4984: [discriminator loss: 0.605804, acc: 0.703125] [adversarial loss: 0.957534, acc: 0.406250]\n",
      "4985: [discriminator loss: 0.580101, acc: 0.695312] [adversarial loss: 1.579558, acc: 0.140625]\n",
      "4986: [discriminator loss: 0.469583, acc: 0.765625] [adversarial loss: 1.075349, acc: 0.281250]\n",
      "4987: [discriminator loss: 0.500251, acc: 0.804688] [adversarial loss: 1.800887, acc: 0.093750]\n",
      "4988: [discriminator loss: 0.531046, acc: 0.773438] [adversarial loss: 1.170535, acc: 0.281250]\n",
      "4989: [discriminator loss: 0.523694, acc: 0.757812] [adversarial loss: 1.181456, acc: 0.328125]\n",
      "4990: [discriminator loss: 0.495935, acc: 0.781250] [adversarial loss: 1.116539, acc: 0.359375]\n",
      "4991: [discriminator loss: 0.613482, acc: 0.664062] [adversarial loss: 1.052612, acc: 0.328125]\n",
      "4992: [discriminator loss: 0.452296, acc: 0.773438] [adversarial loss: 1.533163, acc: 0.125000]\n",
      "4993: [discriminator loss: 0.546293, acc: 0.718750] [adversarial loss: 1.158502, acc: 0.203125]\n",
      "4994: [discriminator loss: 0.553909, acc: 0.703125] [adversarial loss: 1.227908, acc: 0.156250]\n",
      "4995: [discriminator loss: 0.464071, acc: 0.796875] [adversarial loss: 1.314239, acc: 0.250000]\n",
      "4996: [discriminator loss: 0.515687, acc: 0.750000] [adversarial loss: 1.103748, acc: 0.281250]\n",
      "4997: [discriminator loss: 0.514028, acc: 0.703125] [adversarial loss: 1.562613, acc: 0.046875]\n",
      "4998: [discriminator loss: 0.643689, acc: 0.656250] [adversarial loss: 0.762265, acc: 0.562500]\n",
      "4999: [discriminator loss: 0.581952, acc: 0.671875] [adversarial loss: 1.663954, acc: 0.078125]\n",
      "5000: [discriminator loss: 0.533594, acc: 0.695312] [adversarial loss: 1.065576, acc: 0.359375]\n",
      "5001: [discriminator loss: 0.560399, acc: 0.742188] [adversarial loss: 1.683489, acc: 0.125000]\n",
      "5002: [discriminator loss: 0.573810, acc: 0.687500] [adversarial loss: 0.863520, acc: 0.468750]\n",
      "5003: [discriminator loss: 0.576128, acc: 0.632812] [adversarial loss: 1.416032, acc: 0.187500]\n",
      "5004: [discriminator loss: 0.603466, acc: 0.718750] [adversarial loss: 0.930941, acc: 0.375000]\n",
      "5005: [discriminator loss: 0.526974, acc: 0.734375] [adversarial loss: 1.487337, acc: 0.234375]\n",
      "5006: [discriminator loss: 0.619165, acc: 0.679688] [adversarial loss: 0.836073, acc: 0.546875]\n",
      "5007: [discriminator loss: 0.496993, acc: 0.750000] [adversarial loss: 1.529062, acc: 0.187500]\n",
      "5008: [discriminator loss: 0.497322, acc: 0.742188] [adversarial loss: 1.306166, acc: 0.156250]\n",
      "5009: [discriminator loss: 0.477708, acc: 0.750000] [adversarial loss: 1.147951, acc: 0.203125]\n",
      "5010: [discriminator loss: 0.531284, acc: 0.765625] [adversarial loss: 1.538084, acc: 0.140625]\n",
      "5011: [discriminator loss: 0.443987, acc: 0.789062] [adversarial loss: 1.255745, acc: 0.218750]\n",
      "5012: [discriminator loss: 0.426825, acc: 0.757812] [adversarial loss: 1.399881, acc: 0.250000]\n",
      "5013: [discriminator loss: 0.490511, acc: 0.742188] [adversarial loss: 1.195387, acc: 0.203125]\n",
      "5014: [discriminator loss: 0.480464, acc: 0.765625] [adversarial loss: 1.362574, acc: 0.250000]\n",
      "5015: [discriminator loss: 0.524706, acc: 0.734375] [adversarial loss: 0.947056, acc: 0.468750]\n",
      "5016: [discriminator loss: 0.562532, acc: 0.718750] [adversarial loss: 1.581718, acc: 0.171875]\n",
      "5017: [discriminator loss: 0.596805, acc: 0.757812] [adversarial loss: 1.278675, acc: 0.140625]\n",
      "5018: [discriminator loss: 0.480057, acc: 0.773438] [adversarial loss: 1.424007, acc: 0.140625]\n",
      "5019: [discriminator loss: 0.619402, acc: 0.671875] [adversarial loss: 1.194914, acc: 0.296875]\n",
      "5020: [discriminator loss: 0.509939, acc: 0.750000] [adversarial loss: 0.949598, acc: 0.359375]\n",
      "5021: [discriminator loss: 0.535154, acc: 0.757812] [adversarial loss: 1.410326, acc: 0.125000]\n",
      "5022: [discriminator loss: 0.543715, acc: 0.687500] [adversarial loss: 0.965387, acc: 0.296875]\n",
      "5023: [discriminator loss: 0.410199, acc: 0.835938] [adversarial loss: 1.403923, acc: 0.171875]\n",
      "5024: [discriminator loss: 0.494009, acc: 0.742188] [adversarial loss: 1.273324, acc: 0.203125]\n",
      "5025: [discriminator loss: 0.478647, acc: 0.718750] [adversarial loss: 1.309729, acc: 0.203125]\n",
      "5026: [discriminator loss: 0.553612, acc: 0.710938] [adversarial loss: 1.270704, acc: 0.218750]\n",
      "5027: [discriminator loss: 0.513737, acc: 0.765625] [adversarial loss: 1.328009, acc: 0.265625]\n",
      "5028: [discriminator loss: 0.523203, acc: 0.757812] [adversarial loss: 1.159591, acc: 0.234375]\n",
      "5029: [discriminator loss: 0.527154, acc: 0.750000] [adversarial loss: 1.766110, acc: 0.093750]\n",
      "5030: [discriminator loss: 0.519628, acc: 0.734375] [adversarial loss: 1.116917, acc: 0.312500]\n",
      "5031: [discriminator loss: 0.497958, acc: 0.773438] [adversarial loss: 1.125636, acc: 0.250000]\n",
      "5032: [discriminator loss: 0.542167, acc: 0.695312] [adversarial loss: 1.499496, acc: 0.078125]\n",
      "5033: [discriminator loss: 0.398120, acc: 0.820312] [adversarial loss: 1.041072, acc: 0.343750]\n",
      "5034: [discriminator loss: 0.508181, acc: 0.742188] [adversarial loss: 1.632107, acc: 0.109375]\n",
      "5035: [discriminator loss: 0.540260, acc: 0.742188] [adversarial loss: 0.972859, acc: 0.390625]\n",
      "5036: [discriminator loss: 0.587378, acc: 0.742188] [adversarial loss: 1.801850, acc: 0.031250]\n",
      "5037: [discriminator loss: 0.601262, acc: 0.671875] [adversarial loss: 0.946415, acc: 0.406250]\n",
      "5038: [discriminator loss: 0.566000, acc: 0.687500] [adversarial loss: 1.888395, acc: 0.046875]\n",
      "5039: [discriminator loss: 0.498000, acc: 0.757812] [adversarial loss: 0.956930, acc: 0.406250]\n",
      "5040: [discriminator loss: 0.555686, acc: 0.726562] [adversarial loss: 1.330548, acc: 0.265625]\n",
      "5041: [discriminator loss: 0.457908, acc: 0.765625] [adversarial loss: 0.936589, acc: 0.343750]\n",
      "5042: [discriminator loss: 0.477117, acc: 0.781250] [adversarial loss: 1.261525, acc: 0.359375]\n",
      "5043: [discriminator loss: 0.584518, acc: 0.695312] [adversarial loss: 1.222540, acc: 0.250000]\n",
      "5044: [discriminator loss: 0.557380, acc: 0.718750] [adversarial loss: 1.419471, acc: 0.171875]\n",
      "5045: [discriminator loss: 0.530561, acc: 0.734375] [adversarial loss: 1.299685, acc: 0.187500]\n",
      "5046: [discriminator loss: 0.427463, acc: 0.812500] [adversarial loss: 1.129570, acc: 0.218750]\n",
      "5047: [discriminator loss: 0.457227, acc: 0.789062] [adversarial loss: 1.178344, acc: 0.156250]\n",
      "5048: [discriminator loss: 0.440170, acc: 0.820312] [adversarial loss: 1.187882, acc: 0.312500]\n",
      "5049: [discriminator loss: 0.491452, acc: 0.742188] [adversarial loss: 1.258016, acc: 0.234375]\n",
      "5050: [discriminator loss: 0.475196, acc: 0.765625] [adversarial loss: 1.518898, acc: 0.203125]\n",
      "5051: [discriminator loss: 0.550860, acc: 0.718750] [adversarial loss: 1.278279, acc: 0.328125]\n",
      "5052: [discriminator loss: 0.587028, acc: 0.703125] [adversarial loss: 1.643867, acc: 0.109375]\n",
      "5053: [discriminator loss: 0.492681, acc: 0.750000] [adversarial loss: 0.960893, acc: 0.453125]\n",
      "5054: [discriminator loss: 0.506984, acc: 0.765625] [adversarial loss: 1.510801, acc: 0.093750]\n",
      "5055: [discriminator loss: 0.591758, acc: 0.687500] [adversarial loss: 1.111404, acc: 0.312500]\n",
      "5056: [discriminator loss: 0.546822, acc: 0.695312] [adversarial loss: 1.402447, acc: 0.125000]\n",
      "5057: [discriminator loss: 0.437790, acc: 0.804688] [adversarial loss: 1.269119, acc: 0.250000]\n",
      "5058: [discriminator loss: 0.563264, acc: 0.726562] [adversarial loss: 1.352723, acc: 0.234375]\n",
      "5059: [discriminator loss: 0.467243, acc: 0.765625] [adversarial loss: 1.191552, acc: 0.296875]\n",
      "5060: [discriminator loss: 0.567367, acc: 0.687500] [adversarial loss: 1.331509, acc: 0.234375]\n",
      "5061: [discriminator loss: 0.524131, acc: 0.796875] [adversarial loss: 1.185865, acc: 0.265625]\n",
      "5062: [discriminator loss: 0.494115, acc: 0.757812] [adversarial loss: 1.348313, acc: 0.203125]\n",
      "5063: [discriminator loss: 0.545085, acc: 0.687500] [adversarial loss: 1.114794, acc: 0.312500]\n",
      "5064: [discriminator loss: 0.511567, acc: 0.734375] [adversarial loss: 1.733250, acc: 0.156250]\n",
      "5065: [discriminator loss: 0.574580, acc: 0.671875] [adversarial loss: 1.130275, acc: 0.296875]\n",
      "5066: [discriminator loss: 0.550735, acc: 0.757812] [adversarial loss: 1.432711, acc: 0.203125]\n",
      "5067: [discriminator loss: 0.494581, acc: 0.773438] [adversarial loss: 1.188655, acc: 0.218750]\n",
      "5068: [discriminator loss: 0.585605, acc: 0.671875] [adversarial loss: 1.356836, acc: 0.203125]\n",
      "5069: [discriminator loss: 0.541925, acc: 0.718750] [adversarial loss: 0.968394, acc: 0.406250]\n",
      "5070: [discriminator loss: 0.559329, acc: 0.765625] [adversarial loss: 1.367748, acc: 0.156250]\n",
      "5071: [discriminator loss: 0.582869, acc: 0.734375] [adversarial loss: 0.944915, acc: 0.421875]\n",
      "5072: [discriminator loss: 0.589698, acc: 0.703125] [adversarial loss: 1.403872, acc: 0.156250]\n",
      "5073: [discriminator loss: 0.620427, acc: 0.671875] [adversarial loss: 1.198743, acc: 0.187500]\n",
      "5074: [discriminator loss: 0.494586, acc: 0.742188] [adversarial loss: 1.224620, acc: 0.203125]\n",
      "5075: [discriminator loss: 0.600609, acc: 0.625000] [adversarial loss: 1.316168, acc: 0.203125]\n",
      "5076: [discriminator loss: 0.590565, acc: 0.656250] [adversarial loss: 1.182262, acc: 0.203125]\n",
      "5077: [discriminator loss: 0.442212, acc: 0.835938] [adversarial loss: 1.283911, acc: 0.218750]\n",
      "5078: [discriminator loss: 0.521003, acc: 0.726562] [adversarial loss: 1.246508, acc: 0.140625]\n",
      "5079: [discriminator loss: 0.528670, acc: 0.710938] [adversarial loss: 1.388975, acc: 0.125000]\n",
      "5080: [discriminator loss: 0.481731, acc: 0.742188] [adversarial loss: 1.495865, acc: 0.125000]\n",
      "5081: [discriminator loss: 0.458944, acc: 0.781250] [adversarial loss: 1.309753, acc: 0.203125]\n",
      "5082: [discriminator loss: 0.550625, acc: 0.710938] [adversarial loss: 1.501787, acc: 0.109375]\n",
      "5083: [discriminator loss: 0.514230, acc: 0.742188] [adversarial loss: 1.015152, acc: 0.468750]\n",
      "5084: [discriminator loss: 0.539987, acc: 0.703125] [adversarial loss: 1.384470, acc: 0.187500]\n",
      "5085: [discriminator loss: 0.542129, acc: 0.710938] [adversarial loss: 1.144722, acc: 0.250000]\n",
      "5086: [discriminator loss: 0.499020, acc: 0.773438] [adversarial loss: 1.291795, acc: 0.171875]\n",
      "5087: [discriminator loss: 0.420434, acc: 0.820312] [adversarial loss: 1.234377, acc: 0.312500]\n",
      "5088: [discriminator loss: 0.623526, acc: 0.648438] [adversarial loss: 1.475846, acc: 0.218750]\n",
      "5089: [discriminator loss: 0.513155, acc: 0.710938] [adversarial loss: 0.974752, acc: 0.343750]\n",
      "5090: [discriminator loss: 0.485626, acc: 0.781250] [adversarial loss: 1.878169, acc: 0.156250]\n",
      "5091: [discriminator loss: 0.614026, acc: 0.656250] [adversarial loss: 0.735753, acc: 0.562500]\n",
      "5092: [discriminator loss: 0.545310, acc: 0.710938] [adversarial loss: 1.892737, acc: 0.078125]\n",
      "5093: [discriminator loss: 0.455663, acc: 0.796875] [adversarial loss: 1.372308, acc: 0.234375]\n",
      "5094: [discriminator loss: 0.619594, acc: 0.664062] [adversarial loss: 1.403357, acc: 0.125000]\n",
      "5095: [discriminator loss: 0.477372, acc: 0.750000] [adversarial loss: 1.091020, acc: 0.312500]\n",
      "5096: [discriminator loss: 0.472443, acc: 0.812500] [adversarial loss: 1.335429, acc: 0.171875]\n",
      "5097: [discriminator loss: 0.543538, acc: 0.695312] [adversarial loss: 1.047394, acc: 0.281250]\n",
      "5098: [discriminator loss: 0.527656, acc: 0.734375] [adversarial loss: 1.385593, acc: 0.171875]\n",
      "5099: [discriminator loss: 0.532006, acc: 0.734375] [adversarial loss: 1.128134, acc: 0.281250]\n",
      "5100: [discriminator loss: 0.532937, acc: 0.750000] [adversarial loss: 1.335953, acc: 0.140625]\n",
      "5101: [discriminator loss: 0.521943, acc: 0.671875] [adversarial loss: 1.164932, acc: 0.296875]\n",
      "5102: [discriminator loss: 0.531100, acc: 0.718750] [adversarial loss: 1.251740, acc: 0.234375]\n",
      "5103: [discriminator loss: 0.473734, acc: 0.765625] [adversarial loss: 1.666698, acc: 0.093750]\n",
      "5104: [discriminator loss: 0.547010, acc: 0.734375] [adversarial loss: 1.042990, acc: 0.265625]\n",
      "5105: [discriminator loss: 0.524086, acc: 0.718750] [adversarial loss: 1.465362, acc: 0.140625]\n",
      "5106: [discriminator loss: 0.490982, acc: 0.757812] [adversarial loss: 0.869185, acc: 0.421875]\n",
      "5107: [discriminator loss: 0.532592, acc: 0.742188] [adversarial loss: 1.834182, acc: 0.046875]\n",
      "5108: [discriminator loss: 0.591894, acc: 0.625000] [adversarial loss: 1.225035, acc: 0.296875]\n",
      "5109: [discriminator loss: 0.547042, acc: 0.695312] [adversarial loss: 1.580024, acc: 0.109375]\n",
      "5110: [discriminator loss: 0.521473, acc: 0.734375] [adversarial loss: 1.005819, acc: 0.312500]\n",
      "5111: [discriminator loss: 0.530692, acc: 0.742188] [adversarial loss: 1.409057, acc: 0.171875]\n",
      "5112: [discriminator loss: 0.615340, acc: 0.648438] [adversarial loss: 0.992062, acc: 0.343750]\n",
      "5113: [discriminator loss: 0.493526, acc: 0.765625] [adversarial loss: 1.526752, acc: 0.125000]\n",
      "5114: [discriminator loss: 0.555419, acc: 0.703125] [adversarial loss: 0.982850, acc: 0.406250]\n",
      "5115: [discriminator loss: 0.496115, acc: 0.734375] [adversarial loss: 1.455821, acc: 0.187500]\n",
      "5116: [discriminator loss: 0.541780, acc: 0.750000] [adversarial loss: 1.055437, acc: 0.312500]\n",
      "5117: [discriminator loss: 0.524324, acc: 0.765625] [adversarial loss: 1.343006, acc: 0.171875]\n",
      "5118: [discriminator loss: 0.496149, acc: 0.812500] [adversarial loss: 1.045115, acc: 0.296875]\n",
      "5119: [discriminator loss: 0.547010, acc: 0.734375] [adversarial loss: 1.116775, acc: 0.281250]\n",
      "5120: [discriminator loss: 0.472325, acc: 0.757812] [adversarial loss: 1.154985, acc: 0.328125]\n",
      "5121: [discriminator loss: 0.568894, acc: 0.695312] [adversarial loss: 1.294027, acc: 0.203125]\n",
      "5122: [discriminator loss: 0.533121, acc: 0.742188] [adversarial loss: 1.147014, acc: 0.328125]\n",
      "5123: [discriminator loss: 0.522770, acc: 0.734375] [adversarial loss: 1.509643, acc: 0.171875]\n",
      "5124: [discriminator loss: 0.560256, acc: 0.671875] [adversarial loss: 1.000426, acc: 0.281250]\n",
      "5125: [discriminator loss: 0.542502, acc: 0.757812] [adversarial loss: 1.257861, acc: 0.234375]\n",
      "5126: [discriminator loss: 0.453615, acc: 0.820312] [adversarial loss: 1.350271, acc: 0.171875]\n",
      "5127: [discriminator loss: 0.523449, acc: 0.757812] [adversarial loss: 1.344848, acc: 0.156250]\n",
      "5128: [discriminator loss: 0.506380, acc: 0.734375] [adversarial loss: 1.022966, acc: 0.375000]\n",
      "5129: [discriminator loss: 0.491861, acc: 0.734375] [adversarial loss: 1.637682, acc: 0.093750]\n",
      "5130: [discriminator loss: 0.581879, acc: 0.710938] [adversarial loss: 0.991904, acc: 0.437500]\n",
      "5131: [discriminator loss: 0.577197, acc: 0.695312] [adversarial loss: 2.150260, acc: 0.031250]\n",
      "5132: [discriminator loss: 0.679128, acc: 0.617188] [adversarial loss: 0.758446, acc: 0.515625]\n",
      "5133: [discriminator loss: 0.518525, acc: 0.757812] [adversarial loss: 1.223392, acc: 0.250000]\n",
      "5134: [discriminator loss: 0.547610, acc: 0.726562] [adversarial loss: 0.892296, acc: 0.390625]\n",
      "5135: [discriminator loss: 0.580786, acc: 0.695312] [adversarial loss: 1.367505, acc: 0.187500]\n",
      "5136: [discriminator loss: 0.527549, acc: 0.757812] [adversarial loss: 1.183632, acc: 0.312500]\n",
      "5137: [discriminator loss: 0.505880, acc: 0.781250] [adversarial loss: 1.334910, acc: 0.171875]\n",
      "5138: [discriminator loss: 0.514816, acc: 0.734375] [adversarial loss: 1.189089, acc: 0.234375]\n",
      "5139: [discriminator loss: 0.496159, acc: 0.750000] [adversarial loss: 1.275529, acc: 0.265625]\n",
      "5140: [discriminator loss: 0.537095, acc: 0.718750] [adversarial loss: 1.317318, acc: 0.171875]\n",
      "5141: [discriminator loss: 0.419047, acc: 0.835938] [adversarial loss: 1.431855, acc: 0.203125]\n",
      "5142: [discriminator loss: 0.583534, acc: 0.679688] [adversarial loss: 0.998681, acc: 0.406250]\n",
      "5143: [discriminator loss: 0.582902, acc: 0.679688] [adversarial loss: 2.025345, acc: 0.000000]\n",
      "5144: [discriminator loss: 0.536825, acc: 0.703125] [adversarial loss: 0.797204, acc: 0.531250]\n",
      "5145: [discriminator loss: 0.625290, acc: 0.648438] [adversarial loss: 1.628876, acc: 0.062500]\n",
      "5146: [discriminator loss: 0.531832, acc: 0.742188] [adversarial loss: 1.078368, acc: 0.312500]\n",
      "5147: [discriminator loss: 0.471766, acc: 0.757812] [adversarial loss: 1.273196, acc: 0.218750]\n",
      "5148: [discriminator loss: 0.510246, acc: 0.718750] [adversarial loss: 1.167288, acc: 0.203125]\n",
      "5149: [discriminator loss: 0.526074, acc: 0.757812] [adversarial loss: 1.377522, acc: 0.187500]\n",
      "5150: [discriminator loss: 0.450253, acc: 0.820312] [adversarial loss: 1.477926, acc: 0.062500]\n",
      "5151: [discriminator loss: 0.537295, acc: 0.726562] [adversarial loss: 1.177109, acc: 0.281250]\n",
      "5152: [discriminator loss: 0.473738, acc: 0.757812] [adversarial loss: 1.376094, acc: 0.156250]\n",
      "5153: [discriminator loss: 0.501686, acc: 0.765625] [adversarial loss: 1.519988, acc: 0.140625]\n",
      "5154: [discriminator loss: 0.550331, acc: 0.726562] [adversarial loss: 1.144006, acc: 0.234375]\n",
      "5155: [discriminator loss: 0.508075, acc: 0.765625] [adversarial loss: 1.541075, acc: 0.125000]\n",
      "5156: [discriminator loss: 0.520034, acc: 0.781250] [adversarial loss: 1.011937, acc: 0.375000]\n",
      "5157: [discriminator loss: 0.508469, acc: 0.750000] [adversarial loss: 1.727167, acc: 0.156250]\n",
      "5158: [discriminator loss: 0.548473, acc: 0.726562] [adversarial loss: 0.964624, acc: 0.328125]\n",
      "5159: [discriminator loss: 0.462671, acc: 0.789062] [adversarial loss: 1.353726, acc: 0.203125]\n",
      "5160: [discriminator loss: 0.528096, acc: 0.726562] [adversarial loss: 0.970993, acc: 0.390625]\n",
      "5161: [discriminator loss: 0.503943, acc: 0.718750] [adversarial loss: 1.290744, acc: 0.203125]\n",
      "5162: [discriminator loss: 0.594249, acc: 0.648438] [adversarial loss: 1.103216, acc: 0.265625]\n",
      "5163: [discriminator loss: 0.506237, acc: 0.750000] [adversarial loss: 1.362706, acc: 0.265625]\n",
      "5164: [discriminator loss: 0.497526, acc: 0.734375] [adversarial loss: 1.205344, acc: 0.187500]\n",
      "5165: [discriminator loss: 0.531687, acc: 0.710938] [adversarial loss: 1.406424, acc: 0.156250]\n",
      "5166: [discriminator loss: 0.549444, acc: 0.742188] [adversarial loss: 0.923572, acc: 0.390625]\n",
      "5167: [discriminator loss: 0.502851, acc: 0.765625] [adversarial loss: 1.693520, acc: 0.093750]\n",
      "5168: [discriminator loss: 0.557153, acc: 0.687500] [adversarial loss: 0.940634, acc: 0.437500]\n",
      "5169: [discriminator loss: 0.572810, acc: 0.679688] [adversarial loss: 1.369912, acc: 0.171875]\n",
      "5170: [discriminator loss: 0.522813, acc: 0.734375] [adversarial loss: 1.120445, acc: 0.281250]\n",
      "5171: [discriminator loss: 0.516703, acc: 0.710938] [adversarial loss: 1.470869, acc: 0.093750]\n",
      "5172: [discriminator loss: 0.500317, acc: 0.757812] [adversarial loss: 1.195791, acc: 0.234375]\n",
      "5173: [discriminator loss: 0.495865, acc: 0.750000] [adversarial loss: 1.428086, acc: 0.156250]\n",
      "5174: [discriminator loss: 0.493648, acc: 0.789062] [adversarial loss: 1.247303, acc: 0.218750]\n",
      "5175: [discriminator loss: 0.514230, acc: 0.710938] [adversarial loss: 1.240155, acc: 0.234375]\n",
      "5176: [discriminator loss: 0.463642, acc: 0.789062] [adversarial loss: 1.259118, acc: 0.265625]\n",
      "5177: [discriminator loss: 0.531548, acc: 0.718750] [adversarial loss: 1.366702, acc: 0.156250]\n",
      "5178: [discriminator loss: 0.501010, acc: 0.796875] [adversarial loss: 1.066719, acc: 0.359375]\n",
      "5179: [discriminator loss: 0.595166, acc: 0.679688] [adversarial loss: 1.290465, acc: 0.218750]\n",
      "5180: [discriminator loss: 0.489427, acc: 0.773438] [adversarial loss: 1.460993, acc: 0.125000]\n",
      "5181: [discriminator loss: 0.534250, acc: 0.718750] [adversarial loss: 1.104411, acc: 0.328125]\n",
      "5182: [discriminator loss: 0.552679, acc: 0.726562] [adversarial loss: 1.680432, acc: 0.062500]\n",
      "5183: [discriminator loss: 0.509101, acc: 0.750000] [adversarial loss: 1.069209, acc: 0.234375]\n",
      "5184: [discriminator loss: 0.545703, acc: 0.703125] [adversarial loss: 1.509840, acc: 0.140625]\n",
      "5185: [discriminator loss: 0.541125, acc: 0.710938] [adversarial loss: 0.971977, acc: 0.390625]\n",
      "5186: [discriminator loss: 0.540628, acc: 0.750000] [adversarial loss: 1.609676, acc: 0.109375]\n",
      "5187: [discriminator loss: 0.562116, acc: 0.710938] [adversarial loss: 0.995098, acc: 0.343750]\n",
      "5188: [discriminator loss: 0.567013, acc: 0.679688] [adversarial loss: 1.789113, acc: 0.125000]\n",
      "5189: [discriminator loss: 0.511380, acc: 0.734375] [adversarial loss: 0.899034, acc: 0.421875]\n",
      "5190: [discriminator loss: 0.521369, acc: 0.710938] [adversarial loss: 1.668484, acc: 0.078125]\n",
      "5191: [discriminator loss: 0.605726, acc: 0.679688] [adversarial loss: 1.021848, acc: 0.328125]\n",
      "5192: [discriminator loss: 0.565226, acc: 0.656250] [adversarial loss: 1.465055, acc: 0.125000]\n",
      "5193: [discriminator loss: 0.476687, acc: 0.789062] [adversarial loss: 1.183527, acc: 0.265625]\n",
      "5194: [discriminator loss: 0.576486, acc: 0.671875] [adversarial loss: 1.253182, acc: 0.234375]\n",
      "5195: [discriminator loss: 0.466273, acc: 0.750000] [adversarial loss: 1.309347, acc: 0.140625]\n",
      "5196: [discriminator loss: 0.450692, acc: 0.820312] [adversarial loss: 1.262332, acc: 0.125000]\n",
      "5197: [discriminator loss: 0.532690, acc: 0.726562] [adversarial loss: 1.130006, acc: 0.296875]\n",
      "5198: [discriminator loss: 0.463949, acc: 0.765625] [adversarial loss: 1.375859, acc: 0.125000]\n",
      "5199: [discriminator loss: 0.525114, acc: 0.781250] [adversarial loss: 1.046025, acc: 0.343750]\n",
      "5200: [discriminator loss: 0.542193, acc: 0.718750] [adversarial loss: 1.476804, acc: 0.093750]\n",
      "5201: [discriminator loss: 0.546752, acc: 0.710938] [adversarial loss: 1.007359, acc: 0.375000]\n",
      "5202: [discriminator loss: 0.563272, acc: 0.710938] [adversarial loss: 1.651351, acc: 0.109375]\n",
      "5203: [discriminator loss: 0.552250, acc: 0.718750] [adversarial loss: 0.996935, acc: 0.421875]\n",
      "5204: [discriminator loss: 0.493283, acc: 0.734375] [adversarial loss: 1.291522, acc: 0.234375]\n",
      "5205: [discriminator loss: 0.500089, acc: 0.757812] [adversarial loss: 1.360185, acc: 0.140625]\n",
      "5206: [discriminator loss: 0.554707, acc: 0.695312] [adversarial loss: 1.343849, acc: 0.203125]\n",
      "5207: [discriminator loss: 0.610178, acc: 0.687500] [adversarial loss: 0.967401, acc: 0.359375]\n",
      "5208: [discriminator loss: 0.444363, acc: 0.796875] [adversarial loss: 1.294360, acc: 0.218750]\n",
      "5209: [discriminator loss: 0.547054, acc: 0.726562] [adversarial loss: 1.020496, acc: 0.375000]\n",
      "5210: [discriminator loss: 0.586676, acc: 0.695312] [adversarial loss: 1.412262, acc: 0.125000]\n",
      "5211: [discriminator loss: 0.473194, acc: 0.757812] [adversarial loss: 1.491810, acc: 0.156250]\n",
      "5212: [discriminator loss: 0.507433, acc: 0.734375] [adversarial loss: 1.123071, acc: 0.250000]\n",
      "5213: [discriminator loss: 0.533232, acc: 0.726562] [adversarial loss: 1.517312, acc: 0.109375]\n",
      "5214: [discriminator loss: 0.516778, acc: 0.773438] [adversarial loss: 0.953211, acc: 0.375000]\n",
      "5215: [discriminator loss: 0.612449, acc: 0.687500] [adversarial loss: 1.837226, acc: 0.046875]\n",
      "5216: [discriminator loss: 0.550187, acc: 0.703125] [adversarial loss: 0.968286, acc: 0.328125]\n",
      "5217: [discriminator loss: 0.495975, acc: 0.781250] [adversarial loss: 1.516323, acc: 0.093750]\n",
      "5218: [discriminator loss: 0.597975, acc: 0.679688] [adversarial loss: 1.066293, acc: 0.359375]\n",
      "5219: [discriminator loss: 0.525769, acc: 0.757812] [adversarial loss: 1.156131, acc: 0.265625]\n",
      "5220: [discriminator loss: 0.562987, acc: 0.687500] [adversarial loss: 1.331857, acc: 0.187500]\n",
      "5221: [discriminator loss: 0.595460, acc: 0.640625] [adversarial loss: 1.448833, acc: 0.125000]\n",
      "5222: [discriminator loss: 0.501963, acc: 0.804688] [adversarial loss: 1.342968, acc: 0.156250]\n",
      "5223: [discriminator loss: 0.585753, acc: 0.703125] [adversarial loss: 1.090851, acc: 0.234375]\n",
      "5224: [discriminator loss: 0.526255, acc: 0.742188] [adversarial loss: 1.512162, acc: 0.140625]\n",
      "5225: [discriminator loss: 0.501937, acc: 0.773438] [adversarial loss: 1.075224, acc: 0.203125]\n",
      "5226: [discriminator loss: 0.475400, acc: 0.750000] [adversarial loss: 1.445769, acc: 0.171875]\n",
      "5227: [discriminator loss: 0.510811, acc: 0.757812] [adversarial loss: 1.192248, acc: 0.171875]\n",
      "5228: [discriminator loss: 0.484819, acc: 0.734375] [adversarial loss: 1.166985, acc: 0.250000]\n",
      "5229: [discriminator loss: 0.481095, acc: 0.773438] [adversarial loss: 1.085081, acc: 0.265625]\n",
      "5230: [discriminator loss: 0.570020, acc: 0.718750] [adversarial loss: 1.408103, acc: 0.156250]\n",
      "5231: [discriminator loss: 0.505661, acc: 0.726562] [adversarial loss: 0.961778, acc: 0.343750]\n",
      "5232: [discriminator loss: 0.593122, acc: 0.687500] [adversarial loss: 1.725870, acc: 0.078125]\n",
      "5233: [discriminator loss: 0.502180, acc: 0.710938] [adversarial loss: 1.157764, acc: 0.250000]\n",
      "5234: [discriminator loss: 0.472778, acc: 0.773438] [adversarial loss: 0.882771, acc: 0.406250]\n",
      "5235: [discriminator loss: 0.515490, acc: 0.750000] [adversarial loss: 1.249292, acc: 0.218750]\n",
      "5236: [discriminator loss: 0.516137, acc: 0.742188] [adversarial loss: 1.528648, acc: 0.156250]\n",
      "5237: [discriminator loss: 0.506116, acc: 0.710938] [adversarial loss: 0.761943, acc: 0.546875]\n",
      "5238: [discriminator loss: 0.481273, acc: 0.812500] [adversarial loss: 1.360997, acc: 0.140625]\n",
      "5239: [discriminator loss: 0.516167, acc: 0.695312] [adversarial loss: 1.213581, acc: 0.203125]\n",
      "5240: [discriminator loss: 0.555466, acc: 0.750000] [adversarial loss: 1.312947, acc: 0.140625]\n",
      "5241: [discriminator loss: 0.536192, acc: 0.734375] [adversarial loss: 1.086288, acc: 0.312500]\n",
      "5242: [discriminator loss: 0.482133, acc: 0.773438] [adversarial loss: 1.073053, acc: 0.281250]\n",
      "5243: [discriminator loss: 0.489750, acc: 0.726562] [adversarial loss: 1.613765, acc: 0.125000]\n",
      "5244: [discriminator loss: 0.532786, acc: 0.726562] [adversarial loss: 0.767470, acc: 0.531250]\n",
      "5245: [discriminator loss: 0.577361, acc: 0.710938] [adversarial loss: 1.697077, acc: 0.046875]\n",
      "5246: [discriminator loss: 0.575906, acc: 0.710938] [adversarial loss: 0.813591, acc: 0.468750]\n",
      "5247: [discriminator loss: 0.519469, acc: 0.726562] [adversarial loss: 1.599485, acc: 0.125000]\n",
      "5248: [discriminator loss: 0.507035, acc: 0.773438] [adversarial loss: 1.174682, acc: 0.250000]\n",
      "5249: [discriminator loss: 0.472427, acc: 0.781250] [adversarial loss: 1.405777, acc: 0.171875]\n",
      "5250: [discriminator loss: 0.556778, acc: 0.726562] [adversarial loss: 1.151325, acc: 0.265625]\n",
      "5251: [discriminator loss: 0.550560, acc: 0.695312] [adversarial loss: 1.513382, acc: 0.125000]\n",
      "5252: [discriminator loss: 0.505763, acc: 0.679688] [adversarial loss: 1.276114, acc: 0.203125]\n",
      "5253: [discriminator loss: 0.498907, acc: 0.765625] [adversarial loss: 1.394603, acc: 0.109375]\n",
      "5254: [discriminator loss: 0.575753, acc: 0.671875] [adversarial loss: 1.315233, acc: 0.187500]\n",
      "5255: [discriminator loss: 0.478788, acc: 0.773438] [adversarial loss: 1.251050, acc: 0.265625]\n",
      "5256: [discriminator loss: 0.544822, acc: 0.695312] [adversarial loss: 1.429033, acc: 0.218750]\n",
      "5257: [discriminator loss: 0.629078, acc: 0.656250] [adversarial loss: 1.074188, acc: 0.296875]\n",
      "5258: [discriminator loss: 0.508782, acc: 0.734375] [adversarial loss: 1.402444, acc: 0.140625]\n",
      "5259: [discriminator loss: 0.521142, acc: 0.710938] [adversarial loss: 1.048887, acc: 0.359375]\n",
      "5260: [discriminator loss: 0.596905, acc: 0.710938] [adversarial loss: 1.287605, acc: 0.250000]\n",
      "5261: [discriminator loss: 0.544557, acc: 0.726562] [adversarial loss: 1.245769, acc: 0.203125]\n",
      "5262: [discriminator loss: 0.575129, acc: 0.679688] [adversarial loss: 1.414370, acc: 0.234375]\n",
      "5263: [discriminator loss: 0.605361, acc: 0.703125] [adversarial loss: 0.894190, acc: 0.437500]\n",
      "5264: [discriminator loss: 0.532486, acc: 0.726562] [adversarial loss: 1.465064, acc: 0.109375]\n",
      "5265: [discriminator loss: 0.517380, acc: 0.765625] [adversarial loss: 1.016882, acc: 0.453125]\n",
      "5266: [discriminator loss: 0.463740, acc: 0.781250] [adversarial loss: 1.440114, acc: 0.125000]\n",
      "5267: [discriminator loss: 0.552863, acc: 0.703125] [adversarial loss: 0.971779, acc: 0.343750]\n",
      "5268: [discriminator loss: 0.609621, acc: 0.695312] [adversarial loss: 2.001347, acc: 0.078125]\n",
      "5269: [discriminator loss: 0.541349, acc: 0.726562] [adversarial loss: 1.043532, acc: 0.296875]\n",
      "5270: [discriminator loss: 0.526913, acc: 0.671875] [adversarial loss: 1.407880, acc: 0.156250]\n",
      "5271: [discriminator loss: 0.513602, acc: 0.734375] [adversarial loss: 1.096807, acc: 0.281250]\n",
      "5272: [discriminator loss: 0.535490, acc: 0.718750] [adversarial loss: 0.957952, acc: 0.328125]\n",
      "5273: [discriminator loss: 0.495682, acc: 0.773438] [adversarial loss: 1.437034, acc: 0.125000]\n",
      "5274: [discriminator loss: 0.483530, acc: 0.765625] [adversarial loss: 1.189013, acc: 0.296875]\n",
      "5275: [discriminator loss: 0.468891, acc: 0.796875] [adversarial loss: 1.309803, acc: 0.234375]\n",
      "5276: [discriminator loss: 0.549653, acc: 0.734375] [adversarial loss: 1.057364, acc: 0.265625]\n",
      "5277: [discriminator loss: 0.514925, acc: 0.750000] [adversarial loss: 1.130545, acc: 0.218750]\n",
      "5278: [discriminator loss: 0.494262, acc: 0.789062] [adversarial loss: 1.014369, acc: 0.328125]\n",
      "5279: [discriminator loss: 0.550778, acc: 0.679688] [adversarial loss: 1.722324, acc: 0.062500]\n",
      "5280: [discriminator loss: 0.571930, acc: 0.734375] [adversarial loss: 0.858077, acc: 0.453125]\n",
      "5281: [discriminator loss: 0.663143, acc: 0.625000] [adversarial loss: 1.348768, acc: 0.156250]\n",
      "5282: [discriminator loss: 0.549497, acc: 0.726562] [adversarial loss: 1.315694, acc: 0.156250]\n",
      "5283: [discriminator loss: 0.496708, acc: 0.742188] [adversarial loss: 1.283545, acc: 0.281250]\n",
      "5284: [discriminator loss: 0.530941, acc: 0.703125] [adversarial loss: 0.878313, acc: 0.390625]\n",
      "5285: [discriminator loss: 0.466497, acc: 0.765625] [adversarial loss: 1.429097, acc: 0.171875]\n",
      "5286: [discriminator loss: 0.505990, acc: 0.750000] [adversarial loss: 1.314653, acc: 0.171875]\n",
      "5287: [discriminator loss: 0.453552, acc: 0.765625] [adversarial loss: 1.439770, acc: 0.187500]\n",
      "5288: [discriminator loss: 0.517583, acc: 0.750000] [adversarial loss: 1.346691, acc: 0.156250]\n",
      "5289: [discriminator loss: 0.540347, acc: 0.726562] [adversarial loss: 1.415339, acc: 0.250000]\n",
      "5290: [discriminator loss: 0.497064, acc: 0.765625] [adversarial loss: 0.971884, acc: 0.312500]\n",
      "5291: [discriminator loss: 0.558717, acc: 0.687500] [adversarial loss: 1.232809, acc: 0.250000]\n",
      "5292: [discriminator loss: 0.553255, acc: 0.765625] [adversarial loss: 1.305132, acc: 0.203125]\n",
      "5293: [discriminator loss: 0.523953, acc: 0.718750] [adversarial loss: 0.934460, acc: 0.343750]\n",
      "5294: [discriminator loss: 0.523896, acc: 0.726562] [adversarial loss: 1.730393, acc: 0.109375]\n",
      "5295: [discriminator loss: 0.548219, acc: 0.718750] [adversarial loss: 0.648533, acc: 0.609375]\n",
      "5296: [discriminator loss: 0.583108, acc: 0.695312] [adversarial loss: 1.915460, acc: 0.015625]\n",
      "5297: [discriminator loss: 0.549553, acc: 0.734375] [adversarial loss: 0.950628, acc: 0.343750]\n",
      "5298: [discriminator loss: 0.522348, acc: 0.726562] [adversarial loss: 1.677832, acc: 0.078125]\n",
      "5299: [discriminator loss: 0.600538, acc: 0.664062] [adversarial loss: 0.858193, acc: 0.390625]\n",
      "5300: [discriminator loss: 0.460168, acc: 0.726562] [adversarial loss: 1.350738, acc: 0.140625]\n",
      "5301: [discriminator loss: 0.563997, acc: 0.695312] [adversarial loss: 1.125822, acc: 0.218750]\n",
      "5302: [discriminator loss: 0.513823, acc: 0.710938] [adversarial loss: 1.196992, acc: 0.265625]\n",
      "5303: [discriminator loss: 0.529034, acc: 0.750000] [adversarial loss: 1.342645, acc: 0.234375]\n",
      "5304: [discriminator loss: 0.610274, acc: 0.718750] [adversarial loss: 1.057953, acc: 0.296875]\n",
      "5305: [discriminator loss: 0.479537, acc: 0.742188] [adversarial loss: 1.309489, acc: 0.140625]\n",
      "5306: [discriminator loss: 0.468397, acc: 0.773438] [adversarial loss: 1.112774, acc: 0.234375]\n",
      "5307: [discriminator loss: 0.487581, acc: 0.765625] [adversarial loss: 1.510033, acc: 0.125000]\n",
      "5308: [discriminator loss: 0.477337, acc: 0.750000] [adversarial loss: 1.081020, acc: 0.328125]\n",
      "5309: [discriminator loss: 0.557763, acc: 0.710938] [adversarial loss: 1.299592, acc: 0.234375]\n",
      "5310: [discriminator loss: 0.467912, acc: 0.773438] [adversarial loss: 0.960093, acc: 0.359375]\n",
      "5311: [discriminator loss: 0.478974, acc: 0.781250] [adversarial loss: 1.247119, acc: 0.265625]\n",
      "5312: [discriminator loss: 0.531697, acc: 0.757812] [adversarial loss: 1.132623, acc: 0.343750]\n",
      "5313: [discriminator loss: 0.506626, acc: 0.796875] [adversarial loss: 1.279540, acc: 0.250000]\n",
      "5314: [discriminator loss: 0.536071, acc: 0.750000] [adversarial loss: 1.358305, acc: 0.203125]\n",
      "5315: [discriminator loss: 0.468555, acc: 0.781250] [adversarial loss: 1.358066, acc: 0.187500]\n",
      "5316: [discriminator loss: 0.489805, acc: 0.765625] [adversarial loss: 0.987376, acc: 0.312500]\n",
      "5317: [discriminator loss: 0.464252, acc: 0.796875] [adversarial loss: 1.467893, acc: 0.140625]\n",
      "5318: [discriminator loss: 0.490246, acc: 0.773438] [adversarial loss: 0.888196, acc: 0.390625]\n",
      "5319: [discriminator loss: 0.542037, acc: 0.718750] [adversarial loss: 1.475385, acc: 0.187500]\n",
      "5320: [discriminator loss: 0.536347, acc: 0.726562] [adversarial loss: 1.004470, acc: 0.343750]\n",
      "5321: [discriminator loss: 0.515664, acc: 0.750000] [adversarial loss: 1.384560, acc: 0.140625]\n",
      "5322: [discriminator loss: 0.525841, acc: 0.718750] [adversarial loss: 0.997829, acc: 0.343750]\n",
      "5323: [discriminator loss: 0.518592, acc: 0.695312] [adversarial loss: 1.142817, acc: 0.250000]\n",
      "5324: [discriminator loss: 0.519639, acc: 0.742188] [adversarial loss: 1.488470, acc: 0.187500]\n",
      "5325: [discriminator loss: 0.588704, acc: 0.695312] [adversarial loss: 1.182523, acc: 0.296875]\n",
      "5326: [discriminator loss: 0.564387, acc: 0.679688] [adversarial loss: 1.601623, acc: 0.171875]\n",
      "5327: [discriminator loss: 0.551443, acc: 0.734375] [adversarial loss: 1.135746, acc: 0.187500]\n",
      "5328: [discriminator loss: 0.566530, acc: 0.710938] [adversarial loss: 1.473837, acc: 0.156250]\n",
      "5329: [discriminator loss: 0.564008, acc: 0.703125] [adversarial loss: 0.922838, acc: 0.437500]\n",
      "5330: [discriminator loss: 0.515862, acc: 0.726562] [adversarial loss: 1.694325, acc: 0.093750]\n",
      "5331: [discriminator loss: 0.536729, acc: 0.726562] [adversarial loss: 0.909831, acc: 0.421875]\n",
      "5332: [discriminator loss: 0.558527, acc: 0.656250] [adversarial loss: 2.057136, acc: 0.093750]\n",
      "5333: [discriminator loss: 0.489891, acc: 0.781250] [adversarial loss: 1.331458, acc: 0.203125]\n",
      "5334: [discriminator loss: 0.545140, acc: 0.734375] [adversarial loss: 1.149373, acc: 0.281250]\n",
      "5335: [discriminator loss: 0.530821, acc: 0.726562] [adversarial loss: 1.357565, acc: 0.156250]\n",
      "5336: [discriminator loss: 0.499916, acc: 0.757812] [adversarial loss: 1.208349, acc: 0.234375]\n",
      "5337: [discriminator loss: 0.459987, acc: 0.796875] [adversarial loss: 1.315239, acc: 0.140625]\n",
      "5338: [discriminator loss: 0.493071, acc: 0.757812] [adversarial loss: 1.219902, acc: 0.203125]\n",
      "5339: [discriminator loss: 0.532199, acc: 0.726562] [adversarial loss: 1.194381, acc: 0.281250]\n",
      "5340: [discriminator loss: 0.478948, acc: 0.789062] [adversarial loss: 1.190001, acc: 0.187500]\n",
      "5341: [discriminator loss: 0.505441, acc: 0.750000] [adversarial loss: 1.327499, acc: 0.203125]\n",
      "5342: [discriminator loss: 0.448362, acc: 0.773438] [adversarial loss: 1.093750, acc: 0.218750]\n",
      "5343: [discriminator loss: 0.519381, acc: 0.742188] [adversarial loss: 1.241705, acc: 0.203125]\n",
      "5344: [discriminator loss: 0.516080, acc: 0.742188] [adversarial loss: 1.534462, acc: 0.140625]\n",
      "5345: [discriminator loss: 0.642647, acc: 0.703125] [adversarial loss: 0.815436, acc: 0.484375]\n",
      "5346: [discriminator loss: 0.447863, acc: 0.812500] [adversarial loss: 1.682345, acc: 0.093750]\n",
      "5347: [discriminator loss: 0.484657, acc: 0.757812] [adversarial loss: 1.196731, acc: 0.203125]\n",
      "5348: [discriminator loss: 0.522276, acc: 0.726562] [adversarial loss: 1.432895, acc: 0.171875]\n",
      "5349: [discriminator loss: 0.529267, acc: 0.734375] [adversarial loss: 1.092523, acc: 0.328125]\n",
      "5350: [discriminator loss: 0.518995, acc: 0.773438] [adversarial loss: 1.527101, acc: 0.078125]\n",
      "5351: [discriminator loss: 0.591485, acc: 0.687500] [adversarial loss: 0.944496, acc: 0.453125]\n",
      "5352: [discriminator loss: 0.550605, acc: 0.734375] [adversarial loss: 1.636166, acc: 0.093750]\n",
      "5353: [discriminator loss: 0.559334, acc: 0.687500] [adversarial loss: 1.570293, acc: 0.093750]\n",
      "5354: [discriminator loss: 0.566900, acc: 0.703125] [adversarial loss: 0.881746, acc: 0.343750]\n",
      "5355: [discriminator loss: 0.522664, acc: 0.710938] [adversarial loss: 1.350072, acc: 0.125000]\n",
      "5356: [discriminator loss: 0.502724, acc: 0.757812] [adversarial loss: 0.969932, acc: 0.468750]\n",
      "5357: [discriminator loss: 0.555760, acc: 0.734375] [adversarial loss: 1.783046, acc: 0.109375]\n",
      "5358: [discriminator loss: 0.478947, acc: 0.726562] [adversarial loss: 1.188033, acc: 0.218750]\n",
      "5359: [discriminator loss: 0.543941, acc: 0.703125] [adversarial loss: 1.250192, acc: 0.265625]\n",
      "5360: [discriminator loss: 0.513558, acc: 0.757812] [adversarial loss: 1.322435, acc: 0.109375]\n",
      "5361: [discriminator loss: 0.528146, acc: 0.781250] [adversarial loss: 1.247281, acc: 0.187500]\n",
      "5362: [discriminator loss: 0.510404, acc: 0.750000] [adversarial loss: 1.261541, acc: 0.171875]\n",
      "5363: [discriminator loss: 0.557800, acc: 0.710938] [adversarial loss: 1.250468, acc: 0.250000]\n",
      "5364: [discriminator loss: 0.500027, acc: 0.726562] [adversarial loss: 0.999867, acc: 0.234375]\n",
      "5365: [discriminator loss: 0.431575, acc: 0.781250] [adversarial loss: 1.234771, acc: 0.234375]\n",
      "5366: [discriminator loss: 0.470613, acc: 0.773438] [adversarial loss: 1.453291, acc: 0.125000]\n",
      "5367: [discriminator loss: 0.492478, acc: 0.750000] [adversarial loss: 0.913787, acc: 0.421875]\n",
      "5368: [discriminator loss: 0.569989, acc: 0.695312] [adversarial loss: 1.602874, acc: 0.109375]\n",
      "5369: [discriminator loss: 0.530526, acc: 0.710938] [adversarial loss: 0.919874, acc: 0.359375]\n",
      "5370: [discriminator loss: 0.526556, acc: 0.718750] [adversarial loss: 2.006976, acc: 0.062500]\n",
      "5371: [discriminator loss: 0.679688, acc: 0.648438] [adversarial loss: 0.817134, acc: 0.484375]\n",
      "5372: [discriminator loss: 0.536072, acc: 0.742188] [adversarial loss: 1.606610, acc: 0.093750]\n",
      "5373: [discriminator loss: 0.607414, acc: 0.695312] [adversarial loss: 1.040536, acc: 0.296875]\n",
      "5374: [discriminator loss: 0.525620, acc: 0.742188] [adversarial loss: 1.317802, acc: 0.125000]\n",
      "5375: [discriminator loss: 0.552211, acc: 0.718750] [adversarial loss: 1.049303, acc: 0.250000]\n",
      "5376: [discriminator loss: 0.543640, acc: 0.734375] [adversarial loss: 1.314655, acc: 0.171875]\n",
      "5377: [discriminator loss: 0.489097, acc: 0.742188] [adversarial loss: 1.047125, acc: 0.312500]\n",
      "5378: [discriminator loss: 0.566236, acc: 0.718750] [adversarial loss: 1.504131, acc: 0.140625]\n",
      "5379: [discriminator loss: 0.541252, acc: 0.726562] [adversarial loss: 0.961027, acc: 0.406250]\n",
      "5380: [discriminator loss: 0.565110, acc: 0.734375] [adversarial loss: 1.541628, acc: 0.140625]\n",
      "5381: [discriminator loss: 0.491895, acc: 0.742188] [adversarial loss: 1.010046, acc: 0.281250]\n",
      "5382: [discriminator loss: 0.486219, acc: 0.789062] [adversarial loss: 1.295269, acc: 0.250000]\n",
      "5383: [discriminator loss: 0.605428, acc: 0.664062] [adversarial loss: 1.097934, acc: 0.359375]\n",
      "5384: [discriminator loss: 0.527123, acc: 0.726562] [adversarial loss: 1.192707, acc: 0.265625]\n",
      "5385: [discriminator loss: 0.449155, acc: 0.781250] [adversarial loss: 1.092237, acc: 0.296875]\n",
      "5386: [discriminator loss: 0.561448, acc: 0.742188] [adversarial loss: 1.435919, acc: 0.187500]\n",
      "5387: [discriminator loss: 0.572935, acc: 0.695312] [adversarial loss: 0.892261, acc: 0.375000]\n",
      "5388: [discriminator loss: 0.503784, acc: 0.765625] [adversarial loss: 1.493229, acc: 0.109375]\n",
      "5389: [discriminator loss: 0.565438, acc: 0.710938] [adversarial loss: 1.069879, acc: 0.312500]\n",
      "5390: [discriminator loss: 0.501431, acc: 0.734375] [adversarial loss: 1.150718, acc: 0.296875]\n",
      "5391: [discriminator loss: 0.472664, acc: 0.828125] [adversarial loss: 1.371896, acc: 0.078125]\n",
      "5392: [discriminator loss: 0.613375, acc: 0.695312] [adversarial loss: 1.014672, acc: 0.312500]\n",
      "5393: [discriminator loss: 0.625888, acc: 0.671875] [adversarial loss: 1.801391, acc: 0.078125]\n",
      "5394: [discriminator loss: 0.632329, acc: 0.664062] [adversarial loss: 0.983606, acc: 0.359375]\n",
      "5395: [discriminator loss: 0.456927, acc: 0.750000] [adversarial loss: 1.677752, acc: 0.140625]\n",
      "5396: [discriminator loss: 0.576233, acc: 0.703125] [adversarial loss: 0.913346, acc: 0.421875]\n",
      "5397: [discriminator loss: 0.584425, acc: 0.695312] [adversarial loss: 1.674553, acc: 0.078125]\n",
      "5398: [discriminator loss: 0.509741, acc: 0.703125] [adversarial loss: 0.979786, acc: 0.265625]\n",
      "5399: [discriminator loss: 0.572302, acc: 0.695312] [adversarial loss: 1.358084, acc: 0.187500]\n",
      "5400: [discriminator loss: 0.565188, acc: 0.695312] [adversarial loss: 1.097098, acc: 0.234375]\n",
      "5401: [discriminator loss: 0.530631, acc: 0.742188] [adversarial loss: 1.106457, acc: 0.265625]\n",
      "5402: [discriminator loss: 0.532211, acc: 0.734375] [adversarial loss: 0.975840, acc: 0.406250]\n",
      "5403: [discriminator loss: 0.539187, acc: 0.726562] [adversarial loss: 1.300454, acc: 0.187500]\n",
      "5404: [discriminator loss: 0.508030, acc: 0.750000] [adversarial loss: 1.248654, acc: 0.218750]\n",
      "5405: [discriminator loss: 0.445043, acc: 0.781250] [adversarial loss: 1.651099, acc: 0.093750]\n",
      "5406: [discriminator loss: 0.585924, acc: 0.664062] [adversarial loss: 0.855965, acc: 0.453125]\n",
      "5407: [discriminator loss: 0.589568, acc: 0.656250] [adversarial loss: 1.638473, acc: 0.093750]\n",
      "5408: [discriminator loss: 0.480512, acc: 0.773438] [adversarial loss: 1.237770, acc: 0.187500]\n",
      "5409: [discriminator loss: 0.547819, acc: 0.718750] [adversarial loss: 1.159766, acc: 0.265625]\n",
      "5410: [discriminator loss: 0.530605, acc: 0.734375] [adversarial loss: 1.079465, acc: 0.328125]\n",
      "5411: [discriminator loss: 0.617347, acc: 0.585938] [adversarial loss: 1.360095, acc: 0.140625]\n",
      "5412: [discriminator loss: 0.473903, acc: 0.812500] [adversarial loss: 1.261950, acc: 0.156250]\n",
      "5413: [discriminator loss: 0.533527, acc: 0.757812] [adversarial loss: 0.861498, acc: 0.437500]\n",
      "5414: [discriminator loss: 0.549534, acc: 0.695312] [adversarial loss: 1.229707, acc: 0.140625]\n",
      "5415: [discriminator loss: 0.513563, acc: 0.742188] [adversarial loss: 0.943889, acc: 0.390625]\n",
      "5416: [discriminator loss: 0.577569, acc: 0.695312] [adversarial loss: 1.434328, acc: 0.093750]\n",
      "5417: [discriminator loss: 0.510280, acc: 0.726562] [adversarial loss: 1.092177, acc: 0.281250]\n",
      "5418: [discriminator loss: 0.514720, acc: 0.773438] [adversarial loss: 1.139691, acc: 0.296875]\n",
      "5419: [discriminator loss: 0.546905, acc: 0.718750] [adversarial loss: 1.492637, acc: 0.125000]\n",
      "5420: [discriminator loss: 0.569173, acc: 0.710938] [adversarial loss: 0.737341, acc: 0.531250]\n",
      "5421: [discriminator loss: 0.516066, acc: 0.757812] [adversarial loss: 1.691011, acc: 0.031250]\n",
      "5422: [discriminator loss: 0.498427, acc: 0.750000] [adversarial loss: 1.160280, acc: 0.281250]\n",
      "5423: [discriminator loss: 0.468428, acc: 0.757812] [adversarial loss: 1.478597, acc: 0.203125]\n",
      "5424: [discriminator loss: 0.463158, acc: 0.789062] [adversarial loss: 1.120384, acc: 0.218750]\n",
      "5425: [discriminator loss: 0.508599, acc: 0.773438] [adversarial loss: 1.274636, acc: 0.234375]\n",
      "5426: [discriminator loss: 0.538905, acc: 0.718750] [adversarial loss: 1.710964, acc: 0.062500]\n",
      "5427: [discriminator loss: 0.500048, acc: 0.796875] [adversarial loss: 0.981985, acc: 0.390625]\n",
      "5428: [discriminator loss: 0.514005, acc: 0.742188] [adversarial loss: 1.274165, acc: 0.218750]\n",
      "5429: [discriminator loss: 0.527184, acc: 0.765625] [adversarial loss: 1.373585, acc: 0.203125]\n",
      "5430: [discriminator loss: 0.512189, acc: 0.781250] [adversarial loss: 1.334401, acc: 0.218750]\n",
      "5431: [discriminator loss: 0.485639, acc: 0.781250] [adversarial loss: 1.181162, acc: 0.218750]\n",
      "5432: [discriminator loss: 0.522763, acc: 0.750000] [adversarial loss: 1.401165, acc: 0.218750]\n",
      "5433: [discriminator loss: 0.525000, acc: 0.718750] [adversarial loss: 1.163499, acc: 0.250000]\n",
      "5434: [discriminator loss: 0.568888, acc: 0.695312] [adversarial loss: 1.147026, acc: 0.281250]\n",
      "5435: [discriminator loss: 0.549970, acc: 0.726562] [adversarial loss: 1.044410, acc: 0.421875]\n",
      "5436: [discriminator loss: 0.604094, acc: 0.703125] [adversarial loss: 1.705792, acc: 0.156250]\n",
      "5437: [discriminator loss: 0.637395, acc: 0.656250] [adversarial loss: 1.142584, acc: 0.250000]\n",
      "5438: [discriminator loss: 0.533722, acc: 0.718750] [adversarial loss: 1.437359, acc: 0.140625]\n",
      "5439: [discriminator loss: 0.558397, acc: 0.726562] [adversarial loss: 1.257288, acc: 0.296875]\n",
      "5440: [discriminator loss: 0.553908, acc: 0.734375] [adversarial loss: 1.264273, acc: 0.203125]\n",
      "5441: [discriminator loss: 0.573604, acc: 0.695312] [adversarial loss: 1.076679, acc: 0.250000]\n",
      "5442: [discriminator loss: 0.581352, acc: 0.695312] [adversarial loss: 1.646992, acc: 0.046875]\n",
      "5443: [discriminator loss: 0.564823, acc: 0.703125] [adversarial loss: 0.964817, acc: 0.296875]\n",
      "5444: [discriminator loss: 0.537703, acc: 0.710938] [adversarial loss: 1.626375, acc: 0.078125]\n",
      "5445: [discriminator loss: 0.536584, acc: 0.750000] [adversarial loss: 1.322230, acc: 0.281250]\n",
      "5446: [discriminator loss: 0.593231, acc: 0.664062] [adversarial loss: 1.314716, acc: 0.156250]\n",
      "5447: [discriminator loss: 0.517104, acc: 0.726562] [adversarial loss: 1.625654, acc: 0.078125]\n",
      "5448: [discriminator loss: 0.466631, acc: 0.742188] [adversarial loss: 1.199332, acc: 0.218750]\n",
      "5449: [discriminator loss: 0.456903, acc: 0.781250] [adversarial loss: 1.423894, acc: 0.125000]\n",
      "5450: [discriminator loss: 0.514577, acc: 0.734375] [adversarial loss: 1.095103, acc: 0.234375]\n",
      "5451: [discriminator loss: 0.575925, acc: 0.710938] [adversarial loss: 1.576991, acc: 0.125000]\n",
      "5452: [discriminator loss: 0.592596, acc: 0.695312] [adversarial loss: 0.934298, acc: 0.390625]\n",
      "5453: [discriminator loss: 0.533695, acc: 0.734375] [adversarial loss: 1.510827, acc: 0.140625]\n",
      "5454: [discriminator loss: 0.580857, acc: 0.687500] [adversarial loss: 1.101259, acc: 0.375000]\n",
      "5455: [discriminator loss: 0.547875, acc: 0.734375] [adversarial loss: 1.480575, acc: 0.140625]\n",
      "5456: [discriminator loss: 0.533271, acc: 0.710938] [adversarial loss: 1.099598, acc: 0.328125]\n",
      "5457: [discriminator loss: 0.453005, acc: 0.843750] [adversarial loss: 1.548224, acc: 0.109375]\n",
      "5458: [discriminator loss: 0.613996, acc: 0.632812] [adversarial loss: 0.900504, acc: 0.406250]\n",
      "5459: [discriminator loss: 0.638099, acc: 0.632812] [adversarial loss: 1.391235, acc: 0.125000]\n",
      "5460: [discriminator loss: 0.480926, acc: 0.781250] [adversarial loss: 0.962584, acc: 0.375000]\n",
      "5461: [discriminator loss: 0.496175, acc: 0.757812] [adversarial loss: 1.589098, acc: 0.125000]\n",
      "5462: [discriminator loss: 0.563351, acc: 0.687500] [adversarial loss: 1.083617, acc: 0.265625]\n",
      "5463: [discriminator loss: 0.515152, acc: 0.726562] [adversarial loss: 1.303057, acc: 0.125000]\n",
      "5464: [discriminator loss: 0.577264, acc: 0.710938] [adversarial loss: 0.751381, acc: 0.562500]\n",
      "5465: [discriminator loss: 0.492695, acc: 0.742188] [adversarial loss: 1.790345, acc: 0.015625]\n",
      "5466: [discriminator loss: 0.558465, acc: 0.710938] [adversarial loss: 0.921208, acc: 0.375000]\n",
      "5467: [discriminator loss: 0.488165, acc: 0.765625] [adversarial loss: 1.401156, acc: 0.109375]\n",
      "5468: [discriminator loss: 0.512284, acc: 0.750000] [adversarial loss: 1.325754, acc: 0.156250]\n",
      "5469: [discriminator loss: 0.559148, acc: 0.734375] [adversarial loss: 1.109519, acc: 0.281250]\n",
      "5470: [discriminator loss: 0.565432, acc: 0.750000] [adversarial loss: 0.955910, acc: 0.421875]\n",
      "5471: [discriminator loss: 0.513760, acc: 0.734375] [adversarial loss: 1.116045, acc: 0.250000]\n",
      "5472: [discriminator loss: 0.542364, acc: 0.718750] [adversarial loss: 1.206292, acc: 0.218750]\n",
      "5473: [discriminator loss: 0.577803, acc: 0.679688] [adversarial loss: 1.073735, acc: 0.312500]\n",
      "5474: [discriminator loss: 0.535783, acc: 0.718750] [adversarial loss: 1.370809, acc: 0.203125]\n",
      "5475: [discriminator loss: 0.590860, acc: 0.718750] [adversarial loss: 1.175584, acc: 0.265625]\n",
      "5476: [discriminator loss: 0.511633, acc: 0.726562] [adversarial loss: 1.414759, acc: 0.140625]\n",
      "5477: [discriminator loss: 0.615515, acc: 0.695312] [adversarial loss: 0.788288, acc: 0.484375]\n",
      "5478: [discriminator loss: 0.527870, acc: 0.710938] [adversarial loss: 1.734078, acc: 0.093750]\n",
      "5479: [discriminator loss: 0.654329, acc: 0.632812] [adversarial loss: 0.840169, acc: 0.453125]\n",
      "5480: [discriminator loss: 0.585544, acc: 0.687500] [adversarial loss: 1.627270, acc: 0.093750]\n",
      "5481: [discriminator loss: 0.545807, acc: 0.718750] [adversarial loss: 1.089170, acc: 0.312500]\n",
      "5482: [discriminator loss: 0.500686, acc: 0.781250] [adversarial loss: 1.431923, acc: 0.125000]\n",
      "5483: [discriminator loss: 0.423128, acc: 0.804688] [adversarial loss: 1.175030, acc: 0.250000]\n",
      "5484: [discriminator loss: 0.536131, acc: 0.742188] [adversarial loss: 1.102572, acc: 0.281250]\n",
      "5485: [discriminator loss: 0.476921, acc: 0.781250] [adversarial loss: 1.012834, acc: 0.296875]\n",
      "5486: [discriminator loss: 0.477667, acc: 0.765625] [adversarial loss: 1.306835, acc: 0.187500]\n",
      "5487: [discriminator loss: 0.540603, acc: 0.710938] [adversarial loss: 1.193880, acc: 0.203125]\n",
      "5488: [discriminator loss: 0.533093, acc: 0.726562] [adversarial loss: 1.269069, acc: 0.343750]\n",
      "5489: [discriminator loss: 0.571252, acc: 0.718750] [adversarial loss: 1.668367, acc: 0.109375]\n",
      "5490: [discriminator loss: 0.482191, acc: 0.726562] [adversarial loss: 1.069786, acc: 0.359375]\n",
      "5491: [discriminator loss: 0.588423, acc: 0.625000] [adversarial loss: 1.567548, acc: 0.078125]\n",
      "5492: [discriminator loss: 0.492763, acc: 0.671875] [adversarial loss: 1.069566, acc: 0.281250]\n",
      "5493: [discriminator loss: 0.485869, acc: 0.781250] [adversarial loss: 1.531531, acc: 0.171875]\n",
      "5494: [discriminator loss: 0.501935, acc: 0.757812] [adversarial loss: 0.924450, acc: 0.500000]\n",
      "5495: [discriminator loss: 0.487161, acc: 0.765625] [adversarial loss: 1.621492, acc: 0.140625]\n",
      "5496: [discriminator loss: 0.729644, acc: 0.617188] [adversarial loss: 1.048813, acc: 0.296875]\n",
      "5497: [discriminator loss: 0.514042, acc: 0.718750] [adversarial loss: 1.333695, acc: 0.140625]\n",
      "5498: [discriminator loss: 0.482511, acc: 0.781250] [adversarial loss: 0.984214, acc: 0.343750]\n",
      "5499: [discriminator loss: 0.543271, acc: 0.750000] [adversarial loss: 1.630058, acc: 0.046875]\n",
      "5500: [discriminator loss: 0.515259, acc: 0.703125] [adversarial loss: 1.194204, acc: 0.187500]\n",
      "5501: [discriminator loss: 0.576357, acc: 0.695312] [adversarial loss: 1.389062, acc: 0.156250]\n",
      "5502: [discriminator loss: 0.586960, acc: 0.734375] [adversarial loss: 1.002427, acc: 0.296875]\n",
      "5503: [discriminator loss: 0.555702, acc: 0.710938] [adversarial loss: 1.454575, acc: 0.171875]\n",
      "5504: [discriminator loss: 0.542679, acc: 0.726562] [adversarial loss: 0.954169, acc: 0.375000]\n",
      "5505: [discriminator loss: 0.527584, acc: 0.742188] [adversarial loss: 1.183650, acc: 0.218750]\n",
      "5506: [discriminator loss: 0.522678, acc: 0.726562] [adversarial loss: 1.148965, acc: 0.281250]\n",
      "5507: [discriminator loss: 0.528021, acc: 0.765625] [adversarial loss: 1.373172, acc: 0.125000]\n",
      "5508: [discriminator loss: 0.554658, acc: 0.750000] [adversarial loss: 0.881950, acc: 0.437500]\n",
      "5509: [discriminator loss: 0.530036, acc: 0.710938] [adversarial loss: 1.488371, acc: 0.109375]\n",
      "5510: [discriminator loss: 0.526802, acc: 0.703125] [adversarial loss: 0.855356, acc: 0.453125]\n",
      "5511: [discriminator loss: 0.469550, acc: 0.773438] [adversarial loss: 1.337735, acc: 0.156250]\n",
      "5512: [discriminator loss: 0.557588, acc: 0.710938] [adversarial loss: 1.179839, acc: 0.312500]\n",
      "5513: [discriminator loss: 0.559641, acc: 0.710938] [adversarial loss: 1.634761, acc: 0.109375]\n",
      "5514: [discriminator loss: 0.527986, acc: 0.734375] [adversarial loss: 0.988808, acc: 0.390625]\n",
      "5515: [discriminator loss: 0.533788, acc: 0.726562] [adversarial loss: 1.537100, acc: 0.093750]\n",
      "5516: [discriminator loss: 0.532576, acc: 0.726562] [adversarial loss: 0.891059, acc: 0.421875]\n",
      "5517: [discriminator loss: 0.502057, acc: 0.781250] [adversarial loss: 1.256596, acc: 0.281250]\n",
      "5518: [discriminator loss: 0.450934, acc: 0.812500] [adversarial loss: 1.061750, acc: 0.312500]\n",
      "5519: [discriminator loss: 0.610036, acc: 0.648438] [adversarial loss: 1.169278, acc: 0.250000]\n",
      "5520: [discriminator loss: 0.551264, acc: 0.718750] [adversarial loss: 1.304953, acc: 0.203125]\n",
      "5521: [discriminator loss: 0.464277, acc: 0.789062] [adversarial loss: 1.420868, acc: 0.140625]\n",
      "5522: [discriminator loss: 0.540554, acc: 0.671875] [adversarial loss: 1.287583, acc: 0.265625]\n",
      "5523: [discriminator loss: 0.579278, acc: 0.726562] [adversarial loss: 1.823928, acc: 0.125000]\n",
      "5524: [discriminator loss: 0.489647, acc: 0.734375] [adversarial loss: 1.092659, acc: 0.359375]\n",
      "5525: [discriminator loss: 0.548108, acc: 0.742188] [adversarial loss: 1.338970, acc: 0.187500]\n",
      "5526: [discriminator loss: 0.606797, acc: 0.679688] [adversarial loss: 1.142140, acc: 0.343750]\n",
      "5527: [discriminator loss: 0.576879, acc: 0.726562] [adversarial loss: 1.295298, acc: 0.203125]\n",
      "5528: [discriminator loss: 0.563301, acc: 0.695312] [adversarial loss: 1.039748, acc: 0.375000]\n",
      "5529: [discriminator loss: 0.504721, acc: 0.757812] [adversarial loss: 1.253554, acc: 0.140625]\n",
      "5530: [discriminator loss: 0.551378, acc: 0.703125] [adversarial loss: 0.949290, acc: 0.390625]\n",
      "5531: [discriminator loss: 0.541502, acc: 0.742188] [adversarial loss: 1.638009, acc: 0.062500]\n",
      "5532: [discriminator loss: 0.499833, acc: 0.757812] [adversarial loss: 0.912582, acc: 0.421875]\n",
      "5533: [discriminator loss: 0.621061, acc: 0.664062] [adversarial loss: 1.636826, acc: 0.062500]\n",
      "5534: [discriminator loss: 0.531051, acc: 0.757812] [adversarial loss: 0.945787, acc: 0.406250]\n",
      "5535: [discriminator loss: 0.545922, acc: 0.718750] [adversarial loss: 1.307177, acc: 0.203125]\n",
      "5536: [discriminator loss: 0.513412, acc: 0.718750] [adversarial loss: 1.045642, acc: 0.281250]\n",
      "5537: [discriminator loss: 0.531619, acc: 0.773438] [adversarial loss: 1.280578, acc: 0.125000]\n",
      "5538: [discriminator loss: 0.543941, acc: 0.726562] [adversarial loss: 0.822281, acc: 0.500000]\n",
      "5539: [discriminator loss: 0.611578, acc: 0.671875] [adversarial loss: 1.761644, acc: 0.031250]\n",
      "5540: [discriminator loss: 0.608083, acc: 0.695312] [adversarial loss: 0.950887, acc: 0.390625]\n",
      "5541: [discriminator loss: 0.601861, acc: 0.664062] [adversarial loss: 1.440370, acc: 0.109375]\n",
      "5542: [discriminator loss: 0.543300, acc: 0.726562] [adversarial loss: 1.255657, acc: 0.125000]\n",
      "5543: [discriminator loss: 0.494592, acc: 0.773438] [adversarial loss: 0.889678, acc: 0.421875]\n",
      "5544: [discriminator loss: 0.470309, acc: 0.750000] [adversarial loss: 1.419076, acc: 0.125000]\n",
      "5545: [discriminator loss: 0.537444, acc: 0.726562] [adversarial loss: 1.146117, acc: 0.203125]\n",
      "5546: [discriminator loss: 0.576424, acc: 0.718750] [adversarial loss: 1.466705, acc: 0.109375]\n",
      "5547: [discriminator loss: 0.508391, acc: 0.726562] [adversarial loss: 0.905925, acc: 0.359375]\n",
      "5548: [discriminator loss: 0.649383, acc: 0.578125] [adversarial loss: 1.284044, acc: 0.203125]\n",
      "5549: [discriminator loss: 0.535810, acc: 0.695312] [adversarial loss: 1.178030, acc: 0.203125]\n",
      "5550: [discriminator loss: 0.594977, acc: 0.656250] [adversarial loss: 1.360493, acc: 0.203125]\n",
      "5551: [discriminator loss: 0.529436, acc: 0.695312] [adversarial loss: 1.479970, acc: 0.078125]\n",
      "5552: [discriminator loss: 0.504106, acc: 0.734375] [adversarial loss: 0.913456, acc: 0.375000]\n",
      "5553: [discriminator loss: 0.654211, acc: 0.726562] [adversarial loss: 1.402780, acc: 0.171875]\n",
      "5554: [discriminator loss: 0.588495, acc: 0.687500] [adversarial loss: 1.379903, acc: 0.109375]\n",
      "5555: [discriminator loss: 0.573129, acc: 0.695312] [adversarial loss: 0.948585, acc: 0.343750]\n",
      "5556: [discriminator loss: 0.550869, acc: 0.703125] [adversarial loss: 1.529207, acc: 0.140625]\n",
      "5557: [discriminator loss: 0.508752, acc: 0.695312] [adversarial loss: 0.984946, acc: 0.328125]\n",
      "5558: [discriminator loss: 0.496016, acc: 0.742188] [adversarial loss: 1.489943, acc: 0.125000]\n",
      "5559: [discriminator loss: 0.561103, acc: 0.703125] [adversarial loss: 0.908518, acc: 0.406250]\n",
      "5560: [discriminator loss: 0.561557, acc: 0.695312] [adversarial loss: 1.418220, acc: 0.109375]\n",
      "5561: [discriminator loss: 0.577482, acc: 0.687500] [adversarial loss: 0.960394, acc: 0.328125]\n",
      "5562: [discriminator loss: 0.586745, acc: 0.695312] [adversarial loss: 1.474207, acc: 0.093750]\n",
      "5563: [discriminator loss: 0.575540, acc: 0.664062] [adversarial loss: 0.921590, acc: 0.375000]\n",
      "5564: [discriminator loss: 0.501081, acc: 0.710938] [adversarial loss: 1.356570, acc: 0.218750]\n",
      "5565: [discriminator loss: 0.521144, acc: 0.742188] [adversarial loss: 1.032658, acc: 0.250000]\n",
      "5566: [discriminator loss: 0.551787, acc: 0.718750] [adversarial loss: 1.125350, acc: 0.265625]\n",
      "5567: [discriminator loss: 0.531484, acc: 0.718750] [adversarial loss: 0.994089, acc: 0.250000]\n",
      "5568: [discriminator loss: 0.511141, acc: 0.695312] [adversarial loss: 1.264708, acc: 0.187500]\n",
      "5569: [discriminator loss: 0.533471, acc: 0.742188] [adversarial loss: 1.309900, acc: 0.187500]\n",
      "5570: [discriminator loss: 0.510551, acc: 0.726562] [adversarial loss: 0.998846, acc: 0.343750]\n",
      "5571: [discriminator loss: 0.436224, acc: 0.812500] [adversarial loss: 1.190912, acc: 0.234375]\n",
      "5572: [discriminator loss: 0.519772, acc: 0.750000] [adversarial loss: 1.473497, acc: 0.109375]\n",
      "5573: [discriminator loss: 0.572016, acc: 0.726562] [adversarial loss: 0.938405, acc: 0.343750]\n",
      "5574: [discriminator loss: 0.471769, acc: 0.804688] [adversarial loss: 1.308047, acc: 0.156250]\n",
      "5575: [discriminator loss: 0.517518, acc: 0.742188] [adversarial loss: 1.149205, acc: 0.265625]\n",
      "5576: [discriminator loss: 0.564630, acc: 0.687500] [adversarial loss: 1.274340, acc: 0.171875]\n",
      "5577: [discriminator loss: 0.514823, acc: 0.726562] [adversarial loss: 1.261367, acc: 0.171875]\n",
      "5578: [discriminator loss: 0.560765, acc: 0.679688] [adversarial loss: 1.213355, acc: 0.234375]\n",
      "5579: [discriminator loss: 0.571038, acc: 0.726562] [adversarial loss: 1.538978, acc: 0.093750]\n",
      "5580: [discriminator loss: 0.519424, acc: 0.726562] [adversarial loss: 1.070231, acc: 0.187500]\n",
      "5581: [discriminator loss: 0.548599, acc: 0.718750] [adversarial loss: 1.333876, acc: 0.187500]\n",
      "5582: [discriminator loss: 0.457448, acc: 0.781250] [adversarial loss: 1.200652, acc: 0.250000]\n",
      "5583: [discriminator loss: 0.513557, acc: 0.773438] [adversarial loss: 1.243042, acc: 0.156250]\n",
      "5584: [discriminator loss: 0.436088, acc: 0.804688] [adversarial loss: 1.279450, acc: 0.156250]\n",
      "5585: [discriminator loss: 0.456932, acc: 0.812500] [adversarial loss: 1.368767, acc: 0.187500]\n",
      "5586: [discriminator loss: 0.575237, acc: 0.703125] [adversarial loss: 1.493009, acc: 0.078125]\n",
      "5587: [discriminator loss: 0.463913, acc: 0.796875] [adversarial loss: 0.987430, acc: 0.296875]\n",
      "5588: [discriminator loss: 0.595716, acc: 0.734375] [adversarial loss: 1.260276, acc: 0.234375]\n",
      "5589: [discriminator loss: 0.487642, acc: 0.781250] [adversarial loss: 1.003137, acc: 0.250000]\n",
      "5590: [discriminator loss: 0.531638, acc: 0.726562] [adversarial loss: 1.614330, acc: 0.140625]\n",
      "5591: [discriminator loss: 0.592943, acc: 0.695312] [adversarial loss: 0.940396, acc: 0.406250]\n",
      "5592: [discriminator loss: 0.585470, acc: 0.750000] [adversarial loss: 1.471442, acc: 0.140625]\n",
      "5593: [discriminator loss: 0.536762, acc: 0.750000] [adversarial loss: 0.967259, acc: 0.359375]\n",
      "5594: [discriminator loss: 0.418895, acc: 0.789062] [adversarial loss: 1.129032, acc: 0.265625]\n",
      "5595: [discriminator loss: 0.477480, acc: 0.757812] [adversarial loss: 1.063426, acc: 0.281250]\n",
      "5596: [discriminator loss: 0.510382, acc: 0.742188] [adversarial loss: 1.177882, acc: 0.250000]\n",
      "5597: [discriminator loss: 0.499745, acc: 0.781250] [adversarial loss: 1.099507, acc: 0.265625]\n",
      "5598: [discriminator loss: 0.556502, acc: 0.687500] [adversarial loss: 1.415547, acc: 0.156250]\n",
      "5599: [discriminator loss: 0.559577, acc: 0.687500] [adversarial loss: 1.044681, acc: 0.359375]\n",
      "5600: [discriminator loss: 0.560847, acc: 0.718750] [adversarial loss: 1.518323, acc: 0.156250]\n",
      "5601: [discriminator loss: 0.535886, acc: 0.734375] [adversarial loss: 0.937551, acc: 0.437500]\n",
      "5602: [discriminator loss: 0.611663, acc: 0.703125] [adversarial loss: 1.429241, acc: 0.140625]\n",
      "5603: [discriminator loss: 0.510587, acc: 0.734375] [adversarial loss: 0.869709, acc: 0.437500]\n",
      "5604: [discriminator loss: 0.649683, acc: 0.648438] [adversarial loss: 1.657095, acc: 0.046875]\n",
      "5605: [discriminator loss: 0.597690, acc: 0.679688] [adversarial loss: 0.977343, acc: 0.328125]\n",
      "5606: [discriminator loss: 0.523795, acc: 0.750000] [adversarial loss: 1.565313, acc: 0.125000]\n",
      "5607: [discriminator loss: 0.570805, acc: 0.695312] [adversarial loss: 0.957848, acc: 0.359375]\n",
      "5608: [discriminator loss: 0.506566, acc: 0.750000] [adversarial loss: 1.462610, acc: 0.187500]\n",
      "5609: [discriminator loss: 0.497276, acc: 0.757812] [adversarial loss: 1.027626, acc: 0.250000]\n",
      "5610: [discriminator loss: 0.572020, acc: 0.687500] [adversarial loss: 1.533969, acc: 0.031250]\n",
      "5611: [discriminator loss: 0.526987, acc: 0.710938] [adversarial loss: 1.027334, acc: 0.296875]\n",
      "5612: [discriminator loss: 0.485651, acc: 0.734375] [adversarial loss: 1.525638, acc: 0.156250]\n",
      "5613: [discriminator loss: 0.564519, acc: 0.664062] [adversarial loss: 1.049367, acc: 0.328125]\n",
      "5614: [discriminator loss: 0.486887, acc: 0.765625] [adversarial loss: 1.345559, acc: 0.156250]\n",
      "5615: [discriminator loss: 0.491165, acc: 0.765625] [adversarial loss: 1.100078, acc: 0.250000]\n",
      "5616: [discriminator loss: 0.508667, acc: 0.734375] [adversarial loss: 1.633943, acc: 0.125000]\n",
      "5617: [discriminator loss: 0.547692, acc: 0.695312] [adversarial loss: 1.088036, acc: 0.265625]\n",
      "5618: [discriminator loss: 0.492323, acc: 0.750000] [adversarial loss: 1.233860, acc: 0.296875]\n",
      "5619: [discriminator loss: 0.601110, acc: 0.679688] [adversarial loss: 1.407963, acc: 0.109375]\n",
      "5620: [discriminator loss: 0.479305, acc: 0.742188] [adversarial loss: 1.299998, acc: 0.171875]\n",
      "5621: [discriminator loss: 0.543477, acc: 0.742188] [adversarial loss: 1.358038, acc: 0.140625]\n",
      "5622: [discriminator loss: 0.499466, acc: 0.765625] [adversarial loss: 0.861039, acc: 0.437500]\n",
      "5623: [discriminator loss: 0.502437, acc: 0.726562] [adversarial loss: 1.212766, acc: 0.171875]\n",
      "5624: [discriminator loss: 0.481063, acc: 0.765625] [adversarial loss: 1.266079, acc: 0.234375]\n",
      "5625: [discriminator loss: 0.496817, acc: 0.742188] [adversarial loss: 1.234425, acc: 0.250000]\n",
      "5626: [discriminator loss: 0.456727, acc: 0.765625] [adversarial loss: 1.234735, acc: 0.265625]\n",
      "5627: [discriminator loss: 0.613981, acc: 0.656250] [adversarial loss: 1.484560, acc: 0.062500]\n",
      "5628: [discriminator loss: 0.557560, acc: 0.734375] [adversarial loss: 0.912279, acc: 0.375000]\n",
      "5629: [discriminator loss: 0.581589, acc: 0.671875] [adversarial loss: 1.400079, acc: 0.234375]\n",
      "5630: [discriminator loss: 0.586327, acc: 0.703125] [adversarial loss: 0.989708, acc: 0.343750]\n",
      "5631: [discriminator loss: 0.537585, acc: 0.703125] [adversarial loss: 1.347058, acc: 0.156250]\n",
      "5632: [discriminator loss: 0.532334, acc: 0.726562] [adversarial loss: 1.178445, acc: 0.250000]\n",
      "5633: [discriminator loss: 0.539353, acc: 0.726562] [adversarial loss: 1.087993, acc: 0.359375]\n",
      "5634: [discriminator loss: 0.519865, acc: 0.726562] [adversarial loss: 1.332222, acc: 0.140625]\n",
      "5635: [discriminator loss: 0.509084, acc: 0.710938] [adversarial loss: 1.142333, acc: 0.265625]\n",
      "5636: [discriminator loss: 0.522994, acc: 0.742188] [adversarial loss: 1.589205, acc: 0.078125]\n",
      "5637: [discriminator loss: 0.490847, acc: 0.773438] [adversarial loss: 0.791085, acc: 0.546875]\n",
      "5638: [discriminator loss: 0.537995, acc: 0.710938] [adversarial loss: 1.563609, acc: 0.171875]\n",
      "5639: [discriminator loss: 0.465582, acc: 0.781250] [adversarial loss: 1.235585, acc: 0.203125]\n",
      "5640: [discriminator loss: 0.538193, acc: 0.687500] [adversarial loss: 1.335584, acc: 0.125000]\n",
      "5641: [discriminator loss: 0.445381, acc: 0.820312] [adversarial loss: 1.236639, acc: 0.281250]\n",
      "5642: [discriminator loss: 0.560792, acc: 0.703125] [adversarial loss: 1.260047, acc: 0.156250]\n",
      "5643: [discriminator loss: 0.545849, acc: 0.726562] [adversarial loss: 1.207167, acc: 0.250000]\n",
      "5644: [discriminator loss: 0.499950, acc: 0.773438] [adversarial loss: 1.406405, acc: 0.171875]\n",
      "5645: [discriminator loss: 0.516321, acc: 0.750000] [adversarial loss: 1.159879, acc: 0.218750]\n",
      "5646: [discriminator loss: 0.529527, acc: 0.742188] [adversarial loss: 1.493715, acc: 0.203125]\n",
      "5647: [discriminator loss: 0.532300, acc: 0.710938] [adversarial loss: 1.542245, acc: 0.156250]\n",
      "5648: [discriminator loss: 0.545414, acc: 0.710938] [adversarial loss: 1.265111, acc: 0.265625]\n",
      "5649: [discriminator loss: 0.526732, acc: 0.695312] [adversarial loss: 1.353120, acc: 0.203125]\n",
      "5650: [discriminator loss: 0.590772, acc: 0.703125] [adversarial loss: 1.070178, acc: 0.218750]\n",
      "5651: [discriminator loss: 0.535947, acc: 0.718750] [adversarial loss: 1.471719, acc: 0.156250]\n",
      "5652: [discriminator loss: 0.527322, acc: 0.695312] [adversarial loss: 1.192803, acc: 0.203125]\n",
      "5653: [discriminator loss: 0.602783, acc: 0.664062] [adversarial loss: 1.136325, acc: 0.312500]\n",
      "5654: [discriminator loss: 0.527562, acc: 0.726562] [adversarial loss: 1.206037, acc: 0.218750]\n",
      "5655: [discriminator loss: 0.552577, acc: 0.726562] [adversarial loss: 1.008677, acc: 0.296875]\n",
      "5656: [discriminator loss: 0.516388, acc: 0.734375] [adversarial loss: 1.475287, acc: 0.140625]\n",
      "5657: [discriminator loss: 0.553734, acc: 0.710938] [adversarial loss: 1.026192, acc: 0.328125]\n",
      "5658: [discriminator loss: 0.534726, acc: 0.750000] [adversarial loss: 1.507147, acc: 0.156250]\n",
      "5659: [discriminator loss: 0.535224, acc: 0.710938] [adversarial loss: 1.042780, acc: 0.281250]\n",
      "5660: [discriminator loss: 0.626773, acc: 0.703125] [adversarial loss: 1.358525, acc: 0.250000]\n",
      "5661: [discriminator loss: 0.534292, acc: 0.718750] [adversarial loss: 1.282005, acc: 0.203125]\n",
      "5662: [discriminator loss: 0.583171, acc: 0.679688] [adversarial loss: 1.537762, acc: 0.125000]\n",
      "5663: [discriminator loss: 0.461591, acc: 0.812500] [adversarial loss: 0.936415, acc: 0.359375]\n",
      "5664: [discriminator loss: 0.599827, acc: 0.687500] [adversarial loss: 1.402088, acc: 0.140625]\n",
      "5665: [discriminator loss: 0.510782, acc: 0.757812] [adversarial loss: 1.038166, acc: 0.281250]\n",
      "5666: [discriminator loss: 0.587327, acc: 0.687500] [adversarial loss: 1.503950, acc: 0.125000]\n",
      "5667: [discriminator loss: 0.561813, acc: 0.734375] [adversarial loss: 1.404254, acc: 0.125000]\n",
      "5668: [discriminator loss: 0.569149, acc: 0.671875] [adversarial loss: 1.198685, acc: 0.203125]\n",
      "5669: [discriminator loss: 0.531862, acc: 0.710938] [adversarial loss: 1.153908, acc: 0.359375]\n",
      "5670: [discriminator loss: 0.538687, acc: 0.695312] [adversarial loss: 1.458677, acc: 0.140625]\n",
      "5671: [discriminator loss: 0.466047, acc: 0.765625] [adversarial loss: 1.149396, acc: 0.375000]\n",
      "5672: [discriminator loss: 0.495283, acc: 0.757812] [adversarial loss: 1.402949, acc: 0.203125]\n",
      "5673: [discriminator loss: 0.491896, acc: 0.750000] [adversarial loss: 1.098857, acc: 0.312500]\n",
      "5674: [discriminator loss: 0.489796, acc: 0.773438] [adversarial loss: 1.468897, acc: 0.125000]\n",
      "5675: [discriminator loss: 0.581680, acc: 0.671875] [adversarial loss: 1.214521, acc: 0.312500]\n",
      "5676: [discriminator loss: 0.449674, acc: 0.781250] [adversarial loss: 1.123341, acc: 0.343750]\n",
      "5677: [discriminator loss: 0.575682, acc: 0.695312] [adversarial loss: 1.291670, acc: 0.171875]\n",
      "5678: [discriminator loss: 0.462885, acc: 0.796875] [adversarial loss: 1.585561, acc: 0.125000]\n",
      "5679: [discriminator loss: 0.530447, acc: 0.750000] [adversarial loss: 1.143847, acc: 0.187500]\n",
      "5680: [discriminator loss: 0.520031, acc: 0.757812] [adversarial loss: 1.244943, acc: 0.218750]\n",
      "5681: [discriminator loss: 0.497238, acc: 0.734375] [adversarial loss: 1.211606, acc: 0.265625]\n",
      "5682: [discriminator loss: 0.560033, acc: 0.695312] [adversarial loss: 1.703326, acc: 0.140625]\n",
      "5683: [discriminator loss: 0.557129, acc: 0.718750] [adversarial loss: 0.781006, acc: 0.453125]\n",
      "5684: [discriminator loss: 0.547860, acc: 0.679688] [adversarial loss: 1.807232, acc: 0.046875]\n",
      "5685: [discriminator loss: 0.487564, acc: 0.734375] [adversarial loss: 1.034932, acc: 0.281250]\n",
      "5686: [discriminator loss: 0.610250, acc: 0.687500] [adversarial loss: 1.720412, acc: 0.093750]\n",
      "5687: [discriminator loss: 0.561908, acc: 0.679688] [adversarial loss: 1.080406, acc: 0.218750]\n",
      "5688: [discriminator loss: 0.511100, acc: 0.757812] [adversarial loss: 1.411999, acc: 0.125000]\n",
      "5689: [discriminator loss: 0.464269, acc: 0.796875] [adversarial loss: 1.136737, acc: 0.265625]\n",
      "5690: [discriminator loss: 0.601035, acc: 0.679688] [adversarial loss: 1.214538, acc: 0.281250]\n",
      "5691: [discriminator loss: 0.530546, acc: 0.726562] [adversarial loss: 1.292550, acc: 0.156250]\n",
      "5692: [discriminator loss: 0.521117, acc: 0.703125] [adversarial loss: 1.265790, acc: 0.203125]\n",
      "5693: [discriminator loss: 0.537988, acc: 0.695312] [adversarial loss: 1.076967, acc: 0.328125]\n",
      "5694: [discriminator loss: 0.546918, acc: 0.718750] [adversarial loss: 1.172538, acc: 0.250000]\n",
      "5695: [discriminator loss: 0.554626, acc: 0.742188] [adversarial loss: 1.199032, acc: 0.250000]\n",
      "5696: [discriminator loss: 0.482306, acc: 0.828125] [adversarial loss: 1.178385, acc: 0.203125]\n",
      "5697: [discriminator loss: 0.536430, acc: 0.742188] [adversarial loss: 1.268092, acc: 0.203125]\n",
      "5698: [discriminator loss: 0.502419, acc: 0.773438] [adversarial loss: 1.584376, acc: 0.046875]\n",
      "5699: [discriminator loss: 0.538021, acc: 0.726562] [adversarial loss: 1.251612, acc: 0.250000]\n",
      "5700: [discriminator loss: 0.529391, acc: 0.734375] [adversarial loss: 1.385917, acc: 0.296875]\n",
      "5701: [discriminator loss: 0.500703, acc: 0.750000] [adversarial loss: 1.160627, acc: 0.296875]\n",
      "5702: [discriminator loss: 0.463723, acc: 0.757812] [adversarial loss: 1.532420, acc: 0.078125]\n",
      "5703: [discriminator loss: 0.496786, acc: 0.750000] [adversarial loss: 1.191411, acc: 0.312500]\n",
      "5704: [discriminator loss: 0.543139, acc: 0.703125] [adversarial loss: 1.499453, acc: 0.140625]\n",
      "5705: [discriminator loss: 0.538552, acc: 0.695312] [adversarial loss: 1.151562, acc: 0.312500]\n",
      "5706: [discriminator loss: 0.442280, acc: 0.773438] [adversarial loss: 1.501397, acc: 0.078125]\n",
      "5707: [discriminator loss: 0.568577, acc: 0.703125] [adversarial loss: 0.964792, acc: 0.343750]\n",
      "5708: [discriminator loss: 0.618600, acc: 0.664062] [adversarial loss: 1.776189, acc: 0.125000]\n",
      "5709: [discriminator loss: 0.683079, acc: 0.601562] [adversarial loss: 0.930388, acc: 0.343750]\n",
      "5710: [discriminator loss: 0.540229, acc: 0.765625] [adversarial loss: 1.368753, acc: 0.218750]\n",
      "5711: [discriminator loss: 0.521273, acc: 0.742188] [adversarial loss: 0.885105, acc: 0.343750]\n",
      "5712: [discriminator loss: 0.629723, acc: 0.695312] [adversarial loss: 1.570279, acc: 0.062500]\n",
      "5713: [discriminator loss: 0.557592, acc: 0.757812] [adversarial loss: 1.101828, acc: 0.265625]\n",
      "5714: [discriminator loss: 0.498555, acc: 0.757812] [adversarial loss: 1.354979, acc: 0.109375]\n",
      "5715: [discriminator loss: 0.490126, acc: 0.757812] [adversarial loss: 1.382043, acc: 0.078125]\n",
      "5716: [discriminator loss: 0.576682, acc: 0.679688] [adversarial loss: 1.113034, acc: 0.281250]\n",
      "5717: [discriminator loss: 0.571449, acc: 0.695312] [adversarial loss: 1.836251, acc: 0.015625]\n",
      "5718: [discriminator loss: 0.524895, acc: 0.710938] [adversarial loss: 0.992654, acc: 0.265625]\n",
      "5719: [discriminator loss: 0.491441, acc: 0.765625] [adversarial loss: 1.351895, acc: 0.203125]\n",
      "5720: [discriminator loss: 0.565028, acc: 0.742188] [adversarial loss: 1.119931, acc: 0.328125]\n",
      "5721: [discriminator loss: 0.546254, acc: 0.757812] [adversarial loss: 1.456094, acc: 0.140625]\n",
      "5722: [discriminator loss: 0.458193, acc: 0.789062] [adversarial loss: 1.130297, acc: 0.203125]\n",
      "5723: [discriminator loss: 0.500260, acc: 0.750000] [adversarial loss: 1.012056, acc: 0.265625]\n",
      "5724: [discriminator loss: 0.549810, acc: 0.695312] [adversarial loss: 1.085719, acc: 0.296875]\n",
      "5725: [discriminator loss: 0.530871, acc: 0.742188] [adversarial loss: 1.371051, acc: 0.171875]\n",
      "5726: [discriminator loss: 0.598824, acc: 0.679688] [adversarial loss: 1.248047, acc: 0.171875]\n",
      "5727: [discriminator loss: 0.547366, acc: 0.710938] [adversarial loss: 1.275042, acc: 0.218750]\n",
      "5728: [discriminator loss: 0.552595, acc: 0.710938] [adversarial loss: 1.383170, acc: 0.218750]\n",
      "5729: [discriminator loss: 0.491328, acc: 0.750000] [adversarial loss: 1.285068, acc: 0.125000]\n",
      "5730: [discriminator loss: 0.545855, acc: 0.703125] [adversarial loss: 1.028967, acc: 0.343750]\n",
      "5731: [discriminator loss: 0.565014, acc: 0.742188] [adversarial loss: 1.028016, acc: 0.265625]\n",
      "5732: [discriminator loss: 0.545936, acc: 0.679688] [adversarial loss: 1.030897, acc: 0.296875]\n",
      "5733: [discriminator loss: 0.520616, acc: 0.765625] [adversarial loss: 1.266606, acc: 0.187500]\n",
      "5734: [discriminator loss: 0.525263, acc: 0.679688] [adversarial loss: 0.862457, acc: 0.421875]\n",
      "5735: [discriminator loss: 0.599288, acc: 0.648438] [adversarial loss: 1.366694, acc: 0.140625]\n",
      "5736: [discriminator loss: 0.520191, acc: 0.710938] [adversarial loss: 1.183895, acc: 0.218750]\n",
      "5737: [discriminator loss: 0.558506, acc: 0.687500] [adversarial loss: 1.266944, acc: 0.218750]\n",
      "5738: [discriminator loss: 0.602321, acc: 0.671875] [adversarial loss: 1.134883, acc: 0.265625]\n",
      "5739: [discriminator loss: 0.464144, acc: 0.757812] [adversarial loss: 1.417781, acc: 0.140625]\n",
      "5740: [discriminator loss: 0.486595, acc: 0.750000] [adversarial loss: 0.940374, acc: 0.375000]\n",
      "5741: [discriminator loss: 0.536384, acc: 0.710938] [adversarial loss: 1.337397, acc: 0.171875]\n",
      "5742: [discriminator loss: 0.572381, acc: 0.671875] [adversarial loss: 1.099868, acc: 0.328125]\n",
      "5743: [discriminator loss: 0.544353, acc: 0.734375] [adversarial loss: 1.116547, acc: 0.250000]\n",
      "5744: [discriminator loss: 0.515790, acc: 0.750000] [adversarial loss: 1.240317, acc: 0.203125]\n",
      "5745: [discriminator loss: 0.531758, acc: 0.695312] [adversarial loss: 1.401738, acc: 0.187500]\n",
      "5746: [discriminator loss: 0.482360, acc: 0.773438] [adversarial loss: 1.005390, acc: 0.281250]\n",
      "5747: [discriminator loss: 0.572351, acc: 0.718750] [adversarial loss: 1.205806, acc: 0.265625]\n",
      "5748: [discriminator loss: 0.569660, acc: 0.718750] [adversarial loss: 1.172058, acc: 0.265625]\n",
      "5749: [discriminator loss: 0.498625, acc: 0.750000] [adversarial loss: 1.175559, acc: 0.218750]\n",
      "5750: [discriminator loss: 0.581576, acc: 0.710938] [adversarial loss: 1.093345, acc: 0.265625]\n",
      "5751: [discriminator loss: 0.452825, acc: 0.789062] [adversarial loss: 1.423328, acc: 0.171875]\n",
      "5752: [discriminator loss: 0.533626, acc: 0.757812] [adversarial loss: 1.170501, acc: 0.250000]\n",
      "5753: [discriminator loss: 0.507228, acc: 0.750000] [adversarial loss: 1.117128, acc: 0.265625]\n",
      "5754: [discriminator loss: 0.494946, acc: 0.773438] [adversarial loss: 1.070911, acc: 0.296875]\n",
      "5755: [discriminator loss: 0.483760, acc: 0.742188] [adversarial loss: 1.595238, acc: 0.187500]\n",
      "5756: [discriminator loss: 0.592666, acc: 0.671875] [adversarial loss: 0.911448, acc: 0.406250]\n",
      "5757: [discriminator loss: 0.555648, acc: 0.671875] [adversarial loss: 1.621217, acc: 0.109375]\n",
      "5758: [discriminator loss: 0.515769, acc: 0.726562] [adversarial loss: 0.846482, acc: 0.437500]\n",
      "5759: [discriminator loss: 0.532946, acc: 0.703125] [adversarial loss: 1.411459, acc: 0.218750]\n",
      "5760: [discriminator loss: 0.548360, acc: 0.718750] [adversarial loss: 1.144126, acc: 0.234375]\n",
      "5761: [discriminator loss: 0.623554, acc: 0.687500] [adversarial loss: 1.113959, acc: 0.265625]\n",
      "5762: [discriminator loss: 0.553017, acc: 0.710938] [adversarial loss: 1.129858, acc: 0.281250]\n",
      "5763: [discriminator loss: 0.494549, acc: 0.765625] [adversarial loss: 1.542891, acc: 0.109375]\n",
      "5764: [discriminator loss: 0.523483, acc: 0.726562] [adversarial loss: 1.104773, acc: 0.265625]\n",
      "5765: [discriminator loss: 0.571938, acc: 0.695312] [adversarial loss: 1.667259, acc: 0.078125]\n",
      "5766: [discriminator loss: 0.574371, acc: 0.734375] [adversarial loss: 1.559370, acc: 0.109375]\n",
      "5767: [discriminator loss: 0.506824, acc: 0.750000] [adversarial loss: 0.967572, acc: 0.390625]\n",
      "5768: [discriminator loss: 0.503046, acc: 0.773438] [adversarial loss: 1.295411, acc: 0.203125]\n",
      "5769: [discriminator loss: 0.515702, acc: 0.718750] [adversarial loss: 1.427309, acc: 0.171875]\n",
      "5770: [discriminator loss: 0.504380, acc: 0.726562] [adversarial loss: 1.163785, acc: 0.250000]\n",
      "5771: [discriminator loss: 0.544886, acc: 0.718750] [adversarial loss: 1.577372, acc: 0.031250]\n",
      "5772: [discriminator loss: 0.590352, acc: 0.656250] [adversarial loss: 0.902974, acc: 0.375000]\n",
      "5773: [discriminator loss: 0.529163, acc: 0.742188] [adversarial loss: 1.325397, acc: 0.156250]\n",
      "5774: [discriminator loss: 0.578895, acc: 0.656250] [adversarial loss: 1.157079, acc: 0.203125]\n",
      "5775: [discriminator loss: 0.535605, acc: 0.710938] [adversarial loss: 1.551479, acc: 0.140625]\n",
      "5776: [discriminator loss: 0.540942, acc: 0.726562] [adversarial loss: 1.027866, acc: 0.375000]\n",
      "5777: [discriminator loss: 0.505553, acc: 0.765625] [adversarial loss: 1.474757, acc: 0.171875]\n",
      "5778: [discriminator loss: 0.611328, acc: 0.695312] [adversarial loss: 0.756180, acc: 0.625000]\n",
      "5779: [discriminator loss: 0.493562, acc: 0.765625] [adversarial loss: 1.604804, acc: 0.093750]\n",
      "5780: [discriminator loss: 0.574850, acc: 0.679688] [adversarial loss: 0.734439, acc: 0.562500]\n",
      "5781: [discriminator loss: 0.528345, acc: 0.742188] [adversarial loss: 1.509964, acc: 0.125000]\n",
      "5782: [discriminator loss: 0.514673, acc: 0.765625] [adversarial loss: 1.228301, acc: 0.218750]\n",
      "5783: [discriminator loss: 0.558811, acc: 0.703125] [adversarial loss: 1.357841, acc: 0.234375]\n",
      "5784: [discriminator loss: 0.606818, acc: 0.687500] [adversarial loss: 0.998651, acc: 0.281250]\n",
      "5785: [discriminator loss: 0.539201, acc: 0.718750] [adversarial loss: 1.280454, acc: 0.203125]\n",
      "5786: [discriminator loss: 0.532119, acc: 0.742188] [adversarial loss: 0.973570, acc: 0.312500]\n",
      "5787: [discriminator loss: 0.530138, acc: 0.710938] [adversarial loss: 1.340937, acc: 0.125000]\n",
      "5788: [discriminator loss: 0.447709, acc: 0.789062] [adversarial loss: 1.359067, acc: 0.125000]\n",
      "5789: [discriminator loss: 0.500853, acc: 0.765625] [adversarial loss: 1.048943, acc: 0.312500]\n",
      "5790: [discriminator loss: 0.497186, acc: 0.742188] [adversarial loss: 1.280759, acc: 0.265625]\n",
      "5791: [discriminator loss: 0.565135, acc: 0.671875] [adversarial loss: 1.568002, acc: 0.125000]\n",
      "5792: [discriminator loss: 0.546054, acc: 0.742188] [adversarial loss: 0.974936, acc: 0.375000]\n",
      "5793: [discriminator loss: 0.518812, acc: 0.765625] [adversarial loss: 1.324802, acc: 0.156250]\n",
      "5794: [discriminator loss: 0.490979, acc: 0.765625] [adversarial loss: 1.321882, acc: 0.156250]\n",
      "5795: [discriminator loss: 0.542790, acc: 0.742188] [adversarial loss: 1.160612, acc: 0.296875]\n",
      "5796: [discriminator loss: 0.545183, acc: 0.695312] [adversarial loss: 1.478719, acc: 0.125000]\n",
      "5797: [discriminator loss: 0.501294, acc: 0.726562] [adversarial loss: 1.043659, acc: 0.343750]\n",
      "5798: [discriminator loss: 0.565721, acc: 0.695312] [adversarial loss: 1.679860, acc: 0.140625]\n",
      "5799: [discriminator loss: 0.579730, acc: 0.695312] [adversarial loss: 1.435700, acc: 0.218750]\n",
      "5800: [discriminator loss: 0.582724, acc: 0.734375] [adversarial loss: 1.260308, acc: 0.203125]\n",
      "5801: [discriminator loss: 0.538699, acc: 0.687500] [adversarial loss: 1.172912, acc: 0.250000]\n",
      "5802: [discriminator loss: 0.558158, acc: 0.671875] [adversarial loss: 1.667251, acc: 0.031250]\n",
      "5803: [discriminator loss: 0.513018, acc: 0.703125] [adversarial loss: 1.014927, acc: 0.312500]\n",
      "5804: [discriminator loss: 0.518254, acc: 0.765625] [adversarial loss: 1.679430, acc: 0.187500]\n",
      "5805: [discriminator loss: 0.467138, acc: 0.804688] [adversarial loss: 1.357759, acc: 0.140625]\n",
      "5806: [discriminator loss: 0.502507, acc: 0.765625] [adversarial loss: 1.196086, acc: 0.265625]\n",
      "5807: [discriminator loss: 0.494679, acc: 0.742188] [adversarial loss: 1.111276, acc: 0.312500]\n",
      "5808: [discriminator loss: 0.525931, acc: 0.710938] [adversarial loss: 1.380323, acc: 0.203125]\n",
      "5809: [discriminator loss: 0.495060, acc: 0.750000] [adversarial loss: 1.159416, acc: 0.250000]\n",
      "5810: [discriminator loss: 0.542520, acc: 0.695312] [adversarial loss: 1.325200, acc: 0.140625]\n",
      "5811: [discriminator loss: 0.615027, acc: 0.679688] [adversarial loss: 1.052726, acc: 0.312500]\n",
      "5812: [discriminator loss: 0.575310, acc: 0.687500] [adversarial loss: 1.204281, acc: 0.234375]\n",
      "5813: [discriminator loss: 0.460551, acc: 0.781250] [adversarial loss: 1.047047, acc: 0.234375]\n",
      "5814: [discriminator loss: 0.543635, acc: 0.687500] [adversarial loss: 1.572680, acc: 0.078125]\n",
      "5815: [discriminator loss: 0.473397, acc: 0.796875] [adversarial loss: 0.943701, acc: 0.406250]\n",
      "5816: [discriminator loss: 0.593872, acc: 0.718750] [adversarial loss: 1.553756, acc: 0.093750]\n",
      "5817: [discriminator loss: 0.490364, acc: 0.710938] [adversarial loss: 1.045634, acc: 0.281250]\n",
      "5818: [discriminator loss: 0.530790, acc: 0.710938] [adversarial loss: 1.600756, acc: 0.156250]\n",
      "5819: [discriminator loss: 0.496454, acc: 0.726562] [adversarial loss: 1.050305, acc: 0.343750]\n",
      "5820: [discriminator loss: 0.577167, acc: 0.703125] [adversarial loss: 1.556501, acc: 0.109375]\n",
      "5821: [discriminator loss: 0.583847, acc: 0.726562] [adversarial loss: 1.008662, acc: 0.343750]\n",
      "5822: [discriminator loss: 0.529646, acc: 0.734375] [adversarial loss: 1.459337, acc: 0.156250]\n",
      "5823: [discriminator loss: 0.603329, acc: 0.695312] [adversarial loss: 1.056731, acc: 0.281250]\n",
      "5824: [discriminator loss: 0.555775, acc: 0.734375] [adversarial loss: 1.336768, acc: 0.125000]\n",
      "5825: [discriminator loss: 0.479473, acc: 0.796875] [adversarial loss: 1.275873, acc: 0.250000]\n",
      "5826: [discriminator loss: 0.531330, acc: 0.710938] [adversarial loss: 1.132066, acc: 0.312500]\n",
      "5827: [discriminator loss: 0.538889, acc: 0.718750] [adversarial loss: 1.318563, acc: 0.187500]\n",
      "5828: [discriminator loss: 0.489568, acc: 0.796875] [adversarial loss: 1.408861, acc: 0.125000]\n",
      "5829: [discriminator loss: 0.500794, acc: 0.726562] [adversarial loss: 1.271174, acc: 0.203125]\n",
      "5830: [discriminator loss: 0.574237, acc: 0.734375] [adversarial loss: 1.422553, acc: 0.156250]\n",
      "5831: [discriminator loss: 0.634303, acc: 0.640625] [adversarial loss: 1.043940, acc: 0.359375]\n",
      "5832: [discriminator loss: 0.593794, acc: 0.656250] [adversarial loss: 1.730955, acc: 0.078125]\n",
      "5833: [discriminator loss: 0.635948, acc: 0.664062] [adversarial loss: 1.080923, acc: 0.265625]\n",
      "5834: [discriminator loss: 0.520421, acc: 0.742188] [adversarial loss: 1.461992, acc: 0.109375]\n",
      "5835: [discriminator loss: 0.449567, acc: 0.781250] [adversarial loss: 1.126406, acc: 0.265625]\n",
      "5836: [discriminator loss: 0.527803, acc: 0.687500] [adversarial loss: 1.312450, acc: 0.218750]\n",
      "5837: [discriminator loss: 0.539818, acc: 0.695312] [adversarial loss: 1.114761, acc: 0.265625]\n",
      "5838: [discriminator loss: 0.530692, acc: 0.750000] [adversarial loss: 1.348433, acc: 0.203125]\n",
      "5839: [discriminator loss: 0.549864, acc: 0.687500] [adversarial loss: 1.358436, acc: 0.156250]\n",
      "5840: [discriminator loss: 0.442495, acc: 0.796875] [adversarial loss: 1.460011, acc: 0.156250]\n",
      "5841: [discriminator loss: 0.503444, acc: 0.718750] [adversarial loss: 1.418838, acc: 0.218750]\n",
      "5842: [discriminator loss: 0.596907, acc: 0.679688] [adversarial loss: 0.974259, acc: 0.359375]\n",
      "5843: [discriminator loss: 0.474356, acc: 0.765625] [adversarial loss: 1.781111, acc: 0.062500]\n",
      "5844: [discriminator loss: 0.548117, acc: 0.703125] [adversarial loss: 0.929564, acc: 0.421875]\n",
      "5845: [discriminator loss: 0.579731, acc: 0.679688] [adversarial loss: 1.522949, acc: 0.078125]\n",
      "5846: [discriminator loss: 0.531227, acc: 0.757812] [adversarial loss: 1.051117, acc: 0.250000]\n",
      "5847: [discriminator loss: 0.545738, acc: 0.695312] [adversarial loss: 1.543751, acc: 0.109375]\n",
      "5848: [discriminator loss: 0.536271, acc: 0.726562] [adversarial loss: 0.893449, acc: 0.437500]\n",
      "5849: [discriminator loss: 0.559415, acc: 0.773438] [adversarial loss: 1.436512, acc: 0.171875]\n",
      "5850: [discriminator loss: 0.560008, acc: 0.726562] [adversarial loss: 1.320306, acc: 0.218750]\n",
      "5851: [discriminator loss: 0.502485, acc: 0.726562] [adversarial loss: 1.195142, acc: 0.234375]\n",
      "5852: [discriminator loss: 0.510892, acc: 0.757812] [adversarial loss: 1.306288, acc: 0.203125]\n",
      "5853: [discriminator loss: 0.462778, acc: 0.812500] [adversarial loss: 1.301096, acc: 0.250000]\n",
      "5854: [discriminator loss: 0.569922, acc: 0.718750] [adversarial loss: 1.416295, acc: 0.156250]\n",
      "5855: [discriminator loss: 0.486376, acc: 0.718750] [adversarial loss: 1.267801, acc: 0.171875]\n",
      "5856: [discriminator loss: 0.451668, acc: 0.804688] [adversarial loss: 1.029995, acc: 0.343750]\n",
      "5857: [discriminator loss: 0.635471, acc: 0.648438] [adversarial loss: 1.613301, acc: 0.156250]\n",
      "5858: [discriminator loss: 0.534052, acc: 0.718750] [adversarial loss: 0.871748, acc: 0.343750]\n",
      "5859: [discriminator loss: 0.538066, acc: 0.695312] [adversarial loss: 1.134644, acc: 0.265625]\n",
      "5860: [discriminator loss: 0.521515, acc: 0.750000] [adversarial loss: 1.128277, acc: 0.296875]\n",
      "5861: [discriminator loss: 0.518991, acc: 0.726562] [adversarial loss: 1.228901, acc: 0.218750]\n",
      "5862: [discriminator loss: 0.511487, acc: 0.742188] [adversarial loss: 0.924893, acc: 0.390625]\n",
      "5863: [discriminator loss: 0.545259, acc: 0.773438] [adversarial loss: 1.615380, acc: 0.125000]\n",
      "5864: [discriminator loss: 0.570420, acc: 0.679688] [adversarial loss: 0.934369, acc: 0.406250]\n",
      "5865: [discriminator loss: 0.466278, acc: 0.765625] [adversarial loss: 1.547256, acc: 0.140625]\n",
      "5866: [discriminator loss: 0.542108, acc: 0.710938] [adversarial loss: 1.083309, acc: 0.296875]\n",
      "5867: [discriminator loss: 0.516289, acc: 0.750000] [adversarial loss: 1.182601, acc: 0.203125]\n",
      "5868: [discriminator loss: 0.506038, acc: 0.773438] [adversarial loss: 1.266611, acc: 0.234375]\n",
      "5869: [discriminator loss: 0.583711, acc: 0.664062] [adversarial loss: 1.120170, acc: 0.328125]\n",
      "5870: [discriminator loss: 0.536650, acc: 0.726562] [adversarial loss: 1.394625, acc: 0.171875]\n",
      "5871: [discriminator loss: 0.561976, acc: 0.718750] [adversarial loss: 1.017575, acc: 0.312500]\n",
      "5872: [discriminator loss: 0.587036, acc: 0.687500] [adversarial loss: 1.613409, acc: 0.171875]\n",
      "5873: [discriminator loss: 0.556711, acc: 0.671875] [adversarial loss: 0.943559, acc: 0.390625]\n",
      "5874: [discriminator loss: 0.534229, acc: 0.695312] [adversarial loss: 1.422534, acc: 0.281250]\n",
      "5875: [discriminator loss: 0.552007, acc: 0.726562] [adversarial loss: 1.026467, acc: 0.359375]\n",
      "5876: [discriminator loss: 0.526548, acc: 0.734375] [adversarial loss: 1.327016, acc: 0.187500]\n",
      "5877: [discriminator loss: 0.573756, acc: 0.679688] [adversarial loss: 1.535810, acc: 0.093750]\n",
      "5878: [discriminator loss: 0.560798, acc: 0.679688] [adversarial loss: 1.097620, acc: 0.265625]\n",
      "5879: [discriminator loss: 0.546527, acc: 0.679688] [adversarial loss: 1.255768, acc: 0.218750]\n",
      "5880: [discriminator loss: 0.576614, acc: 0.695312] [adversarial loss: 1.165800, acc: 0.265625]\n",
      "5881: [discriminator loss: 0.473376, acc: 0.781250] [adversarial loss: 1.090210, acc: 0.312500]\n",
      "5882: [discriminator loss: 0.535979, acc: 0.695312] [adversarial loss: 1.279092, acc: 0.203125]\n",
      "5883: [discriminator loss: 0.498744, acc: 0.765625] [adversarial loss: 1.007035, acc: 0.312500]\n",
      "5884: [discriminator loss: 0.497593, acc: 0.750000] [adversarial loss: 1.377458, acc: 0.171875]\n",
      "5885: [discriminator loss: 0.588669, acc: 0.695312] [adversarial loss: 1.012618, acc: 0.359375]\n",
      "5886: [discriminator loss: 0.535947, acc: 0.710938] [adversarial loss: 1.646488, acc: 0.140625]\n",
      "5887: [discriminator loss: 0.558691, acc: 0.640625] [adversarial loss: 0.957739, acc: 0.328125]\n",
      "5888: [discriminator loss: 0.530395, acc: 0.742188] [adversarial loss: 1.292062, acc: 0.218750]\n",
      "5889: [discriminator loss: 0.577387, acc: 0.679688] [adversarial loss: 1.218357, acc: 0.171875]\n",
      "5890: [discriminator loss: 0.481075, acc: 0.796875] [adversarial loss: 1.457267, acc: 0.093750]\n",
      "5891: [discriminator loss: 0.551082, acc: 0.710938] [adversarial loss: 1.239486, acc: 0.187500]\n",
      "5892: [discriminator loss: 0.528913, acc: 0.710938] [adversarial loss: 1.340439, acc: 0.156250]\n",
      "5893: [discriminator loss: 0.470010, acc: 0.781250] [adversarial loss: 0.999566, acc: 0.265625]\n",
      "5894: [discriminator loss: 0.584884, acc: 0.671875] [adversarial loss: 1.422710, acc: 0.171875]\n",
      "5895: [discriminator loss: 0.500436, acc: 0.757812] [adversarial loss: 0.971302, acc: 0.343750]\n",
      "5896: [discriminator loss: 0.484112, acc: 0.765625] [adversarial loss: 1.430755, acc: 0.140625]\n",
      "5897: [discriminator loss: 0.517406, acc: 0.703125] [adversarial loss: 0.942477, acc: 0.328125]\n",
      "5898: [discriminator loss: 0.524029, acc: 0.734375] [adversarial loss: 1.791094, acc: 0.109375]\n",
      "5899: [discriminator loss: 0.565289, acc: 0.734375] [adversarial loss: 0.905629, acc: 0.343750]\n",
      "5900: [discriminator loss: 0.517077, acc: 0.750000] [adversarial loss: 1.432451, acc: 0.109375]\n",
      "5901: [discriminator loss: 0.547376, acc: 0.734375] [adversarial loss: 0.876783, acc: 0.375000]\n",
      "5902: [discriminator loss: 0.524499, acc: 0.718750] [adversarial loss: 1.267668, acc: 0.140625]\n",
      "5903: [discriminator loss: 0.532370, acc: 0.710938] [adversarial loss: 0.973442, acc: 0.328125]\n",
      "5904: [discriminator loss: 0.494146, acc: 0.710938] [adversarial loss: 1.560705, acc: 0.140625]\n",
      "5905: [discriminator loss: 0.505433, acc: 0.750000] [adversarial loss: 1.162072, acc: 0.187500]\n",
      "5906: [discriminator loss: 0.490333, acc: 0.750000] [adversarial loss: 1.256359, acc: 0.203125]\n",
      "5907: [discriminator loss: 0.549877, acc: 0.726562] [adversarial loss: 1.270742, acc: 0.125000]\n",
      "5908: [discriminator loss: 0.538826, acc: 0.726562] [adversarial loss: 1.405384, acc: 0.125000]\n",
      "5909: [discriminator loss: 0.491436, acc: 0.781250] [adversarial loss: 1.277890, acc: 0.203125]\n",
      "5910: [discriminator loss: 0.489331, acc: 0.757812] [adversarial loss: 1.136603, acc: 0.250000]\n",
      "5911: [discriminator loss: 0.511522, acc: 0.718750] [adversarial loss: 1.577295, acc: 0.093750]\n",
      "5912: [discriminator loss: 0.532341, acc: 0.742188] [adversarial loss: 1.130012, acc: 0.265625]\n",
      "5913: [discriminator loss: 0.449612, acc: 0.796875] [adversarial loss: 1.528883, acc: 0.140625]\n",
      "5914: [discriminator loss: 0.507720, acc: 0.750000] [adversarial loss: 1.126900, acc: 0.250000]\n",
      "5915: [discriminator loss: 0.630830, acc: 0.671875] [adversarial loss: 1.105208, acc: 0.296875]\n",
      "5916: [discriminator loss: 0.582181, acc: 0.671875] [adversarial loss: 1.444902, acc: 0.125000]\n",
      "5917: [discriminator loss: 0.546726, acc: 0.687500] [adversarial loss: 1.209781, acc: 0.234375]\n",
      "5918: [discriminator loss: 0.475187, acc: 0.804688] [adversarial loss: 1.491193, acc: 0.156250]\n",
      "5919: [discriminator loss: 0.441351, acc: 0.820312] [adversarial loss: 0.937271, acc: 0.312500]\n",
      "5920: [discriminator loss: 0.560363, acc: 0.695312] [adversarial loss: 1.325505, acc: 0.156250]\n",
      "5921: [discriminator loss: 0.517610, acc: 0.718750] [adversarial loss: 0.918519, acc: 0.437500]\n",
      "5922: [discriminator loss: 0.633267, acc: 0.664062] [adversarial loss: 1.639573, acc: 0.046875]\n",
      "5923: [discriminator loss: 0.501731, acc: 0.710938] [adversarial loss: 1.270576, acc: 0.140625]\n",
      "5924: [discriminator loss: 0.545052, acc: 0.757812] [adversarial loss: 1.293670, acc: 0.234375]\n",
      "5925: [discriminator loss: 0.539694, acc: 0.750000] [adversarial loss: 1.448889, acc: 0.171875]\n",
      "5926: [discriminator loss: 0.530806, acc: 0.718750] [adversarial loss: 0.975248, acc: 0.359375]\n",
      "5927: [discriminator loss: 0.553877, acc: 0.718750] [adversarial loss: 1.636644, acc: 0.093750]\n",
      "5928: [discriminator loss: 0.556453, acc: 0.679688] [adversarial loss: 0.924671, acc: 0.437500]\n",
      "5929: [discriminator loss: 0.588808, acc: 0.593750] [adversarial loss: 1.671091, acc: 0.109375]\n",
      "5930: [discriminator loss: 0.556717, acc: 0.703125] [adversarial loss: 0.909420, acc: 0.406250]\n",
      "5931: [discriminator loss: 0.543381, acc: 0.757812] [adversarial loss: 1.472586, acc: 0.109375]\n",
      "5932: [discriminator loss: 0.444922, acc: 0.773438] [adversarial loss: 1.223055, acc: 0.203125]\n",
      "5933: [discriminator loss: 0.538206, acc: 0.750000] [adversarial loss: 1.282800, acc: 0.187500]\n",
      "5934: [discriminator loss: 0.476833, acc: 0.812500] [adversarial loss: 1.433570, acc: 0.093750]\n",
      "5935: [discriminator loss: 0.561424, acc: 0.734375] [adversarial loss: 0.956411, acc: 0.343750]\n",
      "5936: [discriminator loss: 0.597629, acc: 0.703125] [adversarial loss: 1.673399, acc: 0.125000]\n",
      "5937: [discriminator loss: 0.603369, acc: 0.656250] [adversarial loss: 1.068327, acc: 0.328125]\n",
      "5938: [discriminator loss: 0.525507, acc: 0.710938] [adversarial loss: 1.376815, acc: 0.109375]\n",
      "5939: [discriminator loss: 0.584774, acc: 0.664062] [adversarial loss: 1.206885, acc: 0.250000]\n",
      "5940: [discriminator loss: 0.573500, acc: 0.718750] [adversarial loss: 1.207547, acc: 0.218750]\n",
      "5941: [discriminator loss: 0.547180, acc: 0.726562] [adversarial loss: 1.154377, acc: 0.281250]\n",
      "5942: [discriminator loss: 0.513128, acc: 0.726562] [adversarial loss: 1.139649, acc: 0.218750]\n",
      "5943: [discriminator loss: 0.515287, acc: 0.765625] [adversarial loss: 1.209012, acc: 0.234375]\n",
      "5944: [discriminator loss: 0.562646, acc: 0.726562] [adversarial loss: 1.120045, acc: 0.296875]\n",
      "5945: [discriminator loss: 0.534465, acc: 0.703125] [adversarial loss: 1.112743, acc: 0.296875]\n",
      "5946: [discriminator loss: 0.491100, acc: 0.773438] [adversarial loss: 1.367129, acc: 0.203125]\n",
      "5947: [discriminator loss: 0.527554, acc: 0.742188] [adversarial loss: 1.184594, acc: 0.234375]\n",
      "5948: [discriminator loss: 0.645667, acc: 0.648438] [adversarial loss: 1.004256, acc: 0.406250]\n",
      "5949: [discriminator loss: 0.498187, acc: 0.765625] [adversarial loss: 1.683942, acc: 0.062500]\n",
      "5950: [discriminator loss: 0.560658, acc: 0.679688] [adversarial loss: 0.805512, acc: 0.421875]\n",
      "5951: [discriminator loss: 0.499276, acc: 0.757812] [adversarial loss: 1.455684, acc: 0.109375]\n",
      "5952: [discriminator loss: 0.526672, acc: 0.718750] [adversarial loss: 0.840475, acc: 0.500000]\n",
      "5953: [discriminator loss: 0.466679, acc: 0.742188] [adversarial loss: 1.385087, acc: 0.203125]\n",
      "5954: [discriminator loss: 0.431193, acc: 0.812500] [adversarial loss: 1.139084, acc: 0.203125]\n",
      "5955: [discriminator loss: 0.483458, acc: 0.804688] [adversarial loss: 1.184991, acc: 0.203125]\n",
      "5956: [discriminator loss: 0.479357, acc: 0.757812] [adversarial loss: 1.288156, acc: 0.218750]\n",
      "5957: [discriminator loss: 0.524436, acc: 0.734375] [adversarial loss: 1.120667, acc: 0.250000]\n",
      "5958: [discriminator loss: 0.500016, acc: 0.757812] [adversarial loss: 1.330777, acc: 0.140625]\n",
      "5959: [discriminator loss: 0.602361, acc: 0.671875] [adversarial loss: 0.751355, acc: 0.578125]\n",
      "5960: [discriminator loss: 0.636995, acc: 0.656250] [adversarial loss: 1.985967, acc: 0.062500]\n",
      "5961: [discriminator loss: 0.594617, acc: 0.695312] [adversarial loss: 0.833908, acc: 0.500000]\n",
      "5962: [discriminator loss: 0.544361, acc: 0.695312] [adversarial loss: 1.452366, acc: 0.125000]\n",
      "5963: [discriminator loss: 0.544313, acc: 0.734375] [adversarial loss: 0.851374, acc: 0.421875]\n",
      "5964: [discriminator loss: 0.549157, acc: 0.710938] [adversarial loss: 1.429470, acc: 0.109375]\n",
      "5965: [discriminator loss: 0.495198, acc: 0.718750] [adversarial loss: 1.112642, acc: 0.296875]\n",
      "5966: [discriminator loss: 0.582696, acc: 0.726562] [adversarial loss: 1.073059, acc: 0.328125]\n",
      "5967: [discriminator loss: 0.519060, acc: 0.734375] [adversarial loss: 1.137411, acc: 0.250000]\n",
      "5968: [discriminator loss: 0.539673, acc: 0.679688] [adversarial loss: 1.289235, acc: 0.109375]\n",
      "5969: [discriminator loss: 0.530482, acc: 0.679688] [adversarial loss: 1.315424, acc: 0.171875]\n",
      "5970: [discriminator loss: 0.475694, acc: 0.812500] [adversarial loss: 1.192685, acc: 0.250000]\n",
      "5971: [discriminator loss: 0.509617, acc: 0.734375] [adversarial loss: 1.078075, acc: 0.312500]\n",
      "5972: [discriminator loss: 0.579844, acc: 0.710938] [adversarial loss: 1.265374, acc: 0.187500]\n",
      "5973: [discriminator loss: 0.480696, acc: 0.820312] [adversarial loss: 1.097050, acc: 0.265625]\n",
      "5974: [discriminator loss: 0.473533, acc: 0.789062] [adversarial loss: 1.033327, acc: 0.312500]\n",
      "5975: [discriminator loss: 0.522615, acc: 0.750000] [adversarial loss: 1.613258, acc: 0.093750]\n",
      "5976: [discriminator loss: 0.544680, acc: 0.687500] [adversarial loss: 0.797518, acc: 0.562500]\n",
      "5977: [discriminator loss: 0.616263, acc: 0.726562] [adversarial loss: 1.679355, acc: 0.093750]\n",
      "5978: [discriminator loss: 0.540557, acc: 0.695312] [adversarial loss: 1.071200, acc: 0.421875]\n",
      "5979: [discriminator loss: 0.642475, acc: 0.648438] [adversarial loss: 1.517127, acc: 0.171875]\n",
      "5980: [discriminator loss: 0.605429, acc: 0.679688] [adversarial loss: 1.122443, acc: 0.265625]\n",
      "5981: [discriminator loss: 0.513176, acc: 0.718750] [adversarial loss: 1.322162, acc: 0.125000]\n",
      "5982: [discriminator loss: 0.502421, acc: 0.750000] [adversarial loss: 1.010209, acc: 0.328125]\n",
      "5983: [discriminator loss: 0.525416, acc: 0.742188] [adversarial loss: 1.545852, acc: 0.109375]\n",
      "5984: [discriminator loss: 0.483513, acc: 0.750000] [adversarial loss: 0.973463, acc: 0.375000]\n",
      "5985: [discriminator loss: 0.539602, acc: 0.710938] [adversarial loss: 1.280115, acc: 0.250000]\n",
      "5986: [discriminator loss: 0.494503, acc: 0.750000] [adversarial loss: 1.128397, acc: 0.203125]\n",
      "5987: [discriminator loss: 0.529735, acc: 0.710938] [adversarial loss: 1.308054, acc: 0.171875]\n",
      "5988: [discriminator loss: 0.546060, acc: 0.757812] [adversarial loss: 1.414086, acc: 0.156250]\n",
      "5989: [discriminator loss: 0.522159, acc: 0.742188] [adversarial loss: 0.930761, acc: 0.296875]\n",
      "5990: [discriminator loss: 0.511373, acc: 0.742188] [adversarial loss: 1.442328, acc: 0.140625]\n",
      "5991: [discriminator loss: 0.591112, acc: 0.671875] [adversarial loss: 0.961978, acc: 0.343750]\n",
      "5992: [discriminator loss: 0.591586, acc: 0.710938] [adversarial loss: 1.286929, acc: 0.234375]\n",
      "5993: [discriminator loss: 0.528541, acc: 0.773438] [adversarial loss: 1.232632, acc: 0.218750]\n",
      "5994: [discriminator loss: 0.543511, acc: 0.734375] [adversarial loss: 1.156698, acc: 0.234375]\n",
      "5995: [discriminator loss: 0.556799, acc: 0.734375] [adversarial loss: 1.628882, acc: 0.078125]\n",
      "5996: [discriminator loss: 0.573900, acc: 0.742188] [adversarial loss: 0.980938, acc: 0.328125]\n",
      "5997: [discriminator loss: 0.497936, acc: 0.757812] [adversarial loss: 1.317448, acc: 0.156250]\n",
      "5998: [discriminator loss: 0.453440, acc: 0.804688] [adversarial loss: 0.935374, acc: 0.375000]\n",
      "5999: [discriminator loss: 0.530951, acc: 0.726562] [adversarial loss: 1.424481, acc: 0.140625]\n",
      "6000: [discriminator loss: 0.531276, acc: 0.687500] [adversarial loss: 0.859580, acc: 0.437500]\n",
      "6001: [discriminator loss: 0.540008, acc: 0.734375] [adversarial loss: 1.446433, acc: 0.187500]\n",
      "6002: [discriminator loss: 0.573729, acc: 0.710938] [adversarial loss: 1.253983, acc: 0.187500]\n",
      "6003: [discriminator loss: 0.578045, acc: 0.695312] [adversarial loss: 1.651363, acc: 0.093750]\n",
      "6004: [discriminator loss: 0.580583, acc: 0.695312] [adversarial loss: 1.104991, acc: 0.281250]\n",
      "6005: [discriminator loss: 0.529853, acc: 0.734375] [adversarial loss: 1.432773, acc: 0.218750]\n",
      "6006: [discriminator loss: 0.587073, acc: 0.679688] [adversarial loss: 1.095110, acc: 0.343750]\n",
      "6007: [discriminator loss: 0.528666, acc: 0.742188] [adversarial loss: 0.923639, acc: 0.343750]\n",
      "6008: [discriminator loss: 0.486423, acc: 0.781250] [adversarial loss: 1.345106, acc: 0.203125]\n",
      "6009: [discriminator loss: 0.503225, acc: 0.750000] [adversarial loss: 0.983677, acc: 0.359375]\n",
      "6010: [discriminator loss: 0.523888, acc: 0.734375] [adversarial loss: 1.232199, acc: 0.218750]\n",
      "6011: [discriminator loss: 0.552592, acc: 0.679688] [adversarial loss: 1.147694, acc: 0.250000]\n",
      "6012: [discriminator loss: 0.464240, acc: 0.796875] [adversarial loss: 1.267974, acc: 0.250000]\n",
      "6013: [discriminator loss: 0.511083, acc: 0.750000] [adversarial loss: 1.442965, acc: 0.171875]\n",
      "6014: [discriminator loss: 0.538136, acc: 0.718750] [adversarial loss: 1.192835, acc: 0.218750]\n",
      "6015: [discriminator loss: 0.454328, acc: 0.789062] [adversarial loss: 1.489435, acc: 0.218750]\n",
      "6016: [discriminator loss: 0.562668, acc: 0.679688] [adversarial loss: 0.995538, acc: 0.312500]\n",
      "6017: [discriminator loss: 0.521060, acc: 0.671875] [adversarial loss: 1.528268, acc: 0.093750]\n",
      "6018: [discriminator loss: 0.548306, acc: 0.703125] [adversarial loss: 0.953513, acc: 0.375000]\n",
      "6019: [discriminator loss: 0.467239, acc: 0.757812] [adversarial loss: 1.644146, acc: 0.093750]\n",
      "6020: [discriminator loss: 0.573449, acc: 0.703125] [adversarial loss: 0.805870, acc: 0.484375]\n",
      "6021: [discriminator loss: 0.528846, acc: 0.664062] [adversarial loss: 1.764975, acc: 0.078125]\n",
      "6022: [discriminator loss: 0.590544, acc: 0.687500] [adversarial loss: 0.852019, acc: 0.437500]\n",
      "6023: [discriminator loss: 0.560154, acc: 0.750000] [adversarial loss: 1.359629, acc: 0.187500]\n",
      "6024: [discriminator loss: 0.543392, acc: 0.695312] [adversarial loss: 1.221403, acc: 0.187500]\n",
      "6025: [discriminator loss: 0.502822, acc: 0.757812] [adversarial loss: 1.294669, acc: 0.234375]\n",
      "6026: [discriminator loss: 0.479585, acc: 0.742188] [adversarial loss: 1.176687, acc: 0.328125]\n",
      "6027: [discriminator loss: 0.522392, acc: 0.703125] [adversarial loss: 1.202088, acc: 0.250000]\n",
      "6028: [discriminator loss: 0.569412, acc: 0.726562] [adversarial loss: 1.237104, acc: 0.218750]\n",
      "6029: [discriminator loss: 0.501207, acc: 0.812500] [adversarial loss: 1.286182, acc: 0.187500]\n",
      "6030: [discriminator loss: 0.480612, acc: 0.734375] [adversarial loss: 1.127240, acc: 0.265625]\n",
      "6031: [discriminator loss: 0.571815, acc: 0.664062] [adversarial loss: 1.396015, acc: 0.203125]\n",
      "6032: [discriminator loss: 0.500958, acc: 0.734375] [adversarial loss: 1.157255, acc: 0.265625]\n",
      "6033: [discriminator loss: 0.572495, acc: 0.718750] [adversarial loss: 1.172467, acc: 0.203125]\n",
      "6034: [discriminator loss: 0.527253, acc: 0.703125] [adversarial loss: 1.389944, acc: 0.218750]\n",
      "6035: [discriminator loss: 0.590514, acc: 0.687500] [adversarial loss: 0.934638, acc: 0.390625]\n",
      "6036: [discriminator loss: 0.553093, acc: 0.703125] [adversarial loss: 1.606173, acc: 0.062500]\n",
      "6037: [discriminator loss: 0.524703, acc: 0.742188] [adversarial loss: 0.869106, acc: 0.390625]\n",
      "6038: [discriminator loss: 0.560742, acc: 0.710938] [adversarial loss: 1.556670, acc: 0.125000]\n",
      "6039: [discriminator loss: 0.551153, acc: 0.718750] [adversarial loss: 0.850632, acc: 0.437500]\n",
      "6040: [discriminator loss: 0.590086, acc: 0.664062] [adversarial loss: 1.596366, acc: 0.109375]\n",
      "6041: [discriminator loss: 0.500821, acc: 0.726562] [adversarial loss: 1.169036, acc: 0.312500]\n",
      "6042: [discriminator loss: 0.526729, acc: 0.757812] [adversarial loss: 1.463061, acc: 0.171875]\n",
      "6043: [discriminator loss: 0.521588, acc: 0.757812] [adversarial loss: 1.327536, acc: 0.156250]\n",
      "6044: [discriminator loss: 0.517733, acc: 0.757812] [adversarial loss: 1.196268, acc: 0.250000]\n",
      "6045: [discriminator loss: 0.558247, acc: 0.750000] [adversarial loss: 1.154900, acc: 0.328125]\n",
      "6046: [discriminator loss: 0.511614, acc: 0.734375] [adversarial loss: 1.349642, acc: 0.171875]\n",
      "6047: [discriminator loss: 0.496197, acc: 0.742188] [adversarial loss: 0.748922, acc: 0.500000]\n",
      "6048: [discriminator loss: 0.540184, acc: 0.710938] [adversarial loss: 1.510978, acc: 0.062500]\n",
      "6049: [discriminator loss: 0.497855, acc: 0.750000] [adversarial loss: 1.020012, acc: 0.343750]\n",
      "6050: [discriminator loss: 0.652592, acc: 0.617188] [adversarial loss: 1.339017, acc: 0.140625]\n",
      "6051: [discriminator loss: 0.510514, acc: 0.765625] [adversarial loss: 1.295550, acc: 0.156250]\n",
      "6052: [discriminator loss: 0.629584, acc: 0.664062] [adversarial loss: 1.082581, acc: 0.281250]\n",
      "6053: [discriminator loss: 0.434615, acc: 0.796875] [adversarial loss: 1.013116, acc: 0.250000]\n",
      "6054: [discriminator loss: 0.490458, acc: 0.679688] [adversarial loss: 0.977681, acc: 0.312500]\n",
      "6055: [discriminator loss: 0.512211, acc: 0.757812] [adversarial loss: 1.513971, acc: 0.140625]\n",
      "6056: [discriminator loss: 0.519444, acc: 0.703125] [adversarial loss: 1.029067, acc: 0.312500]\n",
      "6057: [discriminator loss: 0.488423, acc: 0.796875] [adversarial loss: 1.405652, acc: 0.187500]\n",
      "6058: [discriminator loss: 0.394536, acc: 0.812500] [adversarial loss: 1.359693, acc: 0.171875]\n",
      "6059: [discriminator loss: 0.580137, acc: 0.718750] [adversarial loss: 1.209753, acc: 0.218750]\n",
      "6060: [discriminator loss: 0.512495, acc: 0.742188] [adversarial loss: 1.234832, acc: 0.218750]\n",
      "6061: [discriminator loss: 0.529893, acc: 0.726562] [adversarial loss: 1.325776, acc: 0.218750]\n",
      "6062: [discriminator loss: 0.498537, acc: 0.710938] [adversarial loss: 1.167416, acc: 0.250000]\n",
      "6063: [discriminator loss: 0.496906, acc: 0.750000] [adversarial loss: 1.429227, acc: 0.171875]\n",
      "6064: [discriminator loss: 0.493062, acc: 0.781250] [adversarial loss: 1.038040, acc: 0.281250]\n",
      "6065: [discriminator loss: 0.540767, acc: 0.671875] [adversarial loss: 1.168632, acc: 0.203125]\n",
      "6066: [discriminator loss: 0.494284, acc: 0.750000] [adversarial loss: 1.414811, acc: 0.171875]\n",
      "6067: [discriminator loss: 0.476984, acc: 0.757812] [adversarial loss: 1.053917, acc: 0.328125]\n",
      "6068: [discriminator loss: 0.636844, acc: 0.625000] [adversarial loss: 1.422202, acc: 0.109375]\n",
      "6069: [discriminator loss: 0.502015, acc: 0.765625] [adversarial loss: 0.971813, acc: 0.390625]\n",
      "6070: [discriminator loss: 0.546720, acc: 0.703125] [adversarial loss: 1.648519, acc: 0.156250]\n",
      "6071: [discriminator loss: 0.469768, acc: 0.773438] [adversarial loss: 1.096582, acc: 0.281250]\n",
      "6072: [discriminator loss: 0.446059, acc: 0.843750] [adversarial loss: 1.566386, acc: 0.156250]\n",
      "6073: [discriminator loss: 0.602117, acc: 0.656250] [adversarial loss: 1.029974, acc: 0.312500]\n",
      "6074: [discriminator loss: 0.547538, acc: 0.710938] [adversarial loss: 1.429966, acc: 0.109375]\n",
      "6075: [discriminator loss: 0.529417, acc: 0.742188] [adversarial loss: 1.374161, acc: 0.093750]\n",
      "6076: [discriminator loss: 0.548973, acc: 0.750000] [adversarial loss: 1.198238, acc: 0.250000]\n",
      "6077: [discriminator loss: 0.563496, acc: 0.695312] [adversarial loss: 0.936821, acc: 0.468750]\n",
      "6078: [discriminator loss: 0.547133, acc: 0.750000] [adversarial loss: 1.428847, acc: 0.109375]\n",
      "6079: [discriminator loss: 0.586557, acc: 0.671875] [adversarial loss: 1.228225, acc: 0.218750]\n",
      "6080: [discriminator loss: 0.562191, acc: 0.695312] [adversarial loss: 1.105552, acc: 0.265625]\n",
      "6081: [discriminator loss: 0.552229, acc: 0.734375] [adversarial loss: 1.500580, acc: 0.218750]\n",
      "6082: [discriminator loss: 0.500106, acc: 0.765625] [adversarial loss: 0.925561, acc: 0.390625]\n",
      "6083: [discriminator loss: 0.608841, acc: 0.648438] [adversarial loss: 1.351714, acc: 0.187500]\n",
      "6084: [discriminator loss: 0.541146, acc: 0.734375] [adversarial loss: 0.873016, acc: 0.500000]\n",
      "6085: [discriminator loss: 0.524660, acc: 0.742188] [adversarial loss: 1.749651, acc: 0.093750]\n",
      "6086: [discriminator loss: 0.528980, acc: 0.726562] [adversarial loss: 1.068309, acc: 0.265625]\n",
      "6087: [discriminator loss: 0.538396, acc: 0.750000] [adversarial loss: 1.310115, acc: 0.125000]\n",
      "6088: [discriminator loss: 0.481715, acc: 0.734375] [adversarial loss: 0.971061, acc: 0.328125]\n",
      "6089: [discriminator loss: 0.521371, acc: 0.734375] [adversarial loss: 1.357011, acc: 0.125000]\n",
      "6090: [discriminator loss: 0.576142, acc: 0.687500] [adversarial loss: 1.153897, acc: 0.265625]\n",
      "6091: [discriminator loss: 0.602546, acc: 0.695312] [adversarial loss: 1.718113, acc: 0.031250]\n",
      "6092: [discriminator loss: 0.484323, acc: 0.750000] [adversarial loss: 1.065802, acc: 0.343750]\n",
      "6093: [discriminator loss: 0.673214, acc: 0.632812] [adversarial loss: 1.501569, acc: 0.109375]\n",
      "6094: [discriminator loss: 0.546724, acc: 0.671875] [adversarial loss: 0.823056, acc: 0.484375]\n",
      "6095: [discriminator loss: 0.563650, acc: 0.695312] [adversarial loss: 1.441592, acc: 0.171875]\n",
      "6096: [discriminator loss: 0.518149, acc: 0.726562] [adversarial loss: 0.976193, acc: 0.343750]\n",
      "6097: [discriminator loss: 0.565231, acc: 0.742188] [adversarial loss: 1.675842, acc: 0.093750]\n",
      "6098: [discriminator loss: 0.541402, acc: 0.742188] [adversarial loss: 1.005944, acc: 0.375000]\n",
      "6099: [discriminator loss: 0.570819, acc: 0.695312] [adversarial loss: 1.443544, acc: 0.140625]\n",
      "6100: [discriminator loss: 0.556017, acc: 0.695312] [adversarial loss: 1.178257, acc: 0.187500]\n",
      "6101: [discriminator loss: 0.510201, acc: 0.750000] [adversarial loss: 1.179875, acc: 0.296875]\n",
      "6102: [discriminator loss: 0.530962, acc: 0.695312] [adversarial loss: 1.393102, acc: 0.187500]\n",
      "6103: [discriminator loss: 0.554080, acc: 0.687500] [adversarial loss: 1.275739, acc: 0.203125]\n",
      "6104: [discriminator loss: 0.535549, acc: 0.703125] [adversarial loss: 1.022588, acc: 0.343750]\n",
      "6105: [discriminator loss: 0.440671, acc: 0.804688] [adversarial loss: 1.534663, acc: 0.140625]\n",
      "6106: [discriminator loss: 0.537561, acc: 0.734375] [adversarial loss: 0.953808, acc: 0.406250]\n",
      "6107: [discriminator loss: 0.561576, acc: 0.718750] [adversarial loss: 1.476061, acc: 0.156250]\n",
      "6108: [discriminator loss: 0.495402, acc: 0.773438] [adversarial loss: 1.001904, acc: 0.390625]\n",
      "6109: [discriminator loss: 0.571280, acc: 0.726562] [adversarial loss: 1.275054, acc: 0.218750]\n",
      "6110: [discriminator loss: 0.518286, acc: 0.734375] [adversarial loss: 1.048418, acc: 0.343750]\n",
      "6111: [discriminator loss: 0.595504, acc: 0.703125] [adversarial loss: 1.513534, acc: 0.156250]\n",
      "6112: [discriminator loss: 0.558113, acc: 0.726562] [adversarial loss: 1.085920, acc: 0.312500]\n",
      "6113: [discriminator loss: 0.561162, acc: 0.687500] [adversarial loss: 1.210207, acc: 0.296875]\n",
      "6114: [discriminator loss: 0.552276, acc: 0.742188] [adversarial loss: 1.187238, acc: 0.265625]\n",
      "6115: [discriminator loss: 0.526463, acc: 0.781250] [adversarial loss: 1.350044, acc: 0.203125]\n",
      "6116: [discriminator loss: 0.604109, acc: 0.718750] [adversarial loss: 1.042621, acc: 0.343750]\n",
      "6117: [discriminator loss: 0.540454, acc: 0.734375] [adversarial loss: 1.311062, acc: 0.218750]\n",
      "6118: [discriminator loss: 0.517356, acc: 0.757812] [adversarial loss: 1.055836, acc: 0.296875]\n",
      "6119: [discriminator loss: 0.507091, acc: 0.726562] [adversarial loss: 1.420207, acc: 0.250000]\n",
      "6120: [discriminator loss: 0.466230, acc: 0.781250] [adversarial loss: 1.090379, acc: 0.296875]\n",
      "6121: [discriminator loss: 0.536419, acc: 0.734375] [adversarial loss: 1.444202, acc: 0.218750]\n",
      "6122: [discriminator loss: 0.617218, acc: 0.695312] [adversarial loss: 0.839345, acc: 0.531250]\n",
      "6123: [discriminator loss: 0.555131, acc: 0.742188] [adversarial loss: 1.487204, acc: 0.125000]\n",
      "6124: [discriminator loss: 0.520167, acc: 0.718750] [adversarial loss: 1.265923, acc: 0.187500]\n",
      "6125: [discriminator loss: 0.525072, acc: 0.765625] [adversarial loss: 1.391457, acc: 0.109375]\n",
      "6126: [discriminator loss: 0.515688, acc: 0.703125] [adversarial loss: 1.091076, acc: 0.296875]\n",
      "6127: [discriminator loss: 0.489454, acc: 0.710938] [adversarial loss: 1.485137, acc: 0.109375]\n",
      "6128: [discriminator loss: 0.552184, acc: 0.718750] [adversarial loss: 0.696729, acc: 0.593750]\n",
      "6129: [discriminator loss: 0.516173, acc: 0.726562] [adversarial loss: 1.351876, acc: 0.156250]\n",
      "6130: [discriminator loss: 0.481040, acc: 0.773438] [adversarial loss: 1.322717, acc: 0.187500]\n",
      "6131: [discriminator loss: 0.482803, acc: 0.773438] [adversarial loss: 1.315644, acc: 0.203125]\n",
      "6132: [discriminator loss: 0.526708, acc: 0.726562] [adversarial loss: 1.346064, acc: 0.187500]\n",
      "6133: [discriminator loss: 0.573089, acc: 0.703125] [adversarial loss: 1.238589, acc: 0.218750]\n",
      "6134: [discriminator loss: 0.464520, acc: 0.789062] [adversarial loss: 1.230558, acc: 0.125000]\n",
      "6135: [discriminator loss: 0.479064, acc: 0.742188] [adversarial loss: 1.188047, acc: 0.218750]\n",
      "6136: [discriminator loss: 0.549270, acc: 0.710938] [adversarial loss: 1.626303, acc: 0.125000]\n",
      "6137: [discriminator loss: 0.512894, acc: 0.695312] [adversarial loss: 1.039385, acc: 0.281250]\n",
      "6138: [discriminator loss: 0.536369, acc: 0.726562] [adversarial loss: 1.270398, acc: 0.281250]\n",
      "6139: [discriminator loss: 0.526115, acc: 0.710938] [adversarial loss: 1.488980, acc: 0.156250]\n",
      "6140: [discriminator loss: 0.519809, acc: 0.750000] [adversarial loss: 1.237048, acc: 0.296875]\n",
      "6141: [discriminator loss: 0.552511, acc: 0.710938] [adversarial loss: 1.427337, acc: 0.156250]\n",
      "6142: [discriminator loss: 0.559135, acc: 0.726562] [adversarial loss: 1.064192, acc: 0.296875]\n",
      "6143: [discriminator loss: 0.556857, acc: 0.726562] [adversarial loss: 1.486603, acc: 0.093750]\n",
      "6144: [discriminator loss: 0.517357, acc: 0.781250] [adversarial loss: 1.155025, acc: 0.109375]\n",
      "6145: [discriminator loss: 0.462335, acc: 0.804688] [adversarial loss: 1.443585, acc: 0.140625]\n",
      "6146: [discriminator loss: 0.523353, acc: 0.742188] [adversarial loss: 0.956885, acc: 0.390625]\n",
      "6147: [discriminator loss: 0.524656, acc: 0.742188] [adversarial loss: 1.443421, acc: 0.140625]\n",
      "6148: [discriminator loss: 0.589905, acc: 0.703125] [adversarial loss: 1.054936, acc: 0.296875]\n",
      "6149: [discriminator loss: 0.564324, acc: 0.703125] [adversarial loss: 1.293869, acc: 0.156250]\n",
      "6150: [discriminator loss: 0.507568, acc: 0.781250] [adversarial loss: 1.730408, acc: 0.125000]\n",
      "6151: [discriminator loss: 0.559224, acc: 0.710938] [adversarial loss: 0.951276, acc: 0.375000]\n",
      "6152: [discriminator loss: 0.568568, acc: 0.656250] [adversarial loss: 1.605323, acc: 0.078125]\n",
      "6153: [discriminator loss: 0.542014, acc: 0.687500] [adversarial loss: 0.828012, acc: 0.453125]\n",
      "6154: [discriminator loss: 0.538815, acc: 0.703125] [adversarial loss: 1.277392, acc: 0.234375]\n",
      "6155: [discriminator loss: 0.482726, acc: 0.757812] [adversarial loss: 1.055405, acc: 0.343750]\n",
      "6156: [discriminator loss: 0.554662, acc: 0.718750] [adversarial loss: 1.268015, acc: 0.171875]\n",
      "6157: [discriminator loss: 0.457380, acc: 0.789062] [adversarial loss: 1.237476, acc: 0.281250]\n",
      "6158: [discriminator loss: 0.572391, acc: 0.734375] [adversarial loss: 1.303973, acc: 0.187500]\n",
      "6159: [discriminator loss: 0.484483, acc: 0.765625] [adversarial loss: 1.032112, acc: 0.328125]\n",
      "6160: [discriminator loss: 0.465850, acc: 0.773438] [adversarial loss: 1.469118, acc: 0.156250]\n",
      "6161: [discriminator loss: 0.636317, acc: 0.656250] [adversarial loss: 1.272009, acc: 0.171875]\n",
      "6162: [discriminator loss: 0.501653, acc: 0.750000] [adversarial loss: 1.055080, acc: 0.359375]\n",
      "6163: [discriminator loss: 0.583795, acc: 0.718750] [adversarial loss: 1.578572, acc: 0.187500]\n",
      "6164: [discriminator loss: 0.581165, acc: 0.710938] [adversarial loss: 0.892619, acc: 0.390625]\n",
      "6165: [discriminator loss: 0.515692, acc: 0.726562] [adversarial loss: 1.406025, acc: 0.171875]\n",
      "6166: [discriminator loss: 0.590891, acc: 0.687500] [adversarial loss: 0.995164, acc: 0.359375]\n",
      "6167: [discriminator loss: 0.599618, acc: 0.687500] [adversarial loss: 1.732641, acc: 0.062500]\n",
      "6168: [discriminator loss: 0.471046, acc: 0.757812] [adversarial loss: 1.189981, acc: 0.218750]\n",
      "6169: [discriminator loss: 0.538296, acc: 0.710938] [adversarial loss: 1.254122, acc: 0.218750]\n",
      "6170: [discriminator loss: 0.564098, acc: 0.695312] [adversarial loss: 1.416441, acc: 0.156250]\n",
      "6171: [discriminator loss: 0.560953, acc: 0.734375] [adversarial loss: 0.973172, acc: 0.343750]\n",
      "6172: [discriminator loss: 0.575218, acc: 0.718750] [adversarial loss: 1.585678, acc: 0.093750]\n",
      "6173: [discriminator loss: 0.563973, acc: 0.726562] [adversarial loss: 1.168784, acc: 0.234375]\n",
      "6174: [discriminator loss: 0.507040, acc: 0.750000] [adversarial loss: 1.388124, acc: 0.187500]\n",
      "6175: [discriminator loss: 0.545588, acc: 0.742188] [adversarial loss: 0.961729, acc: 0.328125]\n",
      "6176: [discriminator loss: 0.588775, acc: 0.656250] [adversarial loss: 1.658684, acc: 0.125000]\n",
      "6177: [discriminator loss: 0.536055, acc: 0.656250] [adversarial loss: 1.091942, acc: 0.312500]\n",
      "6178: [discriminator loss: 0.563922, acc: 0.703125] [adversarial loss: 1.162099, acc: 0.187500]\n",
      "6179: [discriminator loss: 0.467900, acc: 0.750000] [adversarial loss: 1.293483, acc: 0.218750]\n",
      "6180: [discriminator loss: 0.542861, acc: 0.695312] [adversarial loss: 1.495023, acc: 0.109375]\n",
      "6181: [discriminator loss: 0.556245, acc: 0.710938] [adversarial loss: 1.091620, acc: 0.281250]\n",
      "6182: [discriminator loss: 0.526730, acc: 0.750000] [adversarial loss: 1.367100, acc: 0.203125]\n",
      "6183: [discriminator loss: 0.491469, acc: 0.742188] [adversarial loss: 1.147874, acc: 0.250000]\n",
      "6184: [discriminator loss: 0.559411, acc: 0.687500] [adversarial loss: 1.352021, acc: 0.203125]\n",
      "6185: [discriminator loss: 0.517003, acc: 0.750000] [adversarial loss: 0.885717, acc: 0.484375]\n",
      "6186: [discriminator loss: 0.573983, acc: 0.664062] [adversarial loss: 1.156222, acc: 0.265625]\n",
      "6187: [discriminator loss: 0.519245, acc: 0.750000] [adversarial loss: 1.140011, acc: 0.281250]\n",
      "6188: [discriminator loss: 0.489343, acc: 0.765625] [adversarial loss: 1.059411, acc: 0.250000]\n",
      "6189: [discriminator loss: 0.535535, acc: 0.734375] [adversarial loss: 1.180823, acc: 0.250000]\n",
      "6190: [discriminator loss: 0.548275, acc: 0.750000] [adversarial loss: 1.177605, acc: 0.109375]\n",
      "6191: [discriminator loss: 0.519621, acc: 0.726562] [adversarial loss: 0.900529, acc: 0.359375]\n",
      "6192: [discriminator loss: 0.504582, acc: 0.757812] [adversarial loss: 1.664945, acc: 0.125000]\n",
      "6193: [discriminator loss: 0.500197, acc: 0.734375] [adversarial loss: 0.835637, acc: 0.500000]\n",
      "6194: [discriminator loss: 0.631564, acc: 0.640625] [adversarial loss: 1.904280, acc: 0.062500]\n",
      "6195: [discriminator loss: 0.527740, acc: 0.710938] [adversarial loss: 0.970089, acc: 0.375000]\n",
      "6196: [discriminator loss: 0.559159, acc: 0.703125] [adversarial loss: 1.509365, acc: 0.078125]\n",
      "6197: [discriminator loss: 0.592615, acc: 0.671875] [adversarial loss: 0.878238, acc: 0.468750]\n",
      "6198: [discriminator loss: 0.561403, acc: 0.703125] [adversarial loss: 1.377351, acc: 0.109375]\n",
      "6199: [discriminator loss: 0.529016, acc: 0.726562] [adversarial loss: 1.235642, acc: 0.156250]\n",
      "6200: [discriminator loss: 0.505158, acc: 0.789062] [adversarial loss: 1.051506, acc: 0.359375]\n",
      "6201: [discriminator loss: 0.528114, acc: 0.757812] [adversarial loss: 1.340775, acc: 0.156250]\n",
      "6202: [discriminator loss: 0.540828, acc: 0.695312] [adversarial loss: 1.112638, acc: 0.281250]\n",
      "6203: [discriminator loss: 0.507200, acc: 0.734375] [adversarial loss: 1.414448, acc: 0.156250]\n",
      "6204: [discriminator loss: 0.556798, acc: 0.734375] [adversarial loss: 1.033236, acc: 0.343750]\n",
      "6205: [discriminator loss: 0.527318, acc: 0.742188] [adversarial loss: 1.218495, acc: 0.250000]\n",
      "6206: [discriminator loss: 0.500409, acc: 0.750000] [adversarial loss: 1.192638, acc: 0.250000]\n",
      "6207: [discriminator loss: 0.527344, acc: 0.726562] [adversarial loss: 1.535888, acc: 0.046875]\n",
      "6208: [discriminator loss: 0.595908, acc: 0.640625] [adversarial loss: 0.965856, acc: 0.296875]\n",
      "6209: [discriminator loss: 0.551375, acc: 0.703125] [adversarial loss: 1.298720, acc: 0.234375]\n",
      "6210: [discriminator loss: 0.602969, acc: 0.679688] [adversarial loss: 1.223667, acc: 0.203125]\n",
      "6211: [discriminator loss: 0.495725, acc: 0.781250] [adversarial loss: 1.218401, acc: 0.187500]\n",
      "6212: [discriminator loss: 0.540682, acc: 0.695312] [adversarial loss: 1.157974, acc: 0.328125]\n",
      "6213: [discriminator loss: 0.468358, acc: 0.765625] [adversarial loss: 1.208213, acc: 0.203125]\n",
      "6214: [discriminator loss: 0.603587, acc: 0.640625] [adversarial loss: 1.329409, acc: 0.093750]\n",
      "6215: [discriminator loss: 0.479037, acc: 0.781250] [adversarial loss: 0.953364, acc: 0.359375]\n",
      "6216: [discriminator loss: 0.435582, acc: 0.804688] [adversarial loss: 1.352558, acc: 0.125000]\n",
      "6217: [discriminator loss: 0.567812, acc: 0.695312] [adversarial loss: 1.284378, acc: 0.187500]\n",
      "6218: [discriminator loss: 0.553979, acc: 0.679688] [adversarial loss: 0.902949, acc: 0.437500]\n",
      "6219: [discriminator loss: 0.602788, acc: 0.671875] [adversarial loss: 1.713447, acc: 0.140625]\n",
      "6220: [discriminator loss: 0.654521, acc: 0.656250] [adversarial loss: 0.854267, acc: 0.468750]\n",
      "6221: [discriminator loss: 0.562260, acc: 0.718750] [adversarial loss: 1.566717, acc: 0.125000]\n",
      "6222: [discriminator loss: 0.581832, acc: 0.679688] [adversarial loss: 1.142582, acc: 0.265625]\n",
      "6223: [discriminator loss: 0.596966, acc: 0.695312] [adversarial loss: 1.786373, acc: 0.093750]\n",
      "6224: [discriminator loss: 0.509185, acc: 0.710938] [adversarial loss: 0.910656, acc: 0.328125]\n",
      "6225: [discriminator loss: 0.571578, acc: 0.695312] [adversarial loss: 1.243442, acc: 0.187500]\n",
      "6226: [discriminator loss: 0.643601, acc: 0.640625] [adversarial loss: 1.025825, acc: 0.281250]\n",
      "6227: [discriminator loss: 0.563216, acc: 0.710938] [adversarial loss: 1.406924, acc: 0.078125]\n",
      "6228: [discriminator loss: 0.493255, acc: 0.750000] [adversarial loss: 0.984082, acc: 0.296875]\n",
      "6229: [discriminator loss: 0.496262, acc: 0.742188] [adversarial loss: 1.436766, acc: 0.109375]\n",
      "6230: [discriminator loss: 0.489159, acc: 0.726562] [adversarial loss: 1.028477, acc: 0.265625]\n",
      "6231: [discriminator loss: 0.500050, acc: 0.773438] [adversarial loss: 1.274726, acc: 0.156250]\n",
      "6232: [discriminator loss: 0.580220, acc: 0.687500] [adversarial loss: 0.931804, acc: 0.359375]\n",
      "6233: [discriminator loss: 0.534628, acc: 0.695312] [adversarial loss: 1.379699, acc: 0.156250]\n",
      "6234: [discriminator loss: 0.486753, acc: 0.781250] [adversarial loss: 1.125445, acc: 0.234375]\n",
      "6235: [discriminator loss: 0.434199, acc: 0.828125] [adversarial loss: 1.343021, acc: 0.125000]\n",
      "6236: [discriminator loss: 0.523460, acc: 0.718750] [adversarial loss: 1.095301, acc: 0.234375]\n",
      "6237: [discriminator loss: 0.436116, acc: 0.835938] [adversarial loss: 1.367165, acc: 0.109375]\n",
      "6238: [discriminator loss: 0.500569, acc: 0.781250] [adversarial loss: 1.133533, acc: 0.312500]\n",
      "6239: [discriminator loss: 0.544737, acc: 0.679688] [adversarial loss: 1.121083, acc: 0.281250]\n",
      "6240: [discriminator loss: 0.537484, acc: 0.718750] [adversarial loss: 0.979933, acc: 0.406250]\n",
      "6241: [discriminator loss: 0.487547, acc: 0.812500] [adversarial loss: 1.183283, acc: 0.187500]\n",
      "6242: [discriminator loss: 0.570349, acc: 0.664062] [adversarial loss: 1.024525, acc: 0.375000]\n",
      "6243: [discriminator loss: 0.463247, acc: 0.789062] [adversarial loss: 1.379044, acc: 0.156250]\n",
      "6244: [discriminator loss: 0.564549, acc: 0.710938] [adversarial loss: 0.985426, acc: 0.468750]\n",
      "6245: [discriminator loss: 0.594340, acc: 0.703125] [adversarial loss: 1.904918, acc: 0.015625]\n",
      "6246: [discriminator loss: 0.548228, acc: 0.695312] [adversarial loss: 0.901001, acc: 0.390625]\n",
      "6247: [discriminator loss: 0.528196, acc: 0.726562] [adversarial loss: 1.614569, acc: 0.093750]\n",
      "6248: [discriminator loss: 0.598143, acc: 0.710938] [adversarial loss: 1.327845, acc: 0.171875]\n",
      "6249: [discriminator loss: 0.599141, acc: 0.671875] [adversarial loss: 1.504971, acc: 0.062500]\n",
      "6250: [discriminator loss: 0.539297, acc: 0.742188] [adversarial loss: 1.124080, acc: 0.281250]\n",
      "6251: [discriminator loss: 0.502071, acc: 0.718750] [adversarial loss: 1.363909, acc: 0.140625]\n",
      "6252: [discriminator loss: 0.483682, acc: 0.742188] [adversarial loss: 1.314948, acc: 0.187500]\n",
      "6253: [discriminator loss: 0.516045, acc: 0.742188] [adversarial loss: 1.074413, acc: 0.281250]\n",
      "6254: [discriminator loss: 0.507295, acc: 0.726562] [adversarial loss: 1.203237, acc: 0.156250]\n",
      "6255: [discriminator loss: 0.527014, acc: 0.781250] [adversarial loss: 1.313130, acc: 0.171875]\n",
      "6256: [discriminator loss: 0.418429, acc: 0.804688] [adversarial loss: 1.190222, acc: 0.234375]\n",
      "6257: [discriminator loss: 0.493073, acc: 0.781250] [adversarial loss: 1.135778, acc: 0.250000]\n",
      "6258: [discriminator loss: 0.543544, acc: 0.726562] [adversarial loss: 1.534853, acc: 0.093750]\n",
      "6259: [discriminator loss: 0.497745, acc: 0.742188] [adversarial loss: 0.739871, acc: 0.531250]\n",
      "6260: [discriminator loss: 0.598655, acc: 0.687500] [adversarial loss: 1.771116, acc: 0.062500]\n",
      "6261: [discriminator loss: 0.596789, acc: 0.703125] [adversarial loss: 0.915585, acc: 0.312500]\n",
      "6262: [discriminator loss: 0.493967, acc: 0.789062] [adversarial loss: 1.462068, acc: 0.062500]\n",
      "6263: [discriminator loss: 0.540111, acc: 0.726562] [adversarial loss: 1.079846, acc: 0.265625]\n",
      "6264: [discriminator loss: 0.477987, acc: 0.710938] [adversarial loss: 1.211855, acc: 0.234375]\n",
      "6265: [discriminator loss: 0.560204, acc: 0.695312] [adversarial loss: 1.153286, acc: 0.234375]\n",
      "6266: [discriminator loss: 0.559079, acc: 0.742188] [adversarial loss: 1.371770, acc: 0.140625]\n",
      "6267: [discriminator loss: 0.548285, acc: 0.718750] [adversarial loss: 1.194603, acc: 0.234375]\n",
      "6268: [discriminator loss: 0.600720, acc: 0.671875] [adversarial loss: 1.471927, acc: 0.140625]\n",
      "6269: [discriminator loss: 0.519860, acc: 0.750000] [adversarial loss: 1.235299, acc: 0.250000]\n",
      "6270: [discriminator loss: 0.533999, acc: 0.687500] [adversarial loss: 1.390264, acc: 0.171875]\n",
      "6271: [discriminator loss: 0.595565, acc: 0.703125] [adversarial loss: 0.883986, acc: 0.375000]\n",
      "6272: [discriminator loss: 0.622190, acc: 0.679688] [adversarial loss: 1.038623, acc: 0.296875]\n",
      "6273: [discriminator loss: 0.485728, acc: 0.781250] [adversarial loss: 1.136773, acc: 0.265625]\n",
      "6274: [discriminator loss: 0.474190, acc: 0.796875] [adversarial loss: 1.029032, acc: 0.359375]\n",
      "6275: [discriminator loss: 0.521878, acc: 0.703125] [adversarial loss: 1.212795, acc: 0.203125]\n",
      "6276: [discriminator loss: 0.500065, acc: 0.773438] [adversarial loss: 1.248962, acc: 0.156250]\n",
      "6277: [discriminator loss: 0.539018, acc: 0.710938] [adversarial loss: 1.205811, acc: 0.265625]\n",
      "6278: [discriminator loss: 0.515651, acc: 0.742188] [adversarial loss: 1.142454, acc: 0.281250]\n",
      "6279: [discriminator loss: 0.584237, acc: 0.687500] [adversarial loss: 1.249309, acc: 0.281250]\n",
      "6280: [discriminator loss: 0.500726, acc: 0.742188] [adversarial loss: 1.102277, acc: 0.265625]\n",
      "6281: [discriminator loss: 0.548821, acc: 0.757812] [adversarial loss: 1.338852, acc: 0.218750]\n",
      "6282: [discriminator loss: 0.492077, acc: 0.750000] [adversarial loss: 1.058401, acc: 0.359375]\n",
      "6283: [discriminator loss: 0.536468, acc: 0.718750] [adversarial loss: 1.447871, acc: 0.171875]\n",
      "6284: [discriminator loss: 0.444401, acc: 0.781250] [adversarial loss: 1.047117, acc: 0.328125]\n",
      "6285: [discriminator loss: 0.444412, acc: 0.812500] [adversarial loss: 1.587297, acc: 0.109375]\n",
      "6286: [discriminator loss: 0.530904, acc: 0.710938] [adversarial loss: 1.114116, acc: 0.187500]\n",
      "6287: [discriminator loss: 0.465495, acc: 0.757812] [adversarial loss: 1.414019, acc: 0.203125]\n",
      "6288: [discriminator loss: 0.458945, acc: 0.773438] [adversarial loss: 0.871864, acc: 0.375000]\n",
      "6289: [discriminator loss: 0.685615, acc: 0.648438] [adversarial loss: 1.579799, acc: 0.078125]\n",
      "6290: [discriminator loss: 0.576180, acc: 0.695312] [adversarial loss: 0.947303, acc: 0.390625]\n",
      "6291: [discriminator loss: 0.630770, acc: 0.656250] [adversarial loss: 1.669511, acc: 0.078125]\n",
      "6292: [discriminator loss: 0.630898, acc: 0.625000] [adversarial loss: 0.752342, acc: 0.500000]\n",
      "6293: [discriminator loss: 0.476686, acc: 0.757812] [adversarial loss: 1.619266, acc: 0.109375]\n",
      "6294: [discriminator loss: 0.587457, acc: 0.710938] [adversarial loss: 1.037396, acc: 0.390625]\n",
      "6295: [discriminator loss: 0.524492, acc: 0.710938] [adversarial loss: 0.956620, acc: 0.359375]\n",
      "6296: [discriminator loss: 0.496220, acc: 0.773438] [adversarial loss: 1.185391, acc: 0.234375]\n",
      "6297: [discriminator loss: 0.565081, acc: 0.687500] [adversarial loss: 1.403475, acc: 0.109375]\n",
      "6298: [discriminator loss: 0.521813, acc: 0.726562] [adversarial loss: 1.131192, acc: 0.250000]\n",
      "6299: [discriminator loss: 0.530431, acc: 0.757812] [adversarial loss: 1.215375, acc: 0.203125]\n",
      "6300: [discriminator loss: 0.488319, acc: 0.750000] [adversarial loss: 1.359003, acc: 0.093750]\n",
      "6301: [discriminator loss: 0.509597, acc: 0.742188] [adversarial loss: 1.083911, acc: 0.390625]\n",
      "6302: [discriminator loss: 0.623318, acc: 0.656250] [adversarial loss: 0.889691, acc: 0.453125]\n",
      "6303: [discriminator loss: 0.540455, acc: 0.695312] [adversarial loss: 1.645739, acc: 0.109375]\n",
      "6304: [discriminator loss: 0.538606, acc: 0.687500] [adversarial loss: 0.816891, acc: 0.453125]\n",
      "6305: [discriminator loss: 0.618475, acc: 0.687500] [adversarial loss: 1.840769, acc: 0.093750]\n",
      "6306: [discriminator loss: 0.489252, acc: 0.726562] [adversarial loss: 1.200592, acc: 0.296875]\n",
      "6307: [discriminator loss: 0.566612, acc: 0.710938] [adversarial loss: 1.280090, acc: 0.156250]\n",
      "6308: [discriminator loss: 0.578503, acc: 0.664062] [adversarial loss: 1.149949, acc: 0.250000]\n",
      "6309: [discriminator loss: 0.547408, acc: 0.726562] [adversarial loss: 1.269033, acc: 0.125000]\n",
      "6310: [discriminator loss: 0.517297, acc: 0.789062] [adversarial loss: 1.087741, acc: 0.250000]\n",
      "6311: [discriminator loss: 0.448320, acc: 0.804688] [adversarial loss: 1.155926, acc: 0.203125]\n",
      "6312: [discriminator loss: 0.555539, acc: 0.726562] [adversarial loss: 1.152890, acc: 0.187500]\n",
      "6313: [discriminator loss: 0.562714, acc: 0.671875] [adversarial loss: 1.335802, acc: 0.140625]\n",
      "6314: [discriminator loss: 0.481555, acc: 0.757812] [adversarial loss: 0.892269, acc: 0.484375]\n",
      "6315: [discriminator loss: 0.612998, acc: 0.671875] [adversarial loss: 1.825898, acc: 0.046875]\n",
      "6316: [discriminator loss: 0.591221, acc: 0.656250] [adversarial loss: 0.716734, acc: 0.515625]\n",
      "6317: [discriminator loss: 0.561277, acc: 0.734375] [adversarial loss: 1.449678, acc: 0.203125]\n",
      "6318: [discriminator loss: 0.543145, acc: 0.718750] [adversarial loss: 1.152535, acc: 0.203125]\n",
      "6319: [discriminator loss: 0.559525, acc: 0.718750] [adversarial loss: 1.426314, acc: 0.187500]\n",
      "6320: [discriminator loss: 0.486165, acc: 0.734375] [adversarial loss: 1.169328, acc: 0.281250]\n",
      "6321: [discriminator loss: 0.535403, acc: 0.710938] [adversarial loss: 1.295469, acc: 0.203125]\n",
      "6322: [discriminator loss: 0.491820, acc: 0.750000] [adversarial loss: 1.246955, acc: 0.156250]\n",
      "6323: [discriminator loss: 0.592561, acc: 0.687500] [adversarial loss: 1.441077, acc: 0.140625]\n",
      "6324: [discriminator loss: 0.563401, acc: 0.726562] [adversarial loss: 1.198733, acc: 0.234375]\n",
      "6325: [discriminator loss: 0.452037, acc: 0.820312] [adversarial loss: 1.198373, acc: 0.218750]\n",
      "6326: [discriminator loss: 0.488718, acc: 0.750000] [adversarial loss: 1.240359, acc: 0.218750]\n",
      "6327: [discriminator loss: 0.486162, acc: 0.765625] [adversarial loss: 1.206065, acc: 0.281250]\n",
      "6328: [discriminator loss: 0.514248, acc: 0.710938] [adversarial loss: 1.281955, acc: 0.218750]\n",
      "6329: [discriminator loss: 0.520306, acc: 0.734375] [adversarial loss: 0.983623, acc: 0.328125]\n",
      "6330: [discriminator loss: 0.503203, acc: 0.773438] [adversarial loss: 1.686222, acc: 0.125000]\n",
      "6331: [discriminator loss: 0.580596, acc: 0.671875] [adversarial loss: 1.114597, acc: 0.203125]\n",
      "6332: [discriminator loss: 0.540585, acc: 0.726562] [adversarial loss: 1.104676, acc: 0.312500]\n",
      "6333: [discriminator loss: 0.484062, acc: 0.796875] [adversarial loss: 1.204684, acc: 0.218750]\n",
      "6334: [discriminator loss: 0.508247, acc: 0.726562] [adversarial loss: 1.021238, acc: 0.265625]\n",
      "6335: [discriminator loss: 0.599781, acc: 0.687500] [adversarial loss: 1.732414, acc: 0.046875]\n",
      "6336: [discriminator loss: 0.494117, acc: 0.742188] [adversarial loss: 1.099062, acc: 0.218750]\n",
      "6337: [discriminator loss: 0.452960, acc: 0.796875] [adversarial loss: 1.217609, acc: 0.140625]\n",
      "6338: [discriminator loss: 0.551919, acc: 0.710938] [adversarial loss: 1.063593, acc: 0.343750]\n",
      "6339: [discriminator loss: 0.651634, acc: 0.656250] [adversarial loss: 1.710494, acc: 0.093750]\n",
      "6340: [discriminator loss: 0.579866, acc: 0.742188] [adversarial loss: 1.107951, acc: 0.312500]\n",
      "6341: [discriminator loss: 0.551785, acc: 0.718750] [adversarial loss: 1.386617, acc: 0.187500]\n",
      "6342: [discriminator loss: 0.482817, acc: 0.726562] [adversarial loss: 0.981510, acc: 0.312500]\n",
      "6343: [discriminator loss: 0.619714, acc: 0.656250] [adversarial loss: 1.361762, acc: 0.203125]\n",
      "6344: [discriminator loss: 0.533920, acc: 0.726562] [adversarial loss: 1.125610, acc: 0.281250]\n",
      "6345: [discriminator loss: 0.539713, acc: 0.726562] [adversarial loss: 1.357417, acc: 0.203125]\n",
      "6346: [discriminator loss: 0.520338, acc: 0.781250] [adversarial loss: 0.976181, acc: 0.328125]\n",
      "6347: [discriminator loss: 0.480751, acc: 0.765625] [adversarial loss: 1.364721, acc: 0.156250]\n",
      "6348: [discriminator loss: 0.576356, acc: 0.695312] [adversarial loss: 0.966216, acc: 0.375000]\n",
      "6349: [discriminator loss: 0.557103, acc: 0.656250] [adversarial loss: 1.359761, acc: 0.171875]\n",
      "6350: [discriminator loss: 0.614787, acc: 0.640625] [adversarial loss: 1.098965, acc: 0.218750]\n",
      "6351: [discriminator loss: 0.570414, acc: 0.687500] [adversarial loss: 1.225487, acc: 0.234375]\n",
      "6352: [discriminator loss: 0.541926, acc: 0.781250] [adversarial loss: 1.451199, acc: 0.187500]\n",
      "6353: [discriminator loss: 0.579985, acc: 0.687500] [adversarial loss: 0.878610, acc: 0.468750]\n",
      "6354: [discriminator loss: 0.494862, acc: 0.820312] [adversarial loss: 1.556481, acc: 0.109375]\n",
      "6355: [discriminator loss: 0.477132, acc: 0.765625] [adversarial loss: 0.927570, acc: 0.390625]\n",
      "6356: [discriminator loss: 0.505877, acc: 0.773438] [adversarial loss: 1.325957, acc: 0.156250]\n",
      "6357: [discriminator loss: 0.493981, acc: 0.726562] [adversarial loss: 0.990681, acc: 0.359375]\n",
      "6358: [discriminator loss: 0.532851, acc: 0.734375] [adversarial loss: 1.257856, acc: 0.171875]\n",
      "6359: [discriminator loss: 0.483087, acc: 0.781250] [adversarial loss: 1.356467, acc: 0.125000]\n",
      "6360: [discriminator loss: 0.547799, acc: 0.742188] [adversarial loss: 1.227228, acc: 0.203125]\n",
      "6361: [discriminator loss: 0.518262, acc: 0.734375] [adversarial loss: 1.637978, acc: 0.125000]\n",
      "6362: [discriminator loss: 0.623339, acc: 0.679688] [adversarial loss: 1.159119, acc: 0.281250]\n",
      "6363: [discriminator loss: 0.502356, acc: 0.734375] [adversarial loss: 1.415731, acc: 0.140625]\n",
      "6364: [discriminator loss: 0.545182, acc: 0.703125] [adversarial loss: 1.206297, acc: 0.187500]\n",
      "6365: [discriminator loss: 0.528806, acc: 0.765625] [adversarial loss: 0.903369, acc: 0.406250]\n",
      "6366: [discriminator loss: 0.538345, acc: 0.718750] [adversarial loss: 1.754791, acc: 0.125000]\n",
      "6367: [discriminator loss: 0.604672, acc: 0.695312] [adversarial loss: 0.802507, acc: 0.468750]\n",
      "6368: [discriminator loss: 0.537139, acc: 0.750000] [adversarial loss: 1.503801, acc: 0.156250]\n",
      "6369: [discriminator loss: 0.518315, acc: 0.734375] [adversarial loss: 1.096587, acc: 0.296875]\n",
      "6370: [discriminator loss: 0.586739, acc: 0.726562] [adversarial loss: 1.383589, acc: 0.140625]\n",
      "6371: [discriminator loss: 0.487664, acc: 0.726562] [adversarial loss: 1.412879, acc: 0.171875]\n",
      "6372: [discriminator loss: 0.547796, acc: 0.750000] [adversarial loss: 1.068014, acc: 0.250000]\n",
      "6373: [discriminator loss: 0.562228, acc: 0.710938] [adversarial loss: 1.310971, acc: 0.234375]\n",
      "6374: [discriminator loss: 0.509284, acc: 0.757812] [adversarial loss: 1.290457, acc: 0.203125]\n",
      "6375: [discriminator loss: 0.479462, acc: 0.796875] [adversarial loss: 1.339955, acc: 0.234375]\n",
      "6376: [discriminator loss: 0.548021, acc: 0.710938] [adversarial loss: 1.274364, acc: 0.203125]\n",
      "6377: [discriminator loss: 0.623422, acc: 0.640625] [adversarial loss: 1.235146, acc: 0.187500]\n",
      "6378: [discriminator loss: 0.563879, acc: 0.703125] [adversarial loss: 1.324869, acc: 0.187500]\n",
      "6379: [discriminator loss: 0.538532, acc: 0.718750] [adversarial loss: 1.169329, acc: 0.187500]\n",
      "6380: [discriminator loss: 0.528490, acc: 0.734375] [adversarial loss: 1.236588, acc: 0.171875]\n",
      "6381: [discriminator loss: 0.537552, acc: 0.718750] [adversarial loss: 1.203881, acc: 0.250000]\n",
      "6382: [discriminator loss: 0.519816, acc: 0.734375] [adversarial loss: 1.350164, acc: 0.187500]\n",
      "6383: [discriminator loss: 0.510197, acc: 0.687500] [adversarial loss: 1.214823, acc: 0.312500]\n",
      "6384: [discriminator loss: 0.500769, acc: 0.742188] [adversarial loss: 1.112053, acc: 0.328125]\n",
      "6385: [discriminator loss: 0.548534, acc: 0.718750] [adversarial loss: 1.413892, acc: 0.093750]\n",
      "6386: [discriminator loss: 0.564447, acc: 0.695312] [adversarial loss: 1.179981, acc: 0.203125]\n",
      "6387: [discriminator loss: 0.544454, acc: 0.718750] [adversarial loss: 1.367982, acc: 0.203125]\n",
      "6388: [discriminator loss: 0.592086, acc: 0.687500] [adversarial loss: 1.257986, acc: 0.156250]\n",
      "6389: [discriminator loss: 0.482613, acc: 0.773438] [adversarial loss: 1.462366, acc: 0.109375]\n",
      "6390: [discriminator loss: 0.584419, acc: 0.695312] [adversarial loss: 0.998539, acc: 0.328125]\n",
      "6391: [discriminator loss: 0.598489, acc: 0.695312] [adversarial loss: 1.504792, acc: 0.156250]\n",
      "6392: [discriminator loss: 0.528904, acc: 0.750000] [adversarial loss: 0.894034, acc: 0.406250]\n",
      "6393: [discriminator loss: 0.547977, acc: 0.710938] [adversarial loss: 1.585803, acc: 0.156250]\n",
      "6394: [discriminator loss: 0.556506, acc: 0.718750] [adversarial loss: 1.230014, acc: 0.281250]\n",
      "6395: [discriminator loss: 0.569021, acc: 0.679688] [adversarial loss: 1.085832, acc: 0.250000]\n",
      "6396: [discriminator loss: 0.475657, acc: 0.734375] [adversarial loss: 1.251147, acc: 0.187500]\n",
      "6397: [discriminator loss: 0.509757, acc: 0.734375] [adversarial loss: 1.034712, acc: 0.359375]\n",
      "6398: [discriminator loss: 0.505258, acc: 0.742188] [adversarial loss: 1.322768, acc: 0.203125]\n",
      "6399: [discriminator loss: 0.602174, acc: 0.703125] [adversarial loss: 1.137057, acc: 0.218750]\n",
      "6400: [discriminator loss: 0.534026, acc: 0.742188] [adversarial loss: 1.273833, acc: 0.203125]\n",
      "6401: [discriminator loss: 0.533152, acc: 0.718750] [adversarial loss: 0.827819, acc: 0.421875]\n",
      "6402: [discriminator loss: 0.497235, acc: 0.765625] [adversarial loss: 1.437807, acc: 0.171875]\n",
      "6403: [discriminator loss: 0.529694, acc: 0.742188] [adversarial loss: 0.788079, acc: 0.453125]\n",
      "6404: [discriminator loss: 0.546062, acc: 0.718750] [adversarial loss: 1.449665, acc: 0.125000]\n",
      "6405: [discriminator loss: 0.557358, acc: 0.679688] [adversarial loss: 0.949945, acc: 0.421875]\n",
      "6406: [discriminator loss: 0.476696, acc: 0.765625] [adversarial loss: 1.647164, acc: 0.156250]\n",
      "6407: [discriminator loss: 0.595204, acc: 0.695312] [adversarial loss: 0.917722, acc: 0.421875]\n",
      "6408: [discriminator loss: 0.566439, acc: 0.703125] [adversarial loss: 1.163088, acc: 0.312500]\n",
      "6409: [discriminator loss: 0.502778, acc: 0.765625] [adversarial loss: 1.329078, acc: 0.187500]\n",
      "6410: [discriminator loss: 0.556446, acc: 0.718750] [adversarial loss: 1.051121, acc: 0.265625]\n",
      "6411: [discriminator loss: 0.539711, acc: 0.718750] [adversarial loss: 1.454106, acc: 0.109375]\n",
      "6412: [discriminator loss: 0.498880, acc: 0.710938] [adversarial loss: 1.159247, acc: 0.234375]\n",
      "6413: [discriminator loss: 0.543640, acc: 0.718750] [adversarial loss: 1.185502, acc: 0.234375]\n",
      "6414: [discriminator loss: 0.526179, acc: 0.742188] [adversarial loss: 0.993345, acc: 0.390625]\n",
      "6415: [discriminator loss: 0.495190, acc: 0.726562] [adversarial loss: 1.537341, acc: 0.093750]\n",
      "6416: [discriminator loss: 0.571771, acc: 0.703125] [adversarial loss: 0.934435, acc: 0.343750]\n",
      "6417: [discriminator loss: 0.594831, acc: 0.703125] [adversarial loss: 1.146174, acc: 0.281250]\n",
      "6418: [discriminator loss: 0.510266, acc: 0.734375] [adversarial loss: 1.438722, acc: 0.125000]\n",
      "6419: [discriminator loss: 0.566350, acc: 0.703125] [adversarial loss: 0.917410, acc: 0.343750]\n",
      "6420: [discriminator loss: 0.528284, acc: 0.765625] [adversarial loss: 1.302413, acc: 0.093750]\n",
      "6421: [discriminator loss: 0.436886, acc: 0.781250] [adversarial loss: 1.091657, acc: 0.281250]\n",
      "6422: [discriminator loss: 0.560360, acc: 0.718750] [adversarial loss: 1.087998, acc: 0.281250]\n",
      "6423: [discriminator loss: 0.530270, acc: 0.781250] [adversarial loss: 1.239508, acc: 0.171875]\n",
      "6424: [discriminator loss: 0.515754, acc: 0.765625] [adversarial loss: 1.155375, acc: 0.281250]\n",
      "6425: [discriminator loss: 0.529911, acc: 0.742188] [adversarial loss: 1.042137, acc: 0.296875]\n",
      "6426: [discriminator loss: 0.527595, acc: 0.757812] [adversarial loss: 1.441894, acc: 0.125000]\n",
      "6427: [discriminator loss: 0.508683, acc: 0.734375] [adversarial loss: 0.856490, acc: 0.468750]\n",
      "6428: [discriminator loss: 0.581726, acc: 0.656250] [adversarial loss: 1.851452, acc: 0.015625]\n",
      "6429: [discriminator loss: 0.498109, acc: 0.726562] [adversarial loss: 1.019128, acc: 0.250000]\n",
      "6430: [discriminator loss: 0.520394, acc: 0.726562] [adversarial loss: 1.424423, acc: 0.078125]\n",
      "6431: [discriminator loss: 0.553930, acc: 0.671875] [adversarial loss: 1.090451, acc: 0.296875]\n",
      "6432: [discriminator loss: 0.560657, acc: 0.703125] [adversarial loss: 1.293480, acc: 0.187500]\n",
      "6433: [discriminator loss: 0.517104, acc: 0.718750] [adversarial loss: 0.883738, acc: 0.421875]\n",
      "6434: [discriminator loss: 0.495495, acc: 0.757812] [adversarial loss: 1.287098, acc: 0.203125]\n",
      "6435: [discriminator loss: 0.525679, acc: 0.695312] [adversarial loss: 1.140745, acc: 0.203125]\n",
      "6436: [discriminator loss: 0.585253, acc: 0.710938] [adversarial loss: 1.394273, acc: 0.218750]\n",
      "6437: [discriminator loss: 0.544941, acc: 0.765625] [adversarial loss: 0.985898, acc: 0.375000]\n",
      "6438: [discriminator loss: 0.523721, acc: 0.718750] [adversarial loss: 1.383329, acc: 0.218750]\n",
      "6439: [discriminator loss: 0.511068, acc: 0.750000] [adversarial loss: 1.219650, acc: 0.296875]\n",
      "6440: [discriminator loss: 0.536133, acc: 0.734375] [adversarial loss: 0.942928, acc: 0.359375]\n",
      "6441: [discriminator loss: 0.536606, acc: 0.710938] [adversarial loss: 1.514094, acc: 0.171875]\n",
      "6442: [discriminator loss: 0.515153, acc: 0.734375] [adversarial loss: 1.035799, acc: 0.250000]\n",
      "6443: [discriminator loss: 0.563692, acc: 0.679688] [adversarial loss: 1.660009, acc: 0.140625]\n",
      "6444: [discriminator loss: 0.611379, acc: 0.625000] [adversarial loss: 1.149342, acc: 0.281250]\n",
      "6445: [discriminator loss: 0.549826, acc: 0.734375] [adversarial loss: 1.362395, acc: 0.109375]\n",
      "6446: [discriminator loss: 0.483078, acc: 0.710938] [adversarial loss: 1.402455, acc: 0.171875]\n",
      "6447: [discriminator loss: 0.549998, acc: 0.726562] [adversarial loss: 0.921485, acc: 0.375000]\n",
      "6448: [discriminator loss: 0.585336, acc: 0.679688] [adversarial loss: 1.363127, acc: 0.140625]\n",
      "6449: [discriminator loss: 0.463625, acc: 0.796875] [adversarial loss: 1.032894, acc: 0.265625]\n",
      "6450: [discriminator loss: 0.552915, acc: 0.742188] [adversarial loss: 1.439595, acc: 0.109375]\n",
      "6451: [discriminator loss: 0.576885, acc: 0.679688] [adversarial loss: 0.768932, acc: 0.515625]\n",
      "6452: [discriminator loss: 0.560353, acc: 0.718750] [adversarial loss: 1.734148, acc: 0.031250]\n",
      "6453: [discriminator loss: 0.607442, acc: 0.710938] [adversarial loss: 0.901614, acc: 0.500000]\n",
      "6454: [discriminator loss: 0.625944, acc: 0.601562] [adversarial loss: 1.396260, acc: 0.125000]\n",
      "6455: [discriminator loss: 0.502327, acc: 0.765625] [adversarial loss: 1.009515, acc: 0.312500]\n",
      "6456: [discriminator loss: 0.500128, acc: 0.789062] [adversarial loss: 1.509656, acc: 0.109375]\n",
      "6457: [discriminator loss: 0.523197, acc: 0.718750] [adversarial loss: 1.084690, acc: 0.187500]\n",
      "6458: [discriminator loss: 0.598548, acc: 0.664062] [adversarial loss: 1.421629, acc: 0.109375]\n",
      "6459: [discriminator loss: 0.548119, acc: 0.734375] [adversarial loss: 1.096492, acc: 0.265625]\n",
      "6460: [discriminator loss: 0.519266, acc: 0.687500] [adversarial loss: 1.490843, acc: 0.109375]\n",
      "6461: [discriminator loss: 0.563988, acc: 0.703125] [adversarial loss: 0.986271, acc: 0.390625]\n",
      "6462: [discriminator loss: 0.540093, acc: 0.718750] [adversarial loss: 1.449298, acc: 0.125000]\n",
      "6463: [discriminator loss: 0.645404, acc: 0.632812] [adversarial loss: 0.885509, acc: 0.406250]\n",
      "6464: [discriminator loss: 0.522964, acc: 0.773438] [adversarial loss: 1.247938, acc: 0.203125]\n",
      "6465: [discriminator loss: 0.490821, acc: 0.781250] [adversarial loss: 1.232300, acc: 0.234375]\n",
      "6466: [discriminator loss: 0.500514, acc: 0.734375] [adversarial loss: 1.120604, acc: 0.250000]\n",
      "6467: [discriminator loss: 0.542596, acc: 0.687500] [adversarial loss: 1.258203, acc: 0.171875]\n",
      "6468: [discriminator loss: 0.546317, acc: 0.664062] [adversarial loss: 1.173850, acc: 0.203125]\n",
      "6469: [discriminator loss: 0.450418, acc: 0.781250] [adversarial loss: 1.075351, acc: 0.390625]\n",
      "6470: [discriminator loss: 0.553155, acc: 0.718750] [adversarial loss: 1.427027, acc: 0.078125]\n",
      "6471: [discriminator loss: 0.501648, acc: 0.757812] [adversarial loss: 1.255280, acc: 0.218750]\n",
      "6472: [discriminator loss: 0.508927, acc: 0.734375] [adversarial loss: 1.493909, acc: 0.203125]\n",
      "6473: [discriminator loss: 0.544107, acc: 0.742188] [adversarial loss: 1.204053, acc: 0.265625]\n",
      "6474: [discriminator loss: 0.534886, acc: 0.710938] [adversarial loss: 1.193482, acc: 0.281250]\n",
      "6475: [discriminator loss: 0.539319, acc: 0.695312] [adversarial loss: 1.117049, acc: 0.265625]\n",
      "6476: [discriminator loss: 0.577068, acc: 0.687500] [adversarial loss: 0.984651, acc: 0.265625]\n",
      "6477: [discriminator loss: 0.556208, acc: 0.734375] [adversarial loss: 1.393768, acc: 0.125000]\n",
      "6478: [discriminator loss: 0.517565, acc: 0.710938] [adversarial loss: 0.999132, acc: 0.328125]\n",
      "6479: [discriminator loss: 0.520278, acc: 0.703125] [adversarial loss: 1.813221, acc: 0.093750]\n",
      "6480: [discriminator loss: 0.480439, acc: 0.781250] [adversarial loss: 0.957844, acc: 0.437500]\n",
      "6481: [discriminator loss: 0.596027, acc: 0.664062] [adversarial loss: 1.323071, acc: 0.203125]\n",
      "6482: [discriminator loss: 0.553004, acc: 0.703125] [adversarial loss: 1.148295, acc: 0.265625]\n",
      "6483: [discriminator loss: 0.523164, acc: 0.726562] [adversarial loss: 1.525678, acc: 0.187500]\n",
      "6484: [discriminator loss: 0.538544, acc: 0.742188] [adversarial loss: 0.935172, acc: 0.312500]\n",
      "6485: [discriminator loss: 0.572587, acc: 0.671875] [adversarial loss: 1.606312, acc: 0.031250]\n",
      "6486: [discriminator loss: 0.513295, acc: 0.765625] [adversarial loss: 1.111530, acc: 0.250000]\n",
      "6487: [discriminator loss: 0.526002, acc: 0.734375] [adversarial loss: 1.544823, acc: 0.125000]\n",
      "6488: [discriminator loss: 0.585618, acc: 0.703125] [adversarial loss: 0.957594, acc: 0.296875]\n",
      "6489: [discriminator loss: 0.523926, acc: 0.718750] [adversarial loss: 1.470054, acc: 0.140625]\n",
      "6490: [discriminator loss: 0.552515, acc: 0.679688] [adversarial loss: 0.972401, acc: 0.359375]\n",
      "6491: [discriminator loss: 0.612655, acc: 0.632812] [adversarial loss: 1.073330, acc: 0.281250]\n",
      "6492: [discriminator loss: 0.509660, acc: 0.710938] [adversarial loss: 1.094794, acc: 0.250000]\n",
      "6493: [discriminator loss: 0.565209, acc: 0.726562] [adversarial loss: 1.289884, acc: 0.203125]\n",
      "6494: [discriminator loss: 0.560251, acc: 0.726562] [adversarial loss: 1.014000, acc: 0.343750]\n",
      "6495: [discriminator loss: 0.568966, acc: 0.695312] [adversarial loss: 1.371384, acc: 0.125000]\n",
      "6496: [discriminator loss: 0.544885, acc: 0.671875] [adversarial loss: 1.119386, acc: 0.312500]\n",
      "6497: [discriminator loss: 0.611709, acc: 0.632812] [adversarial loss: 1.477881, acc: 0.109375]\n",
      "6498: [discriminator loss: 0.482172, acc: 0.742188] [adversarial loss: 1.258148, acc: 0.250000]\n",
      "6499: [discriminator loss: 0.551089, acc: 0.671875] [adversarial loss: 1.012689, acc: 0.312500]\n",
      "6500: [discriminator loss: 0.598676, acc: 0.718750] [adversarial loss: 1.407493, acc: 0.125000]\n",
      "6501: [discriminator loss: 0.554435, acc: 0.742188] [adversarial loss: 0.957155, acc: 0.390625]\n",
      "6502: [discriminator loss: 0.503262, acc: 0.757812] [adversarial loss: 1.259033, acc: 0.125000]\n",
      "6503: [discriminator loss: 0.510754, acc: 0.742188] [adversarial loss: 1.094861, acc: 0.250000]\n",
      "6504: [discriminator loss: 0.484357, acc: 0.789062] [adversarial loss: 1.323384, acc: 0.203125]\n",
      "6505: [discriminator loss: 0.545942, acc: 0.718750] [adversarial loss: 1.113434, acc: 0.328125]\n",
      "6506: [discriminator loss: 0.556923, acc: 0.726562] [adversarial loss: 1.236782, acc: 0.171875]\n",
      "6507: [discriminator loss: 0.497441, acc: 0.757812] [adversarial loss: 1.389507, acc: 0.140625]\n",
      "6508: [discriminator loss: 0.557886, acc: 0.687500] [adversarial loss: 1.123118, acc: 0.234375]\n",
      "6509: [discriminator loss: 0.492103, acc: 0.757812] [adversarial loss: 1.112207, acc: 0.218750]\n",
      "6510: [discriminator loss: 0.577449, acc: 0.679688] [adversarial loss: 1.395641, acc: 0.171875]\n",
      "6511: [discriminator loss: 0.425745, acc: 0.804688] [adversarial loss: 1.071151, acc: 0.359375]\n",
      "6512: [discriminator loss: 0.498194, acc: 0.773438] [adversarial loss: 1.297914, acc: 0.140625]\n",
      "6513: [discriminator loss: 0.508438, acc: 0.710938] [adversarial loss: 1.278139, acc: 0.171875]\n",
      "6514: [discriminator loss: 0.524436, acc: 0.710938] [adversarial loss: 1.090800, acc: 0.296875]\n",
      "6515: [discriminator loss: 0.586795, acc: 0.710938] [adversarial loss: 1.360597, acc: 0.171875]\n",
      "6516: [discriminator loss: 0.506451, acc: 0.742188] [adversarial loss: 1.190317, acc: 0.218750]\n",
      "6517: [discriminator loss: 0.621750, acc: 0.648438] [adversarial loss: 1.465511, acc: 0.140625]\n",
      "6518: [discriminator loss: 0.594227, acc: 0.695312] [adversarial loss: 0.993262, acc: 0.328125]\n",
      "6519: [discriminator loss: 0.487064, acc: 0.718750] [adversarial loss: 1.227091, acc: 0.171875]\n",
      "6520: [discriminator loss: 0.515989, acc: 0.726562] [adversarial loss: 0.818752, acc: 0.500000]\n",
      "6521: [discriminator loss: 0.573854, acc: 0.703125] [adversarial loss: 1.472927, acc: 0.093750]\n",
      "6522: [discriminator loss: 0.503556, acc: 0.781250] [adversarial loss: 0.979401, acc: 0.343750]\n",
      "6523: [discriminator loss: 0.565662, acc: 0.734375] [adversarial loss: 1.396063, acc: 0.109375]\n",
      "6524: [discriminator loss: 0.641879, acc: 0.632812] [adversarial loss: 0.946933, acc: 0.421875]\n",
      "6525: [discriminator loss: 0.591099, acc: 0.664062] [adversarial loss: 1.582273, acc: 0.187500]\n",
      "6526: [discriminator loss: 0.486699, acc: 0.765625] [adversarial loss: 0.929435, acc: 0.390625]\n",
      "6527: [discriminator loss: 0.461478, acc: 0.765625] [adversarial loss: 1.392854, acc: 0.156250]\n",
      "6528: [discriminator loss: 0.614801, acc: 0.648438] [adversarial loss: 1.021135, acc: 0.359375]\n",
      "6529: [discriminator loss: 0.538427, acc: 0.710938] [adversarial loss: 1.600891, acc: 0.109375]\n",
      "6530: [discriminator loss: 0.562357, acc: 0.687500] [adversarial loss: 0.995174, acc: 0.359375]\n",
      "6531: [discriminator loss: 0.538751, acc: 0.710938] [adversarial loss: 1.123706, acc: 0.281250]\n",
      "6532: [discriminator loss: 0.575787, acc: 0.734375] [adversarial loss: 1.125146, acc: 0.296875]\n",
      "6533: [discriminator loss: 0.521514, acc: 0.726562] [adversarial loss: 1.637474, acc: 0.109375]\n",
      "6534: [discriminator loss: 0.611337, acc: 0.679688] [adversarial loss: 1.110891, acc: 0.203125]\n",
      "6535: [discriminator loss: 0.505747, acc: 0.773438] [adversarial loss: 1.192002, acc: 0.187500]\n",
      "6536: [discriminator loss: 0.551208, acc: 0.703125] [adversarial loss: 1.229748, acc: 0.187500]\n",
      "6537: [discriminator loss: 0.470647, acc: 0.750000] [adversarial loss: 0.917871, acc: 0.453125]\n",
      "6538: [discriminator loss: 0.588415, acc: 0.687500] [adversarial loss: 1.138299, acc: 0.250000]\n",
      "6539: [discriminator loss: 0.529852, acc: 0.734375] [adversarial loss: 1.309877, acc: 0.093750]\n",
      "6540: [discriminator loss: 0.577723, acc: 0.695312] [adversarial loss: 1.209323, acc: 0.281250]\n",
      "6541: [discriminator loss: 0.554206, acc: 0.695312] [adversarial loss: 1.227531, acc: 0.187500]\n",
      "6542: [discriminator loss: 0.634791, acc: 0.640625] [adversarial loss: 1.231270, acc: 0.140625]\n",
      "6543: [discriminator loss: 0.487100, acc: 0.757812] [adversarial loss: 0.870090, acc: 0.468750]\n",
      "6544: [discriminator loss: 0.548664, acc: 0.726562] [adversarial loss: 1.539007, acc: 0.125000]\n",
      "6545: [discriminator loss: 0.531818, acc: 0.710938] [adversarial loss: 1.406634, acc: 0.156250]\n",
      "6546: [discriminator loss: 0.506036, acc: 0.765625] [adversarial loss: 0.901116, acc: 0.406250]\n",
      "6547: [discriminator loss: 0.567396, acc: 0.679688] [adversarial loss: 1.362241, acc: 0.187500]\n",
      "6548: [discriminator loss: 0.468805, acc: 0.765625] [adversarial loss: 1.004637, acc: 0.312500]\n",
      "6549: [discriminator loss: 0.499826, acc: 0.750000] [adversarial loss: 1.477366, acc: 0.093750]\n",
      "6550: [discriminator loss: 0.549065, acc: 0.718750] [adversarial loss: 0.982202, acc: 0.484375]\n",
      "6551: [discriminator loss: 0.598463, acc: 0.640625] [adversarial loss: 1.665142, acc: 0.109375]\n",
      "6552: [discriminator loss: 0.622755, acc: 0.679688] [adversarial loss: 1.018490, acc: 0.359375]\n",
      "6553: [discriminator loss: 0.614278, acc: 0.687500] [adversarial loss: 1.631066, acc: 0.078125]\n",
      "6554: [discriminator loss: 0.474130, acc: 0.742188] [adversarial loss: 0.893945, acc: 0.421875]\n",
      "6555: [discriminator loss: 0.495631, acc: 0.742188] [adversarial loss: 1.299885, acc: 0.234375]\n",
      "6556: [discriminator loss: 0.481026, acc: 0.750000] [adversarial loss: 1.044327, acc: 0.265625]\n",
      "6557: [discriminator loss: 0.601094, acc: 0.679688] [adversarial loss: 1.130059, acc: 0.281250]\n",
      "6558: [discriminator loss: 0.555867, acc: 0.718750] [adversarial loss: 0.864418, acc: 0.390625]\n",
      "6559: [discriminator loss: 0.511520, acc: 0.742188] [adversarial loss: 1.462732, acc: 0.078125]\n",
      "6560: [discriminator loss: 0.548060, acc: 0.742188] [adversarial loss: 0.971458, acc: 0.312500]\n",
      "6561: [discriminator loss: 0.554370, acc: 0.671875] [adversarial loss: 1.271654, acc: 0.250000]\n",
      "6562: [discriminator loss: 0.489842, acc: 0.765625] [adversarial loss: 1.021547, acc: 0.343750]\n",
      "6563: [discriminator loss: 0.465654, acc: 0.789062] [adversarial loss: 0.983647, acc: 0.375000]\n",
      "6564: [discriminator loss: 0.605798, acc: 0.632812] [adversarial loss: 1.255743, acc: 0.218750]\n",
      "6565: [discriminator loss: 0.507413, acc: 0.742188] [adversarial loss: 1.240033, acc: 0.156250]\n",
      "6566: [discriminator loss: 0.496387, acc: 0.742188] [adversarial loss: 1.456748, acc: 0.234375]\n",
      "6567: [discriminator loss: 0.582462, acc: 0.625000] [adversarial loss: 1.111778, acc: 0.250000]\n",
      "6568: [discriminator loss: 0.526356, acc: 0.703125] [adversarial loss: 1.504090, acc: 0.140625]\n",
      "6569: [discriminator loss: 0.494803, acc: 0.757812] [adversarial loss: 0.845754, acc: 0.453125]\n",
      "6570: [discriminator loss: 0.519240, acc: 0.734375] [adversarial loss: 1.234729, acc: 0.218750]\n",
      "6571: [discriminator loss: 0.552898, acc: 0.703125] [adversarial loss: 1.296889, acc: 0.218750]\n",
      "6572: [discriminator loss: 0.523338, acc: 0.734375] [adversarial loss: 1.044400, acc: 0.296875]\n",
      "6573: [discriminator loss: 0.521320, acc: 0.742188] [adversarial loss: 1.027998, acc: 0.265625]\n",
      "6574: [discriminator loss: 0.557156, acc: 0.695312] [adversarial loss: 1.772790, acc: 0.125000]\n",
      "6575: [discriminator loss: 0.605125, acc: 0.671875] [adversarial loss: 0.812768, acc: 0.484375]\n",
      "6576: [discriminator loss: 0.619620, acc: 0.703125] [adversarial loss: 1.547072, acc: 0.109375]\n",
      "6577: [discriminator loss: 0.598281, acc: 0.656250] [adversarial loss: 0.888217, acc: 0.468750]\n",
      "6578: [discriminator loss: 0.573309, acc: 0.671875] [adversarial loss: 1.601781, acc: 0.093750]\n",
      "6579: [discriminator loss: 0.548240, acc: 0.679688] [adversarial loss: 1.093716, acc: 0.265625]\n",
      "6580: [discriminator loss: 0.491913, acc: 0.773438] [adversarial loss: 1.211046, acc: 0.250000]\n",
      "6581: [discriminator loss: 0.616333, acc: 0.640625] [adversarial loss: 1.127584, acc: 0.187500]\n",
      "6582: [discriminator loss: 0.530576, acc: 0.695312] [adversarial loss: 1.245512, acc: 0.218750]\n",
      "6583: [discriminator loss: 0.589888, acc: 0.687500] [adversarial loss: 1.323737, acc: 0.187500]\n",
      "6584: [discriminator loss: 0.522237, acc: 0.734375] [adversarial loss: 0.920116, acc: 0.406250]\n",
      "6585: [discriminator loss: 0.595643, acc: 0.687500] [adversarial loss: 1.093461, acc: 0.296875]\n",
      "6586: [discriminator loss: 0.544433, acc: 0.703125] [adversarial loss: 1.033677, acc: 0.359375]\n",
      "6587: [discriminator loss: 0.567803, acc: 0.648438] [adversarial loss: 1.180718, acc: 0.234375]\n",
      "6588: [discriminator loss: 0.564231, acc: 0.687500] [adversarial loss: 0.975527, acc: 0.328125]\n",
      "6589: [discriminator loss: 0.509994, acc: 0.750000] [adversarial loss: 1.293564, acc: 0.125000]\n",
      "6590: [discriminator loss: 0.555781, acc: 0.742188] [adversarial loss: 1.013808, acc: 0.343750]\n",
      "6591: [discriminator loss: 0.548202, acc: 0.726562] [adversarial loss: 1.608302, acc: 0.046875]\n",
      "6592: [discriminator loss: 0.372286, acc: 0.867188] [adversarial loss: 1.359050, acc: 0.218750]\n",
      "6593: [discriminator loss: 0.679042, acc: 0.664062] [adversarial loss: 1.139356, acc: 0.312500]\n",
      "6594: [discriminator loss: 0.538849, acc: 0.687500] [adversarial loss: 1.096386, acc: 0.296875]\n",
      "6595: [discriminator loss: 0.600659, acc: 0.695312] [adversarial loss: 1.345406, acc: 0.234375]\n",
      "6596: [discriminator loss: 0.533535, acc: 0.750000] [adversarial loss: 1.307904, acc: 0.203125]\n",
      "6597: [discriminator loss: 0.530421, acc: 0.726562] [adversarial loss: 1.148446, acc: 0.250000]\n",
      "6598: [discriminator loss: 0.526815, acc: 0.726562] [adversarial loss: 1.173318, acc: 0.234375]\n",
      "6599: [discriminator loss: 0.561137, acc: 0.679688] [adversarial loss: 0.852198, acc: 0.437500]\n",
      "6600: [discriminator loss: 0.523255, acc: 0.742188] [adversarial loss: 1.550585, acc: 0.093750]\n",
      "6601: [discriminator loss: 0.438776, acc: 0.812500] [adversarial loss: 1.039020, acc: 0.359375]\n",
      "6602: [discriminator loss: 0.480693, acc: 0.781250] [adversarial loss: 1.258225, acc: 0.234375]\n",
      "6603: [discriminator loss: 0.602596, acc: 0.648438] [adversarial loss: 0.830891, acc: 0.484375]\n",
      "6604: [discriminator loss: 0.512415, acc: 0.734375] [adversarial loss: 1.715535, acc: 0.125000]\n",
      "6605: [discriminator loss: 0.585423, acc: 0.710938] [adversarial loss: 0.890224, acc: 0.375000]\n",
      "6606: [discriminator loss: 0.521301, acc: 0.742188] [adversarial loss: 1.346290, acc: 0.125000]\n",
      "6607: [discriminator loss: 0.541344, acc: 0.718750] [adversarial loss: 1.024521, acc: 0.328125]\n",
      "6608: [discriminator loss: 0.558382, acc: 0.671875] [adversarial loss: 1.392384, acc: 0.187500]\n",
      "6609: [discriminator loss: 0.553999, acc: 0.710938] [adversarial loss: 0.868264, acc: 0.343750]\n",
      "6610: [discriminator loss: 0.492830, acc: 0.742188] [adversarial loss: 1.317052, acc: 0.171875]\n",
      "6611: [discriminator loss: 0.505109, acc: 0.742188] [adversarial loss: 1.041625, acc: 0.218750]\n",
      "6612: [discriminator loss: 0.566040, acc: 0.687500] [adversarial loss: 1.205436, acc: 0.296875]\n",
      "6613: [discriminator loss: 0.541504, acc: 0.671875] [adversarial loss: 1.202475, acc: 0.265625]\n",
      "6614: [discriminator loss: 0.532343, acc: 0.710938] [adversarial loss: 1.728616, acc: 0.062500]\n",
      "6615: [discriminator loss: 0.540539, acc: 0.687500] [adversarial loss: 0.997768, acc: 0.296875]\n",
      "6616: [discriminator loss: 0.536197, acc: 0.695312] [adversarial loss: 1.238523, acc: 0.171875]\n",
      "6617: [discriminator loss: 0.527983, acc: 0.726562] [adversarial loss: 1.157782, acc: 0.250000]\n",
      "6618: [discriminator loss: 0.523896, acc: 0.781250] [adversarial loss: 1.484043, acc: 0.078125]\n",
      "6619: [discriminator loss: 0.570148, acc: 0.695312] [adversarial loss: 1.185541, acc: 0.218750]\n",
      "6620: [discriminator loss: 0.431075, acc: 0.851562] [adversarial loss: 0.955763, acc: 0.390625]\n",
      "6621: [discriminator loss: 0.488102, acc: 0.757812] [adversarial loss: 1.490484, acc: 0.093750]\n",
      "6622: [discriminator loss: 0.527713, acc: 0.703125] [adversarial loss: 1.108968, acc: 0.328125]\n",
      "6623: [discriminator loss: 0.525724, acc: 0.734375] [adversarial loss: 1.294538, acc: 0.109375]\n",
      "6624: [discriminator loss: 0.508008, acc: 0.757812] [adversarial loss: 0.971788, acc: 0.359375]\n",
      "6625: [discriminator loss: 0.565861, acc: 0.687500] [adversarial loss: 1.807075, acc: 0.140625]\n",
      "6626: [discriminator loss: 0.584664, acc: 0.664062] [adversarial loss: 0.801957, acc: 0.515625]\n",
      "6627: [discriminator loss: 0.483203, acc: 0.765625] [adversarial loss: 1.598430, acc: 0.062500]\n",
      "6628: [discriminator loss: 0.589220, acc: 0.617188] [adversarial loss: 0.794271, acc: 0.437500]\n",
      "6629: [discriminator loss: 0.557557, acc: 0.656250] [adversarial loss: 1.226097, acc: 0.250000]\n",
      "6630: [discriminator loss: 0.585826, acc: 0.679688] [adversarial loss: 1.196910, acc: 0.250000]\n",
      "6631: [discriminator loss: 0.509892, acc: 0.718750] [adversarial loss: 1.142903, acc: 0.265625]\n",
      "6632: [discriminator loss: 0.617368, acc: 0.640625] [adversarial loss: 1.045404, acc: 0.250000]\n",
      "6633: [discriminator loss: 0.555751, acc: 0.695312] [adversarial loss: 1.147928, acc: 0.140625]\n",
      "6634: [discriminator loss: 0.507561, acc: 0.710938] [adversarial loss: 0.728972, acc: 0.562500]\n",
      "6635: [discriminator loss: 0.568573, acc: 0.664062] [adversarial loss: 1.577539, acc: 0.078125]\n",
      "6636: [discriminator loss: 0.526620, acc: 0.710938] [adversarial loss: 1.238268, acc: 0.234375]\n",
      "6637: [discriminator loss: 0.474360, acc: 0.757812] [adversarial loss: 1.279399, acc: 0.234375]\n",
      "6638: [discriminator loss: 0.530332, acc: 0.750000] [adversarial loss: 0.979395, acc: 0.375000]\n",
      "6639: [discriminator loss: 0.522288, acc: 0.726562] [adversarial loss: 1.418080, acc: 0.250000]\n",
      "6640: [discriminator loss: 0.518847, acc: 0.734375] [adversarial loss: 1.190995, acc: 0.234375]\n",
      "6641: [discriminator loss: 0.530309, acc: 0.734375] [adversarial loss: 1.484961, acc: 0.093750]\n",
      "6642: [discriminator loss: 0.556615, acc: 0.703125] [adversarial loss: 0.959356, acc: 0.359375]\n",
      "6643: [discriminator loss: 0.624816, acc: 0.640625] [adversarial loss: 1.432891, acc: 0.125000]\n",
      "6644: [discriminator loss: 0.567044, acc: 0.726562] [adversarial loss: 1.007398, acc: 0.375000]\n",
      "6645: [discriminator loss: 0.552019, acc: 0.734375] [adversarial loss: 1.604392, acc: 0.140625]\n",
      "6646: [discriminator loss: 0.587650, acc: 0.687500] [adversarial loss: 0.966664, acc: 0.328125]\n",
      "6647: [discriminator loss: 0.465015, acc: 0.789062] [adversarial loss: 1.397452, acc: 0.265625]\n",
      "6648: [discriminator loss: 0.533254, acc: 0.726562] [adversarial loss: 1.170973, acc: 0.281250]\n",
      "6649: [discriminator loss: 0.594656, acc: 0.664062] [adversarial loss: 1.347518, acc: 0.140625]\n",
      "6650: [discriminator loss: 0.521529, acc: 0.734375] [adversarial loss: 1.184908, acc: 0.218750]\n",
      "6651: [discriminator loss: 0.478725, acc: 0.804688] [adversarial loss: 1.241512, acc: 0.171875]\n",
      "6652: [discriminator loss: 0.605828, acc: 0.664062] [adversarial loss: 1.156615, acc: 0.296875]\n",
      "6653: [discriminator loss: 0.553205, acc: 0.703125] [adversarial loss: 1.231634, acc: 0.250000]\n",
      "6654: [discriminator loss: 0.526326, acc: 0.734375] [adversarial loss: 0.855515, acc: 0.453125]\n",
      "6655: [discriminator loss: 0.558294, acc: 0.703125] [adversarial loss: 1.266959, acc: 0.234375]\n",
      "6656: [discriminator loss: 0.555947, acc: 0.718750] [adversarial loss: 1.088681, acc: 0.312500]\n",
      "6657: [discriminator loss: 0.555900, acc: 0.742188] [adversarial loss: 1.323907, acc: 0.140625]\n",
      "6658: [discriminator loss: 0.485679, acc: 0.804688] [adversarial loss: 1.183068, acc: 0.203125]\n",
      "6659: [discriminator loss: 0.567407, acc: 0.695312] [adversarial loss: 1.300976, acc: 0.250000]\n",
      "6660: [discriminator loss: 0.535735, acc: 0.757812] [adversarial loss: 0.740651, acc: 0.531250]\n",
      "6661: [discriminator loss: 0.559426, acc: 0.695312] [adversarial loss: 1.454998, acc: 0.062500]\n",
      "6662: [discriminator loss: 0.599746, acc: 0.687500] [adversarial loss: 1.002481, acc: 0.359375]\n",
      "6663: [discriminator loss: 0.545708, acc: 0.695312] [adversarial loss: 1.468969, acc: 0.156250]\n",
      "6664: [discriminator loss: 0.506265, acc: 0.718750] [adversarial loss: 1.142173, acc: 0.234375]\n",
      "6665: [discriminator loss: 0.506186, acc: 0.796875] [adversarial loss: 1.378323, acc: 0.156250]\n",
      "6666: [discriminator loss: 0.525581, acc: 0.710938] [adversarial loss: 0.905517, acc: 0.453125]\n",
      "6667: [discriminator loss: 0.661259, acc: 0.609375] [adversarial loss: 1.391463, acc: 0.203125]\n",
      "6668: [discriminator loss: 0.502217, acc: 0.742188] [adversarial loss: 1.044054, acc: 0.359375]\n",
      "6669: [discriminator loss: 0.576459, acc: 0.656250] [adversarial loss: 1.090266, acc: 0.203125]\n",
      "6670: [discriminator loss: 0.492925, acc: 0.773438] [adversarial loss: 1.381782, acc: 0.171875]\n",
      "6671: [discriminator loss: 0.525677, acc: 0.750000] [adversarial loss: 1.066868, acc: 0.312500]\n",
      "6672: [discriminator loss: 0.523796, acc: 0.757812] [adversarial loss: 1.452405, acc: 0.125000]\n",
      "6673: [discriminator loss: 0.538407, acc: 0.742188] [adversarial loss: 1.105301, acc: 0.296875]\n",
      "6674: [discriminator loss: 0.502253, acc: 0.734375] [adversarial loss: 1.331156, acc: 0.218750]\n",
      "6675: [discriminator loss: 0.445062, acc: 0.757812] [adversarial loss: 1.492705, acc: 0.125000]\n",
      "6676: [discriminator loss: 0.524610, acc: 0.718750] [adversarial loss: 1.216117, acc: 0.250000]\n",
      "6677: [discriminator loss: 0.503383, acc: 0.742188] [adversarial loss: 1.146777, acc: 0.203125]\n",
      "6678: [discriminator loss: 0.493841, acc: 0.773438] [adversarial loss: 0.970090, acc: 0.328125]\n",
      "6679: [discriminator loss: 0.609174, acc: 0.687500] [adversarial loss: 1.440312, acc: 0.156250]\n",
      "6680: [discriminator loss: 0.541612, acc: 0.757812] [adversarial loss: 1.122006, acc: 0.281250]\n",
      "6681: [discriminator loss: 0.600378, acc: 0.679688] [adversarial loss: 1.193546, acc: 0.250000]\n",
      "6682: [discriminator loss: 0.572094, acc: 0.726562] [adversarial loss: 1.268094, acc: 0.156250]\n",
      "6683: [discriminator loss: 0.567236, acc: 0.687500] [adversarial loss: 1.765910, acc: 0.093750]\n",
      "6684: [discriminator loss: 0.490329, acc: 0.750000] [adversarial loss: 0.903888, acc: 0.281250]\n",
      "6685: [discriminator loss: 0.561330, acc: 0.742188] [adversarial loss: 1.370884, acc: 0.234375]\n",
      "6686: [discriminator loss: 0.589043, acc: 0.687500] [adversarial loss: 1.013732, acc: 0.296875]\n",
      "6687: [discriminator loss: 0.583378, acc: 0.679688] [adversarial loss: 1.197911, acc: 0.203125]\n",
      "6688: [discriminator loss: 0.482698, acc: 0.757812] [adversarial loss: 1.248712, acc: 0.250000]\n",
      "6689: [discriminator loss: 0.500552, acc: 0.757812] [adversarial loss: 1.035460, acc: 0.328125]\n",
      "6690: [discriminator loss: 0.550256, acc: 0.726562] [adversarial loss: 1.736952, acc: 0.156250]\n",
      "6691: [discriminator loss: 0.538750, acc: 0.687500] [adversarial loss: 0.978971, acc: 0.343750]\n",
      "6692: [discriminator loss: 0.564304, acc: 0.687500] [adversarial loss: 1.254700, acc: 0.187500]\n",
      "6693: [discriminator loss: 0.553339, acc: 0.664062] [adversarial loss: 1.108915, acc: 0.234375]\n",
      "6694: [discriminator loss: 0.594067, acc: 0.656250] [adversarial loss: 1.158507, acc: 0.343750]\n",
      "6695: [discriminator loss: 0.524340, acc: 0.726562] [adversarial loss: 1.218202, acc: 0.281250]\n",
      "6696: [discriminator loss: 0.476656, acc: 0.789062] [adversarial loss: 1.620267, acc: 0.093750]\n",
      "6697: [discriminator loss: 0.503755, acc: 0.726562] [adversarial loss: 0.886208, acc: 0.468750]\n",
      "6698: [discriminator loss: 0.555042, acc: 0.695312] [adversarial loss: 1.646168, acc: 0.109375]\n",
      "6699: [discriminator loss: 0.614221, acc: 0.632812] [adversarial loss: 0.799128, acc: 0.453125]\n",
      "6700: [discriminator loss: 0.603628, acc: 0.664062] [adversarial loss: 1.539800, acc: 0.062500]\n",
      "6701: [discriminator loss: 0.543692, acc: 0.734375] [adversarial loss: 1.081886, acc: 0.281250]\n",
      "6702: [discriminator loss: 0.530963, acc: 0.718750] [adversarial loss: 1.623548, acc: 0.062500]\n",
      "6703: [discriminator loss: 0.500145, acc: 0.742188] [adversarial loss: 1.129192, acc: 0.281250]\n",
      "6704: [discriminator loss: 0.642833, acc: 0.656250] [adversarial loss: 1.392115, acc: 0.203125]\n",
      "6705: [discriminator loss: 0.467909, acc: 0.796875] [adversarial loss: 1.262999, acc: 0.234375]\n",
      "6706: [discriminator loss: 0.586321, acc: 0.679688] [adversarial loss: 1.139727, acc: 0.281250]\n",
      "6707: [discriminator loss: 0.541211, acc: 0.734375] [adversarial loss: 1.271742, acc: 0.171875]\n",
      "6708: [discriminator loss: 0.590023, acc: 0.703125] [adversarial loss: 1.059559, acc: 0.343750]\n",
      "6709: [discriminator loss: 0.554561, acc: 0.726562] [adversarial loss: 1.543567, acc: 0.140625]\n",
      "6710: [discriminator loss: 0.592807, acc: 0.695312] [adversarial loss: 0.787612, acc: 0.453125]\n",
      "6711: [discriminator loss: 0.627484, acc: 0.687500] [adversarial loss: 1.585828, acc: 0.046875]\n",
      "6712: [discriminator loss: 0.568071, acc: 0.687500] [adversarial loss: 0.957438, acc: 0.390625]\n",
      "6713: [discriminator loss: 0.537240, acc: 0.734375] [adversarial loss: 1.522096, acc: 0.156250]\n",
      "6714: [discriminator loss: 0.534228, acc: 0.726562] [adversarial loss: 0.931683, acc: 0.343750]\n",
      "6715: [discriminator loss: 0.581041, acc: 0.648438] [adversarial loss: 1.033686, acc: 0.296875]\n",
      "6716: [discriminator loss: 0.566548, acc: 0.695312] [adversarial loss: 1.164893, acc: 0.203125]\n",
      "6717: [discriminator loss: 0.501307, acc: 0.757812] [adversarial loss: 1.276417, acc: 0.140625]\n",
      "6718: [discriminator loss: 0.484734, acc: 0.781250] [adversarial loss: 1.162962, acc: 0.218750]\n",
      "6719: [discriminator loss: 0.539071, acc: 0.703125] [adversarial loss: 0.964066, acc: 0.421875]\n",
      "6720: [discriminator loss: 0.556750, acc: 0.695312] [adversarial loss: 1.348571, acc: 0.125000]\n",
      "6721: [discriminator loss: 0.481841, acc: 0.765625] [adversarial loss: 0.849205, acc: 0.390625]\n",
      "6722: [discriminator loss: 0.538422, acc: 0.773438] [adversarial loss: 1.432375, acc: 0.140625]\n",
      "6723: [discriminator loss: 0.544448, acc: 0.679688] [adversarial loss: 0.886871, acc: 0.484375]\n",
      "6724: [discriminator loss: 0.547064, acc: 0.726562] [adversarial loss: 1.652744, acc: 0.140625]\n",
      "6725: [discriminator loss: 0.555099, acc: 0.695312] [adversarial loss: 0.995086, acc: 0.359375]\n",
      "6726: [discriminator loss: 0.491222, acc: 0.773438] [adversarial loss: 1.015165, acc: 0.375000]\n",
      "6727: [discriminator loss: 0.528866, acc: 0.757812] [adversarial loss: 1.475621, acc: 0.125000]\n",
      "6728: [discriminator loss: 0.512940, acc: 0.734375] [adversarial loss: 0.927243, acc: 0.312500]\n",
      "6729: [discriminator loss: 0.559496, acc: 0.703125] [adversarial loss: 1.590719, acc: 0.093750]\n",
      "6730: [discriminator loss: 0.442506, acc: 0.789062] [adversarial loss: 1.130034, acc: 0.281250]\n",
      "6731: [discriminator loss: 0.482487, acc: 0.796875] [adversarial loss: 1.479680, acc: 0.187500]\n",
      "6732: [discriminator loss: 0.463659, acc: 0.820312] [adversarial loss: 1.271928, acc: 0.218750]\n",
      "6733: [discriminator loss: 0.607943, acc: 0.656250] [adversarial loss: 1.152716, acc: 0.281250]\n",
      "6734: [discriminator loss: 0.499872, acc: 0.734375] [adversarial loss: 1.177494, acc: 0.218750]\n",
      "6735: [discriminator loss: 0.458006, acc: 0.804688] [adversarial loss: 1.033934, acc: 0.250000]\n",
      "6736: [discriminator loss: 0.538712, acc: 0.734375] [adversarial loss: 0.842793, acc: 0.453125]\n",
      "6737: [discriminator loss: 0.511774, acc: 0.742188] [adversarial loss: 1.551289, acc: 0.125000]\n",
      "6738: [discriminator loss: 0.656891, acc: 0.625000] [adversarial loss: 0.609789, acc: 0.656250]\n",
      "6739: [discriminator loss: 0.690290, acc: 0.656250] [adversarial loss: 1.276479, acc: 0.218750]\n",
      "6740: [discriminator loss: 0.479961, acc: 0.796875] [adversarial loss: 1.058693, acc: 0.281250]\n",
      "6741: [discriminator loss: 0.543527, acc: 0.742188] [adversarial loss: 1.292638, acc: 0.203125]\n",
      "6742: [discriminator loss: 0.549076, acc: 0.695312] [adversarial loss: 0.774694, acc: 0.500000]\n",
      "6743: [discriminator loss: 0.587705, acc: 0.656250] [adversarial loss: 1.256292, acc: 0.187500]\n",
      "6744: [discriminator loss: 0.514953, acc: 0.726562] [adversarial loss: 1.253631, acc: 0.140625]\n",
      "6745: [discriminator loss: 0.436267, acc: 0.796875] [adversarial loss: 1.197493, acc: 0.187500]\n",
      "6746: [discriminator loss: 0.548109, acc: 0.710938] [adversarial loss: 1.289849, acc: 0.218750]\n",
      "6747: [discriminator loss: 0.601740, acc: 0.664062] [adversarial loss: 1.142396, acc: 0.250000]\n",
      "6748: [discriminator loss: 0.512716, acc: 0.742188] [adversarial loss: 1.095324, acc: 0.312500]\n",
      "6749: [discriminator loss: 0.601416, acc: 0.664062] [adversarial loss: 0.922776, acc: 0.468750]\n",
      "6750: [discriminator loss: 0.562031, acc: 0.703125] [adversarial loss: 1.540055, acc: 0.125000]\n",
      "6751: [discriminator loss: 0.589584, acc: 0.687500] [adversarial loss: 0.757551, acc: 0.500000]\n",
      "6752: [discriminator loss: 0.529233, acc: 0.734375] [adversarial loss: 1.521841, acc: 0.156250]\n",
      "6753: [discriminator loss: 0.601758, acc: 0.664062] [adversarial loss: 0.826415, acc: 0.484375]\n",
      "6754: [discriminator loss: 0.602495, acc: 0.632812] [adversarial loss: 1.404046, acc: 0.171875]\n",
      "6755: [discriminator loss: 0.577704, acc: 0.640625] [adversarial loss: 1.039689, acc: 0.343750]\n",
      "6756: [discriminator loss: 0.579549, acc: 0.664062] [adversarial loss: 1.118755, acc: 0.296875]\n",
      "6757: [discriminator loss: 0.484180, acc: 0.796875] [adversarial loss: 1.308797, acc: 0.203125]\n",
      "6758: [discriminator loss: 0.521926, acc: 0.734375] [adversarial loss: 1.320948, acc: 0.218750]\n",
      "6759: [discriminator loss: 0.556181, acc: 0.726562] [adversarial loss: 1.130366, acc: 0.218750]\n",
      "6760: [discriminator loss: 0.523267, acc: 0.710938] [adversarial loss: 1.288355, acc: 0.250000]\n",
      "6761: [discriminator loss: 0.480178, acc: 0.828125] [adversarial loss: 1.155795, acc: 0.171875]\n",
      "6762: [discriminator loss: 0.530106, acc: 0.703125] [adversarial loss: 1.174994, acc: 0.250000]\n",
      "6763: [discriminator loss: 0.502434, acc: 0.718750] [adversarial loss: 1.294331, acc: 0.250000]\n",
      "6764: [discriminator loss: 0.552984, acc: 0.710938] [adversarial loss: 1.117078, acc: 0.265625]\n",
      "6765: [discriminator loss: 0.490921, acc: 0.742188] [adversarial loss: 1.351865, acc: 0.156250]\n",
      "6766: [discriminator loss: 0.518642, acc: 0.718750] [adversarial loss: 1.104907, acc: 0.203125]\n",
      "6767: [discriminator loss: 0.484093, acc: 0.773438] [adversarial loss: 1.262940, acc: 0.125000]\n",
      "6768: [discriminator loss: 0.527274, acc: 0.718750] [adversarial loss: 1.260086, acc: 0.218750]\n",
      "6769: [discriminator loss: 0.590913, acc: 0.671875] [adversarial loss: 1.161609, acc: 0.218750]\n",
      "6770: [discriminator loss: 0.549861, acc: 0.703125] [adversarial loss: 1.171676, acc: 0.250000]\n",
      "6771: [discriminator loss: 0.562496, acc: 0.687500] [adversarial loss: 1.201666, acc: 0.203125]\n",
      "6772: [discriminator loss: 0.535503, acc: 0.765625] [adversarial loss: 1.370795, acc: 0.218750]\n",
      "6773: [discriminator loss: 0.501976, acc: 0.750000] [adversarial loss: 1.324607, acc: 0.140625]\n",
      "6774: [discriminator loss: 0.492360, acc: 0.742188] [adversarial loss: 1.167958, acc: 0.281250]\n",
      "6775: [discriminator loss: 0.515719, acc: 0.710938] [adversarial loss: 1.061215, acc: 0.234375]\n",
      "6776: [discriminator loss: 0.449746, acc: 0.789062] [adversarial loss: 1.402840, acc: 0.187500]\n",
      "6777: [discriminator loss: 0.542436, acc: 0.718750] [adversarial loss: 1.150238, acc: 0.250000]\n",
      "6778: [discriminator loss: 0.513007, acc: 0.703125] [adversarial loss: 1.332782, acc: 0.187500]\n",
      "6779: [discriminator loss: 0.520617, acc: 0.742188] [adversarial loss: 0.948225, acc: 0.359375]\n",
      "6780: [discriminator loss: 0.544294, acc: 0.710938] [adversarial loss: 1.814242, acc: 0.000000]\n",
      "6781: [discriminator loss: 0.604586, acc: 0.656250] [adversarial loss: 0.759507, acc: 0.515625]\n",
      "6782: [discriminator loss: 0.607272, acc: 0.648438] [adversarial loss: 1.655864, acc: 0.078125]\n",
      "6783: [discriminator loss: 0.572481, acc: 0.734375] [adversarial loss: 1.013062, acc: 0.375000]\n",
      "6784: [discriminator loss: 0.484630, acc: 0.734375] [adversarial loss: 1.434111, acc: 0.125000]\n",
      "6785: [discriminator loss: 0.535901, acc: 0.734375] [adversarial loss: 0.958897, acc: 0.343750]\n",
      "6786: [discriminator loss: 0.601397, acc: 0.726562] [adversarial loss: 1.461349, acc: 0.109375]\n",
      "6787: [discriminator loss: 0.511733, acc: 0.757812] [adversarial loss: 0.981919, acc: 0.343750]\n",
      "6788: [discriminator loss: 0.585288, acc: 0.718750] [adversarial loss: 1.502817, acc: 0.140625]\n",
      "6789: [discriminator loss: 0.530220, acc: 0.734375] [adversarial loss: 1.161519, acc: 0.250000]\n",
      "6790: [discriminator loss: 0.505247, acc: 0.773438] [adversarial loss: 1.210528, acc: 0.281250]\n",
      "6791: [discriminator loss: 0.523759, acc: 0.718750] [adversarial loss: 1.279932, acc: 0.218750]\n",
      "6792: [discriminator loss: 0.510743, acc: 0.726562] [adversarial loss: 1.455293, acc: 0.093750]\n",
      "6793: [discriminator loss: 0.494634, acc: 0.734375] [adversarial loss: 1.074268, acc: 0.359375]\n",
      "6794: [discriminator loss: 0.490615, acc: 0.742188] [adversarial loss: 1.502505, acc: 0.109375]\n",
      "6795: [discriminator loss: 0.522023, acc: 0.757812] [adversarial loss: 0.859587, acc: 0.468750]\n",
      "6796: [discriminator loss: 0.553109, acc: 0.679688] [adversarial loss: 1.820849, acc: 0.078125]\n",
      "6797: [discriminator loss: 0.505134, acc: 0.687500] [adversarial loss: 1.076622, acc: 0.328125]\n",
      "6798: [discriminator loss: 0.511279, acc: 0.765625] [adversarial loss: 1.530623, acc: 0.078125]\n",
      "6799: [discriminator loss: 0.585645, acc: 0.664062] [adversarial loss: 0.905837, acc: 0.421875]\n",
      "6800: [discriminator loss: 0.600564, acc: 0.679688] [adversarial loss: 1.581374, acc: 0.125000]\n",
      "6801: [discriminator loss: 0.499506, acc: 0.750000] [adversarial loss: 1.116171, acc: 0.296875]\n",
      "6802: [discriminator loss: 0.586169, acc: 0.718750] [adversarial loss: 1.233702, acc: 0.296875]\n",
      "6803: [discriminator loss: 0.528975, acc: 0.718750] [adversarial loss: 0.854422, acc: 0.437500]\n",
      "6804: [discriminator loss: 0.579918, acc: 0.695312] [adversarial loss: 1.414220, acc: 0.140625]\n",
      "6805: [discriminator loss: 0.489733, acc: 0.742188] [adversarial loss: 1.277740, acc: 0.187500]\n",
      "6806: [discriminator loss: 0.497949, acc: 0.750000] [adversarial loss: 1.098085, acc: 0.296875]\n",
      "6807: [discriminator loss: 0.626720, acc: 0.695312] [adversarial loss: 1.220465, acc: 0.250000]\n",
      "6808: [discriminator loss: 0.591602, acc: 0.695312] [adversarial loss: 1.028544, acc: 0.234375]\n",
      "6809: [discriminator loss: 0.531465, acc: 0.742188] [adversarial loss: 1.401682, acc: 0.156250]\n",
      "6810: [discriminator loss: 0.548932, acc: 0.757812] [adversarial loss: 1.053955, acc: 0.343750]\n",
      "6811: [discriminator loss: 0.565238, acc: 0.695312] [adversarial loss: 1.509904, acc: 0.125000]\n",
      "6812: [discriminator loss: 0.518453, acc: 0.742188] [adversarial loss: 1.345496, acc: 0.156250]\n",
      "6813: [discriminator loss: 0.536811, acc: 0.734375] [adversarial loss: 1.135360, acc: 0.312500]\n",
      "6814: [discriminator loss: 0.519225, acc: 0.718750] [adversarial loss: 1.007601, acc: 0.312500]\n",
      "6815: [discriminator loss: 0.530266, acc: 0.718750] [adversarial loss: 0.824887, acc: 0.453125]\n",
      "6816: [discriminator loss: 0.538369, acc: 0.789062] [adversarial loss: 1.330963, acc: 0.093750]\n",
      "6817: [discriminator loss: 0.476288, acc: 0.726562] [adversarial loss: 0.873378, acc: 0.453125]\n",
      "6818: [discriminator loss: 0.608246, acc: 0.640625] [adversarial loss: 1.488232, acc: 0.062500]\n",
      "6819: [discriminator loss: 0.582785, acc: 0.648438] [adversarial loss: 1.122425, acc: 0.328125]\n",
      "6820: [discriminator loss: 0.556180, acc: 0.726562] [adversarial loss: 1.494062, acc: 0.078125]\n",
      "6821: [discriminator loss: 0.449478, acc: 0.796875] [adversarial loss: 1.376331, acc: 0.218750]\n",
      "6822: [discriminator loss: 0.521091, acc: 0.742188] [adversarial loss: 1.104319, acc: 0.281250]\n",
      "6823: [discriminator loss: 0.559944, acc: 0.671875] [adversarial loss: 1.421169, acc: 0.234375]\n",
      "6824: [discriminator loss: 0.492266, acc: 0.734375] [adversarial loss: 1.029650, acc: 0.359375]\n",
      "6825: [discriminator loss: 0.572932, acc: 0.734375] [adversarial loss: 1.210248, acc: 0.187500]\n",
      "6826: [discriminator loss: 0.542080, acc: 0.710938] [adversarial loss: 1.071539, acc: 0.328125]\n",
      "6827: [discriminator loss: 0.544995, acc: 0.695312] [adversarial loss: 1.443048, acc: 0.203125]\n",
      "6828: [discriminator loss: 0.599787, acc: 0.656250] [adversarial loss: 0.883137, acc: 0.437500]\n",
      "6829: [discriminator loss: 0.500904, acc: 0.757812] [adversarial loss: 1.442017, acc: 0.140625]\n",
      "6830: [discriminator loss: 0.581962, acc: 0.656250] [adversarial loss: 0.876199, acc: 0.343750]\n",
      "6831: [discriminator loss: 0.522620, acc: 0.734375] [adversarial loss: 1.259545, acc: 0.203125]\n",
      "6832: [discriminator loss: 0.548298, acc: 0.734375] [adversarial loss: 1.345243, acc: 0.187500]\n",
      "6833: [discriminator loss: 0.479301, acc: 0.781250] [adversarial loss: 1.025872, acc: 0.328125]\n",
      "6834: [discriminator loss: 0.625529, acc: 0.617188] [adversarial loss: 1.275545, acc: 0.234375]\n",
      "6835: [discriminator loss: 0.521159, acc: 0.703125] [adversarial loss: 1.213741, acc: 0.234375]\n",
      "6836: [discriminator loss: 0.546315, acc: 0.718750] [adversarial loss: 1.049130, acc: 0.312500]\n",
      "6837: [discriminator loss: 0.534450, acc: 0.773438] [adversarial loss: 1.279148, acc: 0.234375]\n",
      "6838: [discriminator loss: 0.529057, acc: 0.742188] [adversarial loss: 0.996860, acc: 0.390625]\n",
      "6839: [discriminator loss: 0.535903, acc: 0.750000] [adversarial loss: 1.490458, acc: 0.062500]\n",
      "6840: [discriminator loss: 0.585510, acc: 0.679688] [adversarial loss: 0.752849, acc: 0.562500]\n",
      "6841: [discriminator loss: 0.577147, acc: 0.671875] [adversarial loss: 1.610733, acc: 0.078125]\n",
      "6842: [discriminator loss: 0.562886, acc: 0.695312] [adversarial loss: 0.855512, acc: 0.546875]\n",
      "6843: [discriminator loss: 0.599557, acc: 0.703125] [adversarial loss: 1.345518, acc: 0.156250]\n",
      "6844: [discriminator loss: 0.510668, acc: 0.734375] [adversarial loss: 1.273761, acc: 0.203125]\n",
      "6845: [discriminator loss: 0.481385, acc: 0.773438] [adversarial loss: 1.020593, acc: 0.359375]\n",
      "6846: [discriminator loss: 0.550168, acc: 0.687500] [adversarial loss: 1.132793, acc: 0.203125]\n",
      "6847: [discriminator loss: 0.547930, acc: 0.703125] [adversarial loss: 1.146506, acc: 0.328125]\n",
      "6848: [discriminator loss: 0.576493, acc: 0.703125] [adversarial loss: 1.233046, acc: 0.234375]\n",
      "6849: [discriminator loss: 0.484113, acc: 0.773438] [adversarial loss: 0.935552, acc: 0.375000]\n",
      "6850: [discriminator loss: 0.618742, acc: 0.664062] [adversarial loss: 1.468663, acc: 0.156250]\n",
      "6851: [discriminator loss: 0.525850, acc: 0.765625] [adversarial loss: 1.040289, acc: 0.250000]\n",
      "6852: [discriminator loss: 0.526826, acc: 0.734375] [adversarial loss: 1.576485, acc: 0.078125]\n",
      "6853: [discriminator loss: 0.528593, acc: 0.750000] [adversarial loss: 1.026602, acc: 0.359375]\n",
      "6854: [discriminator loss: 0.544759, acc: 0.695312] [adversarial loss: 1.232249, acc: 0.218750]\n",
      "6855: [discriminator loss: 0.497426, acc: 0.734375] [adversarial loss: 1.069605, acc: 0.250000]\n",
      "6856: [discriminator loss: 0.562678, acc: 0.742188] [adversarial loss: 1.202919, acc: 0.265625]\n",
      "6857: [discriminator loss: 0.525703, acc: 0.804688] [adversarial loss: 1.005994, acc: 0.375000]\n",
      "6858: [discriminator loss: 0.509086, acc: 0.773438] [adversarial loss: 1.201065, acc: 0.218750]\n",
      "6859: [discriminator loss: 0.537376, acc: 0.718750] [adversarial loss: 1.022515, acc: 0.312500]\n",
      "6860: [discriminator loss: 0.485322, acc: 0.742188] [adversarial loss: 1.103836, acc: 0.218750]\n",
      "6861: [discriminator loss: 0.492838, acc: 0.718750] [adversarial loss: 1.601165, acc: 0.093750]\n",
      "6862: [discriminator loss: 0.543547, acc: 0.718750] [adversarial loss: 0.748435, acc: 0.546875]\n",
      "6863: [discriminator loss: 0.655496, acc: 0.632812] [adversarial loss: 1.631266, acc: 0.078125]\n",
      "6864: [discriminator loss: 0.566971, acc: 0.703125] [adversarial loss: 0.938587, acc: 0.359375]\n",
      "6865: [discriminator loss: 0.605855, acc: 0.679688] [adversarial loss: 1.256046, acc: 0.234375]\n",
      "6866: [discriminator loss: 0.559811, acc: 0.726562] [adversarial loss: 1.189278, acc: 0.281250]\n",
      "6867: [discriminator loss: 0.569986, acc: 0.734375] [adversarial loss: 0.932259, acc: 0.359375]\n",
      "6868: [discriminator loss: 0.501613, acc: 0.773438] [adversarial loss: 1.449189, acc: 0.265625]\n",
      "6869: [discriminator loss: 0.566809, acc: 0.718750] [adversarial loss: 1.016454, acc: 0.343750]\n",
      "6870: [discriminator loss: 0.583132, acc: 0.695312] [adversarial loss: 1.257742, acc: 0.250000]\n",
      "6871: [discriminator loss: 0.531275, acc: 0.742188] [adversarial loss: 1.116180, acc: 0.281250]\n",
      "6872: [discriminator loss: 0.511652, acc: 0.687500] [adversarial loss: 1.022058, acc: 0.250000]\n",
      "6873: [discriminator loss: 0.619542, acc: 0.609375] [adversarial loss: 1.174908, acc: 0.234375]\n",
      "6874: [discriminator loss: 0.536986, acc: 0.718750] [adversarial loss: 1.386281, acc: 0.125000]\n",
      "6875: [discriminator loss: 0.565949, acc: 0.679688] [adversarial loss: 0.990278, acc: 0.359375]\n",
      "6876: [discriminator loss: 0.526633, acc: 0.742188] [adversarial loss: 1.553988, acc: 0.109375]\n",
      "6877: [discriminator loss: 0.601718, acc: 0.664062] [adversarial loss: 0.956893, acc: 0.390625]\n",
      "6878: [discriminator loss: 0.561056, acc: 0.695312] [adversarial loss: 1.151396, acc: 0.234375]\n",
      "6879: [discriminator loss: 0.552891, acc: 0.742188] [adversarial loss: 1.060471, acc: 0.375000]\n",
      "6880: [discriminator loss: 0.454274, acc: 0.804688] [adversarial loss: 1.421571, acc: 0.171875]\n",
      "6881: [discriminator loss: 0.496033, acc: 0.773438] [adversarial loss: 0.956945, acc: 0.421875]\n",
      "6882: [discriminator loss: 0.579060, acc: 0.687500] [adversarial loss: 1.528924, acc: 0.125000]\n",
      "6883: [discriminator loss: 0.518282, acc: 0.742188] [adversarial loss: 1.082142, acc: 0.296875]\n",
      "6884: [discriminator loss: 0.558100, acc: 0.718750] [adversarial loss: 1.330415, acc: 0.171875]\n",
      "6885: [discriminator loss: 0.575396, acc: 0.671875] [adversarial loss: 0.911216, acc: 0.421875]\n",
      "6886: [discriminator loss: 0.562892, acc: 0.671875] [adversarial loss: 1.394490, acc: 0.156250]\n",
      "6887: [discriminator loss: 0.568516, acc: 0.687500] [adversarial loss: 0.894323, acc: 0.390625]\n",
      "6888: [discriminator loss: 0.515598, acc: 0.750000] [adversarial loss: 1.760614, acc: 0.109375]\n",
      "6889: [discriminator loss: 0.555839, acc: 0.703125] [adversarial loss: 0.836313, acc: 0.484375]\n",
      "6890: [discriminator loss: 0.572404, acc: 0.726562] [adversarial loss: 1.367069, acc: 0.156250]\n",
      "6891: [discriminator loss: 0.543855, acc: 0.695312] [adversarial loss: 0.735688, acc: 0.484375]\n",
      "6892: [discriminator loss: 0.576120, acc: 0.679688] [adversarial loss: 1.532921, acc: 0.125000]\n",
      "6893: [discriminator loss: 0.566674, acc: 0.718750] [adversarial loss: 0.897858, acc: 0.453125]\n",
      "6894: [discriminator loss: 0.611269, acc: 0.679688] [adversarial loss: 1.371929, acc: 0.140625]\n",
      "6895: [discriminator loss: 0.519839, acc: 0.765625] [adversarial loss: 1.267437, acc: 0.171875]\n",
      "6896: [discriminator loss: 0.551428, acc: 0.710938] [adversarial loss: 1.010408, acc: 0.265625]\n",
      "6897: [discriminator loss: 0.602975, acc: 0.710938] [adversarial loss: 1.512772, acc: 0.062500]\n",
      "6898: [discriminator loss: 0.575290, acc: 0.695312] [adversarial loss: 1.031360, acc: 0.234375]\n",
      "6899: [discriminator loss: 0.531280, acc: 0.726562] [adversarial loss: 1.183383, acc: 0.203125]\n",
      "6900: [discriminator loss: 0.536271, acc: 0.718750] [adversarial loss: 1.574260, acc: 0.156250]\n",
      "6901: [discriminator loss: 0.629166, acc: 0.687500] [adversarial loss: 0.971951, acc: 0.406250]\n",
      "6902: [discriminator loss: 0.508201, acc: 0.718750] [adversarial loss: 1.336043, acc: 0.234375]\n",
      "6903: [discriminator loss: 0.543867, acc: 0.695312] [adversarial loss: 1.014820, acc: 0.281250]\n",
      "6904: [discriminator loss: 0.590504, acc: 0.648438] [adversarial loss: 1.393579, acc: 0.125000]\n",
      "6905: [discriminator loss: 0.556035, acc: 0.718750] [adversarial loss: 1.017932, acc: 0.312500]\n",
      "6906: [discriminator loss: 0.478238, acc: 0.757812] [adversarial loss: 1.015767, acc: 0.296875]\n",
      "6907: [discriminator loss: 0.527101, acc: 0.765625] [adversarial loss: 1.189717, acc: 0.156250]\n",
      "6908: [discriminator loss: 0.566694, acc: 0.718750] [adversarial loss: 1.134486, acc: 0.234375]\n",
      "6909: [discriminator loss: 0.535428, acc: 0.710938] [adversarial loss: 1.205384, acc: 0.203125]\n",
      "6910: [discriminator loss: 0.556248, acc: 0.734375] [adversarial loss: 1.218042, acc: 0.265625]\n",
      "6911: [discriminator loss: 0.488698, acc: 0.765625] [adversarial loss: 1.167871, acc: 0.281250]\n",
      "6912: [discriminator loss: 0.520865, acc: 0.718750] [adversarial loss: 1.026226, acc: 0.328125]\n",
      "6913: [discriminator loss: 0.539562, acc: 0.695312] [adversarial loss: 1.258224, acc: 0.250000]\n",
      "6914: [discriminator loss: 0.574274, acc: 0.718750] [adversarial loss: 1.004410, acc: 0.359375]\n",
      "6915: [discriminator loss: 0.567529, acc: 0.710938] [adversarial loss: 1.686152, acc: 0.093750]\n",
      "6916: [discriminator loss: 0.542249, acc: 0.734375] [adversarial loss: 1.056243, acc: 0.296875]\n",
      "6917: [discriminator loss: 0.538871, acc: 0.710938] [adversarial loss: 1.335396, acc: 0.156250]\n",
      "6918: [discriminator loss: 0.661078, acc: 0.664062] [adversarial loss: 0.745866, acc: 0.531250]\n",
      "6919: [discriminator loss: 0.588943, acc: 0.703125] [adversarial loss: 1.467895, acc: 0.109375]\n",
      "6920: [discriminator loss: 0.575150, acc: 0.687500] [adversarial loss: 0.930291, acc: 0.375000]\n",
      "6921: [discriminator loss: 0.510016, acc: 0.742188] [adversarial loss: 1.258967, acc: 0.265625]\n",
      "6922: [discriminator loss: 0.506150, acc: 0.804688] [adversarial loss: 1.249724, acc: 0.218750]\n",
      "6923: [discriminator loss: 0.464882, acc: 0.781250] [adversarial loss: 1.373791, acc: 0.187500]\n",
      "6924: [discriminator loss: 0.534312, acc: 0.734375] [adversarial loss: 1.109236, acc: 0.296875]\n",
      "6925: [discriminator loss: 0.508290, acc: 0.703125] [adversarial loss: 1.330676, acc: 0.125000]\n",
      "6926: [discriminator loss: 0.598865, acc: 0.703125] [adversarial loss: 1.403267, acc: 0.140625]\n",
      "6927: [discriminator loss: 0.571044, acc: 0.710938] [adversarial loss: 0.961926, acc: 0.421875]\n",
      "6928: [discriminator loss: 0.553361, acc: 0.703125] [adversarial loss: 1.746800, acc: 0.031250]\n",
      "6929: [discriminator loss: 0.540064, acc: 0.703125] [adversarial loss: 0.905312, acc: 0.375000]\n",
      "6930: [discriminator loss: 0.546624, acc: 0.703125] [adversarial loss: 1.283158, acc: 0.156250]\n",
      "6931: [discriminator loss: 0.558024, acc: 0.679688] [adversarial loss: 1.142250, acc: 0.234375]\n",
      "6932: [discriminator loss: 0.515477, acc: 0.765625] [adversarial loss: 1.282711, acc: 0.218750]\n",
      "6933: [discriminator loss: 0.493599, acc: 0.773438] [adversarial loss: 1.342837, acc: 0.156250]\n",
      "6934: [discriminator loss: 0.570996, acc: 0.695312] [adversarial loss: 1.240852, acc: 0.234375]\n",
      "6935: [discriminator loss: 0.501310, acc: 0.789062] [adversarial loss: 1.076389, acc: 0.328125]\n",
      "6936: [discriminator loss: 0.559093, acc: 0.703125] [adversarial loss: 1.696510, acc: 0.046875]\n",
      "6937: [discriminator loss: 0.581155, acc: 0.687500] [adversarial loss: 1.303803, acc: 0.171875]\n",
      "6938: [discriminator loss: 0.543945, acc: 0.679688] [adversarial loss: 1.352907, acc: 0.171875]\n",
      "6939: [discriminator loss: 0.469562, acc: 0.796875] [adversarial loss: 1.062742, acc: 0.281250]\n",
      "6940: [discriminator loss: 0.537184, acc: 0.710938] [adversarial loss: 1.514030, acc: 0.093750]\n",
      "6941: [discriminator loss: 0.596534, acc: 0.687500] [adversarial loss: 1.161299, acc: 0.234375]\n",
      "6942: [discriminator loss: 0.530722, acc: 0.710938] [adversarial loss: 1.269228, acc: 0.171875]\n",
      "6943: [discriminator loss: 0.534297, acc: 0.656250] [adversarial loss: 1.065183, acc: 0.296875]\n",
      "6944: [discriminator loss: 0.464964, acc: 0.781250] [adversarial loss: 1.342651, acc: 0.234375]\n",
      "6945: [discriminator loss: 0.498830, acc: 0.742188] [adversarial loss: 0.994942, acc: 0.390625]\n",
      "6946: [discriminator loss: 0.506649, acc: 0.695312] [adversarial loss: 1.290111, acc: 0.218750]\n",
      "6947: [discriminator loss: 0.583887, acc: 0.679688] [adversarial loss: 1.120852, acc: 0.250000]\n",
      "6948: [discriminator loss: 0.504665, acc: 0.773438] [adversarial loss: 1.672684, acc: 0.062500]\n",
      "6949: [discriminator loss: 0.595534, acc: 0.664062] [adversarial loss: 0.640281, acc: 0.578125]\n",
      "6950: [discriminator loss: 0.574758, acc: 0.656250] [adversarial loss: 1.362003, acc: 0.125000]\n",
      "6951: [discriminator loss: 0.554404, acc: 0.710938] [adversarial loss: 1.075383, acc: 0.218750]\n",
      "6952: [discriminator loss: 0.506359, acc: 0.781250] [adversarial loss: 1.096778, acc: 0.203125]\n",
      "6953: [discriminator loss: 0.462956, acc: 0.851562] [adversarial loss: 1.477274, acc: 0.171875]\n",
      "6954: [discriminator loss: 0.568130, acc: 0.695312] [adversarial loss: 0.946736, acc: 0.328125]\n",
      "6955: [discriminator loss: 0.624284, acc: 0.656250] [adversarial loss: 1.593605, acc: 0.078125]\n",
      "6956: [discriminator loss: 0.590659, acc: 0.679688] [adversarial loss: 0.896132, acc: 0.406250]\n",
      "6957: [discriminator loss: 0.560101, acc: 0.679688] [adversarial loss: 1.273918, acc: 0.187500]\n",
      "6958: [discriminator loss: 0.544743, acc: 0.726562] [adversarial loss: 1.306369, acc: 0.234375]\n",
      "6959: [discriminator loss: 0.465831, acc: 0.796875] [adversarial loss: 1.275868, acc: 0.156250]\n",
      "6960: [discriminator loss: 0.545892, acc: 0.710938] [adversarial loss: 1.008179, acc: 0.375000]\n",
      "6961: [discriminator loss: 0.503981, acc: 0.765625] [adversarial loss: 1.359019, acc: 0.125000]\n",
      "6962: [discriminator loss: 0.517767, acc: 0.734375] [adversarial loss: 1.303291, acc: 0.156250]\n",
      "6963: [discriminator loss: 0.587132, acc: 0.703125] [adversarial loss: 1.343852, acc: 0.171875]\n",
      "6964: [discriminator loss: 0.586622, acc: 0.695312] [adversarial loss: 0.889486, acc: 0.421875]\n",
      "6965: [discriminator loss: 0.522067, acc: 0.710938] [adversarial loss: 1.278456, acc: 0.265625]\n",
      "6966: [discriminator loss: 0.590327, acc: 0.695312] [adversarial loss: 1.174175, acc: 0.265625]\n",
      "6967: [discriminator loss: 0.576911, acc: 0.710938] [adversarial loss: 1.374469, acc: 0.125000]\n",
      "6968: [discriminator loss: 0.534272, acc: 0.710938] [adversarial loss: 1.045566, acc: 0.343750]\n",
      "6969: [discriminator loss: 0.560217, acc: 0.718750] [adversarial loss: 1.257252, acc: 0.171875]\n",
      "6970: [discriminator loss: 0.635691, acc: 0.664062] [adversarial loss: 1.248434, acc: 0.218750]\n",
      "6971: [discriminator loss: 0.528991, acc: 0.718750] [adversarial loss: 1.252738, acc: 0.156250]\n",
      "6972: [discriminator loss: 0.517168, acc: 0.726562] [adversarial loss: 1.272950, acc: 0.109375]\n",
      "6973: [discriminator loss: 0.512549, acc: 0.742188] [adversarial loss: 0.907533, acc: 0.437500]\n",
      "6974: [discriminator loss: 0.490147, acc: 0.765625] [adversarial loss: 1.403363, acc: 0.140625]\n",
      "6975: [discriminator loss: 0.565823, acc: 0.703125] [adversarial loss: 0.920234, acc: 0.453125]\n",
      "6976: [discriminator loss: 0.538512, acc: 0.703125] [adversarial loss: 1.688164, acc: 0.093750]\n",
      "6977: [discriminator loss: 0.578688, acc: 0.742188] [adversarial loss: 0.930714, acc: 0.406250]\n",
      "6978: [discriminator loss: 0.550355, acc: 0.773438] [adversarial loss: 1.331378, acc: 0.187500]\n",
      "6979: [discriminator loss: 0.554006, acc: 0.718750] [adversarial loss: 1.046056, acc: 0.328125]\n",
      "6980: [discriminator loss: 0.647390, acc: 0.601562] [adversarial loss: 1.206613, acc: 0.234375]\n",
      "6981: [discriminator loss: 0.548389, acc: 0.710938] [adversarial loss: 0.978743, acc: 0.375000]\n",
      "6982: [discriminator loss: 0.596777, acc: 0.656250] [adversarial loss: 1.476012, acc: 0.109375]\n",
      "6983: [discriminator loss: 0.541821, acc: 0.687500] [adversarial loss: 1.058785, acc: 0.250000]\n",
      "6984: [discriminator loss: 0.576923, acc: 0.679688] [adversarial loss: 1.593311, acc: 0.171875]\n",
      "6985: [discriminator loss: 0.593428, acc: 0.679688] [adversarial loss: 0.832170, acc: 0.406250]\n",
      "6986: [discriminator loss: 0.580390, acc: 0.664062] [adversarial loss: 1.384171, acc: 0.187500]\n",
      "6987: [discriminator loss: 0.601185, acc: 0.703125] [adversarial loss: 0.896735, acc: 0.406250]\n",
      "6988: [discriminator loss: 0.589998, acc: 0.703125] [adversarial loss: 1.315086, acc: 0.187500]\n",
      "6989: [discriminator loss: 0.530835, acc: 0.718750] [adversarial loss: 1.042856, acc: 0.312500]\n",
      "6990: [discriminator loss: 0.591674, acc: 0.695312] [adversarial loss: 1.237604, acc: 0.265625]\n",
      "6991: [discriminator loss: 0.547682, acc: 0.679688] [adversarial loss: 1.124366, acc: 0.296875]\n",
      "6992: [discriminator loss: 0.486832, acc: 0.773438] [adversarial loss: 1.282434, acc: 0.140625]\n",
      "6993: [discriminator loss: 0.592669, acc: 0.656250] [adversarial loss: 1.181139, acc: 0.250000]\n",
      "6994: [discriminator loss: 0.512533, acc: 0.765625] [adversarial loss: 1.060001, acc: 0.250000]\n",
      "6995: [discriminator loss: 0.578033, acc: 0.687500] [adversarial loss: 1.413731, acc: 0.125000]\n",
      "6996: [discriminator loss: 0.610998, acc: 0.632812] [adversarial loss: 1.000840, acc: 0.437500]\n",
      "6997: [discriminator loss: 0.584074, acc: 0.679688] [adversarial loss: 1.464909, acc: 0.109375]\n",
      "6998: [discriminator loss: 0.528712, acc: 0.726562] [adversarial loss: 1.102049, acc: 0.312500]\n",
      "6999: [discriminator loss: 0.542866, acc: 0.687500] [adversarial loss: 1.138026, acc: 0.265625]\n",
      "7000: [discriminator loss: 0.504357, acc: 0.757812] [adversarial loss: 1.183029, acc: 0.203125]\n",
      "7001: [discriminator loss: 0.589414, acc: 0.710938] [adversarial loss: 1.117473, acc: 0.171875]\n",
      "7002: [discriminator loss: 0.564967, acc: 0.710938] [adversarial loss: 1.020674, acc: 0.359375]\n",
      "7003: [discriminator loss: 0.543122, acc: 0.726562] [adversarial loss: 1.249469, acc: 0.125000]\n",
      "7004: [discriminator loss: 0.516655, acc: 0.710938] [adversarial loss: 0.975474, acc: 0.312500]\n",
      "7005: [discriminator loss: 0.590014, acc: 0.695312] [adversarial loss: 1.496342, acc: 0.156250]\n",
      "7006: [discriminator loss: 0.601621, acc: 0.632812] [adversarial loss: 0.901493, acc: 0.375000]\n",
      "7007: [discriminator loss: 0.684896, acc: 0.640625] [adversarial loss: 1.616186, acc: 0.062500]\n",
      "7008: [discriminator loss: 0.581188, acc: 0.695312] [adversarial loss: 0.869815, acc: 0.437500]\n",
      "7009: [discriminator loss: 0.536500, acc: 0.773438] [adversarial loss: 1.306972, acc: 0.171875]\n",
      "7010: [discriminator loss: 0.504136, acc: 0.765625] [adversarial loss: 0.964942, acc: 0.359375]\n",
      "7011: [discriminator loss: 0.537275, acc: 0.718750] [adversarial loss: 1.149424, acc: 0.234375]\n",
      "7012: [discriminator loss: 0.505717, acc: 0.742188] [adversarial loss: 1.115186, acc: 0.250000]\n",
      "7013: [discriminator loss: 0.544095, acc: 0.710938] [adversarial loss: 1.298357, acc: 0.171875]\n",
      "7014: [discriminator loss: 0.487702, acc: 0.726562] [adversarial loss: 1.108533, acc: 0.250000]\n",
      "7015: [discriminator loss: 0.514603, acc: 0.726562] [adversarial loss: 1.033561, acc: 0.296875]\n",
      "7016: [discriminator loss: 0.572805, acc: 0.710938] [adversarial loss: 1.370130, acc: 0.187500]\n",
      "7017: [discriminator loss: 0.524240, acc: 0.765625] [adversarial loss: 1.175890, acc: 0.296875]\n",
      "7018: [discriminator loss: 0.532207, acc: 0.726562] [adversarial loss: 1.215907, acc: 0.171875]\n",
      "7019: [discriminator loss: 0.547394, acc: 0.695312] [adversarial loss: 1.150596, acc: 0.296875]\n",
      "7020: [discriminator loss: 0.492147, acc: 0.773438] [adversarial loss: 1.279485, acc: 0.171875]\n",
      "7021: [discriminator loss: 0.496101, acc: 0.765625] [adversarial loss: 1.172348, acc: 0.218750]\n",
      "7022: [discriminator loss: 0.548178, acc: 0.679688] [adversarial loss: 1.590588, acc: 0.156250]\n",
      "7023: [discriminator loss: 0.525971, acc: 0.703125] [adversarial loss: 1.035240, acc: 0.312500]\n",
      "7024: [discriminator loss: 0.464342, acc: 0.796875] [adversarial loss: 1.103260, acc: 0.343750]\n",
      "7025: [discriminator loss: 0.635705, acc: 0.671875] [adversarial loss: 1.377706, acc: 0.171875]\n",
      "7026: [discriminator loss: 0.553489, acc: 0.726562] [adversarial loss: 1.052022, acc: 0.296875]\n",
      "7027: [discriminator loss: 0.551960, acc: 0.726562] [adversarial loss: 1.250044, acc: 0.281250]\n",
      "7028: [discriminator loss: 0.553173, acc: 0.742188] [adversarial loss: 1.031866, acc: 0.296875]\n",
      "7029: [discriminator loss: 0.506551, acc: 0.773438] [adversarial loss: 1.281098, acc: 0.140625]\n",
      "7030: [discriminator loss: 0.502956, acc: 0.734375] [adversarial loss: 1.039642, acc: 0.296875]\n",
      "7031: [discriminator loss: 0.497735, acc: 0.773438] [adversarial loss: 0.903584, acc: 0.421875]\n",
      "7032: [discriminator loss: 0.525286, acc: 0.726562] [adversarial loss: 1.287334, acc: 0.203125]\n",
      "7033: [discriminator loss: 0.610969, acc: 0.671875] [adversarial loss: 0.988479, acc: 0.375000]\n",
      "7034: [discriminator loss: 0.513310, acc: 0.742188] [adversarial loss: 1.485926, acc: 0.140625]\n",
      "7035: [discriminator loss: 0.549112, acc: 0.695312] [adversarial loss: 0.931404, acc: 0.359375]\n",
      "7036: [discriminator loss: 0.591624, acc: 0.679688] [adversarial loss: 1.583330, acc: 0.078125]\n",
      "7037: [discriminator loss: 0.538583, acc: 0.726562] [adversarial loss: 1.133333, acc: 0.281250]\n",
      "7038: [discriminator loss: 0.536687, acc: 0.695312] [adversarial loss: 1.536995, acc: 0.187500]\n",
      "7039: [discriminator loss: 0.565358, acc: 0.695312] [adversarial loss: 1.011868, acc: 0.328125]\n",
      "7040: [discriminator loss: 0.553680, acc: 0.726562] [adversarial loss: 1.349555, acc: 0.187500]\n",
      "7041: [discriminator loss: 0.508794, acc: 0.750000] [adversarial loss: 1.177833, acc: 0.281250]\n",
      "7042: [discriminator loss: 0.530391, acc: 0.726562] [adversarial loss: 0.934884, acc: 0.312500]\n",
      "7043: [discriminator loss: 0.530206, acc: 0.757812] [adversarial loss: 1.372009, acc: 0.125000]\n",
      "7044: [discriminator loss: 0.540867, acc: 0.710938] [adversarial loss: 1.013092, acc: 0.328125]\n",
      "7045: [discriminator loss: 0.547758, acc: 0.796875] [adversarial loss: 1.244922, acc: 0.140625]\n",
      "7046: [discriminator loss: 0.604860, acc: 0.671875] [adversarial loss: 1.367339, acc: 0.203125]\n",
      "7047: [discriminator loss: 0.509007, acc: 0.734375] [adversarial loss: 1.104050, acc: 0.265625]\n",
      "7048: [discriminator loss: 0.550268, acc: 0.703125] [adversarial loss: 1.214856, acc: 0.156250]\n",
      "7049: [discriminator loss: 0.534883, acc: 0.718750] [adversarial loss: 1.132665, acc: 0.265625]\n",
      "7050: [discriminator loss: 0.576821, acc: 0.695312] [adversarial loss: 1.416623, acc: 0.140625]\n",
      "7051: [discriminator loss: 0.614087, acc: 0.640625] [adversarial loss: 0.891738, acc: 0.437500]\n",
      "7052: [discriminator loss: 0.565127, acc: 0.671875] [adversarial loss: 1.209112, acc: 0.218750]\n",
      "7053: [discriminator loss: 0.554329, acc: 0.718750] [adversarial loss: 1.210494, acc: 0.218750]\n",
      "7054: [discriminator loss: 0.558756, acc: 0.757812] [adversarial loss: 1.101781, acc: 0.234375]\n",
      "7055: [discriminator loss: 0.498195, acc: 0.750000] [adversarial loss: 1.235834, acc: 0.234375]\n",
      "7056: [discriminator loss: 0.478483, acc: 0.765625] [adversarial loss: 1.070242, acc: 0.265625]\n",
      "7057: [discriminator loss: 0.556216, acc: 0.734375] [adversarial loss: 1.375903, acc: 0.140625]\n",
      "7058: [discriminator loss: 0.537235, acc: 0.710938] [adversarial loss: 1.070730, acc: 0.406250]\n",
      "7059: [discriminator loss: 0.523322, acc: 0.710938] [adversarial loss: 1.404664, acc: 0.156250]\n",
      "7060: [discriminator loss: 0.539536, acc: 0.726562] [adversarial loss: 1.111995, acc: 0.265625]\n",
      "7061: [discriminator loss: 0.581850, acc: 0.695312] [adversarial loss: 1.283665, acc: 0.218750]\n",
      "7062: [discriminator loss: 0.517133, acc: 0.757812] [adversarial loss: 0.814892, acc: 0.515625]\n",
      "7063: [discriminator loss: 0.576336, acc: 0.703125] [adversarial loss: 1.568245, acc: 0.156250]\n",
      "7064: [discriminator loss: 0.593282, acc: 0.710938] [adversarial loss: 0.717625, acc: 0.609375]\n",
      "7065: [discriminator loss: 0.611384, acc: 0.640625] [adversarial loss: 1.384853, acc: 0.078125]\n",
      "7066: [discriminator loss: 0.564083, acc: 0.648438] [adversarial loss: 1.189882, acc: 0.171875]\n",
      "7067: [discriminator loss: 0.504290, acc: 0.750000] [adversarial loss: 1.180747, acc: 0.250000]\n",
      "7068: [discriminator loss: 0.501295, acc: 0.812500] [adversarial loss: 1.008733, acc: 0.265625]\n",
      "7069: [discriminator loss: 0.484303, acc: 0.781250] [adversarial loss: 1.326550, acc: 0.109375]\n",
      "7070: [discriminator loss: 0.550735, acc: 0.710938] [adversarial loss: 1.077712, acc: 0.218750]\n",
      "7071: [discriminator loss: 0.536968, acc: 0.750000] [adversarial loss: 1.405583, acc: 0.140625]\n",
      "7072: [discriminator loss: 0.519951, acc: 0.734375] [adversarial loss: 1.049899, acc: 0.265625]\n",
      "7073: [discriminator loss: 0.529684, acc: 0.742188] [adversarial loss: 1.203994, acc: 0.171875]\n",
      "7074: [discriminator loss: 0.528961, acc: 0.765625] [adversarial loss: 1.383431, acc: 0.156250]\n",
      "7075: [discriminator loss: 0.540904, acc: 0.703125] [adversarial loss: 0.970897, acc: 0.343750]\n",
      "7076: [discriminator loss: 0.488941, acc: 0.796875] [adversarial loss: 1.347818, acc: 0.234375]\n",
      "7077: [discriminator loss: 0.598327, acc: 0.695312] [adversarial loss: 1.073448, acc: 0.359375]\n",
      "7078: [discriminator loss: 0.581517, acc: 0.687500] [adversarial loss: 1.527080, acc: 0.078125]\n",
      "7079: [discriminator loss: 0.533936, acc: 0.656250] [adversarial loss: 1.185885, acc: 0.234375]\n",
      "7080: [discriminator loss: 0.515866, acc: 0.742188] [adversarial loss: 1.300198, acc: 0.140625]\n",
      "7081: [discriminator loss: 0.574675, acc: 0.757812] [adversarial loss: 0.912675, acc: 0.437500]\n",
      "7082: [discriminator loss: 0.526486, acc: 0.750000] [adversarial loss: 1.359768, acc: 0.125000]\n",
      "7083: [discriminator loss: 0.539670, acc: 0.710938] [adversarial loss: 1.301441, acc: 0.203125]\n",
      "7084: [discriminator loss: 0.570199, acc: 0.671875] [adversarial loss: 1.136368, acc: 0.312500]\n",
      "7085: [discriminator loss: 0.553200, acc: 0.703125] [adversarial loss: 0.987147, acc: 0.406250]\n",
      "7086: [discriminator loss: 0.574495, acc: 0.718750] [adversarial loss: 1.477364, acc: 0.156250]\n",
      "7087: [discriminator loss: 0.551246, acc: 0.703125] [adversarial loss: 1.254514, acc: 0.125000]\n",
      "7088: [discriminator loss: 0.589909, acc: 0.671875] [adversarial loss: 1.287248, acc: 0.281250]\n",
      "7089: [discriminator loss: 0.552649, acc: 0.750000] [adversarial loss: 0.930735, acc: 0.453125]\n",
      "7090: [discriminator loss: 0.548820, acc: 0.710938] [adversarial loss: 1.069206, acc: 0.265625]\n",
      "7091: [discriminator loss: 0.501435, acc: 0.750000] [adversarial loss: 1.247314, acc: 0.187500]\n",
      "7092: [discriminator loss: 0.524271, acc: 0.734375] [adversarial loss: 1.123053, acc: 0.281250]\n",
      "7093: [discriminator loss: 0.528584, acc: 0.703125] [adversarial loss: 1.218683, acc: 0.218750]\n",
      "7094: [discriminator loss: 0.608951, acc: 0.656250] [adversarial loss: 1.004182, acc: 0.265625]\n",
      "7095: [discriminator loss: 0.634133, acc: 0.648438] [adversarial loss: 1.326027, acc: 0.203125]\n",
      "7096: [discriminator loss: 0.546411, acc: 0.734375] [adversarial loss: 0.903623, acc: 0.359375]\n",
      "7097: [discriminator loss: 0.515649, acc: 0.726562] [adversarial loss: 1.396013, acc: 0.062500]\n",
      "7098: [discriminator loss: 0.591937, acc: 0.632812] [adversarial loss: 0.949198, acc: 0.390625]\n",
      "7099: [discriminator loss: 0.610087, acc: 0.687500] [adversarial loss: 1.569252, acc: 0.140625]\n",
      "7100: [discriminator loss: 0.508410, acc: 0.750000] [adversarial loss: 1.056508, acc: 0.406250]\n",
      "7101: [discriminator loss: 0.586115, acc: 0.664062] [adversarial loss: 1.495543, acc: 0.109375]\n",
      "7102: [discriminator loss: 0.517598, acc: 0.695312] [adversarial loss: 1.040327, acc: 0.312500]\n",
      "7103: [discriminator loss: 0.577684, acc: 0.703125] [adversarial loss: 1.182864, acc: 0.203125]\n",
      "7104: [discriminator loss: 0.548395, acc: 0.734375] [adversarial loss: 0.962841, acc: 0.343750]\n",
      "7105: [discriminator loss: 0.539685, acc: 0.710938] [adversarial loss: 1.429119, acc: 0.218750]\n",
      "7106: [discriminator loss: 0.550424, acc: 0.695312] [adversarial loss: 1.010094, acc: 0.265625]\n",
      "7107: [discriminator loss: 0.540238, acc: 0.710938] [adversarial loss: 1.337108, acc: 0.140625]\n",
      "7108: [discriminator loss: 0.521047, acc: 0.781250] [adversarial loss: 0.982967, acc: 0.281250]\n",
      "7109: [discriminator loss: 0.574022, acc: 0.703125] [adversarial loss: 1.280964, acc: 0.203125]\n",
      "7110: [discriminator loss: 0.536870, acc: 0.718750] [adversarial loss: 1.182123, acc: 0.156250]\n",
      "7111: [discriminator loss: 0.523587, acc: 0.750000] [adversarial loss: 1.183348, acc: 0.203125]\n",
      "7112: [discriminator loss: 0.523950, acc: 0.726562] [adversarial loss: 1.009109, acc: 0.281250]\n",
      "7113: [discriminator loss: 0.518829, acc: 0.742188] [adversarial loss: 1.216370, acc: 0.203125]\n",
      "7114: [discriminator loss: 0.588742, acc: 0.640625] [adversarial loss: 1.041958, acc: 0.343750]\n",
      "7115: [discriminator loss: 0.510044, acc: 0.726562] [adversarial loss: 1.584090, acc: 0.015625]\n",
      "7116: [discriminator loss: 0.598980, acc: 0.695312] [adversarial loss: 1.032095, acc: 0.281250]\n",
      "7117: [discriminator loss: 0.532480, acc: 0.734375] [adversarial loss: 1.357275, acc: 0.234375]\n",
      "7118: [discriminator loss: 0.489220, acc: 0.781250] [adversarial loss: 1.096568, acc: 0.187500]\n",
      "7119: [discriminator loss: 0.505254, acc: 0.765625] [adversarial loss: 1.310851, acc: 0.187500]\n",
      "7120: [discriminator loss: 0.562162, acc: 0.679688] [adversarial loss: 1.004119, acc: 0.343750]\n",
      "7121: [discriminator loss: 0.592568, acc: 0.679688] [adversarial loss: 1.165558, acc: 0.281250]\n",
      "7122: [discriminator loss: 0.552514, acc: 0.742188] [adversarial loss: 1.235750, acc: 0.328125]\n",
      "7123: [discriminator loss: 0.538531, acc: 0.687500] [adversarial loss: 1.313268, acc: 0.125000]\n",
      "7124: [discriminator loss: 0.543794, acc: 0.765625] [adversarial loss: 1.426178, acc: 0.171875]\n",
      "7125: [discriminator loss: 0.557105, acc: 0.734375] [adversarial loss: 1.013636, acc: 0.265625]\n",
      "7126: [discriminator loss: 0.582011, acc: 0.718750] [adversarial loss: 1.345587, acc: 0.140625]\n",
      "7127: [discriminator loss: 0.510896, acc: 0.757812] [adversarial loss: 1.078822, acc: 0.203125]\n",
      "7128: [discriminator loss: 0.485309, acc: 0.812500] [adversarial loss: 1.297214, acc: 0.281250]\n",
      "7129: [discriminator loss: 0.477988, acc: 0.773438] [adversarial loss: 0.913999, acc: 0.343750]\n",
      "7130: [discriminator loss: 0.627909, acc: 0.632812] [adversarial loss: 1.437501, acc: 0.171875]\n",
      "7131: [discriminator loss: 0.518948, acc: 0.718750] [adversarial loss: 0.840094, acc: 0.468750]\n",
      "7132: [discriminator loss: 0.577482, acc: 0.703125] [adversarial loss: 1.538654, acc: 0.109375]\n",
      "7133: [discriminator loss: 0.640959, acc: 0.648438] [adversarial loss: 0.856587, acc: 0.421875]\n",
      "7134: [discriminator loss: 0.514821, acc: 0.742188] [adversarial loss: 1.362999, acc: 0.203125]\n",
      "7135: [discriminator loss: 0.598026, acc: 0.687500] [adversarial loss: 1.087524, acc: 0.265625]\n",
      "7136: [discriminator loss: 0.503821, acc: 0.757812] [adversarial loss: 1.306223, acc: 0.171875]\n",
      "7137: [discriminator loss: 0.652819, acc: 0.695312] [adversarial loss: 1.082188, acc: 0.265625]\n",
      "7138: [discriminator loss: 0.496357, acc: 0.773438] [adversarial loss: 1.241852, acc: 0.234375]\n",
      "7139: [discriminator loss: 0.589282, acc: 0.710938] [adversarial loss: 1.371686, acc: 0.156250]\n",
      "7140: [discriminator loss: 0.546740, acc: 0.750000] [adversarial loss: 0.943980, acc: 0.390625]\n",
      "7141: [discriminator loss: 0.508784, acc: 0.765625] [adversarial loss: 1.334531, acc: 0.203125]\n",
      "7142: [discriminator loss: 0.531967, acc: 0.718750] [adversarial loss: 0.830729, acc: 0.484375]\n",
      "7143: [discriminator loss: 0.619057, acc: 0.648438] [adversarial loss: 1.440974, acc: 0.187500]\n",
      "7144: [discriminator loss: 0.589165, acc: 0.734375] [adversarial loss: 1.118459, acc: 0.296875]\n",
      "7145: [discriminator loss: 0.553816, acc: 0.734375] [adversarial loss: 1.261941, acc: 0.187500]\n",
      "7146: [discriminator loss: 0.490563, acc: 0.781250] [adversarial loss: 1.139067, acc: 0.296875]\n",
      "7147: [discriminator loss: 0.504075, acc: 0.750000] [adversarial loss: 1.372546, acc: 0.187500]\n",
      "7148: [discriminator loss: 0.489349, acc: 0.742188] [adversarial loss: 0.811271, acc: 0.531250]\n",
      "7149: [discriminator loss: 0.586002, acc: 0.656250] [adversarial loss: 1.633328, acc: 0.093750]\n",
      "7150: [discriminator loss: 0.507489, acc: 0.773438] [adversarial loss: 1.196766, acc: 0.218750]\n",
      "7151: [discriminator loss: 0.561216, acc: 0.703125] [adversarial loss: 1.359159, acc: 0.203125]\n",
      "7152: [discriminator loss: 0.509132, acc: 0.757812] [adversarial loss: 1.150673, acc: 0.234375]\n",
      "7153: [discriminator loss: 0.536773, acc: 0.710938] [adversarial loss: 1.145768, acc: 0.218750]\n",
      "7154: [discriminator loss: 0.536671, acc: 0.726562] [adversarial loss: 1.243687, acc: 0.171875]\n",
      "7155: [discriminator loss: 0.588289, acc: 0.679688] [adversarial loss: 1.193901, acc: 0.265625]\n",
      "7156: [discriminator loss: 0.518626, acc: 0.726562] [adversarial loss: 1.203048, acc: 0.187500]\n",
      "7157: [discriminator loss: 0.571445, acc: 0.687500] [adversarial loss: 1.127555, acc: 0.203125]\n",
      "7158: [discriminator loss: 0.550998, acc: 0.742188] [adversarial loss: 1.024549, acc: 0.375000]\n",
      "7159: [discriminator loss: 0.580747, acc: 0.687500] [adversarial loss: 1.345087, acc: 0.156250]\n",
      "7160: [discriminator loss: 0.515891, acc: 0.750000] [adversarial loss: 1.010072, acc: 0.250000]\n",
      "7161: [discriminator loss: 0.534812, acc: 0.757812] [adversarial loss: 1.315227, acc: 0.234375]\n",
      "7162: [discriminator loss: 0.478737, acc: 0.757812] [adversarial loss: 0.835790, acc: 0.468750]\n",
      "7163: [discriminator loss: 0.500215, acc: 0.757812] [adversarial loss: 1.319261, acc: 0.187500]\n",
      "7164: [discriminator loss: 0.542343, acc: 0.695312] [adversarial loss: 1.118784, acc: 0.265625]\n",
      "7165: [discriminator loss: 0.501631, acc: 0.796875] [adversarial loss: 0.970126, acc: 0.390625]\n",
      "7166: [discriminator loss: 0.500813, acc: 0.726562] [adversarial loss: 1.295756, acc: 0.296875]\n",
      "7167: [discriminator loss: 0.556955, acc: 0.726562] [adversarial loss: 1.189479, acc: 0.328125]\n",
      "7168: [discriminator loss: 0.540176, acc: 0.734375] [adversarial loss: 1.194704, acc: 0.218750]\n",
      "7169: [discriminator loss: 0.444749, acc: 0.828125] [adversarial loss: 1.199478, acc: 0.250000]\n",
      "7170: [discriminator loss: 0.589246, acc: 0.632812] [adversarial loss: 1.020780, acc: 0.328125]\n",
      "7171: [discriminator loss: 0.573735, acc: 0.695312] [adversarial loss: 1.277849, acc: 0.156250]\n",
      "7172: [discriminator loss: 0.655731, acc: 0.664062] [adversarial loss: 0.849570, acc: 0.406250]\n",
      "7173: [discriminator loss: 0.587964, acc: 0.710938] [adversarial loss: 1.638342, acc: 0.140625]\n",
      "7174: [discriminator loss: 0.576750, acc: 0.656250] [adversarial loss: 0.896178, acc: 0.375000]\n",
      "7175: [discriminator loss: 0.507752, acc: 0.765625] [adversarial loss: 1.205622, acc: 0.203125]\n",
      "7176: [discriminator loss: 0.548773, acc: 0.703125] [adversarial loss: 1.305788, acc: 0.093750]\n",
      "7177: [discriminator loss: 0.552649, acc: 0.695312] [adversarial loss: 0.880463, acc: 0.484375]\n",
      "7178: [discriminator loss: 0.517295, acc: 0.726562] [adversarial loss: 1.373000, acc: 0.125000]\n",
      "7179: [discriminator loss: 0.529521, acc: 0.718750] [adversarial loss: 0.939363, acc: 0.421875]\n",
      "7180: [discriminator loss: 0.544213, acc: 0.703125] [adversarial loss: 1.521950, acc: 0.109375]\n",
      "7181: [discriminator loss: 0.532342, acc: 0.734375] [adversarial loss: 1.001939, acc: 0.265625]\n",
      "7182: [discriminator loss: 0.474995, acc: 0.789062] [adversarial loss: 1.380281, acc: 0.093750]\n",
      "7183: [discriminator loss: 0.541448, acc: 0.710938] [adversarial loss: 0.937140, acc: 0.375000]\n",
      "7184: [discriminator loss: 0.505976, acc: 0.734375] [adversarial loss: 1.713878, acc: 0.093750]\n",
      "7185: [discriminator loss: 0.692075, acc: 0.625000] [adversarial loss: 0.662869, acc: 0.593750]\n",
      "7186: [discriminator loss: 0.598384, acc: 0.648438] [adversarial loss: 1.163290, acc: 0.250000]\n",
      "7187: [discriminator loss: 0.550974, acc: 0.687500] [adversarial loss: 1.016433, acc: 0.265625]\n",
      "7188: [discriminator loss: 0.552492, acc: 0.718750] [adversarial loss: 1.464392, acc: 0.140625]\n",
      "7189: [discriminator loss: 0.542377, acc: 0.718750] [adversarial loss: 0.988554, acc: 0.296875]\n",
      "7190: [discriminator loss: 0.548729, acc: 0.695312] [adversarial loss: 1.212660, acc: 0.250000]\n",
      "7191: [discriminator loss: 0.520039, acc: 0.703125] [adversarial loss: 1.030981, acc: 0.265625]\n",
      "7192: [discriminator loss: 0.444298, acc: 0.820312] [adversarial loss: 1.004476, acc: 0.250000]\n",
      "7193: [discriminator loss: 0.531822, acc: 0.695312] [adversarial loss: 1.091392, acc: 0.281250]\n",
      "7194: [discriminator loss: 0.588266, acc: 0.726562] [adversarial loss: 1.239595, acc: 0.203125]\n",
      "7195: [discriminator loss: 0.531427, acc: 0.710938] [adversarial loss: 1.397658, acc: 0.187500]\n",
      "7196: [discriminator loss: 0.541079, acc: 0.703125] [adversarial loss: 1.026227, acc: 0.296875]\n",
      "7197: [discriminator loss: 0.563450, acc: 0.687500] [adversarial loss: 1.075086, acc: 0.265625]\n",
      "7198: [discriminator loss: 0.593078, acc: 0.671875] [adversarial loss: 1.093289, acc: 0.234375]\n",
      "7199: [discriminator loss: 0.568923, acc: 0.687500] [adversarial loss: 1.485292, acc: 0.187500]\n",
      "7200: [discriminator loss: 0.569827, acc: 0.679688] [adversarial loss: 1.009369, acc: 0.296875]\n",
      "7201: [discriminator loss: 0.494073, acc: 0.789062] [adversarial loss: 1.159895, acc: 0.218750]\n",
      "7202: [discriminator loss: 0.516353, acc: 0.718750] [adversarial loss: 1.526781, acc: 0.078125]\n",
      "7203: [discriminator loss: 0.500165, acc: 0.765625] [adversarial loss: 0.878055, acc: 0.390625]\n",
      "7204: [discriminator loss: 0.538415, acc: 0.695312] [adversarial loss: 1.439121, acc: 0.062500]\n",
      "7205: [discriminator loss: 0.510876, acc: 0.750000] [adversarial loss: 0.820888, acc: 0.453125]\n",
      "7206: [discriminator loss: 0.495812, acc: 0.789062] [adversarial loss: 1.323684, acc: 0.187500]\n",
      "7207: [discriminator loss: 0.562245, acc: 0.734375] [adversarial loss: 0.989506, acc: 0.375000]\n",
      "7208: [discriminator loss: 0.499106, acc: 0.734375] [adversarial loss: 1.351120, acc: 0.234375]\n",
      "7209: [discriminator loss: 0.525695, acc: 0.765625] [adversarial loss: 0.969800, acc: 0.390625]\n",
      "7210: [discriminator loss: 0.516181, acc: 0.679688] [adversarial loss: 1.337585, acc: 0.234375]\n",
      "7211: [discriminator loss: 0.604844, acc: 0.648438] [adversarial loss: 1.054141, acc: 0.375000]\n",
      "7212: [discriminator loss: 0.567217, acc: 0.710938] [adversarial loss: 1.216651, acc: 0.234375]\n",
      "7213: [discriminator loss: 0.533236, acc: 0.718750] [adversarial loss: 0.817469, acc: 0.468750]\n",
      "7214: [discriminator loss: 0.535298, acc: 0.726562] [adversarial loss: 1.379366, acc: 0.093750]\n",
      "7215: [discriminator loss: 0.527764, acc: 0.710938] [adversarial loss: 1.039414, acc: 0.343750]\n",
      "7216: [discriminator loss: 0.564559, acc: 0.726562] [adversarial loss: 1.457641, acc: 0.187500]\n",
      "7217: [discriminator loss: 0.650287, acc: 0.640625] [adversarial loss: 0.843763, acc: 0.437500]\n",
      "7218: [discriminator loss: 0.559977, acc: 0.726562] [adversarial loss: 1.635076, acc: 0.046875]\n",
      "7219: [discriminator loss: 0.534237, acc: 0.726562] [adversarial loss: 1.013016, acc: 0.328125]\n",
      "7220: [discriminator loss: 0.551846, acc: 0.726562] [adversarial loss: 1.069481, acc: 0.328125]\n",
      "7221: [discriminator loss: 0.517746, acc: 0.742188] [adversarial loss: 1.062579, acc: 0.234375]\n",
      "7222: [discriminator loss: 0.527214, acc: 0.734375] [adversarial loss: 1.382246, acc: 0.109375]\n",
      "7223: [discriminator loss: 0.546950, acc: 0.750000] [adversarial loss: 1.309675, acc: 0.171875]\n",
      "7224: [discriminator loss: 0.617797, acc: 0.656250] [adversarial loss: 0.930698, acc: 0.421875]\n",
      "7225: [discriminator loss: 0.483435, acc: 0.757812] [adversarial loss: 1.456333, acc: 0.187500]\n",
      "7226: [discriminator loss: 0.640286, acc: 0.617188] [adversarial loss: 0.985436, acc: 0.375000]\n",
      "7227: [discriminator loss: 0.674651, acc: 0.601562] [adversarial loss: 1.753947, acc: 0.093750]\n",
      "7228: [discriminator loss: 0.642056, acc: 0.664062] [adversarial loss: 0.831580, acc: 0.453125]\n",
      "7229: [discriminator loss: 0.598991, acc: 0.703125] [adversarial loss: 1.441191, acc: 0.078125]\n",
      "7230: [discriminator loss: 0.658453, acc: 0.632812] [adversarial loss: 0.875995, acc: 0.406250]\n",
      "7231: [discriminator loss: 0.514369, acc: 0.734375] [adversarial loss: 1.271785, acc: 0.156250]\n",
      "7232: [discriminator loss: 0.565220, acc: 0.703125] [adversarial loss: 1.192963, acc: 0.265625]\n",
      "7233: [discriminator loss: 0.508535, acc: 0.789062] [adversarial loss: 1.129813, acc: 0.187500]\n",
      "7234: [discriminator loss: 0.542405, acc: 0.773438] [adversarial loss: 0.995523, acc: 0.281250]\n",
      "7235: [discriminator loss: 0.568256, acc: 0.679688] [adversarial loss: 1.204921, acc: 0.218750]\n",
      "7236: [discriminator loss: 0.480924, acc: 0.765625] [adversarial loss: 1.223990, acc: 0.203125]\n",
      "7237: [discriminator loss: 0.556048, acc: 0.734375] [adversarial loss: 0.955632, acc: 0.359375]\n",
      "7238: [discriminator loss: 0.540278, acc: 0.734375] [adversarial loss: 1.158212, acc: 0.281250]\n",
      "7239: [discriminator loss: 0.478719, acc: 0.773438] [adversarial loss: 1.201784, acc: 0.218750]\n",
      "7240: [discriminator loss: 0.549127, acc: 0.718750] [adversarial loss: 1.169097, acc: 0.250000]\n",
      "7241: [discriminator loss: 0.503784, acc: 0.773438] [adversarial loss: 1.355169, acc: 0.156250]\n",
      "7242: [discriminator loss: 0.500866, acc: 0.757812] [adversarial loss: 1.113695, acc: 0.281250]\n",
      "7243: [discriminator loss: 0.537227, acc: 0.742188] [adversarial loss: 1.394988, acc: 0.171875]\n",
      "7244: [discriminator loss: 0.691244, acc: 0.593750] [adversarial loss: 0.862230, acc: 0.453125]\n",
      "7245: [discriminator loss: 0.575105, acc: 0.703125] [adversarial loss: 1.460429, acc: 0.078125]\n",
      "7246: [discriminator loss: 0.586990, acc: 0.703125] [adversarial loss: 1.022668, acc: 0.281250]\n",
      "7247: [discriminator loss: 0.517627, acc: 0.742188] [adversarial loss: 1.187464, acc: 0.187500]\n",
      "7248: [discriminator loss: 0.518461, acc: 0.742188] [adversarial loss: 1.022673, acc: 0.296875]\n",
      "7249: [discriminator loss: 0.465792, acc: 0.796875] [adversarial loss: 1.372722, acc: 0.093750]\n",
      "7250: [discriminator loss: 0.532519, acc: 0.687500] [adversarial loss: 1.003708, acc: 0.343750]\n",
      "7251: [discriminator loss: 0.533480, acc: 0.710938] [adversarial loss: 1.171233, acc: 0.265625]\n",
      "7252: [discriminator loss: 0.521233, acc: 0.687500] [adversarial loss: 1.290255, acc: 0.109375]\n",
      "7253: [discriminator loss: 0.487482, acc: 0.750000] [adversarial loss: 1.077302, acc: 0.312500]\n",
      "7254: [discriminator loss: 0.604359, acc: 0.679688] [adversarial loss: 0.969080, acc: 0.343750]\n",
      "7255: [discriminator loss: 0.464263, acc: 0.781250] [adversarial loss: 1.284372, acc: 0.125000]\n",
      "7256: [discriminator loss: 0.483409, acc: 0.773438] [adversarial loss: 1.193223, acc: 0.265625]\n",
      "7257: [discriminator loss: 0.532870, acc: 0.656250] [adversarial loss: 1.396635, acc: 0.187500]\n",
      "7258: [discriminator loss: 0.540426, acc: 0.757812] [adversarial loss: 1.016883, acc: 0.296875]\n",
      "7259: [discriminator loss: 0.529713, acc: 0.703125] [adversarial loss: 1.183399, acc: 0.234375]\n",
      "7260: [discriminator loss: 0.470617, acc: 0.742188] [adversarial loss: 1.044822, acc: 0.328125]\n",
      "7261: [discriminator loss: 0.577723, acc: 0.664062] [adversarial loss: 1.256632, acc: 0.234375]\n",
      "7262: [discriminator loss: 0.472960, acc: 0.773438] [adversarial loss: 0.972878, acc: 0.312500]\n",
      "7263: [discriminator loss: 0.548058, acc: 0.710938] [adversarial loss: 1.665227, acc: 0.171875]\n",
      "7264: [discriminator loss: 0.587283, acc: 0.632812] [adversarial loss: 0.796014, acc: 0.546875]\n",
      "7265: [discriminator loss: 0.608035, acc: 0.656250] [adversarial loss: 1.421849, acc: 0.187500]\n",
      "7266: [discriminator loss: 0.571623, acc: 0.656250] [adversarial loss: 0.815881, acc: 0.515625]\n",
      "7267: [discriminator loss: 0.694896, acc: 0.625000] [adversarial loss: 1.510069, acc: 0.171875]\n",
      "7268: [discriminator loss: 0.570041, acc: 0.718750] [adversarial loss: 1.039743, acc: 0.312500]\n",
      "7269: [discriminator loss: 0.592938, acc: 0.632812] [adversarial loss: 1.194078, acc: 0.250000]\n",
      "7270: [discriminator loss: 0.570208, acc: 0.671875] [adversarial loss: 0.944628, acc: 0.328125]\n",
      "7271: [discriminator loss: 0.446621, acc: 0.812500] [adversarial loss: 0.999490, acc: 0.343750]\n",
      "7272: [discriminator loss: 0.536185, acc: 0.695312] [adversarial loss: 1.035157, acc: 0.296875]\n",
      "7273: [discriminator loss: 0.563101, acc: 0.710938] [adversarial loss: 1.339355, acc: 0.171875]\n",
      "7274: [discriminator loss: 0.529914, acc: 0.757812] [adversarial loss: 0.952956, acc: 0.359375]\n",
      "7275: [discriminator loss: 0.551159, acc: 0.757812] [adversarial loss: 1.333551, acc: 0.093750]\n",
      "7276: [discriminator loss: 0.555644, acc: 0.742188] [adversarial loss: 0.780482, acc: 0.437500]\n",
      "7277: [discriminator loss: 0.565361, acc: 0.742188] [adversarial loss: 1.339991, acc: 0.171875]\n",
      "7278: [discriminator loss: 0.518973, acc: 0.750000] [adversarial loss: 1.314384, acc: 0.156250]\n",
      "7279: [discriminator loss: 0.555578, acc: 0.726562] [adversarial loss: 1.180179, acc: 0.171875]\n",
      "7280: [discriminator loss: 0.538596, acc: 0.679688] [adversarial loss: 1.135925, acc: 0.234375]\n",
      "7281: [discriminator loss: 0.598479, acc: 0.734375] [adversarial loss: 1.344579, acc: 0.187500]\n",
      "7282: [discriminator loss: 0.502130, acc: 0.773438] [adversarial loss: 1.169328, acc: 0.218750]\n",
      "7283: [discriminator loss: 0.556659, acc: 0.757812] [adversarial loss: 1.103254, acc: 0.234375]\n",
      "7284: [discriminator loss: 0.585412, acc: 0.703125] [adversarial loss: 1.004679, acc: 0.296875]\n",
      "7285: [discriminator loss: 0.588921, acc: 0.742188] [adversarial loss: 1.440511, acc: 0.171875]\n",
      "7286: [discriminator loss: 0.542842, acc: 0.703125] [adversarial loss: 0.881426, acc: 0.437500]\n",
      "7287: [discriminator loss: 0.486410, acc: 0.757812] [adversarial loss: 1.404007, acc: 0.109375]\n",
      "7288: [discriminator loss: 0.501753, acc: 0.726562] [adversarial loss: 1.097349, acc: 0.265625]\n",
      "7289: [discriminator loss: 0.504287, acc: 0.750000] [adversarial loss: 1.187984, acc: 0.218750]\n",
      "7290: [discriminator loss: 0.601699, acc: 0.664062] [adversarial loss: 0.863101, acc: 0.406250]\n",
      "7291: [discriminator loss: 0.541441, acc: 0.664062] [adversarial loss: 1.565353, acc: 0.125000]\n",
      "7292: [discriminator loss: 0.555598, acc: 0.679688] [adversarial loss: 1.056611, acc: 0.250000]\n",
      "7293: [discriminator loss: 0.517718, acc: 0.781250] [adversarial loss: 0.932462, acc: 0.375000]\n",
      "7294: [discriminator loss: 0.551253, acc: 0.734375] [adversarial loss: 1.417697, acc: 0.078125]\n",
      "7295: [discriminator loss: 0.645911, acc: 0.632812] [adversarial loss: 0.765629, acc: 0.468750]\n",
      "7296: [discriminator loss: 0.594947, acc: 0.703125] [adversarial loss: 1.542992, acc: 0.109375]\n",
      "7297: [discriminator loss: 0.598726, acc: 0.687500] [adversarial loss: 0.848861, acc: 0.484375]\n",
      "7298: [discriminator loss: 0.575264, acc: 0.718750] [adversarial loss: 1.522021, acc: 0.078125]\n",
      "7299: [discriminator loss: 0.530802, acc: 0.734375] [adversarial loss: 0.956627, acc: 0.406250]\n",
      "7300: [discriminator loss: 0.528798, acc: 0.671875] [adversarial loss: 1.250056, acc: 0.203125]\n",
      "7301: [discriminator loss: 0.560368, acc: 0.671875] [adversarial loss: 0.955234, acc: 0.406250]\n",
      "7302: [discriminator loss: 0.662501, acc: 0.617188] [adversarial loss: 1.418882, acc: 0.093750]\n",
      "7303: [discriminator loss: 0.548561, acc: 0.687500] [adversarial loss: 1.019762, acc: 0.265625]\n",
      "7304: [discriminator loss: 0.529808, acc: 0.710938] [adversarial loss: 1.385564, acc: 0.125000]\n",
      "7305: [discriminator loss: 0.484431, acc: 0.796875] [adversarial loss: 1.021000, acc: 0.328125]\n",
      "7306: [discriminator loss: 0.539592, acc: 0.726562] [adversarial loss: 1.354688, acc: 0.203125]\n",
      "7307: [discriminator loss: 0.532945, acc: 0.710938] [adversarial loss: 0.926732, acc: 0.390625]\n",
      "7308: [discriminator loss: 0.589444, acc: 0.703125] [adversarial loss: 1.192758, acc: 0.265625]\n",
      "7309: [discriminator loss: 0.504390, acc: 0.781250] [adversarial loss: 1.377168, acc: 0.125000]\n",
      "7310: [discriminator loss: 0.554395, acc: 0.750000] [adversarial loss: 1.368956, acc: 0.156250]\n",
      "7311: [discriminator loss: 0.510909, acc: 0.750000] [adversarial loss: 1.121706, acc: 0.156250]\n",
      "7312: [discriminator loss: 0.617984, acc: 0.734375] [adversarial loss: 1.182638, acc: 0.250000]\n",
      "7313: [discriminator loss: 0.607776, acc: 0.625000] [adversarial loss: 1.252729, acc: 0.281250]\n",
      "7314: [discriminator loss: 0.603390, acc: 0.648438] [adversarial loss: 1.555673, acc: 0.046875]\n",
      "7315: [discriminator loss: 0.565063, acc: 0.734375] [adversarial loss: 1.076633, acc: 0.359375]\n",
      "7316: [discriminator loss: 0.529222, acc: 0.710938] [adversarial loss: 1.160385, acc: 0.187500]\n",
      "7317: [discriminator loss: 0.532554, acc: 0.757812] [adversarial loss: 1.484628, acc: 0.187500]\n",
      "7318: [discriminator loss: 0.538268, acc: 0.710938] [adversarial loss: 0.886751, acc: 0.406250]\n",
      "7319: [discriminator loss: 0.571028, acc: 0.687500] [adversarial loss: 1.291642, acc: 0.125000]\n",
      "7320: [discriminator loss: 0.518729, acc: 0.703125] [adversarial loss: 0.872092, acc: 0.312500]\n",
      "7321: [discriminator loss: 0.484747, acc: 0.734375] [adversarial loss: 1.433752, acc: 0.156250]\n",
      "7322: [discriminator loss: 0.573836, acc: 0.703125] [adversarial loss: 1.076038, acc: 0.343750]\n",
      "7323: [discriminator loss: 0.533932, acc: 0.757812] [adversarial loss: 1.428742, acc: 0.156250]\n",
      "7324: [discriminator loss: 0.498993, acc: 0.765625] [adversarial loss: 0.875684, acc: 0.484375]\n",
      "7325: [discriminator loss: 0.599162, acc: 0.679688] [adversarial loss: 1.272911, acc: 0.109375]\n",
      "7326: [discriminator loss: 0.554928, acc: 0.687500] [adversarial loss: 1.146815, acc: 0.171875]\n",
      "7327: [discriminator loss: 0.526491, acc: 0.757812] [adversarial loss: 0.966268, acc: 0.328125]\n",
      "7328: [discriminator loss: 0.536018, acc: 0.718750] [adversarial loss: 1.080423, acc: 0.281250]\n",
      "7329: [discriminator loss: 0.520981, acc: 0.742188] [adversarial loss: 1.161919, acc: 0.265625]\n",
      "7330: [discriminator loss: 0.508528, acc: 0.726562] [adversarial loss: 1.203164, acc: 0.218750]\n",
      "7331: [discriminator loss: 0.544933, acc: 0.726562] [adversarial loss: 0.997850, acc: 0.359375]\n",
      "7332: [discriminator loss: 0.536096, acc: 0.710938] [adversarial loss: 1.460612, acc: 0.140625]\n",
      "7333: [discriminator loss: 0.465243, acc: 0.734375] [adversarial loss: 1.190121, acc: 0.250000]\n",
      "7334: [discriminator loss: 0.517899, acc: 0.734375] [adversarial loss: 1.464372, acc: 0.171875]\n",
      "7335: [discriminator loss: 0.521408, acc: 0.695312] [adversarial loss: 0.886172, acc: 0.437500]\n",
      "7336: [discriminator loss: 0.605513, acc: 0.671875] [adversarial loss: 1.540027, acc: 0.156250]\n",
      "7337: [discriminator loss: 0.533721, acc: 0.718750] [adversarial loss: 1.045961, acc: 0.406250]\n",
      "7338: [discriminator loss: 0.556607, acc: 0.671875] [adversarial loss: 1.518353, acc: 0.078125]\n",
      "7339: [discriminator loss: 0.523165, acc: 0.773438] [adversarial loss: 1.073285, acc: 0.328125]\n",
      "7340: [discriminator loss: 0.538781, acc: 0.742188] [adversarial loss: 1.269989, acc: 0.171875]\n",
      "7341: [discriminator loss: 0.525558, acc: 0.742188] [adversarial loss: 0.837125, acc: 0.453125]\n",
      "7342: [discriminator loss: 0.566744, acc: 0.687500] [adversarial loss: 1.204701, acc: 0.218750]\n",
      "7343: [discriminator loss: 0.550288, acc: 0.687500] [adversarial loss: 0.912326, acc: 0.421875]\n",
      "7344: [discriminator loss: 0.569847, acc: 0.664062] [adversarial loss: 1.449644, acc: 0.109375]\n",
      "7345: [discriminator loss: 0.585948, acc: 0.648438] [adversarial loss: 1.135828, acc: 0.265625]\n",
      "7346: [discriminator loss: 0.549354, acc: 0.703125] [adversarial loss: 1.278985, acc: 0.125000]\n",
      "7347: [discriminator loss: 0.594998, acc: 0.648438] [adversarial loss: 0.862487, acc: 0.468750]\n",
      "7348: [discriminator loss: 0.569713, acc: 0.687500] [adversarial loss: 1.504326, acc: 0.156250]\n",
      "7349: [discriminator loss: 0.623205, acc: 0.632812] [adversarial loss: 0.828174, acc: 0.453125]\n",
      "7350: [discriminator loss: 0.590905, acc: 0.703125] [adversarial loss: 1.469945, acc: 0.140625]\n",
      "7351: [discriminator loss: 0.601046, acc: 0.718750] [adversarial loss: 0.902560, acc: 0.359375]\n",
      "7352: [discriminator loss: 0.549067, acc: 0.726562] [adversarial loss: 1.223751, acc: 0.187500]\n",
      "7353: [discriminator loss: 0.568092, acc: 0.695312] [adversarial loss: 1.019463, acc: 0.312500]\n",
      "7354: [discriminator loss: 0.563402, acc: 0.734375] [adversarial loss: 1.177940, acc: 0.156250]\n",
      "7355: [discriminator loss: 0.515465, acc: 0.781250] [adversarial loss: 1.008979, acc: 0.343750]\n",
      "7356: [discriminator loss: 0.513806, acc: 0.750000] [adversarial loss: 1.251084, acc: 0.171875]\n",
      "7357: [discriminator loss: 0.557056, acc: 0.726562] [adversarial loss: 1.331043, acc: 0.171875]\n",
      "7358: [discriminator loss: 0.546902, acc: 0.734375] [adversarial loss: 0.875027, acc: 0.453125]\n",
      "7359: [discriminator loss: 0.485273, acc: 0.742188] [adversarial loss: 1.403191, acc: 0.109375]\n",
      "7360: [discriminator loss: 0.446401, acc: 0.820312] [adversarial loss: 1.036175, acc: 0.359375]\n",
      "7361: [discriminator loss: 0.595888, acc: 0.656250] [adversarial loss: 1.494615, acc: 0.140625]\n",
      "7362: [discriminator loss: 0.550857, acc: 0.726562] [adversarial loss: 0.934189, acc: 0.390625]\n",
      "7363: [discriminator loss: 0.591137, acc: 0.710938] [adversarial loss: 1.360141, acc: 0.125000]\n",
      "7364: [discriminator loss: 0.524161, acc: 0.734375] [adversarial loss: 0.865401, acc: 0.375000]\n",
      "7365: [discriminator loss: 0.459093, acc: 0.789062] [adversarial loss: 1.334228, acc: 0.250000]\n",
      "7366: [discriminator loss: 0.537297, acc: 0.726562] [adversarial loss: 0.842812, acc: 0.406250]\n",
      "7367: [discriminator loss: 0.520113, acc: 0.734375] [adversarial loss: 1.349696, acc: 0.156250]\n",
      "7368: [discriminator loss: 0.540325, acc: 0.710938] [adversarial loss: 0.938476, acc: 0.421875]\n",
      "7369: [discriminator loss: 0.500591, acc: 0.757812] [adversarial loss: 1.577497, acc: 0.078125]\n",
      "7370: [discriminator loss: 0.544195, acc: 0.679688] [adversarial loss: 0.901249, acc: 0.406250]\n",
      "7371: [discriminator loss: 0.517502, acc: 0.703125] [adversarial loss: 1.450729, acc: 0.125000]\n",
      "7372: [discriminator loss: 0.507313, acc: 0.742188] [adversarial loss: 0.972460, acc: 0.328125]\n",
      "7373: [discriminator loss: 0.569061, acc: 0.765625] [adversarial loss: 1.484117, acc: 0.125000]\n",
      "7374: [discriminator loss: 0.534849, acc: 0.671875] [adversarial loss: 1.236179, acc: 0.171875]\n",
      "7375: [discriminator loss: 0.522827, acc: 0.695312] [adversarial loss: 1.359013, acc: 0.187500]\n",
      "7376: [discriminator loss: 0.527671, acc: 0.757812] [adversarial loss: 1.080223, acc: 0.187500]\n",
      "7377: [discriminator loss: 0.449371, acc: 0.804688] [adversarial loss: 1.257384, acc: 0.234375]\n",
      "7378: [discriminator loss: 0.545716, acc: 0.671875] [adversarial loss: 1.065583, acc: 0.312500]\n",
      "7379: [discriminator loss: 0.500861, acc: 0.757812] [adversarial loss: 1.261122, acc: 0.218750]\n",
      "7380: [discriminator loss: 0.564448, acc: 0.703125] [adversarial loss: 0.750054, acc: 0.531250]\n",
      "7381: [discriminator loss: 0.638211, acc: 0.679688] [adversarial loss: 1.491394, acc: 0.187500]\n",
      "7382: [discriminator loss: 0.522073, acc: 0.710938] [adversarial loss: 1.153907, acc: 0.296875]\n",
      "7383: [discriminator loss: 0.556658, acc: 0.695312] [adversarial loss: 1.455042, acc: 0.125000]\n",
      "7384: [discriminator loss: 0.590685, acc: 0.656250] [adversarial loss: 1.014123, acc: 0.343750]\n",
      "7385: [discriminator loss: 0.572996, acc: 0.742188] [adversarial loss: 1.254586, acc: 0.171875]\n",
      "7386: [discriminator loss: 0.548713, acc: 0.734375] [adversarial loss: 1.110573, acc: 0.187500]\n",
      "7387: [discriminator loss: 0.514179, acc: 0.750000] [adversarial loss: 1.173291, acc: 0.234375]\n",
      "7388: [discriminator loss: 0.502475, acc: 0.734375] [adversarial loss: 1.162366, acc: 0.281250]\n",
      "7389: [discriminator loss: 0.587078, acc: 0.679688] [adversarial loss: 1.260248, acc: 0.109375]\n",
      "7390: [discriminator loss: 0.549845, acc: 0.718750] [adversarial loss: 1.417242, acc: 0.093750]\n",
      "7391: [discriminator loss: 0.550541, acc: 0.703125] [adversarial loss: 0.963336, acc: 0.359375]\n",
      "7392: [discriminator loss: 0.527792, acc: 0.703125] [adversarial loss: 1.751100, acc: 0.093750]\n",
      "7393: [discriminator loss: 0.538625, acc: 0.742188] [adversarial loss: 0.807749, acc: 0.437500]\n",
      "7394: [discriminator loss: 0.583135, acc: 0.679688] [adversarial loss: 1.384249, acc: 0.140625]\n",
      "7395: [discriminator loss: 0.567807, acc: 0.648438] [adversarial loss: 1.003352, acc: 0.296875]\n",
      "7396: [discriminator loss: 0.485119, acc: 0.781250] [adversarial loss: 1.528942, acc: 0.171875]\n",
      "7397: [discriminator loss: 0.573563, acc: 0.679688] [adversarial loss: 1.044889, acc: 0.328125]\n",
      "7398: [discriminator loss: 0.561362, acc: 0.671875] [adversarial loss: 1.440486, acc: 0.250000]\n",
      "7399: [discriminator loss: 0.550527, acc: 0.695312] [adversarial loss: 0.837462, acc: 0.468750]\n",
      "7400: [discriminator loss: 0.456693, acc: 0.789062] [adversarial loss: 1.501278, acc: 0.109375]\n",
      "7401: [discriminator loss: 0.653558, acc: 0.703125] [adversarial loss: 0.786219, acc: 0.500000]\n",
      "7402: [discriminator loss: 0.597385, acc: 0.695312] [adversarial loss: 1.118835, acc: 0.281250]\n",
      "7403: [discriminator loss: 0.533404, acc: 0.726562] [adversarial loss: 1.251926, acc: 0.234375]\n",
      "7404: [discriminator loss: 0.505383, acc: 0.742188] [adversarial loss: 1.078715, acc: 0.281250]\n",
      "7405: [discriminator loss: 0.545858, acc: 0.718750] [adversarial loss: 1.397341, acc: 0.171875]\n",
      "7406: [discriminator loss: 0.588341, acc: 0.664062] [adversarial loss: 0.934849, acc: 0.421875]\n",
      "7407: [discriminator loss: 0.601080, acc: 0.687500] [adversarial loss: 1.486564, acc: 0.125000]\n",
      "7408: [discriminator loss: 0.547009, acc: 0.718750] [adversarial loss: 1.049689, acc: 0.265625]\n",
      "7409: [discriminator loss: 0.586663, acc: 0.703125] [adversarial loss: 1.485688, acc: 0.171875]\n",
      "7410: [discriminator loss: 0.542811, acc: 0.718750] [adversarial loss: 0.883522, acc: 0.328125]\n",
      "7411: [discriminator loss: 0.511551, acc: 0.734375] [adversarial loss: 1.569849, acc: 0.109375]\n",
      "7412: [discriminator loss: 0.552141, acc: 0.687500] [adversarial loss: 0.668280, acc: 0.531250]\n",
      "7413: [discriminator loss: 0.606514, acc: 0.687500] [adversarial loss: 1.382968, acc: 0.140625]\n",
      "7414: [discriminator loss: 0.527930, acc: 0.734375] [adversarial loss: 1.280760, acc: 0.140625]\n",
      "7415: [discriminator loss: 0.553836, acc: 0.710938] [adversarial loss: 1.213610, acc: 0.140625]\n",
      "7416: [discriminator loss: 0.464971, acc: 0.781250] [adversarial loss: 1.294974, acc: 0.234375]\n",
      "7417: [discriminator loss: 0.496298, acc: 0.789062] [adversarial loss: 1.420201, acc: 0.171875]\n",
      "7418: [discriminator loss: 0.602592, acc: 0.710938] [adversarial loss: 1.254110, acc: 0.203125]\n",
      "7419: [discriminator loss: 0.511927, acc: 0.757812] [adversarial loss: 1.203176, acc: 0.281250]\n",
      "7420: [discriminator loss: 0.599722, acc: 0.679688] [adversarial loss: 0.945137, acc: 0.328125]\n",
      "7421: [discriminator loss: 0.534835, acc: 0.781250] [adversarial loss: 1.456180, acc: 0.156250]\n",
      "7422: [discriminator loss: 0.479759, acc: 0.765625] [adversarial loss: 0.912280, acc: 0.359375]\n",
      "7423: [discriminator loss: 0.492319, acc: 0.765625] [adversarial loss: 1.447911, acc: 0.140625]\n",
      "7424: [discriminator loss: 0.636561, acc: 0.625000] [adversarial loss: 0.853418, acc: 0.484375]\n",
      "7425: [discriminator loss: 0.558907, acc: 0.687500] [adversarial loss: 1.429582, acc: 0.140625]\n",
      "7426: [discriminator loss: 0.553523, acc: 0.710938] [adversarial loss: 0.994795, acc: 0.281250]\n",
      "7427: [discriminator loss: 0.494858, acc: 0.718750] [adversarial loss: 1.235281, acc: 0.218750]\n",
      "7428: [discriminator loss: 0.462807, acc: 0.781250] [adversarial loss: 1.159333, acc: 0.234375]\n",
      "7429: [discriminator loss: 0.518484, acc: 0.726562] [adversarial loss: 1.151135, acc: 0.265625]\n",
      "7430: [discriminator loss: 0.542315, acc: 0.710938] [adversarial loss: 0.935831, acc: 0.343750]\n",
      "7431: [discriminator loss: 0.524466, acc: 0.695312] [adversarial loss: 1.360624, acc: 0.140625]\n",
      "7432: [discriminator loss: 0.500510, acc: 0.765625] [adversarial loss: 1.007613, acc: 0.343750]\n",
      "7433: [discriminator loss: 0.618230, acc: 0.625000] [adversarial loss: 1.254639, acc: 0.281250]\n",
      "7434: [discriminator loss: 0.527796, acc: 0.726562] [adversarial loss: 1.184238, acc: 0.265625]\n",
      "7435: [discriminator loss: 0.502938, acc: 0.773438] [adversarial loss: 1.231056, acc: 0.218750]\n",
      "7436: [discriminator loss: 0.546805, acc: 0.742188] [adversarial loss: 1.258684, acc: 0.203125]\n",
      "7437: [discriminator loss: 0.599851, acc: 0.648438] [adversarial loss: 1.114220, acc: 0.296875]\n",
      "7438: [discriminator loss: 0.508887, acc: 0.750000] [adversarial loss: 1.158846, acc: 0.218750]\n",
      "7439: [discriminator loss: 0.562080, acc: 0.726562] [adversarial loss: 1.446589, acc: 0.156250]\n",
      "7440: [discriminator loss: 0.546511, acc: 0.703125] [adversarial loss: 0.915120, acc: 0.359375]\n",
      "7441: [discriminator loss: 0.583677, acc: 0.695312] [adversarial loss: 1.067520, acc: 0.218750]\n",
      "7442: [discriminator loss: 0.571225, acc: 0.703125] [adversarial loss: 1.041526, acc: 0.281250]\n",
      "7443: [discriminator loss: 0.624173, acc: 0.656250] [adversarial loss: 1.351473, acc: 0.203125]\n",
      "7444: [discriminator loss: 0.474460, acc: 0.742188] [adversarial loss: 0.996464, acc: 0.312500]\n",
      "7445: [discriminator loss: 0.518114, acc: 0.726562] [adversarial loss: 1.604383, acc: 0.125000]\n",
      "7446: [discriminator loss: 0.557257, acc: 0.687500] [adversarial loss: 0.879709, acc: 0.421875]\n",
      "7447: [discriminator loss: 0.559371, acc: 0.687500] [adversarial loss: 1.275160, acc: 0.250000]\n",
      "7448: [discriminator loss: 0.502923, acc: 0.703125] [adversarial loss: 1.204702, acc: 0.234375]\n",
      "7449: [discriminator loss: 0.510027, acc: 0.750000] [adversarial loss: 0.998516, acc: 0.328125]\n",
      "7450: [discriminator loss: 0.550987, acc: 0.734375] [adversarial loss: 1.380631, acc: 0.156250]\n",
      "7451: [discriminator loss: 0.542640, acc: 0.734375] [adversarial loss: 0.903057, acc: 0.421875]\n",
      "7452: [discriminator loss: 0.558882, acc: 0.718750] [adversarial loss: 1.592352, acc: 0.109375]\n",
      "7453: [discriminator loss: 0.629442, acc: 0.640625] [adversarial loss: 0.882578, acc: 0.375000]\n",
      "7454: [discriminator loss: 0.580046, acc: 0.679688] [adversarial loss: 1.625164, acc: 0.078125]\n",
      "7455: [discriminator loss: 0.572531, acc: 0.648438] [adversarial loss: 0.862856, acc: 0.437500]\n",
      "7456: [discriminator loss: 0.543998, acc: 0.742188] [adversarial loss: 1.503755, acc: 0.109375]\n",
      "7457: [discriminator loss: 0.514017, acc: 0.773438] [adversarial loss: 1.083777, acc: 0.234375]\n",
      "7458: [discriminator loss: 0.566061, acc: 0.742188] [adversarial loss: 0.955030, acc: 0.296875]\n",
      "7459: [discriminator loss: 0.540723, acc: 0.687500] [adversarial loss: 1.165329, acc: 0.203125]\n",
      "7460: [discriminator loss: 0.599262, acc: 0.664062] [adversarial loss: 1.146300, acc: 0.281250]\n",
      "7461: [discriminator loss: 0.565140, acc: 0.679688] [adversarial loss: 1.332485, acc: 0.156250]\n",
      "7462: [discriminator loss: 0.515872, acc: 0.718750] [adversarial loss: 1.306546, acc: 0.203125]\n",
      "7463: [discriminator loss: 0.560563, acc: 0.679688] [adversarial loss: 1.139458, acc: 0.234375]\n",
      "7464: [discriminator loss: 0.506336, acc: 0.765625] [adversarial loss: 1.075008, acc: 0.296875]\n",
      "7465: [discriminator loss: 0.559256, acc: 0.695312] [adversarial loss: 1.304824, acc: 0.125000]\n",
      "7466: [discriminator loss: 0.522704, acc: 0.750000] [adversarial loss: 0.985692, acc: 0.312500]\n",
      "7467: [discriminator loss: 0.564081, acc: 0.679688] [adversarial loss: 1.529855, acc: 0.140625]\n",
      "7468: [discriminator loss: 0.499828, acc: 0.695312] [adversarial loss: 0.943290, acc: 0.343750]\n",
      "7469: [discriminator loss: 0.630061, acc: 0.625000] [adversarial loss: 1.262850, acc: 0.218750]\n",
      "7470: [discriminator loss: 0.564830, acc: 0.726562] [adversarial loss: 1.092168, acc: 0.265625]\n",
      "7471: [discriminator loss: 0.536086, acc: 0.742188] [adversarial loss: 1.541053, acc: 0.078125]\n",
      "7472: [discriminator loss: 0.567866, acc: 0.664062] [adversarial loss: 0.906638, acc: 0.375000]\n",
      "7473: [discriminator loss: 0.592999, acc: 0.656250] [adversarial loss: 1.633330, acc: 0.093750]\n",
      "7474: [discriminator loss: 0.558475, acc: 0.703125] [adversarial loss: 1.019886, acc: 0.390625]\n",
      "7475: [discriminator loss: 0.539299, acc: 0.718750] [adversarial loss: 1.470299, acc: 0.218750]\n",
      "7476: [discriminator loss: 0.570769, acc: 0.687500] [adversarial loss: 0.975888, acc: 0.328125]\n",
      "7477: [discriminator loss: 0.563164, acc: 0.687500] [adversarial loss: 1.083649, acc: 0.281250]\n",
      "7478: [discriminator loss: 0.590734, acc: 0.632812] [adversarial loss: 0.958071, acc: 0.390625]\n",
      "7479: [discriminator loss: 0.578483, acc: 0.671875] [adversarial loss: 1.392553, acc: 0.171875]\n",
      "7480: [discriminator loss: 0.568933, acc: 0.648438] [adversarial loss: 0.779032, acc: 0.515625]\n",
      "7481: [discriminator loss: 0.648820, acc: 0.632812] [adversarial loss: 1.236037, acc: 0.187500]\n",
      "7482: [discriminator loss: 0.616162, acc: 0.664062] [adversarial loss: 0.965790, acc: 0.359375]\n",
      "7483: [discriminator loss: 0.507279, acc: 0.773438] [adversarial loss: 1.211368, acc: 0.296875]\n",
      "7484: [discriminator loss: 0.496077, acc: 0.789062] [adversarial loss: 1.003513, acc: 0.328125]\n",
      "7485: [discriminator loss: 0.497596, acc: 0.812500] [adversarial loss: 1.309147, acc: 0.140625]\n",
      "7486: [discriminator loss: 0.607970, acc: 0.703125] [adversarial loss: 1.158323, acc: 0.281250]\n",
      "7487: [discriminator loss: 0.492962, acc: 0.742188] [adversarial loss: 1.119450, acc: 0.265625]\n",
      "7488: [discriminator loss: 0.490926, acc: 0.781250] [adversarial loss: 0.996276, acc: 0.296875]\n",
      "7489: [discriminator loss: 0.475714, acc: 0.843750] [adversarial loss: 1.264826, acc: 0.187500]\n",
      "7490: [discriminator loss: 0.484632, acc: 0.781250] [adversarial loss: 1.032536, acc: 0.359375]\n",
      "7491: [discriminator loss: 0.548328, acc: 0.710938] [adversarial loss: 1.306392, acc: 0.125000]\n",
      "7492: [discriminator loss: 0.570776, acc: 0.750000] [adversarial loss: 0.844416, acc: 0.500000]\n",
      "7493: [discriminator loss: 0.556144, acc: 0.742188] [adversarial loss: 1.383927, acc: 0.125000]\n",
      "7494: [discriminator loss: 0.589828, acc: 0.679688] [adversarial loss: 0.860506, acc: 0.468750]\n",
      "7495: [discriminator loss: 0.615180, acc: 0.687500] [adversarial loss: 1.489928, acc: 0.062500]\n",
      "7496: [discriminator loss: 0.528596, acc: 0.726562] [adversarial loss: 0.704035, acc: 0.531250]\n",
      "7497: [discriminator loss: 0.578963, acc: 0.695312] [adversarial loss: 1.377394, acc: 0.203125]\n",
      "7498: [discriminator loss: 0.563851, acc: 0.734375] [adversarial loss: 1.270690, acc: 0.187500]\n",
      "7499: [discriminator loss: 0.566022, acc: 0.726562] [adversarial loss: 1.029479, acc: 0.328125]\n",
      "7500: [discriminator loss: 0.505669, acc: 0.750000] [adversarial loss: 1.360501, acc: 0.125000]\n",
      "7501: [discriminator loss: 0.608669, acc: 0.679688] [adversarial loss: 1.018819, acc: 0.296875]\n",
      "7502: [discriminator loss: 0.493374, acc: 0.757812] [adversarial loss: 1.397671, acc: 0.125000]\n",
      "7503: [discriminator loss: 0.530688, acc: 0.710938] [adversarial loss: 0.937941, acc: 0.390625]\n",
      "7504: [discriminator loss: 0.546205, acc: 0.734375] [adversarial loss: 1.395073, acc: 0.125000]\n",
      "7505: [discriminator loss: 0.581603, acc: 0.695312] [adversarial loss: 1.181420, acc: 0.234375]\n",
      "7506: [discriminator loss: 0.553299, acc: 0.718750] [adversarial loss: 1.366065, acc: 0.171875]\n",
      "7507: [discriminator loss: 0.573178, acc: 0.734375] [adversarial loss: 1.170902, acc: 0.203125]\n",
      "7508: [discriminator loss: 0.569899, acc: 0.687500] [adversarial loss: 1.084372, acc: 0.296875]\n",
      "7509: [discriminator loss: 0.473724, acc: 0.773438] [adversarial loss: 1.158760, acc: 0.218750]\n",
      "7510: [discriminator loss: 0.514718, acc: 0.812500] [adversarial loss: 1.486090, acc: 0.203125]\n",
      "7511: [discriminator loss: 0.639014, acc: 0.625000] [adversarial loss: 0.932812, acc: 0.296875]\n",
      "7512: [discriminator loss: 0.528498, acc: 0.734375] [adversarial loss: 1.755593, acc: 0.109375]\n",
      "7513: [discriminator loss: 0.529172, acc: 0.710938] [adversarial loss: 1.169896, acc: 0.281250]\n",
      "7514: [discriminator loss: 0.559562, acc: 0.734375] [adversarial loss: 1.283367, acc: 0.109375]\n",
      "7515: [discriminator loss: 0.498869, acc: 0.742188] [adversarial loss: 1.216800, acc: 0.218750]\n",
      "7516: [discriminator loss: 0.610296, acc: 0.687500] [adversarial loss: 1.333923, acc: 0.203125]\n",
      "7517: [discriminator loss: 0.555807, acc: 0.671875] [adversarial loss: 1.190696, acc: 0.250000]\n",
      "7518: [discriminator loss: 0.563653, acc: 0.671875] [adversarial loss: 0.858069, acc: 0.500000]\n",
      "7519: [discriminator loss: 0.587993, acc: 0.656250] [adversarial loss: 1.229506, acc: 0.203125]\n",
      "7520: [discriminator loss: 0.552927, acc: 0.710938] [adversarial loss: 0.807243, acc: 0.578125]\n",
      "7521: [discriminator loss: 0.624633, acc: 0.664062] [adversarial loss: 1.598069, acc: 0.093750]\n",
      "7522: [discriminator loss: 0.628428, acc: 0.554688] [adversarial loss: 1.001547, acc: 0.406250]\n",
      "7523: [discriminator loss: 0.545578, acc: 0.765625] [adversarial loss: 1.418716, acc: 0.171875]\n",
      "7524: [discriminator loss: 0.604065, acc: 0.656250] [adversarial loss: 0.866604, acc: 0.437500]\n",
      "7525: [discriminator loss: 0.600623, acc: 0.671875] [adversarial loss: 1.305186, acc: 0.171875]\n",
      "7526: [discriminator loss: 0.536167, acc: 0.718750] [adversarial loss: 1.064166, acc: 0.250000]\n",
      "7527: [discriminator loss: 0.590704, acc: 0.695312] [adversarial loss: 1.025497, acc: 0.359375]\n",
      "7528: [discriminator loss: 0.592606, acc: 0.687500] [adversarial loss: 1.021625, acc: 0.343750]\n",
      "7529: [discriminator loss: 0.543383, acc: 0.734375] [adversarial loss: 1.580568, acc: 0.078125]\n",
      "7530: [discriminator loss: 0.493707, acc: 0.789062] [adversarial loss: 1.059642, acc: 0.218750]\n",
      "7531: [discriminator loss: 0.530361, acc: 0.710938] [adversarial loss: 1.108383, acc: 0.265625]\n",
      "7532: [discriminator loss: 0.540934, acc: 0.703125] [adversarial loss: 1.039912, acc: 0.265625]\n",
      "7533: [discriminator loss: 0.539663, acc: 0.726562] [adversarial loss: 1.483686, acc: 0.156250]\n",
      "7534: [discriminator loss: 0.570064, acc: 0.703125] [adversarial loss: 1.044320, acc: 0.281250]\n",
      "7535: [discriminator loss: 0.507947, acc: 0.742188] [adversarial loss: 1.174780, acc: 0.265625]\n",
      "7536: [discriminator loss: 0.618916, acc: 0.625000] [adversarial loss: 1.070037, acc: 0.250000]\n",
      "7537: [discriminator loss: 0.575074, acc: 0.671875] [adversarial loss: 1.178692, acc: 0.156250]\n",
      "7538: [discriminator loss: 0.555925, acc: 0.695312] [adversarial loss: 1.285829, acc: 0.203125]\n",
      "7539: [discriminator loss: 0.529638, acc: 0.718750] [adversarial loss: 1.173411, acc: 0.250000]\n",
      "7540: [discriminator loss: 0.587028, acc: 0.710938] [adversarial loss: 0.994716, acc: 0.265625]\n",
      "7541: [discriminator loss: 0.554689, acc: 0.703125] [adversarial loss: 1.167498, acc: 0.281250]\n",
      "7542: [discriminator loss: 0.564777, acc: 0.664062] [adversarial loss: 1.304310, acc: 0.140625]\n",
      "7543: [discriminator loss: 0.563787, acc: 0.687500] [adversarial loss: 1.002909, acc: 0.359375]\n",
      "7544: [discriminator loss: 0.540947, acc: 0.734375] [adversarial loss: 1.491976, acc: 0.125000]\n",
      "7545: [discriminator loss: 0.525968, acc: 0.695312] [adversarial loss: 1.228718, acc: 0.203125]\n",
      "7546: [discriminator loss: 0.459515, acc: 0.796875] [adversarial loss: 1.604771, acc: 0.093750]\n",
      "7547: [discriminator loss: 0.632535, acc: 0.687500] [adversarial loss: 0.885061, acc: 0.406250]\n",
      "7548: [discriminator loss: 0.525974, acc: 0.765625] [adversarial loss: 1.591463, acc: 0.015625]\n",
      "7549: [discriminator loss: 0.486747, acc: 0.765625] [adversarial loss: 1.150053, acc: 0.296875]\n",
      "7550: [discriminator loss: 0.555280, acc: 0.703125] [adversarial loss: 1.169329, acc: 0.359375]\n",
      "7551: [discriminator loss: 0.515129, acc: 0.703125] [adversarial loss: 0.936818, acc: 0.343750]\n",
      "7552: [discriminator loss: 0.497491, acc: 0.796875] [adversarial loss: 1.421088, acc: 0.093750]\n",
      "7553: [discriminator loss: 0.553935, acc: 0.703125] [adversarial loss: 1.284507, acc: 0.203125]\n",
      "7554: [discriminator loss: 0.484161, acc: 0.781250] [adversarial loss: 1.097124, acc: 0.281250]\n",
      "7555: [discriminator loss: 0.487721, acc: 0.765625] [adversarial loss: 1.339397, acc: 0.140625]\n",
      "7556: [discriminator loss: 0.596745, acc: 0.718750] [adversarial loss: 0.795173, acc: 0.515625]\n",
      "7557: [discriminator loss: 0.554929, acc: 0.695312] [adversarial loss: 1.456041, acc: 0.125000]\n",
      "7558: [discriminator loss: 0.582528, acc: 0.695312] [adversarial loss: 0.959109, acc: 0.328125]\n",
      "7559: [discriminator loss: 0.544001, acc: 0.718750] [adversarial loss: 1.457505, acc: 0.093750]\n",
      "7560: [discriminator loss: 0.567491, acc: 0.679688] [adversarial loss: 0.966129, acc: 0.359375]\n",
      "7561: [discriminator loss: 0.546546, acc: 0.671875] [adversarial loss: 1.452389, acc: 0.156250]\n",
      "7562: [discriminator loss: 0.645979, acc: 0.585938] [adversarial loss: 0.927028, acc: 0.390625]\n",
      "7563: [discriminator loss: 0.570264, acc: 0.679688] [adversarial loss: 1.395403, acc: 0.093750]\n",
      "7564: [discriminator loss: 0.540963, acc: 0.703125] [adversarial loss: 1.030505, acc: 0.375000]\n",
      "7565: [discriminator loss: 0.558292, acc: 0.718750] [adversarial loss: 1.205224, acc: 0.171875]\n",
      "7566: [discriminator loss: 0.497675, acc: 0.757812] [adversarial loss: 1.153106, acc: 0.187500]\n",
      "7567: [discriminator loss: 0.489380, acc: 0.789062] [adversarial loss: 1.127248, acc: 0.171875]\n",
      "7568: [discriminator loss: 0.529570, acc: 0.742188] [adversarial loss: 1.363694, acc: 0.140625]\n",
      "7569: [discriminator loss: 0.475191, acc: 0.789062] [adversarial loss: 0.957484, acc: 0.328125]\n",
      "7570: [discriminator loss: 0.537604, acc: 0.695312] [adversarial loss: 1.000934, acc: 0.343750]\n",
      "7571: [discriminator loss: 0.481046, acc: 0.750000] [adversarial loss: 1.220489, acc: 0.218750]\n",
      "7572: [discriminator loss: 0.511355, acc: 0.734375] [adversarial loss: 1.169124, acc: 0.265625]\n",
      "7573: [discriminator loss: 0.496296, acc: 0.750000] [adversarial loss: 0.876692, acc: 0.343750]\n",
      "7574: [discriminator loss: 0.506912, acc: 0.765625] [adversarial loss: 1.337173, acc: 0.187500]\n",
      "7575: [discriminator loss: 0.569056, acc: 0.687500] [adversarial loss: 1.167918, acc: 0.171875]\n",
      "7576: [discriminator loss: 0.555842, acc: 0.718750] [adversarial loss: 1.200788, acc: 0.250000]\n",
      "7577: [discriminator loss: 0.577124, acc: 0.671875] [adversarial loss: 0.888662, acc: 0.375000]\n",
      "7578: [discriminator loss: 0.570548, acc: 0.742188] [adversarial loss: 1.589784, acc: 0.125000]\n",
      "7579: [discriminator loss: 0.580352, acc: 0.695312] [adversarial loss: 1.085071, acc: 0.328125]\n",
      "7580: [discriminator loss: 0.554311, acc: 0.718750] [adversarial loss: 1.586962, acc: 0.203125]\n",
      "7581: [discriminator loss: 0.596922, acc: 0.695312] [adversarial loss: 1.143604, acc: 0.265625]\n",
      "7582: [discriminator loss: 0.529196, acc: 0.742188] [adversarial loss: 1.207117, acc: 0.218750]\n",
      "7583: [discriminator loss: 0.508877, acc: 0.695312] [adversarial loss: 1.098255, acc: 0.281250]\n",
      "7584: [discriminator loss: 0.527379, acc: 0.703125] [adversarial loss: 1.153192, acc: 0.234375]\n",
      "7585: [discriminator loss: 0.520845, acc: 0.710938] [adversarial loss: 1.116736, acc: 0.250000]\n",
      "7586: [discriminator loss: 0.516141, acc: 0.710938] [adversarial loss: 1.067142, acc: 0.406250]\n",
      "7587: [discriminator loss: 0.550598, acc: 0.703125] [adversarial loss: 1.198744, acc: 0.312500]\n",
      "7588: [discriminator loss: 0.581495, acc: 0.664062] [adversarial loss: 1.204830, acc: 0.234375]\n",
      "7589: [discriminator loss: 0.494278, acc: 0.765625] [adversarial loss: 1.215920, acc: 0.187500]\n",
      "7590: [discriminator loss: 0.541888, acc: 0.710938] [adversarial loss: 1.278450, acc: 0.187500]\n",
      "7591: [discriminator loss: 0.582764, acc: 0.671875] [adversarial loss: 0.900360, acc: 0.406250]\n",
      "7592: [discriminator loss: 0.529444, acc: 0.703125] [adversarial loss: 1.500306, acc: 0.031250]\n",
      "7593: [discriminator loss: 0.558909, acc: 0.687500] [adversarial loss: 1.187739, acc: 0.296875]\n",
      "7594: [discriminator loss: 0.543896, acc: 0.679688] [adversarial loss: 1.429437, acc: 0.093750]\n",
      "7595: [discriminator loss: 0.558376, acc: 0.703125] [adversarial loss: 1.099610, acc: 0.234375]\n",
      "7596: [discriminator loss: 0.586478, acc: 0.671875] [adversarial loss: 1.071933, acc: 0.234375]\n",
      "7597: [discriminator loss: 0.526689, acc: 0.703125] [adversarial loss: 1.250492, acc: 0.171875]\n",
      "7598: [discriminator loss: 0.518231, acc: 0.773438] [adversarial loss: 0.930743, acc: 0.375000]\n",
      "7599: [discriminator loss: 0.532068, acc: 0.750000] [adversarial loss: 1.458980, acc: 0.156250]\n",
      "7600: [discriminator loss: 0.568359, acc: 0.726562] [adversarial loss: 1.357773, acc: 0.109375]\n",
      "7601: [discriminator loss: 0.528745, acc: 0.718750] [adversarial loss: 1.571858, acc: 0.078125]\n",
      "7602: [discriminator loss: 0.524202, acc: 0.757812] [adversarial loss: 0.964542, acc: 0.359375]\n",
      "7603: [discriminator loss: 0.520126, acc: 0.734375] [adversarial loss: 1.476756, acc: 0.125000]\n",
      "7604: [discriminator loss: 0.548035, acc: 0.695312] [adversarial loss: 0.730116, acc: 0.531250]\n",
      "7605: [discriminator loss: 0.531283, acc: 0.710938] [adversarial loss: 1.569845, acc: 0.125000]\n",
      "7606: [discriminator loss: 0.584569, acc: 0.679688] [adversarial loss: 0.786200, acc: 0.453125]\n",
      "7607: [discriminator loss: 0.578353, acc: 0.671875] [adversarial loss: 1.468231, acc: 0.156250]\n",
      "7608: [discriminator loss: 0.519428, acc: 0.726562] [adversarial loss: 1.269671, acc: 0.187500]\n",
      "7609: [discriminator loss: 0.608994, acc: 0.664062] [adversarial loss: 0.824173, acc: 0.484375]\n",
      "7610: [discriminator loss: 0.569314, acc: 0.656250] [adversarial loss: 1.413908, acc: 0.109375]\n",
      "7611: [discriminator loss: 0.537078, acc: 0.726562] [adversarial loss: 0.877684, acc: 0.406250]\n",
      "7612: [discriminator loss: 0.568867, acc: 0.695312] [adversarial loss: 1.480994, acc: 0.171875]\n",
      "7613: [discriminator loss: 0.541196, acc: 0.742188] [adversarial loss: 0.829233, acc: 0.437500]\n",
      "7614: [discriminator loss: 0.588650, acc: 0.664062] [adversarial loss: 1.412853, acc: 0.171875]\n",
      "7615: [discriminator loss: 0.597846, acc: 0.679688] [adversarial loss: 0.971097, acc: 0.312500]\n",
      "7616: [discriminator loss: 0.570911, acc: 0.710938] [adversarial loss: 1.239372, acc: 0.171875]\n",
      "7617: [discriminator loss: 0.587356, acc: 0.695312] [adversarial loss: 1.043186, acc: 0.265625]\n",
      "7618: [discriminator loss: 0.528306, acc: 0.718750] [adversarial loss: 1.089838, acc: 0.234375]\n",
      "7619: [discriminator loss: 0.579549, acc: 0.679688] [adversarial loss: 1.211471, acc: 0.140625]\n",
      "7620: [discriminator loss: 0.549008, acc: 0.695312] [adversarial loss: 1.102378, acc: 0.250000]\n",
      "7621: [discriminator loss: 0.506261, acc: 0.757812] [adversarial loss: 1.101398, acc: 0.281250]\n",
      "7622: [discriminator loss: 0.462551, acc: 0.804688] [adversarial loss: 1.048394, acc: 0.281250]\n",
      "7623: [discriminator loss: 0.608738, acc: 0.695312] [adversarial loss: 1.405321, acc: 0.171875]\n",
      "7624: [discriminator loss: 0.567902, acc: 0.750000] [adversarial loss: 0.832497, acc: 0.453125]\n",
      "7625: [discriminator loss: 0.565615, acc: 0.679688] [adversarial loss: 1.619076, acc: 0.109375]\n",
      "7626: [discriminator loss: 0.513482, acc: 0.757812] [adversarial loss: 0.916338, acc: 0.375000]\n",
      "7627: [discriminator loss: 0.644615, acc: 0.640625] [adversarial loss: 1.602213, acc: 0.031250]\n",
      "7628: [discriminator loss: 0.588720, acc: 0.671875] [adversarial loss: 1.132587, acc: 0.250000]\n",
      "7629: [discriminator loss: 0.512147, acc: 0.757812] [adversarial loss: 1.151634, acc: 0.203125]\n",
      "7630: [discriminator loss: 0.585968, acc: 0.671875] [adversarial loss: 1.219907, acc: 0.218750]\n",
      "7631: [discriminator loss: 0.539568, acc: 0.679688] [adversarial loss: 1.002965, acc: 0.390625]\n",
      "7632: [discriminator loss: 0.552070, acc: 0.726562] [adversarial loss: 1.344107, acc: 0.187500]\n",
      "7633: [discriminator loss: 0.567518, acc: 0.679688] [adversarial loss: 0.842205, acc: 0.468750]\n",
      "7634: [discriminator loss: 0.592445, acc: 0.671875] [adversarial loss: 1.541804, acc: 0.171875]\n",
      "7635: [discriminator loss: 0.583010, acc: 0.625000] [adversarial loss: 0.904195, acc: 0.328125]\n",
      "7636: [discriminator loss: 0.539533, acc: 0.742188] [adversarial loss: 1.433478, acc: 0.140625]\n",
      "7637: [discriminator loss: 0.571203, acc: 0.679688] [adversarial loss: 0.802520, acc: 0.546875]\n",
      "7638: [discriminator loss: 0.580081, acc: 0.671875] [adversarial loss: 1.275666, acc: 0.187500]\n",
      "7639: [discriminator loss: 0.533015, acc: 0.726562] [adversarial loss: 0.993449, acc: 0.296875]\n",
      "7640: [discriminator loss: 0.527124, acc: 0.679688] [adversarial loss: 1.264512, acc: 0.203125]\n",
      "7641: [discriminator loss: 0.587931, acc: 0.648438] [adversarial loss: 0.963980, acc: 0.312500]\n",
      "7642: [discriminator loss: 0.514849, acc: 0.734375] [adversarial loss: 1.495939, acc: 0.125000]\n",
      "7643: [discriminator loss: 0.527214, acc: 0.773438] [adversarial loss: 1.136679, acc: 0.203125]\n",
      "7644: [discriminator loss: 0.512300, acc: 0.718750] [adversarial loss: 1.096725, acc: 0.296875]\n",
      "7645: [discriminator loss: 0.508479, acc: 0.765625] [adversarial loss: 1.342003, acc: 0.187500]\n",
      "7646: [discriminator loss: 0.524608, acc: 0.718750] [adversarial loss: 1.123877, acc: 0.234375]\n",
      "7647: [discriminator loss: 0.469886, acc: 0.812500] [adversarial loss: 1.415074, acc: 0.187500]\n",
      "7648: [discriminator loss: 0.533835, acc: 0.671875] [adversarial loss: 1.235770, acc: 0.203125]\n",
      "7649: [discriminator loss: 0.498759, acc: 0.765625] [adversarial loss: 1.193580, acc: 0.093750]\n",
      "7650: [discriminator loss: 0.546162, acc: 0.664062] [adversarial loss: 1.050381, acc: 0.250000]\n",
      "7651: [discriminator loss: 0.517947, acc: 0.750000] [adversarial loss: 1.104110, acc: 0.296875]\n",
      "7652: [discriminator loss: 0.488642, acc: 0.734375] [adversarial loss: 1.313838, acc: 0.093750]\n",
      "7653: [discriminator loss: 0.556483, acc: 0.703125] [adversarial loss: 1.223084, acc: 0.218750]\n",
      "7654: [discriminator loss: 0.548991, acc: 0.734375] [adversarial loss: 1.057875, acc: 0.343750]\n",
      "7655: [discriminator loss: 0.549502, acc: 0.765625] [adversarial loss: 1.108014, acc: 0.281250]\n",
      "7656: [discriminator loss: 0.558241, acc: 0.710938] [adversarial loss: 1.199881, acc: 0.375000]\n",
      "7657: [discriminator loss: 0.573724, acc: 0.687500] [adversarial loss: 1.012421, acc: 0.296875]\n",
      "7658: [discriminator loss: 0.619380, acc: 0.687500] [adversarial loss: 1.073923, acc: 0.250000]\n",
      "7659: [discriminator loss: 0.525609, acc: 0.718750] [adversarial loss: 1.261699, acc: 0.218750]\n",
      "7660: [discriminator loss: 0.606920, acc: 0.695312] [adversarial loss: 1.585024, acc: 0.109375]\n",
      "7661: [discriminator loss: 0.570300, acc: 0.679688] [adversarial loss: 0.911008, acc: 0.359375]\n",
      "7662: [discriminator loss: 0.546350, acc: 0.710938] [adversarial loss: 1.395682, acc: 0.125000]\n",
      "7663: [discriminator loss: 0.587089, acc: 0.695312] [adversarial loss: 1.117661, acc: 0.265625]\n",
      "7664: [discriminator loss: 0.593834, acc: 0.679688] [adversarial loss: 1.306730, acc: 0.140625]\n",
      "7665: [discriminator loss: 0.565965, acc: 0.703125] [adversarial loss: 1.164104, acc: 0.281250]\n",
      "7666: [discriminator loss: 0.584196, acc: 0.671875] [adversarial loss: 1.237219, acc: 0.265625]\n",
      "7667: [discriminator loss: 0.590692, acc: 0.695312] [adversarial loss: 1.074413, acc: 0.328125]\n",
      "7668: [discriminator loss: 0.529845, acc: 0.765625] [adversarial loss: 1.276216, acc: 0.218750]\n",
      "7669: [discriminator loss: 0.560548, acc: 0.710938] [adversarial loss: 1.127058, acc: 0.296875]\n",
      "7670: [discriminator loss: 0.519242, acc: 0.750000] [adversarial loss: 1.333323, acc: 0.203125]\n",
      "7671: [discriminator loss: 0.561023, acc: 0.742188] [adversarial loss: 1.163967, acc: 0.281250]\n",
      "7672: [discriminator loss: 0.563831, acc: 0.742188] [adversarial loss: 1.119936, acc: 0.265625]\n",
      "7673: [discriminator loss: 0.596686, acc: 0.679688] [adversarial loss: 1.086753, acc: 0.312500]\n",
      "7674: [discriminator loss: 0.658205, acc: 0.687500] [adversarial loss: 1.039215, acc: 0.218750]\n",
      "7675: [discriminator loss: 0.564973, acc: 0.687500] [adversarial loss: 1.139118, acc: 0.250000]\n",
      "7676: [discriminator loss: 0.555709, acc: 0.687500] [adversarial loss: 0.858591, acc: 0.406250]\n",
      "7677: [discriminator loss: 0.630586, acc: 0.648438] [adversarial loss: 1.212499, acc: 0.203125]\n",
      "7678: [discriminator loss: 0.506592, acc: 0.765625] [adversarial loss: 0.963687, acc: 0.281250]\n",
      "7679: [discriminator loss: 0.485408, acc: 0.781250] [adversarial loss: 1.319535, acc: 0.140625]\n",
      "7680: [discriminator loss: 0.587573, acc: 0.703125] [adversarial loss: 0.876544, acc: 0.437500]\n",
      "7681: [discriminator loss: 0.584325, acc: 0.703125] [adversarial loss: 1.437354, acc: 0.187500]\n",
      "7682: [discriminator loss: 0.514929, acc: 0.726562] [adversarial loss: 1.080899, acc: 0.296875]\n",
      "7683: [discriminator loss: 0.588695, acc: 0.679688] [adversarial loss: 1.324336, acc: 0.187500]\n",
      "7684: [discriminator loss: 0.566720, acc: 0.687500] [adversarial loss: 0.925278, acc: 0.390625]\n",
      "7685: [discriminator loss: 0.472937, acc: 0.734375] [adversarial loss: 1.276839, acc: 0.203125]\n",
      "7686: [discriminator loss: 0.523279, acc: 0.734375] [adversarial loss: 0.977046, acc: 0.312500]\n",
      "7687: [discriminator loss: 0.534078, acc: 0.734375] [adversarial loss: 1.334552, acc: 0.218750]\n",
      "7688: [discriminator loss: 0.558277, acc: 0.687500] [adversarial loss: 1.160293, acc: 0.312500]\n",
      "7689: [discriminator loss: 0.643986, acc: 0.609375] [adversarial loss: 1.191688, acc: 0.250000]\n",
      "7690: [discriminator loss: 0.581679, acc: 0.679688] [adversarial loss: 0.724377, acc: 0.593750]\n",
      "7691: [discriminator loss: 0.534465, acc: 0.703125] [adversarial loss: 1.469048, acc: 0.140625]\n",
      "7692: [discriminator loss: 0.576990, acc: 0.648438] [adversarial loss: 1.116318, acc: 0.312500]\n",
      "7693: [discriminator loss: 0.494613, acc: 0.710938] [adversarial loss: 1.336078, acc: 0.281250]\n",
      "7694: [discriminator loss: 0.540336, acc: 0.710938] [adversarial loss: 0.949867, acc: 0.375000]\n",
      "7695: [discriminator loss: 0.565594, acc: 0.687500] [adversarial loss: 1.505590, acc: 0.140625]\n",
      "7696: [discriminator loss: 0.514453, acc: 0.734375] [adversarial loss: 0.810821, acc: 0.421875]\n",
      "7697: [discriminator loss: 0.513172, acc: 0.734375] [adversarial loss: 1.561285, acc: 0.046875]\n",
      "7698: [discriminator loss: 0.661919, acc: 0.632812] [adversarial loss: 0.811077, acc: 0.484375]\n",
      "7699: [discriminator loss: 0.598418, acc: 0.679688] [adversarial loss: 1.720961, acc: 0.062500]\n",
      "7700: [discriminator loss: 0.522407, acc: 0.734375] [adversarial loss: 1.161835, acc: 0.234375]\n",
      "7701: [discriminator loss: 0.592243, acc: 0.679688] [adversarial loss: 1.045685, acc: 0.312500]\n",
      "7702: [discriminator loss: 0.510769, acc: 0.773438] [adversarial loss: 1.351026, acc: 0.203125]\n",
      "7703: [discriminator loss: 0.538981, acc: 0.710938] [adversarial loss: 1.035873, acc: 0.281250]\n",
      "7704: [discriminator loss: 0.570806, acc: 0.687500] [adversarial loss: 1.133148, acc: 0.265625]\n",
      "7705: [discriminator loss: 0.552861, acc: 0.726562] [adversarial loss: 0.997488, acc: 0.265625]\n",
      "7706: [discriminator loss: 0.507116, acc: 0.789062] [adversarial loss: 1.389669, acc: 0.109375]\n",
      "7707: [discriminator loss: 0.623201, acc: 0.695312] [adversarial loss: 1.264812, acc: 0.156250]\n",
      "7708: [discriminator loss: 0.540949, acc: 0.687500] [adversarial loss: 1.277941, acc: 0.171875]\n",
      "7709: [discriminator loss: 0.544291, acc: 0.734375] [adversarial loss: 1.134129, acc: 0.203125]\n",
      "7710: [discriminator loss: 0.587879, acc: 0.640625] [adversarial loss: 1.095444, acc: 0.265625]\n",
      "7711: [discriminator loss: 0.518422, acc: 0.757812] [adversarial loss: 1.033106, acc: 0.296875]\n",
      "7712: [discriminator loss: 0.481187, acc: 0.750000] [adversarial loss: 1.562914, acc: 0.093750]\n",
      "7713: [discriminator loss: 0.596653, acc: 0.648438] [adversarial loss: 0.874557, acc: 0.421875]\n",
      "7714: [discriminator loss: 0.615710, acc: 0.679688] [adversarial loss: 1.486385, acc: 0.140625]\n",
      "7715: [discriminator loss: 0.632339, acc: 0.687500] [adversarial loss: 0.991219, acc: 0.218750]\n",
      "7716: [discriminator loss: 0.542040, acc: 0.750000] [adversarial loss: 1.222447, acc: 0.187500]\n",
      "7717: [discriminator loss: 0.515250, acc: 0.742188] [adversarial loss: 1.223178, acc: 0.250000]\n",
      "7718: [discriminator loss: 0.559756, acc: 0.695312] [adversarial loss: 1.176857, acc: 0.281250]\n",
      "7719: [discriminator loss: 0.549485, acc: 0.742188] [adversarial loss: 1.511754, acc: 0.203125]\n",
      "7720: [discriminator loss: 0.540449, acc: 0.734375] [adversarial loss: 1.185635, acc: 0.265625]\n",
      "7721: [discriminator loss: 0.511182, acc: 0.804688] [adversarial loss: 1.189153, acc: 0.187500]\n",
      "7722: [discriminator loss: 0.536483, acc: 0.742188] [adversarial loss: 1.331225, acc: 0.187500]\n",
      "7723: [discriminator loss: 0.572846, acc: 0.710938] [adversarial loss: 1.179229, acc: 0.203125]\n",
      "7724: [discriminator loss: 0.512636, acc: 0.750000] [adversarial loss: 1.016078, acc: 0.375000]\n",
      "7725: [discriminator loss: 0.570909, acc: 0.687500] [adversarial loss: 1.272393, acc: 0.234375]\n",
      "7726: [discriminator loss: 0.552932, acc: 0.695312] [adversarial loss: 0.897843, acc: 0.468750]\n",
      "7727: [discriminator loss: 0.590371, acc: 0.664062] [adversarial loss: 1.376760, acc: 0.078125]\n",
      "7728: [discriminator loss: 0.505043, acc: 0.773438] [adversarial loss: 1.020521, acc: 0.312500]\n",
      "7729: [discriminator loss: 0.550493, acc: 0.726562] [adversarial loss: 1.507651, acc: 0.125000]\n",
      "7730: [discriminator loss: 0.609548, acc: 0.687500] [adversarial loss: 0.918260, acc: 0.343750]\n",
      "7731: [discriminator loss: 0.537201, acc: 0.718750] [adversarial loss: 1.136709, acc: 0.265625]\n",
      "7732: [discriminator loss: 0.559230, acc: 0.718750] [adversarial loss: 0.860043, acc: 0.437500]\n",
      "7733: [discriminator loss: 0.553823, acc: 0.695312] [adversarial loss: 1.418147, acc: 0.203125]\n",
      "7734: [discriminator loss: 0.590558, acc: 0.687500] [adversarial loss: 1.333888, acc: 0.171875]\n",
      "7735: [discriminator loss: 0.522779, acc: 0.765625] [adversarial loss: 1.203096, acc: 0.171875]\n",
      "7736: [discriminator loss: 0.475449, acc: 0.781250] [adversarial loss: 1.107594, acc: 0.265625]\n",
      "7737: [discriminator loss: 0.488946, acc: 0.765625] [adversarial loss: 1.019940, acc: 0.375000]\n",
      "7738: [discriminator loss: 0.572265, acc: 0.656250] [adversarial loss: 1.265628, acc: 0.281250]\n",
      "7739: [discriminator loss: 0.538874, acc: 0.718750] [adversarial loss: 1.373651, acc: 0.140625]\n",
      "7740: [discriminator loss: 0.604983, acc: 0.664062] [adversarial loss: 0.982528, acc: 0.375000]\n",
      "7741: [discriminator loss: 0.515227, acc: 0.750000] [adversarial loss: 1.355402, acc: 0.187500]\n",
      "7742: [discriminator loss: 0.557987, acc: 0.718750] [adversarial loss: 0.989670, acc: 0.406250]\n",
      "7743: [discriminator loss: 0.579269, acc: 0.679688] [adversarial loss: 1.107035, acc: 0.250000]\n",
      "7744: [discriminator loss: 0.551459, acc: 0.710938] [adversarial loss: 1.217949, acc: 0.218750]\n",
      "7745: [discriminator loss: 0.476295, acc: 0.789062] [adversarial loss: 1.434653, acc: 0.171875]\n",
      "7746: [discriminator loss: 0.598030, acc: 0.664062] [adversarial loss: 1.075528, acc: 0.250000]\n",
      "7747: [discriminator loss: 0.547356, acc: 0.710938] [adversarial loss: 1.319950, acc: 0.203125]\n",
      "7748: [discriminator loss: 0.503388, acc: 0.750000] [adversarial loss: 0.928240, acc: 0.406250]\n",
      "7749: [discriminator loss: 0.560673, acc: 0.695312] [adversarial loss: 1.450082, acc: 0.093750]\n",
      "7750: [discriminator loss: 0.542679, acc: 0.718750] [adversarial loss: 0.928751, acc: 0.343750]\n",
      "7751: [discriminator loss: 0.512058, acc: 0.710938] [adversarial loss: 1.250820, acc: 0.218750]\n",
      "7752: [discriminator loss: 0.531564, acc: 0.726562] [adversarial loss: 1.022223, acc: 0.312500]\n",
      "7753: [discriminator loss: 0.520837, acc: 0.742188] [adversarial loss: 1.335947, acc: 0.156250]\n",
      "7754: [discriminator loss: 0.597232, acc: 0.664062] [adversarial loss: 1.078032, acc: 0.265625]\n",
      "7755: [discriminator loss: 0.494567, acc: 0.781250] [adversarial loss: 1.177547, acc: 0.281250]\n",
      "7756: [discriminator loss: 0.581726, acc: 0.734375] [adversarial loss: 1.237988, acc: 0.250000]\n",
      "7757: [discriminator loss: 0.491903, acc: 0.710938] [adversarial loss: 1.298778, acc: 0.203125]\n",
      "7758: [discriminator loss: 0.478653, acc: 0.765625] [adversarial loss: 0.758028, acc: 0.500000]\n",
      "7759: [discriminator loss: 0.567099, acc: 0.687500] [adversarial loss: 1.225530, acc: 0.203125]\n",
      "7760: [discriminator loss: 0.460924, acc: 0.773438] [adversarial loss: 1.199949, acc: 0.250000]\n",
      "7761: [discriminator loss: 0.562626, acc: 0.718750] [adversarial loss: 1.270077, acc: 0.125000]\n",
      "7762: [discriminator loss: 0.537993, acc: 0.671875] [adversarial loss: 1.241637, acc: 0.296875]\n",
      "7763: [discriminator loss: 0.516505, acc: 0.718750] [adversarial loss: 1.414855, acc: 0.171875]\n",
      "7764: [discriminator loss: 0.495782, acc: 0.750000] [adversarial loss: 1.068925, acc: 0.281250]\n",
      "7765: [discriminator loss: 0.576348, acc: 0.695312] [adversarial loss: 0.908167, acc: 0.406250]\n",
      "7766: [discriminator loss: 0.616266, acc: 0.679688] [adversarial loss: 1.736881, acc: 0.046875]\n",
      "7767: [discriminator loss: 0.577870, acc: 0.710938] [adversarial loss: 0.742541, acc: 0.500000]\n",
      "7768: [discriminator loss: 0.523940, acc: 0.750000] [adversarial loss: 1.380283, acc: 0.187500]\n",
      "7769: [discriminator loss: 0.564533, acc: 0.703125] [adversarial loss: 1.021341, acc: 0.312500]\n",
      "7770: [discriminator loss: 0.493832, acc: 0.750000] [adversarial loss: 1.550165, acc: 0.125000]\n",
      "7771: [discriminator loss: 0.590126, acc: 0.687500] [adversarial loss: 0.799761, acc: 0.500000]\n",
      "7772: [discriminator loss: 0.591125, acc: 0.648438] [adversarial loss: 1.319347, acc: 0.203125]\n",
      "7773: [discriminator loss: 0.507012, acc: 0.726562] [adversarial loss: 1.116648, acc: 0.265625]\n",
      "7774: [discriminator loss: 0.565571, acc: 0.687500] [adversarial loss: 1.280657, acc: 0.187500]\n",
      "7775: [discriminator loss: 0.574860, acc: 0.687500] [adversarial loss: 1.013880, acc: 0.343750]\n",
      "7776: [discriminator loss: 0.532396, acc: 0.695312] [adversarial loss: 1.299892, acc: 0.203125]\n",
      "7777: [discriminator loss: 0.506055, acc: 0.757812] [adversarial loss: 1.502995, acc: 0.156250]\n",
      "7778: [discriminator loss: 0.547820, acc: 0.703125] [adversarial loss: 1.071990, acc: 0.312500]\n",
      "7779: [discriminator loss: 0.515620, acc: 0.734375] [adversarial loss: 1.192556, acc: 0.234375]\n",
      "7780: [discriminator loss: 0.597546, acc: 0.656250] [adversarial loss: 1.041493, acc: 0.312500]\n",
      "7781: [discriminator loss: 0.507071, acc: 0.757812] [adversarial loss: 1.658756, acc: 0.093750]\n",
      "7782: [discriminator loss: 0.638881, acc: 0.656250] [adversarial loss: 1.023537, acc: 0.296875]\n",
      "7783: [discriminator loss: 0.536714, acc: 0.750000] [adversarial loss: 1.211311, acc: 0.234375]\n",
      "7784: [discriminator loss: 0.549096, acc: 0.671875] [adversarial loss: 0.997940, acc: 0.390625]\n",
      "7785: [discriminator loss: 0.507413, acc: 0.750000] [adversarial loss: 1.213162, acc: 0.156250]\n",
      "7786: [discriminator loss: 0.485691, acc: 0.757812] [adversarial loss: 1.198860, acc: 0.296875]\n",
      "7787: [discriminator loss: 0.477502, acc: 0.765625] [adversarial loss: 1.120387, acc: 0.312500]\n",
      "7788: [discriminator loss: 0.610777, acc: 0.648438] [adversarial loss: 1.241722, acc: 0.234375]\n",
      "7789: [discriminator loss: 0.502601, acc: 0.726562] [adversarial loss: 1.014967, acc: 0.359375]\n",
      "7790: [discriminator loss: 0.517257, acc: 0.750000] [adversarial loss: 1.286564, acc: 0.171875]\n",
      "7791: [discriminator loss: 0.519941, acc: 0.734375] [adversarial loss: 1.126417, acc: 0.296875]\n",
      "7792: [discriminator loss: 0.556379, acc: 0.687500] [adversarial loss: 1.261382, acc: 0.203125]\n",
      "7793: [discriminator loss: 0.544719, acc: 0.664062] [adversarial loss: 0.712752, acc: 0.500000]\n",
      "7794: [discriminator loss: 0.591406, acc: 0.664062] [adversarial loss: 1.660611, acc: 0.093750]\n",
      "7795: [discriminator loss: 0.642034, acc: 0.664062] [adversarial loss: 0.827772, acc: 0.468750]\n",
      "7796: [discriminator loss: 0.571416, acc: 0.710938] [adversarial loss: 1.607402, acc: 0.046875]\n",
      "7797: [discriminator loss: 0.602611, acc: 0.656250] [adversarial loss: 1.065280, acc: 0.296875]\n",
      "7798: [discriminator loss: 0.530906, acc: 0.734375] [adversarial loss: 1.437722, acc: 0.125000]\n",
      "7799: [discriminator loss: 0.504556, acc: 0.742188] [adversarial loss: 1.264957, acc: 0.187500]\n",
      "7800: [discriminator loss: 0.580363, acc: 0.687500] [adversarial loss: 1.212734, acc: 0.265625]\n",
      "7801: [discriminator loss: 0.489768, acc: 0.781250] [adversarial loss: 1.310596, acc: 0.156250]\n",
      "7802: [discriminator loss: 0.516862, acc: 0.718750] [adversarial loss: 1.144004, acc: 0.218750]\n",
      "7803: [discriminator loss: 0.544409, acc: 0.734375] [adversarial loss: 0.989876, acc: 0.343750]\n",
      "7804: [discriminator loss: 0.586823, acc: 0.656250] [adversarial loss: 1.486741, acc: 0.078125]\n",
      "7805: [discriminator loss: 0.517397, acc: 0.687500] [adversarial loss: 0.995437, acc: 0.265625]\n",
      "7806: [discriminator loss: 0.524514, acc: 0.734375] [adversarial loss: 1.582222, acc: 0.109375]\n",
      "7807: [discriminator loss: 0.508534, acc: 0.718750] [adversarial loss: 0.960671, acc: 0.328125]\n",
      "7808: [discriminator loss: 0.556144, acc: 0.726562] [adversarial loss: 1.232592, acc: 0.234375]\n",
      "7809: [discriminator loss: 0.587105, acc: 0.664062] [adversarial loss: 1.002620, acc: 0.328125]\n",
      "7810: [discriminator loss: 0.556729, acc: 0.671875] [adversarial loss: 1.338455, acc: 0.187500]\n",
      "7811: [discriminator loss: 0.558170, acc: 0.718750] [adversarial loss: 0.823843, acc: 0.390625]\n",
      "7812: [discriminator loss: 0.570025, acc: 0.687500] [adversarial loss: 1.188404, acc: 0.250000]\n",
      "7813: [discriminator loss: 0.523068, acc: 0.726562] [adversarial loss: 0.994403, acc: 0.375000]\n",
      "7814: [discriminator loss: 0.516267, acc: 0.757812] [adversarial loss: 1.285193, acc: 0.187500]\n",
      "7815: [discriminator loss: 0.601642, acc: 0.640625] [adversarial loss: 1.070954, acc: 0.265625]\n",
      "7816: [discriminator loss: 0.584665, acc: 0.664062] [adversarial loss: 1.306413, acc: 0.218750]\n",
      "7817: [discriminator loss: 0.581766, acc: 0.671875] [adversarial loss: 0.955930, acc: 0.390625]\n",
      "7818: [discriminator loss: 0.534103, acc: 0.710938] [adversarial loss: 1.336316, acc: 0.140625]\n",
      "7819: [discriminator loss: 0.493360, acc: 0.765625] [adversarial loss: 0.953446, acc: 0.359375]\n",
      "7820: [discriminator loss: 0.547162, acc: 0.695312] [adversarial loss: 1.189956, acc: 0.218750]\n",
      "7821: [discriminator loss: 0.532165, acc: 0.734375] [adversarial loss: 0.933573, acc: 0.343750]\n",
      "7822: [discriminator loss: 0.557111, acc: 0.664062] [adversarial loss: 1.204752, acc: 0.281250]\n",
      "7823: [discriminator loss: 0.556365, acc: 0.687500] [adversarial loss: 1.317511, acc: 0.140625]\n",
      "7824: [discriminator loss: 0.583176, acc: 0.703125] [adversarial loss: 1.127904, acc: 0.312500]\n",
      "7825: [discriminator loss: 0.628584, acc: 0.609375] [adversarial loss: 1.289136, acc: 0.218750]\n",
      "7826: [discriminator loss: 0.537228, acc: 0.742188] [adversarial loss: 1.142037, acc: 0.250000]\n",
      "7827: [discriminator loss: 0.520008, acc: 0.718750] [adversarial loss: 1.168032, acc: 0.281250]\n",
      "7828: [discriminator loss: 0.530425, acc: 0.726562] [adversarial loss: 1.177012, acc: 0.250000]\n",
      "7829: [discriminator loss: 0.573943, acc: 0.695312] [adversarial loss: 1.303820, acc: 0.203125]\n",
      "7830: [discriminator loss: 0.509006, acc: 0.757812] [adversarial loss: 0.918968, acc: 0.390625]\n",
      "7831: [discriminator loss: 0.608221, acc: 0.664062] [adversarial loss: 1.458253, acc: 0.078125]\n",
      "7832: [discriminator loss: 0.548274, acc: 0.718750] [adversarial loss: 1.056671, acc: 0.312500]\n",
      "7833: [discriminator loss: 0.490480, acc: 0.757812] [adversarial loss: 1.137430, acc: 0.250000]\n",
      "7834: [discriminator loss: 0.487770, acc: 0.796875] [adversarial loss: 1.331637, acc: 0.125000]\n",
      "7835: [discriminator loss: 0.556687, acc: 0.734375] [adversarial loss: 1.004337, acc: 0.328125]\n",
      "7836: [discriminator loss: 0.564640, acc: 0.695312] [adversarial loss: 1.203283, acc: 0.187500]\n",
      "7837: [discriminator loss: 0.595575, acc: 0.648438] [adversarial loss: 1.096349, acc: 0.328125]\n",
      "7838: [discriminator loss: 0.531993, acc: 0.773438] [adversarial loss: 1.293830, acc: 0.187500]\n",
      "7839: [discriminator loss: 0.499324, acc: 0.773438] [adversarial loss: 1.125002, acc: 0.187500]\n",
      "7840: [discriminator loss: 0.528242, acc: 0.710938] [adversarial loss: 1.190751, acc: 0.203125]\n",
      "7841: [discriminator loss: 0.549430, acc: 0.664062] [adversarial loss: 0.974380, acc: 0.375000]\n",
      "7842: [discriminator loss: 0.527454, acc: 0.765625] [adversarial loss: 1.357722, acc: 0.171875]\n",
      "7843: [discriminator loss: 0.586098, acc: 0.695312] [adversarial loss: 0.941948, acc: 0.375000]\n",
      "7844: [discriminator loss: 0.563518, acc: 0.710938] [adversarial loss: 1.491918, acc: 0.125000]\n",
      "7845: [discriminator loss: 0.568421, acc: 0.687500] [adversarial loss: 1.025725, acc: 0.312500]\n",
      "7846: [discriminator loss: 0.532244, acc: 0.742188] [adversarial loss: 1.590302, acc: 0.031250]\n",
      "7847: [discriminator loss: 0.549682, acc: 0.718750] [adversarial loss: 1.039545, acc: 0.312500]\n",
      "7848: [discriminator loss: 0.517703, acc: 0.703125] [adversarial loss: 1.574287, acc: 0.046875]\n",
      "7849: [discriminator loss: 0.525538, acc: 0.718750] [adversarial loss: 1.085119, acc: 0.234375]\n",
      "7850: [discriminator loss: 0.578149, acc: 0.671875] [adversarial loss: 1.540590, acc: 0.078125]\n",
      "7851: [discriminator loss: 0.512317, acc: 0.710938] [adversarial loss: 1.130741, acc: 0.187500]\n",
      "7852: [discriminator loss: 0.538696, acc: 0.679688] [adversarial loss: 0.962965, acc: 0.390625]\n",
      "7853: [discriminator loss: 0.505206, acc: 0.742188] [adversarial loss: 1.309705, acc: 0.140625]\n",
      "7854: [discriminator loss: 0.503088, acc: 0.742188] [adversarial loss: 1.106761, acc: 0.265625]\n",
      "7855: [discriminator loss: 0.552212, acc: 0.710938] [adversarial loss: 1.282223, acc: 0.156250]\n",
      "7856: [discriminator loss: 0.501917, acc: 0.726562] [adversarial loss: 1.083941, acc: 0.218750]\n",
      "7857: [discriminator loss: 0.578170, acc: 0.687500] [adversarial loss: 1.264422, acc: 0.218750]\n",
      "7858: [discriminator loss: 0.547479, acc: 0.703125] [adversarial loss: 0.937396, acc: 0.296875]\n",
      "7859: [discriminator loss: 0.532110, acc: 0.718750] [adversarial loss: 1.402116, acc: 0.140625]\n",
      "7860: [discriminator loss: 0.564788, acc: 0.703125] [adversarial loss: 0.986848, acc: 0.375000]\n",
      "7861: [discriminator loss: 0.532263, acc: 0.750000] [adversarial loss: 1.479542, acc: 0.171875]\n",
      "7862: [discriminator loss: 0.601290, acc: 0.656250] [adversarial loss: 0.885997, acc: 0.421875]\n",
      "7863: [discriminator loss: 0.606269, acc: 0.648438] [adversarial loss: 1.303001, acc: 0.187500]\n",
      "7864: [discriminator loss: 0.571344, acc: 0.679688] [adversarial loss: 1.222862, acc: 0.218750]\n",
      "7865: [discriminator loss: 0.502789, acc: 0.750000] [adversarial loss: 1.125046, acc: 0.171875]\n",
      "7866: [discriminator loss: 0.567839, acc: 0.687500] [adversarial loss: 1.161084, acc: 0.218750]\n",
      "7867: [discriminator loss: 0.504838, acc: 0.726562] [adversarial loss: 1.119780, acc: 0.296875]\n",
      "7868: [discriminator loss: 0.553199, acc: 0.671875] [adversarial loss: 1.159584, acc: 0.203125]\n",
      "7869: [discriminator loss: 0.529885, acc: 0.734375] [adversarial loss: 1.207471, acc: 0.218750]\n",
      "7870: [discriminator loss: 0.513336, acc: 0.734375] [adversarial loss: 0.945077, acc: 0.359375]\n",
      "7871: [discriminator loss: 0.503816, acc: 0.773438] [adversarial loss: 1.291148, acc: 0.203125]\n",
      "7872: [discriminator loss: 0.599858, acc: 0.656250] [adversarial loss: 1.184695, acc: 0.265625]\n",
      "7873: [discriminator loss: 0.476607, acc: 0.757812] [adversarial loss: 1.092625, acc: 0.312500]\n",
      "7874: [discriminator loss: 0.521047, acc: 0.750000] [adversarial loss: 1.229797, acc: 0.187500]\n",
      "7875: [discriminator loss: 0.509711, acc: 0.726562] [adversarial loss: 1.017454, acc: 0.359375]\n",
      "7876: [discriminator loss: 0.505213, acc: 0.765625] [adversarial loss: 1.323919, acc: 0.187500]\n",
      "7877: [discriminator loss: 0.578993, acc: 0.687500] [adversarial loss: 0.897173, acc: 0.312500]\n",
      "7878: [discriminator loss: 0.554706, acc: 0.679688] [adversarial loss: 1.386353, acc: 0.156250]\n",
      "7879: [discriminator loss: 0.511156, acc: 0.742188] [adversarial loss: 0.985032, acc: 0.296875]\n",
      "7880: [discriminator loss: 0.433767, acc: 0.750000] [adversarial loss: 1.563232, acc: 0.156250]\n",
      "7881: [discriminator loss: 0.598711, acc: 0.687500] [adversarial loss: 0.952659, acc: 0.484375]\n",
      "7882: [discriminator loss: 0.635701, acc: 0.648438] [adversarial loss: 1.514121, acc: 0.125000]\n",
      "7883: [discriminator loss: 0.575756, acc: 0.718750] [adversarial loss: 0.974768, acc: 0.390625]\n",
      "7884: [discriminator loss: 0.604969, acc: 0.656250] [adversarial loss: 1.460856, acc: 0.109375]\n",
      "7885: [discriminator loss: 0.522107, acc: 0.734375] [adversarial loss: 0.885617, acc: 0.421875]\n",
      "7886: [discriminator loss: 0.559432, acc: 0.726562] [adversarial loss: 1.302336, acc: 0.203125]\n",
      "7887: [discriminator loss: 0.553896, acc: 0.742188] [adversarial loss: 1.224303, acc: 0.171875]\n",
      "7888: [discriminator loss: 0.516793, acc: 0.726562] [adversarial loss: 0.985860, acc: 0.296875]\n",
      "7889: [discriminator loss: 0.495288, acc: 0.765625] [adversarial loss: 1.335244, acc: 0.187500]\n",
      "7890: [discriminator loss: 0.523750, acc: 0.687500] [adversarial loss: 1.034930, acc: 0.312500]\n",
      "7891: [discriminator loss: 0.588431, acc: 0.679688] [adversarial loss: 1.355487, acc: 0.140625]\n",
      "7892: [discriminator loss: 0.595342, acc: 0.718750] [adversarial loss: 0.953703, acc: 0.343750]\n",
      "7893: [discriminator loss: 0.593960, acc: 0.648438] [adversarial loss: 1.319177, acc: 0.125000]\n",
      "7894: [discriminator loss: 0.505352, acc: 0.742188] [adversarial loss: 1.049405, acc: 0.203125]\n",
      "7895: [discriminator loss: 0.575816, acc: 0.703125] [adversarial loss: 1.028210, acc: 0.250000]\n",
      "7896: [discriminator loss: 0.542861, acc: 0.718750] [adversarial loss: 1.040585, acc: 0.296875]\n",
      "7897: [discriminator loss: 0.525864, acc: 0.710938] [adversarial loss: 1.344766, acc: 0.187500]\n",
      "7898: [discriminator loss: 0.574363, acc: 0.664062] [adversarial loss: 1.310683, acc: 0.265625]\n",
      "7899: [discriminator loss: 0.467994, acc: 0.796875] [adversarial loss: 1.131324, acc: 0.234375]\n",
      "7900: [discriminator loss: 0.529804, acc: 0.750000] [adversarial loss: 1.263350, acc: 0.171875]\n",
      "7901: [discriminator loss: 0.507086, acc: 0.734375] [adversarial loss: 1.170655, acc: 0.265625]\n",
      "7902: [discriminator loss: 0.582966, acc: 0.679688] [adversarial loss: 1.298101, acc: 0.171875]\n",
      "7903: [discriminator loss: 0.579871, acc: 0.687500] [adversarial loss: 0.856444, acc: 0.406250]\n",
      "7904: [discriminator loss: 0.628917, acc: 0.671875] [adversarial loss: 1.278450, acc: 0.203125]\n",
      "7905: [discriminator loss: 0.552899, acc: 0.726562] [adversarial loss: 1.045906, acc: 0.296875]\n",
      "7906: [discriminator loss: 0.579852, acc: 0.671875] [adversarial loss: 1.184565, acc: 0.156250]\n",
      "7907: [discriminator loss: 0.430380, acc: 0.835938] [adversarial loss: 1.366928, acc: 0.234375]\n",
      "7908: [discriminator loss: 0.561250, acc: 0.695312] [adversarial loss: 1.163973, acc: 0.281250]\n",
      "7909: [discriminator loss: 0.512056, acc: 0.757812] [adversarial loss: 1.333053, acc: 0.187500]\n",
      "7910: [discriminator loss: 0.530287, acc: 0.742188] [adversarial loss: 0.859691, acc: 0.390625]\n",
      "7911: [discriminator loss: 0.555577, acc: 0.695312] [adversarial loss: 1.472735, acc: 0.140625]\n",
      "7912: [discriminator loss: 0.525475, acc: 0.734375] [adversarial loss: 0.803124, acc: 0.515625]\n",
      "7913: [discriminator loss: 0.592215, acc: 0.679688] [adversarial loss: 1.265140, acc: 0.234375]\n",
      "7914: [discriminator loss: 0.552350, acc: 0.742188] [adversarial loss: 0.938292, acc: 0.390625]\n",
      "7915: [discriminator loss: 0.609026, acc: 0.687500] [adversarial loss: 1.542107, acc: 0.156250]\n",
      "7916: [discriminator loss: 0.573380, acc: 0.710938] [adversarial loss: 1.009810, acc: 0.328125]\n",
      "7917: [discriminator loss: 0.577776, acc: 0.703125] [adversarial loss: 1.191130, acc: 0.203125]\n",
      "7918: [discriminator loss: 0.551284, acc: 0.710938] [adversarial loss: 0.873171, acc: 0.406250]\n",
      "7919: [discriminator loss: 0.581739, acc: 0.679688] [adversarial loss: 1.325644, acc: 0.187500]\n",
      "7920: [discriminator loss: 0.518622, acc: 0.703125] [adversarial loss: 1.218172, acc: 0.187500]\n",
      "7921: [discriminator loss: 0.538369, acc: 0.750000] [adversarial loss: 1.159589, acc: 0.250000]\n",
      "7922: [discriminator loss: 0.501091, acc: 0.742188] [adversarial loss: 1.435641, acc: 0.203125]\n",
      "7923: [discriminator loss: 0.573643, acc: 0.695312] [adversarial loss: 1.053477, acc: 0.343750]\n",
      "7924: [discriminator loss: 0.534790, acc: 0.726562] [adversarial loss: 1.312249, acc: 0.203125]\n",
      "7925: [discriminator loss: 0.572751, acc: 0.710938] [adversarial loss: 1.038666, acc: 0.453125]\n",
      "7926: [discriminator loss: 0.528104, acc: 0.734375] [adversarial loss: 1.314779, acc: 0.156250]\n",
      "7927: [discriminator loss: 0.561211, acc: 0.695312] [adversarial loss: 1.145521, acc: 0.234375]\n",
      "7928: [discriminator loss: 0.540218, acc: 0.718750] [adversarial loss: 0.964375, acc: 0.406250]\n",
      "7929: [discriminator loss: 0.519492, acc: 0.765625] [adversarial loss: 1.237121, acc: 0.218750]\n",
      "7930: [discriminator loss: 0.521202, acc: 0.757812] [adversarial loss: 0.979531, acc: 0.375000]\n",
      "7931: [discriminator loss: 0.588938, acc: 0.710938] [adversarial loss: 1.243086, acc: 0.203125]\n",
      "7932: [discriminator loss: 0.551018, acc: 0.664062] [adversarial loss: 0.980943, acc: 0.343750]\n",
      "7933: [discriminator loss: 0.511744, acc: 0.742188] [adversarial loss: 1.581196, acc: 0.156250]\n",
      "7934: [discriminator loss: 0.556226, acc: 0.695312] [adversarial loss: 0.745396, acc: 0.468750]\n",
      "7935: [discriminator loss: 0.567807, acc: 0.687500] [adversarial loss: 1.676289, acc: 0.093750]\n",
      "7936: [discriminator loss: 0.513655, acc: 0.687500] [adversarial loss: 1.175808, acc: 0.250000]\n",
      "7937: [discriminator loss: 0.533729, acc: 0.757812] [adversarial loss: 1.065230, acc: 0.328125]\n",
      "7938: [discriminator loss: 0.615955, acc: 0.656250] [adversarial loss: 1.108658, acc: 0.296875]\n",
      "7939: [discriminator loss: 0.526326, acc: 0.773438] [adversarial loss: 1.440941, acc: 0.109375]\n",
      "7940: [discriminator loss: 0.564617, acc: 0.695312] [adversarial loss: 1.004215, acc: 0.250000]\n",
      "7941: [discriminator loss: 0.630108, acc: 0.671875] [adversarial loss: 1.053216, acc: 0.281250]\n",
      "7942: [discriminator loss: 0.548253, acc: 0.726562] [adversarial loss: 1.341076, acc: 0.125000]\n",
      "7943: [discriminator loss: 0.488575, acc: 0.781250] [adversarial loss: 1.006210, acc: 0.390625]\n",
      "7944: [discriminator loss: 0.590610, acc: 0.648438] [adversarial loss: 1.487599, acc: 0.062500]\n",
      "7945: [discriminator loss: 0.626265, acc: 0.609375] [adversarial loss: 1.091174, acc: 0.203125]\n",
      "7946: [discriminator loss: 0.529781, acc: 0.718750] [adversarial loss: 1.332857, acc: 0.156250]\n",
      "7947: [discriminator loss: 0.566063, acc: 0.671875] [adversarial loss: 1.367436, acc: 0.125000]\n",
      "7948: [discriminator loss: 0.555350, acc: 0.687500] [adversarial loss: 0.892348, acc: 0.437500]\n",
      "7949: [discriminator loss: 0.488619, acc: 0.742188] [adversarial loss: 1.180036, acc: 0.187500]\n",
      "7950: [discriminator loss: 0.521440, acc: 0.664062] [adversarial loss: 1.170367, acc: 0.265625]\n",
      "7951: [discriminator loss: 0.575249, acc: 0.656250] [adversarial loss: 1.138269, acc: 0.171875]\n",
      "7952: [discriminator loss: 0.522900, acc: 0.718750] [adversarial loss: 1.238962, acc: 0.250000]\n",
      "7953: [discriminator loss: 0.575824, acc: 0.687500] [adversarial loss: 1.149234, acc: 0.265625]\n",
      "7954: [discriminator loss: 0.488355, acc: 0.773438] [adversarial loss: 1.458278, acc: 0.203125]\n",
      "7955: [discriminator loss: 0.631510, acc: 0.656250] [adversarial loss: 0.826665, acc: 0.468750]\n",
      "7956: [discriminator loss: 0.561771, acc: 0.718750] [adversarial loss: 1.504611, acc: 0.078125]\n",
      "7957: [discriminator loss: 0.476936, acc: 0.750000] [adversarial loss: 1.025648, acc: 0.359375]\n",
      "7958: [discriminator loss: 0.597070, acc: 0.687500] [adversarial loss: 1.674526, acc: 0.078125]\n",
      "7959: [discriminator loss: 0.601520, acc: 0.687500] [adversarial loss: 0.858940, acc: 0.484375]\n",
      "7960: [discriminator loss: 0.586335, acc: 0.640625] [adversarial loss: 1.463495, acc: 0.109375]\n",
      "7961: [discriminator loss: 0.657499, acc: 0.632812] [adversarial loss: 0.842735, acc: 0.437500]\n",
      "7962: [discriminator loss: 0.545699, acc: 0.703125] [adversarial loss: 1.394207, acc: 0.093750]\n",
      "7963: [discriminator loss: 0.469791, acc: 0.750000] [adversarial loss: 1.061802, acc: 0.265625]\n",
      "7964: [discriminator loss: 0.582897, acc: 0.710938] [adversarial loss: 1.163961, acc: 0.281250]\n",
      "7965: [discriminator loss: 0.544255, acc: 0.734375] [adversarial loss: 1.180839, acc: 0.218750]\n",
      "7966: [discriminator loss: 0.561776, acc: 0.710938] [adversarial loss: 1.116843, acc: 0.250000]\n",
      "7967: [discriminator loss: 0.538068, acc: 0.726562] [adversarial loss: 1.462348, acc: 0.109375]\n",
      "7968: [discriminator loss: 0.618929, acc: 0.632812] [adversarial loss: 0.925624, acc: 0.375000]\n",
      "7969: [discriminator loss: 0.616939, acc: 0.609375] [adversarial loss: 1.551666, acc: 0.109375]\n",
      "7970: [discriminator loss: 0.497891, acc: 0.742188] [adversarial loss: 0.939028, acc: 0.343750]\n",
      "7971: [discriminator loss: 0.566060, acc: 0.671875] [adversarial loss: 1.134878, acc: 0.343750]\n",
      "7972: [discriminator loss: 0.517979, acc: 0.750000] [adversarial loss: 1.331750, acc: 0.140625]\n",
      "7973: [discriminator loss: 0.467856, acc: 0.789062] [adversarial loss: 1.114898, acc: 0.218750]\n",
      "7974: [discriminator loss: 0.649144, acc: 0.656250] [adversarial loss: 1.254311, acc: 0.203125]\n",
      "7975: [discriminator loss: 0.544518, acc: 0.679688] [adversarial loss: 1.051399, acc: 0.328125]\n",
      "7976: [discriminator loss: 0.518029, acc: 0.726562] [adversarial loss: 1.362953, acc: 0.140625]\n",
      "7977: [discriminator loss: 0.530052, acc: 0.695312] [adversarial loss: 1.121998, acc: 0.296875]\n",
      "7978: [discriminator loss: 0.560075, acc: 0.679688] [adversarial loss: 1.280006, acc: 0.109375]\n",
      "7979: [discriminator loss: 0.546636, acc: 0.687500] [adversarial loss: 0.947587, acc: 0.390625]\n",
      "7980: [discriminator loss: 0.574471, acc: 0.726562] [adversarial loss: 1.533497, acc: 0.109375]\n",
      "7981: [discriminator loss: 0.624357, acc: 0.671875] [adversarial loss: 0.815701, acc: 0.437500]\n",
      "7982: [discriminator loss: 0.586992, acc: 0.671875] [adversarial loss: 1.582085, acc: 0.140625]\n",
      "7983: [discriminator loss: 0.502623, acc: 0.742188] [adversarial loss: 1.038329, acc: 0.328125]\n",
      "7984: [discriminator loss: 0.504526, acc: 0.710938] [adversarial loss: 1.440825, acc: 0.171875]\n",
      "7985: [discriminator loss: 0.558069, acc: 0.695312] [adversarial loss: 1.194770, acc: 0.234375]\n",
      "7986: [discriminator loss: 0.551801, acc: 0.750000] [adversarial loss: 1.250983, acc: 0.187500]\n",
      "7987: [discriminator loss: 0.545917, acc: 0.734375] [adversarial loss: 1.148238, acc: 0.171875]\n",
      "7988: [discriminator loss: 0.553245, acc: 0.679688] [adversarial loss: 1.128729, acc: 0.250000]\n",
      "7989: [discriminator loss: 0.507093, acc: 0.750000] [adversarial loss: 1.233765, acc: 0.203125]\n",
      "7990: [discriminator loss: 0.581061, acc: 0.671875] [adversarial loss: 1.074444, acc: 0.328125]\n",
      "7991: [discriminator loss: 0.561705, acc: 0.656250] [adversarial loss: 1.175526, acc: 0.187500]\n",
      "7992: [discriminator loss: 0.600457, acc: 0.632812] [adversarial loss: 1.174306, acc: 0.265625]\n",
      "7993: [discriminator loss: 0.486743, acc: 0.734375] [adversarial loss: 1.263287, acc: 0.140625]\n",
      "7994: [discriminator loss: 0.498184, acc: 0.781250] [adversarial loss: 0.917235, acc: 0.359375]\n",
      "7995: [discriminator loss: 0.535703, acc: 0.742188] [adversarial loss: 1.361153, acc: 0.187500]\n",
      "7996: [discriminator loss: 0.549562, acc: 0.695312] [adversarial loss: 0.892073, acc: 0.343750]\n",
      "7997: [discriminator loss: 0.505377, acc: 0.773438] [adversarial loss: 1.520087, acc: 0.140625]\n",
      "7998: [discriminator loss: 0.507777, acc: 0.750000] [adversarial loss: 0.952106, acc: 0.343750]\n",
      "7999: [discriminator loss: 0.558001, acc: 0.718750] [adversarial loss: 1.440803, acc: 0.109375]\n",
      "8000: [discriminator loss: 0.575581, acc: 0.710938] [adversarial loss: 0.694022, acc: 0.640625]\n",
      "8001: [discriminator loss: 0.545507, acc: 0.718750] [adversarial loss: 1.442054, acc: 0.109375]\n",
      "8002: [discriminator loss: 0.488736, acc: 0.789062] [adversarial loss: 1.127987, acc: 0.312500]\n",
      "8003: [discriminator loss: 0.570125, acc: 0.679688] [adversarial loss: 1.125326, acc: 0.281250]\n",
      "8004: [discriminator loss: 0.489015, acc: 0.742188] [adversarial loss: 1.254653, acc: 0.234375]\n",
      "8005: [discriminator loss: 0.539956, acc: 0.703125] [adversarial loss: 1.335188, acc: 0.171875]\n",
      "8006: [discriminator loss: 0.494625, acc: 0.710938] [adversarial loss: 1.144940, acc: 0.296875]\n",
      "8007: [discriminator loss: 0.574966, acc: 0.671875] [adversarial loss: 1.220564, acc: 0.218750]\n",
      "8008: [discriminator loss: 0.539032, acc: 0.742188] [adversarial loss: 1.080732, acc: 0.390625]\n",
      "8009: [discriminator loss: 0.519255, acc: 0.718750] [adversarial loss: 1.256567, acc: 0.203125]\n",
      "8010: [discriminator loss: 0.570342, acc: 0.695312] [adversarial loss: 1.078819, acc: 0.281250]\n",
      "8011: [discriminator loss: 0.530627, acc: 0.726562] [adversarial loss: 1.458773, acc: 0.187500]\n",
      "8012: [discriminator loss: 0.599515, acc: 0.703125] [adversarial loss: 1.144818, acc: 0.234375]\n",
      "8013: [discriminator loss: 0.564010, acc: 0.679688] [adversarial loss: 1.199609, acc: 0.234375]\n",
      "8014: [discriminator loss: 0.541658, acc: 0.710938] [adversarial loss: 1.479410, acc: 0.156250]\n",
      "8015: [discriminator loss: 0.605497, acc: 0.671875] [adversarial loss: 0.746649, acc: 0.437500]\n",
      "8016: [discriminator loss: 0.571252, acc: 0.703125] [adversarial loss: 1.522332, acc: 0.109375]\n",
      "8017: [discriminator loss: 0.609252, acc: 0.648438] [adversarial loss: 0.868979, acc: 0.421875]\n",
      "8018: [discriminator loss: 0.607365, acc: 0.703125] [adversarial loss: 1.336569, acc: 0.125000]\n",
      "8019: [discriminator loss: 0.594722, acc: 0.687500] [adversarial loss: 1.089570, acc: 0.359375]\n",
      "8020: [discriminator loss: 0.557165, acc: 0.679688] [adversarial loss: 1.368382, acc: 0.125000]\n",
      "8021: [discriminator loss: 0.508601, acc: 0.757812] [adversarial loss: 1.171158, acc: 0.265625]\n",
      "8022: [discriminator loss: 0.545361, acc: 0.718750] [adversarial loss: 1.018619, acc: 0.281250]\n",
      "8023: [discriminator loss: 0.521668, acc: 0.742188] [adversarial loss: 1.387402, acc: 0.125000]\n",
      "8024: [discriminator loss: 0.570624, acc: 0.695312] [adversarial loss: 0.998271, acc: 0.328125]\n",
      "8025: [discriminator loss: 0.504341, acc: 0.757812] [adversarial loss: 1.440360, acc: 0.125000]\n",
      "8026: [discriminator loss: 0.543599, acc: 0.750000] [adversarial loss: 0.947476, acc: 0.406250]\n",
      "8027: [discriminator loss: 0.487724, acc: 0.781250] [adversarial loss: 1.361151, acc: 0.140625]\n",
      "8028: [discriminator loss: 0.584929, acc: 0.695312] [adversarial loss: 1.118660, acc: 0.187500]\n",
      "8029: [discriminator loss: 0.576030, acc: 0.710938] [adversarial loss: 1.207684, acc: 0.187500]\n",
      "8030: [discriminator loss: 0.517259, acc: 0.734375] [adversarial loss: 1.136160, acc: 0.203125]\n",
      "8031: [discriminator loss: 0.612188, acc: 0.695312] [adversarial loss: 1.157248, acc: 0.281250]\n",
      "8032: [discriminator loss: 0.517557, acc: 0.750000] [adversarial loss: 0.988573, acc: 0.359375]\n",
      "8033: [discriminator loss: 0.653453, acc: 0.640625] [adversarial loss: 1.830031, acc: 0.046875]\n",
      "8034: [discriminator loss: 0.595865, acc: 0.671875] [adversarial loss: 1.003058, acc: 0.406250]\n",
      "8035: [discriminator loss: 0.527524, acc: 0.757812] [adversarial loss: 1.019227, acc: 0.296875]\n",
      "8036: [discriminator loss: 0.513716, acc: 0.781250] [adversarial loss: 1.317716, acc: 0.187500]\n",
      "8037: [discriminator loss: 0.631009, acc: 0.617188] [adversarial loss: 1.184835, acc: 0.187500]\n",
      "8038: [discriminator loss: 0.606561, acc: 0.687500] [adversarial loss: 1.190847, acc: 0.218750]\n",
      "8039: [discriminator loss: 0.546330, acc: 0.710938] [adversarial loss: 1.107893, acc: 0.312500]\n",
      "8040: [discriminator loss: 0.550065, acc: 0.695312] [adversarial loss: 1.153622, acc: 0.234375]\n",
      "8041: [discriminator loss: 0.491133, acc: 0.757812] [adversarial loss: 1.221310, acc: 0.250000]\n",
      "8042: [discriminator loss: 0.538500, acc: 0.734375] [adversarial loss: 1.172783, acc: 0.218750]\n",
      "8043: [discriminator loss: 0.540334, acc: 0.734375] [adversarial loss: 0.992603, acc: 0.390625]\n",
      "8044: [discriminator loss: 0.604714, acc: 0.640625] [adversarial loss: 1.441048, acc: 0.140625]\n",
      "8045: [discriminator loss: 0.597198, acc: 0.640625] [adversarial loss: 0.847388, acc: 0.468750]\n",
      "8046: [discriminator loss: 0.624304, acc: 0.648438] [adversarial loss: 1.523186, acc: 0.171875]\n",
      "8047: [discriminator loss: 0.511242, acc: 0.765625] [adversarial loss: 1.098643, acc: 0.234375]\n",
      "8048: [discriminator loss: 0.511680, acc: 0.710938] [adversarial loss: 1.138591, acc: 0.218750]\n",
      "8049: [discriminator loss: 0.506732, acc: 0.757812] [adversarial loss: 1.478955, acc: 0.156250]\n",
      "8050: [discriminator loss: 0.587247, acc: 0.656250] [adversarial loss: 1.350540, acc: 0.156250]\n",
      "8051: [discriminator loss: 0.505756, acc: 0.765625] [adversarial loss: 1.029873, acc: 0.265625]\n",
      "8052: [discriminator loss: 0.538328, acc: 0.726562] [adversarial loss: 1.183073, acc: 0.265625]\n",
      "8053: [discriminator loss: 0.534270, acc: 0.726562] [adversarial loss: 1.077937, acc: 0.203125]\n",
      "8054: [discriminator loss: 0.542526, acc: 0.750000] [adversarial loss: 1.181606, acc: 0.250000]\n",
      "8055: [discriminator loss: 0.608777, acc: 0.664062] [adversarial loss: 0.930856, acc: 0.328125]\n",
      "8056: [discriminator loss: 0.592247, acc: 0.671875] [adversarial loss: 1.592762, acc: 0.109375]\n",
      "8057: [discriminator loss: 0.594609, acc: 0.664062] [adversarial loss: 0.835843, acc: 0.468750]\n",
      "8058: [discriminator loss: 0.556218, acc: 0.703125] [adversarial loss: 1.318832, acc: 0.234375]\n",
      "8059: [discriminator loss: 0.513269, acc: 0.734375] [adversarial loss: 1.192075, acc: 0.218750]\n",
      "8060: [discriminator loss: 0.533732, acc: 0.703125] [adversarial loss: 1.094494, acc: 0.265625]\n",
      "8061: [discriminator loss: 0.492319, acc: 0.718750] [adversarial loss: 1.082413, acc: 0.296875]\n",
      "8062: [discriminator loss: 0.559027, acc: 0.664062] [adversarial loss: 1.093851, acc: 0.312500]\n",
      "8063: [discriminator loss: 0.583100, acc: 0.710938] [adversarial loss: 1.380804, acc: 0.171875]\n",
      "8064: [discriminator loss: 0.523038, acc: 0.687500] [adversarial loss: 1.044528, acc: 0.281250]\n",
      "8065: [discriminator loss: 0.539832, acc: 0.710938] [adversarial loss: 1.079922, acc: 0.250000]\n",
      "8066: [discriminator loss: 0.626525, acc: 0.640625] [adversarial loss: 1.360737, acc: 0.171875]\n",
      "8067: [discriminator loss: 0.521222, acc: 0.750000] [adversarial loss: 1.140723, acc: 0.187500]\n",
      "8068: [discriminator loss: 0.516486, acc: 0.703125] [adversarial loss: 0.858088, acc: 0.453125]\n",
      "8069: [discriminator loss: 0.590506, acc: 0.679688] [adversarial loss: 1.510495, acc: 0.140625]\n",
      "8070: [discriminator loss: 0.574804, acc: 0.703125] [adversarial loss: 1.027997, acc: 0.343750]\n",
      "8071: [discriminator loss: 0.537469, acc: 0.703125] [adversarial loss: 1.501268, acc: 0.078125]\n",
      "8072: [discriminator loss: 0.513263, acc: 0.750000] [adversarial loss: 1.012594, acc: 0.328125]\n",
      "8073: [discriminator loss: 0.571026, acc: 0.656250] [adversarial loss: 1.234771, acc: 0.171875]\n",
      "8074: [discriminator loss: 0.534197, acc: 0.703125] [adversarial loss: 1.326829, acc: 0.156250]\n",
      "8075: [discriminator loss: 0.511536, acc: 0.757812] [adversarial loss: 1.141530, acc: 0.234375]\n",
      "8076: [discriminator loss: 0.483531, acc: 0.773438] [adversarial loss: 0.991329, acc: 0.265625]\n",
      "8077: [discriminator loss: 0.630727, acc: 0.664062] [adversarial loss: 1.137394, acc: 0.234375]\n",
      "8078: [discriminator loss: 0.563378, acc: 0.703125] [adversarial loss: 1.020585, acc: 0.265625]\n",
      "8079: [discriminator loss: 0.544744, acc: 0.671875] [adversarial loss: 1.256636, acc: 0.171875]\n",
      "8080: [discriminator loss: 0.544830, acc: 0.734375] [adversarial loss: 1.120916, acc: 0.359375]\n",
      "8081: [discriminator loss: 0.521219, acc: 0.726562] [adversarial loss: 1.127678, acc: 0.312500]\n",
      "8082: [discriminator loss: 0.586347, acc: 0.687500] [adversarial loss: 1.348926, acc: 0.203125]\n",
      "8083: [discriminator loss: 0.571397, acc: 0.656250] [adversarial loss: 0.963543, acc: 0.343750]\n",
      "8084: [discriminator loss: 0.561747, acc: 0.695312] [adversarial loss: 1.408638, acc: 0.171875]\n",
      "8085: [discriminator loss: 0.623441, acc: 0.632812] [adversarial loss: 1.068086, acc: 0.250000]\n",
      "8086: [discriminator loss: 0.495215, acc: 0.726562] [adversarial loss: 1.240531, acc: 0.156250]\n",
      "8087: [discriminator loss: 0.567028, acc: 0.648438] [adversarial loss: 1.134366, acc: 0.234375]\n",
      "8088: [discriminator loss: 0.523506, acc: 0.750000] [adversarial loss: 1.581765, acc: 0.093750]\n",
      "8089: [discriminator loss: 0.646837, acc: 0.648438] [adversarial loss: 0.877477, acc: 0.406250]\n",
      "8090: [discriminator loss: 0.617867, acc: 0.640625] [adversarial loss: 1.610288, acc: 0.078125]\n",
      "8091: [discriminator loss: 0.591178, acc: 0.679688] [adversarial loss: 0.821675, acc: 0.437500]\n",
      "8092: [discriminator loss: 0.577838, acc: 0.679688] [adversarial loss: 1.482255, acc: 0.078125]\n",
      "8093: [discriminator loss: 0.502523, acc: 0.757812] [adversarial loss: 1.157955, acc: 0.250000]\n",
      "8094: [discriminator loss: 0.483660, acc: 0.804688] [adversarial loss: 1.551497, acc: 0.078125]\n",
      "8095: [discriminator loss: 0.566394, acc: 0.718750] [adversarial loss: 1.091322, acc: 0.281250]\n",
      "8096: [discriminator loss: 0.568099, acc: 0.703125] [adversarial loss: 1.295769, acc: 0.218750]\n",
      "8097: [discriminator loss: 0.561718, acc: 0.664062] [adversarial loss: 1.129848, acc: 0.203125]\n",
      "8098: [discriminator loss: 0.522335, acc: 0.757812] [adversarial loss: 1.393885, acc: 0.234375]\n",
      "8099: [discriminator loss: 0.602958, acc: 0.632812] [adversarial loss: 1.168562, acc: 0.250000]\n",
      "8100: [discriminator loss: 0.501278, acc: 0.765625] [adversarial loss: 1.050122, acc: 0.296875]\n",
      "8101: [discriminator loss: 0.574250, acc: 0.710938] [adversarial loss: 1.274832, acc: 0.234375]\n",
      "8102: [discriminator loss: 0.555368, acc: 0.718750] [adversarial loss: 1.034486, acc: 0.359375]\n",
      "8103: [discriminator loss: 0.491244, acc: 0.757812] [adversarial loss: 1.414019, acc: 0.140625]\n",
      "8104: [discriminator loss: 0.568520, acc: 0.656250] [adversarial loss: 1.092038, acc: 0.296875]\n",
      "8105: [discriminator loss: 0.539437, acc: 0.734375] [adversarial loss: 1.126533, acc: 0.187500]\n",
      "8106: [discriminator loss: 0.501356, acc: 0.765625] [adversarial loss: 1.174289, acc: 0.218750]\n",
      "8107: [discriminator loss: 0.618327, acc: 0.671875] [adversarial loss: 1.164812, acc: 0.203125]\n",
      "8108: [discriminator loss: 0.531385, acc: 0.742188] [adversarial loss: 1.143913, acc: 0.187500]\n",
      "8109: [discriminator loss: 0.556173, acc: 0.703125] [adversarial loss: 1.299570, acc: 0.156250]\n",
      "8110: [discriminator loss: 0.527635, acc: 0.773438] [adversarial loss: 0.964952, acc: 0.359375]\n",
      "8111: [discriminator loss: 0.576207, acc: 0.656250] [adversarial loss: 1.404392, acc: 0.203125]\n",
      "8112: [discriminator loss: 0.603094, acc: 0.664062] [adversarial loss: 0.868762, acc: 0.468750]\n",
      "8113: [discriminator loss: 0.548868, acc: 0.687500] [adversarial loss: 1.323684, acc: 0.125000]\n",
      "8114: [discriminator loss: 0.546338, acc: 0.742188] [adversarial loss: 0.961850, acc: 0.453125]\n",
      "8115: [discriminator loss: 0.485673, acc: 0.804688] [adversarial loss: 1.263282, acc: 0.187500]\n",
      "8116: [discriminator loss: 0.578063, acc: 0.671875] [adversarial loss: 1.018310, acc: 0.250000]\n",
      "8117: [discriminator loss: 0.564433, acc: 0.726562] [adversarial loss: 1.292730, acc: 0.234375]\n",
      "8118: [discriminator loss: 0.561795, acc: 0.679688] [adversarial loss: 1.046365, acc: 0.234375]\n",
      "8119: [discriminator loss: 0.563785, acc: 0.648438] [adversarial loss: 1.332637, acc: 0.125000]\n",
      "8120: [discriminator loss: 0.532289, acc: 0.726562] [adversarial loss: 0.918746, acc: 0.343750]\n",
      "8121: [discriminator loss: 0.541492, acc: 0.750000] [adversarial loss: 1.608193, acc: 0.093750]\n",
      "8122: [discriminator loss: 0.617777, acc: 0.687500] [adversarial loss: 0.713287, acc: 0.515625]\n",
      "8123: [discriminator loss: 0.573914, acc: 0.671875] [adversarial loss: 1.477036, acc: 0.109375]\n",
      "8124: [discriminator loss: 0.584569, acc: 0.648438] [adversarial loss: 0.829113, acc: 0.406250]\n",
      "8125: [discriminator loss: 0.600769, acc: 0.695312] [adversarial loss: 1.307166, acc: 0.156250]\n",
      "8126: [discriminator loss: 0.504493, acc: 0.726562] [adversarial loss: 0.971738, acc: 0.343750]\n",
      "8127: [discriminator loss: 0.559005, acc: 0.703125] [adversarial loss: 1.470699, acc: 0.125000]\n",
      "8128: [discriminator loss: 0.520105, acc: 0.765625] [adversarial loss: 1.019347, acc: 0.375000]\n",
      "8129: [discriminator loss: 0.550576, acc: 0.687500] [adversarial loss: 1.238326, acc: 0.234375]\n",
      "8130: [discriminator loss: 0.535443, acc: 0.710938] [adversarial loss: 1.219489, acc: 0.250000]\n",
      "8131: [discriminator loss: 0.467725, acc: 0.750000] [adversarial loss: 1.267111, acc: 0.265625]\n",
      "8132: [discriminator loss: 0.501739, acc: 0.703125] [adversarial loss: 1.143634, acc: 0.296875]\n",
      "8133: [discriminator loss: 0.547821, acc: 0.679688] [adversarial loss: 1.027469, acc: 0.375000]\n",
      "8134: [discriminator loss: 0.532065, acc: 0.710938] [adversarial loss: 1.147739, acc: 0.250000]\n",
      "8135: [discriminator loss: 0.524059, acc: 0.703125] [adversarial loss: 1.190316, acc: 0.156250]\n",
      "8136: [discriminator loss: 0.484890, acc: 0.710938] [adversarial loss: 1.006806, acc: 0.328125]\n",
      "8137: [discriminator loss: 0.601161, acc: 0.671875] [adversarial loss: 1.180390, acc: 0.218750]\n",
      "8138: [discriminator loss: 0.491431, acc: 0.750000] [adversarial loss: 1.056906, acc: 0.328125]\n",
      "8139: [discriminator loss: 0.606588, acc: 0.656250] [adversarial loss: 1.132874, acc: 0.296875]\n",
      "8140: [discriminator loss: 0.589835, acc: 0.679688] [adversarial loss: 0.881951, acc: 0.421875]\n",
      "8141: [discriminator loss: 0.627154, acc: 0.648438] [adversarial loss: 1.446590, acc: 0.109375]\n",
      "8142: [discriminator loss: 0.513523, acc: 0.710938] [adversarial loss: 1.060865, acc: 0.390625]\n",
      "8143: [discriminator loss: 0.558826, acc: 0.718750] [adversarial loss: 1.442642, acc: 0.234375]\n",
      "8144: [discriminator loss: 0.527899, acc: 0.742188] [adversarial loss: 1.540805, acc: 0.156250]\n",
      "8145: [discriminator loss: 0.521801, acc: 0.726562] [adversarial loss: 0.790269, acc: 0.453125]\n",
      "8146: [discriminator loss: 0.578728, acc: 0.687500] [adversarial loss: 1.286623, acc: 0.171875]\n",
      "8147: [discriminator loss: 0.613037, acc: 0.648438] [adversarial loss: 0.915842, acc: 0.375000]\n",
      "8148: [discriminator loss: 0.614645, acc: 0.632812] [adversarial loss: 1.248893, acc: 0.171875]\n",
      "8149: [discriminator loss: 0.583406, acc: 0.703125] [adversarial loss: 0.976582, acc: 0.328125]\n",
      "8150: [discriminator loss: 0.517757, acc: 0.796875] [adversarial loss: 1.314275, acc: 0.156250]\n",
      "8151: [discriminator loss: 0.513081, acc: 0.726562] [adversarial loss: 0.915187, acc: 0.390625]\n",
      "8152: [discriminator loss: 0.594555, acc: 0.609375] [adversarial loss: 1.390657, acc: 0.140625]\n",
      "8153: [discriminator loss: 0.579253, acc: 0.679688] [adversarial loss: 0.905581, acc: 0.406250]\n",
      "8154: [discriminator loss: 0.630294, acc: 0.664062] [adversarial loss: 1.567354, acc: 0.093750]\n",
      "8155: [discriminator loss: 0.598833, acc: 0.625000] [adversarial loss: 0.977908, acc: 0.343750]\n",
      "8156: [discriminator loss: 0.556748, acc: 0.726562] [adversarial loss: 1.605857, acc: 0.078125]\n",
      "8157: [discriminator loss: 0.597504, acc: 0.718750] [adversarial loss: 1.064744, acc: 0.375000]\n",
      "8158: [discriminator loss: 0.512579, acc: 0.757812] [adversarial loss: 1.256284, acc: 0.156250]\n",
      "8159: [discriminator loss: 0.509827, acc: 0.718750] [adversarial loss: 1.216461, acc: 0.218750]\n",
      "8160: [discriminator loss: 0.580202, acc: 0.703125] [adversarial loss: 1.001882, acc: 0.265625]\n",
      "8161: [discriminator loss: 0.608172, acc: 0.648438] [adversarial loss: 1.212964, acc: 0.156250]\n",
      "8162: [discriminator loss: 0.563473, acc: 0.695312] [adversarial loss: 1.069365, acc: 0.343750]\n",
      "8163: [discriminator loss: 0.596595, acc: 0.640625] [adversarial loss: 1.128072, acc: 0.343750]\n",
      "8164: [discriminator loss: 0.557732, acc: 0.703125] [adversarial loss: 1.295681, acc: 0.125000]\n",
      "8165: [discriminator loss: 0.489700, acc: 0.765625] [adversarial loss: 1.263931, acc: 0.203125]\n",
      "8166: [discriminator loss: 0.519302, acc: 0.710938] [adversarial loss: 1.130713, acc: 0.281250]\n",
      "8167: [discriminator loss: 0.441388, acc: 0.820312] [adversarial loss: 1.227548, acc: 0.250000]\n",
      "8168: [discriminator loss: 0.624728, acc: 0.671875] [adversarial loss: 0.943625, acc: 0.328125]\n",
      "8169: [discriminator loss: 0.506932, acc: 0.742188] [adversarial loss: 1.303008, acc: 0.171875]\n",
      "8170: [discriminator loss: 0.490791, acc: 0.773438] [adversarial loss: 1.235569, acc: 0.218750]\n",
      "8171: [discriminator loss: 0.584665, acc: 0.679688] [adversarial loss: 1.221764, acc: 0.203125]\n",
      "8172: [discriminator loss: 0.500171, acc: 0.804688] [adversarial loss: 1.006562, acc: 0.328125]\n",
      "8173: [discriminator loss: 0.538658, acc: 0.734375] [adversarial loss: 1.104201, acc: 0.343750]\n",
      "8174: [discriminator loss: 0.609777, acc: 0.656250] [adversarial loss: 1.236593, acc: 0.234375]\n",
      "8175: [discriminator loss: 0.599728, acc: 0.703125] [adversarial loss: 1.188214, acc: 0.281250]\n",
      "8176: [discriminator loss: 0.542378, acc: 0.757812] [adversarial loss: 1.217750, acc: 0.265625]\n",
      "8177: [discriminator loss: 0.525688, acc: 0.757812] [adversarial loss: 1.218307, acc: 0.281250]\n",
      "8178: [discriminator loss: 0.603803, acc: 0.664062] [adversarial loss: 1.020016, acc: 0.359375]\n",
      "8179: [discriminator loss: 0.532693, acc: 0.765625] [adversarial loss: 1.449940, acc: 0.156250]\n",
      "8180: [discriminator loss: 0.566904, acc: 0.687500] [adversarial loss: 0.843580, acc: 0.437500]\n",
      "8181: [discriminator loss: 0.481295, acc: 0.734375] [adversarial loss: 1.501480, acc: 0.109375]\n",
      "8182: [discriminator loss: 0.565793, acc: 0.671875] [adversarial loss: 1.030698, acc: 0.312500]\n",
      "8183: [discriminator loss: 0.593579, acc: 0.710938] [adversarial loss: 1.359982, acc: 0.187500]\n",
      "8184: [discriminator loss: 0.599426, acc: 0.703125] [adversarial loss: 0.924027, acc: 0.421875]\n",
      "8185: [discriminator loss: 0.488086, acc: 0.789062] [adversarial loss: 1.239534, acc: 0.125000]\n",
      "8186: [discriminator loss: 0.560509, acc: 0.664062] [adversarial loss: 1.104249, acc: 0.234375]\n",
      "8187: [discriminator loss: 0.551551, acc: 0.726562] [adversarial loss: 1.125575, acc: 0.359375]\n",
      "8188: [discriminator loss: 0.539065, acc: 0.695312] [adversarial loss: 0.899728, acc: 0.406250]\n",
      "8189: [discriminator loss: 0.532508, acc: 0.726562] [adversarial loss: 1.276791, acc: 0.218750]\n",
      "8190: [discriminator loss: 0.545736, acc: 0.695312] [adversarial loss: 0.644776, acc: 0.656250]\n",
      "8191: [discriminator loss: 0.671298, acc: 0.625000] [adversarial loss: 1.647813, acc: 0.125000]\n",
      "8192: [discriminator loss: 0.498258, acc: 0.765625] [adversarial loss: 1.013593, acc: 0.343750]\n",
      "8193: [discriminator loss: 0.525206, acc: 0.726562] [adversarial loss: 1.041624, acc: 0.328125]\n",
      "8194: [discriminator loss: 0.555316, acc: 0.742188] [adversarial loss: 1.124317, acc: 0.187500]\n",
      "8195: [discriminator loss: 0.562591, acc: 0.695312] [adversarial loss: 1.194048, acc: 0.203125]\n",
      "8196: [discriminator loss: 0.487398, acc: 0.781250] [adversarial loss: 1.419979, acc: 0.125000]\n",
      "8197: [discriminator loss: 0.550881, acc: 0.695312] [adversarial loss: 0.870567, acc: 0.375000]\n",
      "8198: [discriminator loss: 0.580488, acc: 0.679688] [adversarial loss: 1.279044, acc: 0.187500]\n",
      "8199: [discriminator loss: 0.539119, acc: 0.734375] [adversarial loss: 0.906069, acc: 0.453125]\n",
      "8200: [discriminator loss: 0.486813, acc: 0.750000] [adversarial loss: 1.769504, acc: 0.078125]\n",
      "8201: [discriminator loss: 0.574660, acc: 0.671875] [adversarial loss: 1.028086, acc: 0.328125]\n",
      "8202: [discriminator loss: 0.500046, acc: 0.812500] [adversarial loss: 1.398798, acc: 0.171875]\n",
      "8203: [discriminator loss: 0.596797, acc: 0.687500] [adversarial loss: 0.877118, acc: 0.437500]\n",
      "8204: [discriminator loss: 0.518015, acc: 0.757812] [adversarial loss: 1.372116, acc: 0.203125]\n",
      "8205: [discriminator loss: 0.587299, acc: 0.687500] [adversarial loss: 1.131407, acc: 0.187500]\n",
      "8206: [discriminator loss: 0.526534, acc: 0.765625] [adversarial loss: 1.226095, acc: 0.156250]\n",
      "8207: [discriminator loss: 0.529650, acc: 0.687500] [adversarial loss: 1.157869, acc: 0.281250]\n",
      "8208: [discriminator loss: 0.528454, acc: 0.726562] [adversarial loss: 1.226220, acc: 0.203125]\n",
      "8209: [discriminator loss: 0.540192, acc: 0.710938] [adversarial loss: 1.018317, acc: 0.421875]\n",
      "8210: [discriminator loss: 0.485128, acc: 0.789062] [adversarial loss: 1.279314, acc: 0.156250]\n",
      "8211: [discriminator loss: 0.535357, acc: 0.710938] [adversarial loss: 1.061461, acc: 0.359375]\n",
      "8212: [discriminator loss: 0.506895, acc: 0.726562] [adversarial loss: 1.212263, acc: 0.140625]\n",
      "8213: [discriminator loss: 0.588688, acc: 0.656250] [adversarial loss: 1.164183, acc: 0.281250]\n",
      "8214: [discriminator loss: 0.612038, acc: 0.695312] [adversarial loss: 1.288173, acc: 0.187500]\n",
      "8215: [discriminator loss: 0.526719, acc: 0.773438] [adversarial loss: 1.115271, acc: 0.265625]\n",
      "8216: [discriminator loss: 0.493389, acc: 0.757812] [adversarial loss: 1.204563, acc: 0.187500]\n",
      "8217: [discriminator loss: 0.547249, acc: 0.695312] [adversarial loss: 1.162897, acc: 0.218750]\n",
      "8218: [discriminator loss: 0.534692, acc: 0.710938] [adversarial loss: 1.178527, acc: 0.171875]\n",
      "8219: [discriminator loss: 0.541710, acc: 0.718750] [adversarial loss: 1.311883, acc: 0.109375]\n",
      "8220: [discriminator loss: 0.537183, acc: 0.742188] [adversarial loss: 0.952152, acc: 0.375000]\n",
      "8221: [discriminator loss: 0.437965, acc: 0.804688] [adversarial loss: 1.292900, acc: 0.218750]\n",
      "8222: [discriminator loss: 0.494546, acc: 0.773438] [adversarial loss: 0.923031, acc: 0.359375]\n",
      "8223: [discriminator loss: 0.546189, acc: 0.671875] [adversarial loss: 1.394702, acc: 0.171875]\n",
      "8224: [discriminator loss: 0.530744, acc: 0.726562] [adversarial loss: 1.168843, acc: 0.312500]\n",
      "8225: [discriminator loss: 0.509180, acc: 0.703125] [adversarial loss: 1.129629, acc: 0.265625]\n",
      "8226: [discriminator loss: 0.592349, acc: 0.671875] [adversarial loss: 1.322723, acc: 0.125000]\n",
      "8227: [discriminator loss: 0.559278, acc: 0.687500] [adversarial loss: 0.845263, acc: 0.437500]\n",
      "8228: [discriminator loss: 0.610686, acc: 0.640625] [adversarial loss: 1.466466, acc: 0.156250]\n",
      "8229: [discriminator loss: 0.591514, acc: 0.625000] [adversarial loss: 1.066446, acc: 0.359375]\n",
      "8230: [discriminator loss: 0.638607, acc: 0.656250] [adversarial loss: 1.424385, acc: 0.125000]\n",
      "8231: [discriminator loss: 0.586713, acc: 0.687500] [adversarial loss: 0.920309, acc: 0.390625]\n",
      "8232: [discriminator loss: 0.594451, acc: 0.679688] [adversarial loss: 1.257387, acc: 0.203125]\n",
      "8233: [discriminator loss: 0.502422, acc: 0.734375] [adversarial loss: 1.100819, acc: 0.265625]\n",
      "8234: [discriminator loss: 0.548071, acc: 0.726562] [adversarial loss: 1.113013, acc: 0.234375]\n",
      "8235: [discriminator loss: 0.524092, acc: 0.781250] [adversarial loss: 0.975480, acc: 0.359375]\n",
      "8236: [discriminator loss: 0.543787, acc: 0.718750] [adversarial loss: 1.255584, acc: 0.203125]\n",
      "8237: [discriminator loss: 0.562974, acc: 0.726562] [adversarial loss: 1.000266, acc: 0.312500]\n",
      "8238: [discriminator loss: 0.549421, acc: 0.679688] [adversarial loss: 1.348342, acc: 0.125000]\n",
      "8239: [discriminator loss: 0.541240, acc: 0.726562] [adversarial loss: 1.039121, acc: 0.296875]\n",
      "8240: [discriminator loss: 0.525584, acc: 0.742188] [adversarial loss: 1.183400, acc: 0.125000]\n",
      "8241: [discriminator loss: 0.569204, acc: 0.671875] [adversarial loss: 1.083764, acc: 0.250000]\n",
      "8242: [discriminator loss: 0.489699, acc: 0.726562] [adversarial loss: 1.085612, acc: 0.296875]\n",
      "8243: [discriminator loss: 0.564771, acc: 0.703125] [adversarial loss: 1.137006, acc: 0.265625]\n",
      "8244: [discriminator loss: 0.577268, acc: 0.703125] [adversarial loss: 1.375334, acc: 0.171875]\n",
      "8245: [discriminator loss: 0.547792, acc: 0.664062] [adversarial loss: 1.046792, acc: 0.234375]\n",
      "8246: [discriminator loss: 0.477254, acc: 0.781250] [adversarial loss: 1.185842, acc: 0.234375]\n",
      "8247: [discriminator loss: 0.537928, acc: 0.703125] [adversarial loss: 1.467839, acc: 0.093750]\n",
      "8248: [discriminator loss: 0.614848, acc: 0.656250] [adversarial loss: 0.844799, acc: 0.453125]\n",
      "8249: [discriminator loss: 0.575652, acc: 0.679688] [adversarial loss: 1.578973, acc: 0.109375]\n",
      "8250: [discriminator loss: 0.608000, acc: 0.687500] [adversarial loss: 0.832831, acc: 0.421875]\n",
      "8251: [discriminator loss: 0.604338, acc: 0.648438] [adversarial loss: 1.121651, acc: 0.281250]\n",
      "8252: [discriminator loss: 0.538853, acc: 0.765625] [adversarial loss: 0.879969, acc: 0.453125]\n",
      "8253: [discriminator loss: 0.552701, acc: 0.750000] [adversarial loss: 1.266319, acc: 0.218750]\n",
      "8254: [discriminator loss: 0.521296, acc: 0.796875] [adversarial loss: 1.214775, acc: 0.234375]\n",
      "8255: [discriminator loss: 0.526261, acc: 0.718750] [adversarial loss: 1.079906, acc: 0.250000]\n",
      "8256: [discriminator loss: 0.509998, acc: 0.718750] [adversarial loss: 1.116998, acc: 0.203125]\n",
      "8257: [discriminator loss: 0.572972, acc: 0.734375] [adversarial loss: 0.808070, acc: 0.484375]\n",
      "8258: [discriminator loss: 0.517388, acc: 0.742188] [adversarial loss: 1.617308, acc: 0.062500]\n",
      "8259: [discriminator loss: 0.512738, acc: 0.726562] [adversarial loss: 1.210449, acc: 0.203125]\n",
      "8260: [discriminator loss: 0.588143, acc: 0.679688] [adversarial loss: 1.177919, acc: 0.125000]\n",
      "8261: [discriminator loss: 0.513109, acc: 0.742188] [adversarial loss: 1.088654, acc: 0.343750]\n",
      "8262: [discriminator loss: 0.609825, acc: 0.640625] [adversarial loss: 1.502297, acc: 0.093750]\n",
      "8263: [discriminator loss: 0.540915, acc: 0.718750] [adversarial loss: 0.939130, acc: 0.390625]\n",
      "8264: [discriminator loss: 0.546138, acc: 0.695312] [adversarial loss: 1.242222, acc: 0.234375]\n",
      "8265: [discriminator loss: 0.482822, acc: 0.718750] [adversarial loss: 0.932228, acc: 0.359375]\n",
      "8266: [discriminator loss: 0.581003, acc: 0.671875] [adversarial loss: 1.327152, acc: 0.171875]\n",
      "8267: [discriminator loss: 0.624754, acc: 0.625000] [adversarial loss: 0.988229, acc: 0.359375]\n",
      "8268: [discriminator loss: 0.578669, acc: 0.679688] [adversarial loss: 1.416380, acc: 0.171875]\n",
      "8269: [discriminator loss: 0.497626, acc: 0.765625] [adversarial loss: 1.138321, acc: 0.234375]\n",
      "8270: [discriminator loss: 0.560931, acc: 0.718750] [adversarial loss: 1.165358, acc: 0.234375]\n",
      "8271: [discriminator loss: 0.530787, acc: 0.750000] [adversarial loss: 1.130294, acc: 0.234375]\n",
      "8272: [discriminator loss: 0.545005, acc: 0.687500] [adversarial loss: 1.188787, acc: 0.187500]\n",
      "8273: [discriminator loss: 0.561494, acc: 0.718750] [adversarial loss: 0.955643, acc: 0.375000]\n",
      "8274: [discriminator loss: 0.536742, acc: 0.695312] [adversarial loss: 1.568469, acc: 0.109375]\n",
      "8275: [discriminator loss: 0.541324, acc: 0.742188] [adversarial loss: 0.936497, acc: 0.390625]\n",
      "8276: [discriminator loss: 0.547746, acc: 0.734375] [adversarial loss: 1.460721, acc: 0.140625]\n",
      "8277: [discriminator loss: 0.580337, acc: 0.664062] [adversarial loss: 0.783091, acc: 0.593750]\n",
      "8278: [discriminator loss: 0.610416, acc: 0.648438] [adversarial loss: 1.397011, acc: 0.218750]\n",
      "8279: [discriminator loss: 0.506364, acc: 0.750000] [adversarial loss: 1.104620, acc: 0.171875]\n",
      "8280: [discriminator loss: 0.480538, acc: 0.742188] [adversarial loss: 1.472218, acc: 0.125000]\n",
      "8281: [discriminator loss: 0.469484, acc: 0.757812] [adversarial loss: 1.088809, acc: 0.281250]\n",
      "8282: [discriminator loss: 0.513074, acc: 0.773438] [adversarial loss: 1.267014, acc: 0.234375]\n",
      "8283: [discriminator loss: 0.563878, acc: 0.703125] [adversarial loss: 0.951001, acc: 0.343750]\n",
      "8284: [discriminator loss: 0.505421, acc: 0.718750] [adversarial loss: 1.302113, acc: 0.203125]\n",
      "8285: [discriminator loss: 0.513876, acc: 0.750000] [adversarial loss: 1.085274, acc: 0.281250]\n",
      "8286: [discriminator loss: 0.461040, acc: 0.812500] [adversarial loss: 1.333542, acc: 0.156250]\n",
      "8287: [discriminator loss: 0.525268, acc: 0.742188] [adversarial loss: 0.821636, acc: 0.468750]\n",
      "8288: [discriminator loss: 0.561336, acc: 0.757812] [adversarial loss: 1.623862, acc: 0.109375]\n",
      "8289: [discriminator loss: 0.512919, acc: 0.757812] [adversarial loss: 0.948931, acc: 0.375000]\n",
      "8290: [discriminator loss: 0.539573, acc: 0.734375] [adversarial loss: 1.251106, acc: 0.234375]\n",
      "8291: [discriminator loss: 0.558054, acc: 0.710938] [adversarial loss: 1.089362, acc: 0.281250]\n",
      "8292: [discriminator loss: 0.586751, acc: 0.718750] [adversarial loss: 1.288176, acc: 0.234375]\n",
      "8293: [discriminator loss: 0.507478, acc: 0.695312] [adversarial loss: 1.128587, acc: 0.250000]\n",
      "8294: [discriminator loss: 0.535912, acc: 0.726562] [adversarial loss: 1.090608, acc: 0.250000]\n",
      "8295: [discriminator loss: 0.574518, acc: 0.734375] [adversarial loss: 1.422554, acc: 0.140625]\n",
      "8296: [discriminator loss: 0.502069, acc: 0.726562] [adversarial loss: 1.226012, acc: 0.171875]\n",
      "8297: [discriminator loss: 0.530434, acc: 0.734375] [adversarial loss: 1.105288, acc: 0.343750]\n",
      "8298: [discriminator loss: 0.520405, acc: 0.742188] [adversarial loss: 1.057084, acc: 0.359375]\n",
      "8299: [discriminator loss: 0.560591, acc: 0.726562] [adversarial loss: 1.181690, acc: 0.250000]\n",
      "8300: [discriminator loss: 0.610190, acc: 0.656250] [adversarial loss: 0.897354, acc: 0.437500]\n",
      "8301: [discriminator loss: 0.577613, acc: 0.734375] [adversarial loss: 1.483552, acc: 0.140625]\n",
      "8302: [discriminator loss: 0.582749, acc: 0.664062] [adversarial loss: 1.213590, acc: 0.187500]\n",
      "8303: [discriminator loss: 0.512046, acc: 0.734375] [adversarial loss: 1.101408, acc: 0.234375]\n",
      "8304: [discriminator loss: 0.549049, acc: 0.750000] [adversarial loss: 1.139849, acc: 0.281250]\n",
      "8305: [discriminator loss: 0.545007, acc: 0.742188] [adversarial loss: 1.172801, acc: 0.250000]\n",
      "8306: [discriminator loss: 0.577472, acc: 0.632812] [adversarial loss: 1.482700, acc: 0.109375]\n",
      "8307: [discriminator loss: 0.613519, acc: 0.632812] [adversarial loss: 0.773217, acc: 0.500000]\n",
      "8308: [discriminator loss: 0.554529, acc: 0.695312] [adversarial loss: 1.353540, acc: 0.140625]\n",
      "8309: [discriminator loss: 0.488232, acc: 0.757812] [adversarial loss: 1.066671, acc: 0.312500]\n",
      "8310: [discriminator loss: 0.524549, acc: 0.710938] [adversarial loss: 1.570007, acc: 0.125000]\n",
      "8311: [discriminator loss: 0.534695, acc: 0.734375] [adversarial loss: 0.740664, acc: 0.515625]\n",
      "8312: [discriminator loss: 0.614813, acc: 0.656250] [adversarial loss: 1.521983, acc: 0.109375]\n",
      "8313: [discriminator loss: 0.573362, acc: 0.664062] [adversarial loss: 1.095665, acc: 0.296875]\n",
      "8314: [discriminator loss: 0.564648, acc: 0.671875] [adversarial loss: 1.308911, acc: 0.187500]\n",
      "8315: [discriminator loss: 0.551017, acc: 0.734375] [adversarial loss: 0.929163, acc: 0.421875]\n",
      "8316: [discriminator loss: 0.509396, acc: 0.781250] [adversarial loss: 1.277876, acc: 0.125000]\n",
      "8317: [discriminator loss: 0.501564, acc: 0.703125] [adversarial loss: 1.118979, acc: 0.250000]\n",
      "8318: [discriminator loss: 0.574050, acc: 0.648438] [adversarial loss: 1.231364, acc: 0.218750]\n",
      "8319: [discriminator loss: 0.573483, acc: 0.671875] [adversarial loss: 1.012570, acc: 0.328125]\n",
      "8320: [discriminator loss: 0.569592, acc: 0.695312] [adversarial loss: 1.272404, acc: 0.140625]\n",
      "8321: [discriminator loss: 0.541094, acc: 0.742188] [adversarial loss: 1.095060, acc: 0.203125]\n",
      "8322: [discriminator loss: 0.587578, acc: 0.640625] [adversarial loss: 1.430678, acc: 0.109375]\n",
      "8323: [discriminator loss: 0.522420, acc: 0.734375] [adversarial loss: 1.049189, acc: 0.265625]\n",
      "8324: [discriminator loss: 0.641728, acc: 0.640625] [adversarial loss: 1.071218, acc: 0.265625]\n",
      "8325: [discriminator loss: 0.587469, acc: 0.687500] [adversarial loss: 1.006456, acc: 0.312500]\n",
      "8326: [discriminator loss: 0.540230, acc: 0.710938] [adversarial loss: 1.639328, acc: 0.109375]\n",
      "8327: [discriminator loss: 0.590306, acc: 0.679688] [adversarial loss: 1.046503, acc: 0.328125]\n",
      "8328: [discriminator loss: 0.576535, acc: 0.703125] [adversarial loss: 1.488891, acc: 0.156250]\n",
      "8329: [discriminator loss: 0.572475, acc: 0.718750] [adversarial loss: 1.097566, acc: 0.328125]\n",
      "8330: [discriminator loss: 0.506361, acc: 0.742188] [adversarial loss: 1.375723, acc: 0.156250]\n",
      "8331: [discriminator loss: 0.552846, acc: 0.710938] [adversarial loss: 1.344360, acc: 0.187500]\n",
      "8332: [discriminator loss: 0.510746, acc: 0.750000] [adversarial loss: 1.155583, acc: 0.234375]\n",
      "8333: [discriminator loss: 0.563436, acc: 0.726562] [adversarial loss: 1.291766, acc: 0.234375]\n",
      "8334: [discriminator loss: 0.540802, acc: 0.734375] [adversarial loss: 1.289719, acc: 0.140625]\n",
      "8335: [discriminator loss: 0.542662, acc: 0.703125] [adversarial loss: 0.993687, acc: 0.250000]\n",
      "8336: [discriminator loss: 0.566063, acc: 0.640625] [adversarial loss: 1.091457, acc: 0.343750]\n",
      "8337: [discriminator loss: 0.609243, acc: 0.695312] [adversarial loss: 1.128368, acc: 0.250000]\n",
      "8338: [discriminator loss: 0.463739, acc: 0.796875] [adversarial loss: 1.542146, acc: 0.125000]\n",
      "8339: [discriminator loss: 0.558612, acc: 0.718750] [adversarial loss: 1.194250, acc: 0.234375]\n",
      "8340: [discriminator loss: 0.583021, acc: 0.718750] [adversarial loss: 1.333768, acc: 0.187500]\n",
      "8341: [discriminator loss: 0.517638, acc: 0.757812] [adversarial loss: 1.192507, acc: 0.187500]\n",
      "8342: [discriminator loss: 0.595676, acc: 0.671875] [adversarial loss: 1.245789, acc: 0.203125]\n",
      "8343: [discriminator loss: 0.548380, acc: 0.648438] [adversarial loss: 1.407486, acc: 0.203125]\n",
      "8344: [discriminator loss: 0.592398, acc: 0.695312] [adversarial loss: 1.037536, acc: 0.234375]\n",
      "8345: [discriminator loss: 0.549265, acc: 0.703125] [adversarial loss: 1.306751, acc: 0.140625]\n",
      "8346: [discriminator loss: 0.646267, acc: 0.640625] [adversarial loss: 0.997050, acc: 0.312500]\n",
      "8347: [discriminator loss: 0.587671, acc: 0.710938] [adversarial loss: 1.166247, acc: 0.265625]\n",
      "8348: [discriminator loss: 0.531471, acc: 0.750000] [adversarial loss: 1.221019, acc: 0.171875]\n",
      "8349: [discriminator loss: 0.499409, acc: 0.757812] [adversarial loss: 1.354696, acc: 0.125000]\n",
      "8350: [discriminator loss: 0.546569, acc: 0.695312] [adversarial loss: 1.015350, acc: 0.375000]\n",
      "8351: [discriminator loss: 0.543319, acc: 0.734375] [adversarial loss: 1.399679, acc: 0.078125]\n",
      "8352: [discriminator loss: 0.553776, acc: 0.695312] [adversarial loss: 1.030343, acc: 0.296875]\n",
      "8353: [discriminator loss: 0.555399, acc: 0.757812] [adversarial loss: 1.326740, acc: 0.203125]\n",
      "8354: [discriminator loss: 0.509022, acc: 0.750000] [adversarial loss: 1.237732, acc: 0.187500]\n",
      "8355: [discriminator loss: 0.499578, acc: 0.773438] [adversarial loss: 1.086372, acc: 0.281250]\n",
      "8356: [discriminator loss: 0.487462, acc: 0.781250] [adversarial loss: 1.102513, acc: 0.296875]\n",
      "8357: [discriminator loss: 0.504916, acc: 0.757812] [adversarial loss: 1.144461, acc: 0.265625]\n",
      "8358: [discriminator loss: 0.513483, acc: 0.718750] [adversarial loss: 1.212402, acc: 0.187500]\n",
      "8359: [discriminator loss: 0.538954, acc: 0.703125] [adversarial loss: 1.014907, acc: 0.375000]\n",
      "8360: [discriminator loss: 0.597450, acc: 0.687500] [adversarial loss: 1.231213, acc: 0.343750]\n",
      "8361: [discriminator loss: 0.569138, acc: 0.695312] [adversarial loss: 1.266215, acc: 0.234375]\n",
      "8362: [discriminator loss: 0.591338, acc: 0.656250] [adversarial loss: 0.808141, acc: 0.500000]\n",
      "8363: [discriminator loss: 0.497047, acc: 0.750000] [adversarial loss: 1.496918, acc: 0.109375]\n",
      "8364: [discriminator loss: 0.586474, acc: 0.679688] [adversarial loss: 0.892414, acc: 0.406250]\n",
      "8365: [discriminator loss: 0.521999, acc: 0.757812] [adversarial loss: 1.175887, acc: 0.250000]\n",
      "8366: [discriminator loss: 0.556508, acc: 0.687500] [adversarial loss: 1.318129, acc: 0.156250]\n",
      "8367: [discriminator loss: 0.585297, acc: 0.664062] [adversarial loss: 0.880608, acc: 0.406250]\n",
      "8368: [discriminator loss: 0.576974, acc: 0.664062] [adversarial loss: 1.477453, acc: 0.109375]\n",
      "8369: [discriminator loss: 0.578166, acc: 0.710938] [adversarial loss: 1.024119, acc: 0.421875]\n",
      "8370: [discriminator loss: 0.563263, acc: 0.671875] [adversarial loss: 1.186602, acc: 0.296875]\n",
      "8371: [discriminator loss: 0.491975, acc: 0.734375] [adversarial loss: 0.878670, acc: 0.406250]\n",
      "8372: [discriminator loss: 0.486005, acc: 0.781250] [adversarial loss: 1.241053, acc: 0.171875]\n",
      "8373: [discriminator loss: 0.574801, acc: 0.703125] [adversarial loss: 1.157029, acc: 0.234375]\n",
      "8374: [discriminator loss: 0.525839, acc: 0.757812] [adversarial loss: 1.330258, acc: 0.156250]\n",
      "8375: [discriminator loss: 0.526004, acc: 0.703125] [adversarial loss: 0.797499, acc: 0.531250]\n",
      "8376: [discriminator loss: 0.555898, acc: 0.710938] [adversarial loss: 1.422467, acc: 0.156250]\n",
      "8377: [discriminator loss: 0.644516, acc: 0.601562] [adversarial loss: 0.786646, acc: 0.546875]\n",
      "8378: [discriminator loss: 0.581780, acc: 0.703125] [adversarial loss: 1.536704, acc: 0.125000]\n",
      "8379: [discriminator loss: 0.565582, acc: 0.687500] [adversarial loss: 0.831593, acc: 0.515625]\n",
      "8380: [discriminator loss: 0.634520, acc: 0.648438] [adversarial loss: 1.533889, acc: 0.125000]\n",
      "8381: [discriminator loss: 0.568042, acc: 0.695312] [adversarial loss: 0.917059, acc: 0.328125]\n",
      "8382: [discriminator loss: 0.510865, acc: 0.781250] [adversarial loss: 1.237586, acc: 0.203125]\n",
      "8383: [discriminator loss: 0.515364, acc: 0.750000] [adversarial loss: 1.050413, acc: 0.312500]\n",
      "8384: [discriminator loss: 0.549518, acc: 0.718750] [adversarial loss: 1.217890, acc: 0.203125]\n",
      "8385: [discriminator loss: 0.602010, acc: 0.687500] [adversarial loss: 1.340362, acc: 0.187500]\n",
      "8386: [discriminator loss: 0.543935, acc: 0.734375] [adversarial loss: 1.011143, acc: 0.312500]\n",
      "8387: [discriminator loss: 0.554254, acc: 0.734375] [adversarial loss: 1.097970, acc: 0.187500]\n",
      "8388: [discriminator loss: 0.518697, acc: 0.765625] [adversarial loss: 1.205934, acc: 0.265625]\n",
      "8389: [discriminator loss: 0.535231, acc: 0.679688] [adversarial loss: 0.907774, acc: 0.359375]\n",
      "8390: [discriminator loss: 0.595861, acc: 0.695312] [adversarial loss: 1.318262, acc: 0.203125]\n",
      "8391: [discriminator loss: 0.511649, acc: 0.773438] [adversarial loss: 1.231871, acc: 0.125000]\n",
      "8392: [discriminator loss: 0.544207, acc: 0.710938] [adversarial loss: 1.065665, acc: 0.343750]\n",
      "8393: [discriminator loss: 0.575412, acc: 0.687500] [adversarial loss: 1.091442, acc: 0.265625]\n",
      "8394: [discriminator loss: 0.564914, acc: 0.710938] [adversarial loss: 1.150211, acc: 0.265625]\n",
      "8395: [discriminator loss: 0.543721, acc: 0.703125] [adversarial loss: 1.232579, acc: 0.218750]\n",
      "8396: [discriminator loss: 0.543714, acc: 0.695312] [adversarial loss: 1.027455, acc: 0.343750]\n",
      "8397: [discriminator loss: 0.576536, acc: 0.734375] [adversarial loss: 1.319680, acc: 0.156250]\n",
      "8398: [discriminator loss: 0.562289, acc: 0.703125] [adversarial loss: 1.153974, acc: 0.218750]\n",
      "8399: [discriminator loss: 0.562551, acc: 0.695312] [adversarial loss: 1.423027, acc: 0.156250]\n",
      "8400: [discriminator loss: 0.509560, acc: 0.781250] [adversarial loss: 1.073258, acc: 0.265625]\n",
      "8401: [discriminator loss: 0.607256, acc: 0.703125] [adversarial loss: 1.267093, acc: 0.265625]\n",
      "8402: [discriminator loss: 0.493404, acc: 0.750000] [adversarial loss: 1.043915, acc: 0.328125]\n",
      "8403: [discriminator loss: 0.519869, acc: 0.726562] [adversarial loss: 1.129229, acc: 0.265625]\n",
      "8404: [discriminator loss: 0.550861, acc: 0.703125] [adversarial loss: 0.849669, acc: 0.468750]\n",
      "8405: [discriminator loss: 0.475507, acc: 0.765625] [adversarial loss: 1.532436, acc: 0.140625]\n",
      "8406: [discriminator loss: 0.495275, acc: 0.710938] [adversarial loss: 1.248729, acc: 0.203125]\n",
      "8407: [discriminator loss: 0.500296, acc: 0.765625] [adversarial loss: 1.243515, acc: 0.250000]\n",
      "8408: [discriminator loss: 0.553240, acc: 0.718750] [adversarial loss: 1.110466, acc: 0.250000]\n",
      "8409: [discriminator loss: 0.585885, acc: 0.718750] [adversarial loss: 1.367262, acc: 0.187500]\n",
      "8410: [discriminator loss: 0.545950, acc: 0.703125] [adversarial loss: 1.111053, acc: 0.187500]\n",
      "8411: [discriminator loss: 0.573230, acc: 0.695312] [adversarial loss: 1.026062, acc: 0.375000]\n",
      "8412: [discriminator loss: 0.510582, acc: 0.773438] [adversarial loss: 1.165990, acc: 0.234375]\n",
      "8413: [discriminator loss: 0.564037, acc: 0.695312] [adversarial loss: 1.202129, acc: 0.187500]\n",
      "8414: [discriminator loss: 0.550789, acc: 0.687500] [adversarial loss: 1.196702, acc: 0.265625]\n",
      "8415: [discriminator loss: 0.525994, acc: 0.726562] [adversarial loss: 0.802200, acc: 0.421875]\n",
      "8416: [discriminator loss: 0.613034, acc: 0.648438] [adversarial loss: 1.481784, acc: 0.171875]\n",
      "8417: [discriminator loss: 0.587755, acc: 0.656250] [adversarial loss: 0.942365, acc: 0.406250]\n",
      "8418: [discriminator loss: 0.557731, acc: 0.718750] [adversarial loss: 1.228660, acc: 0.156250]\n",
      "8419: [discriminator loss: 0.550560, acc: 0.742188] [adversarial loss: 1.134341, acc: 0.312500]\n",
      "8420: [discriminator loss: 0.581303, acc: 0.703125] [adversarial loss: 1.197117, acc: 0.265625]\n",
      "8421: [discriminator loss: 0.521510, acc: 0.718750] [adversarial loss: 1.004970, acc: 0.328125]\n",
      "8422: [discriminator loss: 0.532148, acc: 0.750000] [adversarial loss: 1.152748, acc: 0.203125]\n",
      "8423: [discriminator loss: 0.601253, acc: 0.710938] [adversarial loss: 1.493212, acc: 0.187500]\n",
      "8424: [discriminator loss: 0.572643, acc: 0.718750] [adversarial loss: 0.898012, acc: 0.406250]\n",
      "8425: [discriminator loss: 0.535668, acc: 0.695312] [adversarial loss: 1.531052, acc: 0.109375]\n",
      "8426: [discriminator loss: 0.632169, acc: 0.648438] [adversarial loss: 1.057450, acc: 0.375000]\n",
      "8427: [discriminator loss: 0.537763, acc: 0.687500] [adversarial loss: 1.206105, acc: 0.250000]\n",
      "8428: [discriminator loss: 0.626525, acc: 0.640625] [adversarial loss: 1.191676, acc: 0.203125]\n",
      "8429: [discriminator loss: 0.523151, acc: 0.710938] [adversarial loss: 1.218768, acc: 0.140625]\n",
      "8430: [discriminator loss: 0.562436, acc: 0.710938] [adversarial loss: 0.835183, acc: 0.453125]\n",
      "8431: [discriminator loss: 0.484599, acc: 0.742188] [adversarial loss: 1.162262, acc: 0.234375]\n",
      "8432: [discriminator loss: 0.569457, acc: 0.687500] [adversarial loss: 1.023067, acc: 0.359375]\n",
      "8433: [discriminator loss: 0.583582, acc: 0.718750] [adversarial loss: 1.265464, acc: 0.250000]\n",
      "8434: [discriminator loss: 0.581806, acc: 0.718750] [adversarial loss: 1.174600, acc: 0.265625]\n",
      "8435: [discriminator loss: 0.547119, acc: 0.695312] [adversarial loss: 1.191071, acc: 0.218750]\n",
      "8436: [discriminator loss: 0.542046, acc: 0.718750] [adversarial loss: 1.199531, acc: 0.234375]\n",
      "8437: [discriminator loss: 0.548185, acc: 0.757812] [adversarial loss: 0.997936, acc: 0.343750]\n",
      "8438: [discriminator loss: 0.436570, acc: 0.828125] [adversarial loss: 1.274000, acc: 0.140625]\n",
      "8439: [discriminator loss: 0.566224, acc: 0.695312] [adversarial loss: 1.074610, acc: 0.343750]\n",
      "8440: [discriminator loss: 0.583410, acc: 0.703125] [adversarial loss: 1.373960, acc: 0.250000]\n",
      "8441: [discriminator loss: 0.575602, acc: 0.632812] [adversarial loss: 0.913315, acc: 0.375000]\n",
      "8442: [discriminator loss: 0.509228, acc: 0.718750] [adversarial loss: 1.497599, acc: 0.109375]\n",
      "8443: [discriminator loss: 0.593739, acc: 0.679688] [adversarial loss: 0.937175, acc: 0.406250]\n",
      "8444: [discriminator loss: 0.514970, acc: 0.734375] [adversarial loss: 1.469617, acc: 0.156250]\n",
      "8445: [discriminator loss: 0.615847, acc: 0.679688] [adversarial loss: 1.106599, acc: 0.343750]\n",
      "8446: [discriminator loss: 0.514343, acc: 0.757812] [adversarial loss: 1.328426, acc: 0.171875]\n",
      "8447: [discriminator loss: 0.553096, acc: 0.695312] [adversarial loss: 0.879627, acc: 0.421875]\n",
      "8448: [discriminator loss: 0.539329, acc: 0.726562] [adversarial loss: 1.599377, acc: 0.125000]\n",
      "8449: [discriminator loss: 0.555817, acc: 0.710938] [adversarial loss: 1.032753, acc: 0.312500]\n",
      "8450: [discriminator loss: 0.605894, acc: 0.695312] [adversarial loss: 1.462589, acc: 0.093750]\n",
      "8451: [discriminator loss: 0.542748, acc: 0.671875] [adversarial loss: 1.063463, acc: 0.296875]\n",
      "8452: [discriminator loss: 0.570918, acc: 0.664062] [adversarial loss: 1.272712, acc: 0.281250]\n",
      "8453: [discriminator loss: 0.535270, acc: 0.695312] [adversarial loss: 1.184280, acc: 0.171875]\n",
      "8454: [discriminator loss: 0.530800, acc: 0.781250] [adversarial loss: 1.143347, acc: 0.250000]\n",
      "8455: [discriminator loss: 0.497543, acc: 0.742188] [adversarial loss: 1.108720, acc: 0.218750]\n",
      "8456: [discriminator loss: 0.585497, acc: 0.656250] [adversarial loss: 1.230892, acc: 0.218750]\n",
      "8457: [discriminator loss: 0.571407, acc: 0.695312] [adversarial loss: 1.444809, acc: 0.109375]\n",
      "8458: [discriminator loss: 0.481676, acc: 0.757812] [adversarial loss: 1.154535, acc: 0.218750]\n",
      "8459: [discriminator loss: 0.511341, acc: 0.703125] [adversarial loss: 1.149694, acc: 0.343750]\n",
      "8460: [discriminator loss: 0.578384, acc: 0.703125] [adversarial loss: 1.223056, acc: 0.296875]\n",
      "8461: [discriminator loss: 0.525333, acc: 0.765625] [adversarial loss: 1.305917, acc: 0.218750]\n",
      "8462: [discriminator loss: 0.526788, acc: 0.734375] [adversarial loss: 0.961524, acc: 0.328125]\n",
      "8463: [discriminator loss: 0.582446, acc: 0.679688] [adversarial loss: 1.375100, acc: 0.093750]\n",
      "8464: [discriminator loss: 0.597955, acc: 0.648438] [adversarial loss: 0.866305, acc: 0.328125]\n",
      "8465: [discriminator loss: 0.542678, acc: 0.718750] [adversarial loss: 1.303300, acc: 0.234375]\n",
      "8466: [discriminator loss: 0.570339, acc: 0.679688] [adversarial loss: 0.993085, acc: 0.375000]\n",
      "8467: [discriminator loss: 0.502485, acc: 0.789062] [adversarial loss: 1.395547, acc: 0.156250]\n",
      "8468: [discriminator loss: 0.577370, acc: 0.710938] [adversarial loss: 0.934917, acc: 0.406250]\n",
      "8469: [discriminator loss: 0.529910, acc: 0.726562] [adversarial loss: 1.242444, acc: 0.187500]\n",
      "8470: [discriminator loss: 0.491791, acc: 0.828125] [adversarial loss: 1.228203, acc: 0.156250]\n",
      "8471: [discriminator loss: 0.539832, acc: 0.734375] [adversarial loss: 0.927254, acc: 0.375000]\n",
      "8472: [discriminator loss: 0.590542, acc: 0.664062] [adversarial loss: 1.191881, acc: 0.203125]\n",
      "8473: [discriminator loss: 0.532003, acc: 0.750000] [adversarial loss: 0.988381, acc: 0.312500]\n",
      "8474: [discriminator loss: 0.592870, acc: 0.703125] [adversarial loss: 1.424769, acc: 0.109375]\n",
      "8475: [discriminator loss: 0.537605, acc: 0.757812] [adversarial loss: 1.071121, acc: 0.281250]\n",
      "8476: [discriminator loss: 0.554729, acc: 0.710938] [adversarial loss: 1.460837, acc: 0.156250]\n",
      "8477: [discriminator loss: 0.642618, acc: 0.617188] [adversarial loss: 0.743405, acc: 0.578125]\n",
      "8478: [discriminator loss: 0.538859, acc: 0.679688] [adversarial loss: 1.340029, acc: 0.078125]\n",
      "8479: [discriminator loss: 0.577315, acc: 0.664062] [adversarial loss: 0.953162, acc: 0.390625]\n",
      "8480: [discriminator loss: 0.591052, acc: 0.687500] [adversarial loss: 1.315493, acc: 0.156250]\n",
      "8481: [discriminator loss: 0.501730, acc: 0.718750] [adversarial loss: 1.259793, acc: 0.125000]\n",
      "8482: [discriminator loss: 0.606337, acc: 0.625000] [adversarial loss: 1.073677, acc: 0.234375]\n",
      "8483: [discriminator loss: 0.531894, acc: 0.734375] [adversarial loss: 1.296885, acc: 0.218750]\n",
      "8484: [discriminator loss: 0.578678, acc: 0.656250] [adversarial loss: 0.924809, acc: 0.328125]\n",
      "8485: [discriminator loss: 0.574580, acc: 0.687500] [adversarial loss: 1.471802, acc: 0.093750]\n",
      "8486: [discriminator loss: 0.536119, acc: 0.750000] [adversarial loss: 0.851187, acc: 0.390625]\n",
      "8487: [discriminator loss: 0.572115, acc: 0.687500] [adversarial loss: 1.519063, acc: 0.109375]\n",
      "8488: [discriminator loss: 0.549814, acc: 0.734375] [adversarial loss: 1.050615, acc: 0.281250]\n",
      "8489: [discriminator loss: 0.533895, acc: 0.757812] [adversarial loss: 1.165816, acc: 0.234375]\n",
      "8490: [discriminator loss: 0.494573, acc: 0.757812] [adversarial loss: 1.456367, acc: 0.125000]\n",
      "8491: [discriminator loss: 0.596377, acc: 0.671875] [adversarial loss: 1.026183, acc: 0.375000]\n",
      "8492: [discriminator loss: 0.531952, acc: 0.695312] [adversarial loss: 1.208976, acc: 0.156250]\n",
      "8493: [discriminator loss: 0.554517, acc: 0.734375] [adversarial loss: 1.122669, acc: 0.281250]\n",
      "8494: [discriminator loss: 0.492610, acc: 0.757812] [adversarial loss: 1.086880, acc: 0.265625]\n",
      "8495: [discriminator loss: 0.495394, acc: 0.750000] [adversarial loss: 1.310181, acc: 0.234375]\n",
      "8496: [discriminator loss: 0.635212, acc: 0.640625] [adversarial loss: 0.848518, acc: 0.453125]\n",
      "8497: [discriminator loss: 0.648288, acc: 0.648438] [adversarial loss: 1.422484, acc: 0.140625]\n",
      "8498: [discriminator loss: 0.559777, acc: 0.656250] [adversarial loss: 0.898489, acc: 0.484375]\n",
      "8499: [discriminator loss: 0.543263, acc: 0.734375] [adversarial loss: 1.260744, acc: 0.203125]\n",
      "8500: [discriminator loss: 0.524836, acc: 0.734375] [adversarial loss: 1.216109, acc: 0.250000]\n",
      "8501: [discriminator loss: 0.612560, acc: 0.679688] [adversarial loss: 1.208406, acc: 0.171875]\n",
      "8502: [discriminator loss: 0.529432, acc: 0.687500] [adversarial loss: 1.064678, acc: 0.234375]\n",
      "8503: [discriminator loss: 0.536300, acc: 0.742188] [adversarial loss: 1.250431, acc: 0.109375]\n",
      "8504: [discriminator loss: 0.586327, acc: 0.718750] [adversarial loss: 1.212266, acc: 0.203125]\n",
      "8505: [discriminator loss: 0.580985, acc: 0.679688] [adversarial loss: 1.224976, acc: 0.171875]\n",
      "8506: [discriminator loss: 0.521272, acc: 0.750000] [adversarial loss: 0.900092, acc: 0.375000]\n",
      "8507: [discriminator loss: 0.550397, acc: 0.648438] [adversarial loss: 1.053667, acc: 0.187500]\n",
      "8508: [discriminator loss: 0.542368, acc: 0.656250] [adversarial loss: 1.099512, acc: 0.312500]\n",
      "8509: [discriminator loss: 0.604484, acc: 0.617188] [adversarial loss: 1.460545, acc: 0.078125]\n",
      "8510: [discriminator loss: 0.524040, acc: 0.703125] [adversarial loss: 1.017787, acc: 0.250000]\n",
      "8511: [discriminator loss: 0.584428, acc: 0.664062] [adversarial loss: 1.331367, acc: 0.156250]\n",
      "8512: [discriminator loss: 0.559862, acc: 0.742188] [adversarial loss: 1.159151, acc: 0.250000]\n",
      "8513: [discriminator loss: 0.491545, acc: 0.781250] [adversarial loss: 1.352244, acc: 0.171875]\n",
      "8514: [discriminator loss: 0.548723, acc: 0.695312] [adversarial loss: 1.096519, acc: 0.265625]\n",
      "8515: [discriminator loss: 0.553230, acc: 0.710938] [adversarial loss: 1.305461, acc: 0.125000]\n",
      "8516: [discriminator loss: 0.480504, acc: 0.781250] [adversarial loss: 0.997940, acc: 0.359375]\n",
      "8517: [discriminator loss: 0.470140, acc: 0.796875] [adversarial loss: 1.417931, acc: 0.109375]\n",
      "8518: [discriminator loss: 0.569800, acc: 0.687500] [adversarial loss: 0.751808, acc: 0.484375]\n",
      "8519: [discriminator loss: 0.618934, acc: 0.671875] [adversarial loss: 1.332285, acc: 0.171875]\n",
      "8520: [discriminator loss: 0.510040, acc: 0.781250] [adversarial loss: 0.934339, acc: 0.328125]\n",
      "8521: [discriminator loss: 0.527420, acc: 0.710938] [adversarial loss: 1.290352, acc: 0.156250]\n",
      "8522: [discriminator loss: 0.546842, acc: 0.718750] [adversarial loss: 1.363454, acc: 0.218750]\n",
      "8523: [discriminator loss: 0.601391, acc: 0.671875] [adversarial loss: 1.203081, acc: 0.171875]\n",
      "8524: [discriminator loss: 0.505963, acc: 0.789062] [adversarial loss: 1.369783, acc: 0.156250]\n",
      "8525: [discriminator loss: 0.582454, acc: 0.718750] [adversarial loss: 0.901780, acc: 0.359375]\n",
      "8526: [discriminator loss: 0.480239, acc: 0.789062] [adversarial loss: 1.590638, acc: 0.109375]\n",
      "8527: [discriminator loss: 0.516189, acc: 0.710938] [adversarial loss: 0.945055, acc: 0.390625]\n",
      "8528: [discriminator loss: 0.516984, acc: 0.750000] [adversarial loss: 1.150879, acc: 0.312500]\n",
      "8529: [discriminator loss: 0.659725, acc: 0.640625] [adversarial loss: 1.068237, acc: 0.250000]\n",
      "8530: [discriminator loss: 0.635454, acc: 0.648438] [adversarial loss: 1.357725, acc: 0.125000]\n",
      "8531: [discriminator loss: 0.612756, acc: 0.656250] [adversarial loss: 1.029761, acc: 0.296875]\n",
      "8532: [discriminator loss: 0.623549, acc: 0.703125] [adversarial loss: 1.014415, acc: 0.281250]\n",
      "8533: [discriminator loss: 0.594602, acc: 0.640625] [adversarial loss: 0.953611, acc: 0.375000]\n",
      "8534: [discriminator loss: 0.538845, acc: 0.726562] [adversarial loss: 1.316483, acc: 0.187500]\n",
      "8535: [discriminator loss: 0.568017, acc: 0.734375] [adversarial loss: 1.014036, acc: 0.312500]\n",
      "8536: [discriminator loss: 0.570636, acc: 0.679688] [adversarial loss: 1.232534, acc: 0.203125]\n",
      "8537: [discriminator loss: 0.566310, acc: 0.750000] [adversarial loss: 0.822311, acc: 0.406250]\n",
      "8538: [discriminator loss: 0.560958, acc: 0.742188] [adversarial loss: 1.175881, acc: 0.218750]\n",
      "8539: [discriminator loss: 0.573545, acc: 0.710938] [adversarial loss: 0.823396, acc: 0.437500]\n",
      "8540: [discriminator loss: 0.538790, acc: 0.734375] [adversarial loss: 1.342104, acc: 0.218750]\n",
      "8541: [discriminator loss: 0.501504, acc: 0.750000] [adversarial loss: 1.313227, acc: 0.171875]\n",
      "8542: [discriminator loss: 0.547851, acc: 0.710938] [adversarial loss: 1.240111, acc: 0.203125]\n",
      "8543: [discriminator loss: 0.528901, acc: 0.765625] [adversarial loss: 1.008309, acc: 0.359375]\n",
      "8544: [discriminator loss: 0.543211, acc: 0.718750] [adversarial loss: 1.334927, acc: 0.187500]\n",
      "8545: [discriminator loss: 0.562452, acc: 0.718750] [adversarial loss: 1.151818, acc: 0.312500]\n",
      "8546: [discriminator loss: 0.529086, acc: 0.679688] [adversarial loss: 1.544211, acc: 0.078125]\n",
      "8547: [discriminator loss: 0.582989, acc: 0.710938] [adversarial loss: 0.832748, acc: 0.468750]\n",
      "8548: [discriminator loss: 0.568507, acc: 0.710938] [adversarial loss: 1.168862, acc: 0.281250]\n",
      "8549: [discriminator loss: 0.534707, acc: 0.718750] [adversarial loss: 0.941873, acc: 0.359375]\n",
      "8550: [discriminator loss: 0.629426, acc: 0.640625] [adversarial loss: 1.317895, acc: 0.296875]\n",
      "8551: [discriminator loss: 0.552183, acc: 0.695312] [adversarial loss: 0.970163, acc: 0.437500]\n",
      "8552: [discriminator loss: 0.507710, acc: 0.718750] [adversarial loss: 1.424188, acc: 0.156250]\n",
      "8553: [discriminator loss: 0.533591, acc: 0.703125] [adversarial loss: 0.992418, acc: 0.359375]\n",
      "8554: [discriminator loss: 0.556224, acc: 0.726562] [adversarial loss: 1.165772, acc: 0.187500]\n",
      "8555: [discriminator loss: 0.511338, acc: 0.718750] [adversarial loss: 1.046300, acc: 0.296875]\n",
      "8556: [discriminator loss: 0.485113, acc: 0.757812] [adversarial loss: 1.583750, acc: 0.109375]\n",
      "8557: [discriminator loss: 0.662999, acc: 0.656250] [adversarial loss: 0.900099, acc: 0.406250]\n",
      "8558: [discriminator loss: 0.590562, acc: 0.695312] [adversarial loss: 1.523018, acc: 0.093750]\n",
      "8559: [discriminator loss: 0.596605, acc: 0.703125] [adversarial loss: 0.962745, acc: 0.421875]\n",
      "8560: [discriminator loss: 0.499305, acc: 0.734375] [adversarial loss: 1.099943, acc: 0.312500]\n",
      "8561: [discriminator loss: 0.547360, acc: 0.734375] [adversarial loss: 1.170022, acc: 0.312500]\n",
      "8562: [discriminator loss: 0.615142, acc: 0.640625] [adversarial loss: 1.055563, acc: 0.359375]\n",
      "8563: [discriminator loss: 0.528175, acc: 0.742188] [adversarial loss: 0.927153, acc: 0.343750]\n",
      "8564: [discriminator loss: 0.496349, acc: 0.796875] [adversarial loss: 1.121999, acc: 0.250000]\n",
      "8565: [discriminator loss: 0.514220, acc: 0.726562] [adversarial loss: 1.171208, acc: 0.171875]\n",
      "8566: [discriminator loss: 0.589259, acc: 0.671875] [adversarial loss: 1.264839, acc: 0.171875]\n",
      "8567: [discriminator loss: 0.570535, acc: 0.671875] [adversarial loss: 1.101920, acc: 0.328125]\n",
      "8568: [discriminator loss: 0.581922, acc: 0.664062] [adversarial loss: 1.276879, acc: 0.156250]\n",
      "8569: [discriminator loss: 0.555174, acc: 0.695312] [adversarial loss: 1.030003, acc: 0.281250]\n",
      "8570: [discriminator loss: 0.510252, acc: 0.765625] [adversarial loss: 1.525186, acc: 0.140625]\n",
      "8571: [discriminator loss: 0.547825, acc: 0.750000] [adversarial loss: 0.839247, acc: 0.375000]\n",
      "8572: [discriminator loss: 0.560652, acc: 0.671875] [adversarial loss: 1.419211, acc: 0.125000]\n",
      "8573: [discriminator loss: 0.584509, acc: 0.710938] [adversarial loss: 0.923362, acc: 0.375000]\n",
      "8574: [discriminator loss: 0.596036, acc: 0.695312] [adversarial loss: 1.223395, acc: 0.265625]\n",
      "8575: [discriminator loss: 0.629863, acc: 0.617188] [adversarial loss: 1.001066, acc: 0.343750]\n",
      "8576: [discriminator loss: 0.449853, acc: 0.789062] [adversarial loss: 1.309423, acc: 0.218750]\n",
      "8577: [discriminator loss: 0.479779, acc: 0.804688] [adversarial loss: 1.324384, acc: 0.109375]\n",
      "8578: [discriminator loss: 0.566387, acc: 0.734375] [adversarial loss: 1.217109, acc: 0.296875]\n",
      "8579: [discriminator loss: 0.589376, acc: 0.648438] [adversarial loss: 1.061975, acc: 0.250000]\n",
      "8580: [discriminator loss: 0.554207, acc: 0.734375] [adversarial loss: 1.092337, acc: 0.296875]\n",
      "8581: [discriminator loss: 0.544749, acc: 0.734375] [adversarial loss: 1.009968, acc: 0.359375]\n",
      "8582: [discriminator loss: 0.552150, acc: 0.718750] [adversarial loss: 1.138108, acc: 0.250000]\n",
      "8583: [discriminator loss: 0.561544, acc: 0.718750] [adversarial loss: 1.220287, acc: 0.265625]\n",
      "8584: [discriminator loss: 0.592691, acc: 0.695312] [adversarial loss: 0.789945, acc: 0.484375]\n",
      "8585: [discriminator loss: 0.541469, acc: 0.742188] [adversarial loss: 1.324229, acc: 0.140625]\n",
      "8586: [discriminator loss: 0.586271, acc: 0.679688] [adversarial loss: 1.141782, acc: 0.234375]\n",
      "8587: [discriminator loss: 0.650510, acc: 0.617188] [adversarial loss: 1.304260, acc: 0.140625]\n",
      "8588: [discriminator loss: 0.623971, acc: 0.671875] [adversarial loss: 0.957825, acc: 0.375000]\n",
      "8589: [discriminator loss: 0.568682, acc: 0.726562] [adversarial loss: 1.062588, acc: 0.343750]\n",
      "8590: [discriminator loss: 0.515666, acc: 0.765625] [adversarial loss: 1.295884, acc: 0.187500]\n",
      "8591: [discriminator loss: 0.559898, acc: 0.695312] [adversarial loss: 0.877476, acc: 0.406250]\n",
      "8592: [discriminator loss: 0.548928, acc: 0.718750] [adversarial loss: 1.429574, acc: 0.078125]\n",
      "8593: [discriminator loss: 0.594011, acc: 0.671875] [adversarial loss: 0.704765, acc: 0.609375]\n",
      "8594: [discriminator loss: 0.585703, acc: 0.703125] [adversarial loss: 1.319878, acc: 0.156250]\n",
      "8595: [discriminator loss: 0.547132, acc: 0.726562] [adversarial loss: 1.060696, acc: 0.218750]\n",
      "8596: [discriminator loss: 0.545514, acc: 0.757812] [adversarial loss: 1.237880, acc: 0.140625]\n",
      "8597: [discriminator loss: 0.567462, acc: 0.710938] [adversarial loss: 1.092797, acc: 0.265625]\n",
      "8598: [discriminator loss: 0.593667, acc: 0.703125] [adversarial loss: 1.432001, acc: 0.125000]\n",
      "8599: [discriminator loss: 0.576848, acc: 0.710938] [adversarial loss: 0.940159, acc: 0.375000]\n",
      "8600: [discriminator loss: 0.500659, acc: 0.757812] [adversarial loss: 1.405143, acc: 0.125000]\n",
      "8601: [discriminator loss: 0.588200, acc: 0.687500] [adversarial loss: 0.980118, acc: 0.312500]\n",
      "8602: [discriminator loss: 0.653819, acc: 0.656250] [adversarial loss: 1.297449, acc: 0.187500]\n",
      "8603: [discriminator loss: 0.562473, acc: 0.671875] [adversarial loss: 1.166353, acc: 0.250000]\n",
      "8604: [discriminator loss: 0.540184, acc: 0.687500] [adversarial loss: 1.177988, acc: 0.234375]\n",
      "8605: [discriminator loss: 0.581490, acc: 0.695312] [adversarial loss: 1.167844, acc: 0.203125]\n",
      "8606: [discriminator loss: 0.522371, acc: 0.710938] [adversarial loss: 1.363427, acc: 0.156250]\n",
      "8607: [discriminator loss: 0.542437, acc: 0.750000] [adversarial loss: 1.080230, acc: 0.234375]\n",
      "8608: [discriminator loss: 0.600770, acc: 0.671875] [adversarial loss: 1.292463, acc: 0.109375]\n",
      "8609: [discriminator loss: 0.621878, acc: 0.679688] [adversarial loss: 1.235685, acc: 0.125000]\n",
      "8610: [discriminator loss: 0.584532, acc: 0.671875] [adversarial loss: 1.014643, acc: 0.296875]\n",
      "8611: [discriminator loss: 0.542874, acc: 0.710938] [adversarial loss: 1.215494, acc: 0.187500]\n",
      "8612: [discriminator loss: 0.606047, acc: 0.671875] [adversarial loss: 1.181243, acc: 0.234375]\n",
      "8613: [discriminator loss: 0.614499, acc: 0.640625] [adversarial loss: 1.268700, acc: 0.171875]\n",
      "8614: [discriminator loss: 0.500460, acc: 0.742188] [adversarial loss: 1.230921, acc: 0.187500]\n",
      "8615: [discriminator loss: 0.529546, acc: 0.742188] [adversarial loss: 1.091971, acc: 0.359375]\n",
      "8616: [discriminator loss: 0.581944, acc: 0.687500] [adversarial loss: 1.396444, acc: 0.140625]\n",
      "8617: [discriminator loss: 0.574414, acc: 0.710938] [adversarial loss: 0.964233, acc: 0.312500]\n",
      "8618: [discriminator loss: 0.498473, acc: 0.742188] [adversarial loss: 1.316070, acc: 0.218750]\n",
      "8619: [discriminator loss: 0.497257, acc: 0.765625] [adversarial loss: 1.016771, acc: 0.312500]\n",
      "8620: [discriminator loss: 0.579172, acc: 0.671875] [adversarial loss: 1.460414, acc: 0.125000]\n",
      "8621: [discriminator loss: 0.518156, acc: 0.742188] [adversarial loss: 0.895154, acc: 0.421875]\n",
      "8622: [discriminator loss: 0.536318, acc: 0.750000] [adversarial loss: 1.371691, acc: 0.156250]\n",
      "8623: [discriminator loss: 0.569311, acc: 0.695312] [adversarial loss: 0.984754, acc: 0.390625]\n",
      "8624: [discriminator loss: 0.515181, acc: 0.718750] [adversarial loss: 1.183236, acc: 0.265625]\n",
      "8625: [discriminator loss: 0.511310, acc: 0.742188] [adversarial loss: 1.091789, acc: 0.203125]\n",
      "8626: [discriminator loss: 0.633292, acc: 0.632812] [adversarial loss: 1.073926, acc: 0.296875]\n",
      "8627: [discriminator loss: 0.580972, acc: 0.703125] [adversarial loss: 1.024490, acc: 0.390625]\n",
      "8628: [discriminator loss: 0.592963, acc: 0.664062] [adversarial loss: 1.413215, acc: 0.140625]\n",
      "8629: [discriminator loss: 0.537105, acc: 0.695312] [adversarial loss: 1.030771, acc: 0.296875]\n",
      "8630: [discriminator loss: 0.588230, acc: 0.679688] [adversarial loss: 1.250869, acc: 0.218750]\n",
      "8631: [discriminator loss: 0.482324, acc: 0.781250] [adversarial loss: 1.144260, acc: 0.234375]\n",
      "8632: [discriminator loss: 0.527389, acc: 0.710938] [adversarial loss: 1.495607, acc: 0.109375]\n",
      "8633: [discriminator loss: 0.603666, acc: 0.609375] [adversarial loss: 0.855887, acc: 0.453125]\n",
      "8634: [discriminator loss: 0.563224, acc: 0.710938] [adversarial loss: 1.529151, acc: 0.093750]\n",
      "8635: [discriminator loss: 0.612856, acc: 0.687500] [adversarial loss: 0.948731, acc: 0.390625]\n",
      "8636: [discriminator loss: 0.560162, acc: 0.726562] [adversarial loss: 1.429830, acc: 0.171875]\n",
      "8637: [discriminator loss: 0.590189, acc: 0.679688] [adversarial loss: 0.932203, acc: 0.312500]\n",
      "8638: [discriminator loss: 0.555307, acc: 0.703125] [adversarial loss: 1.618291, acc: 0.062500]\n",
      "8639: [discriminator loss: 0.539844, acc: 0.679688] [adversarial loss: 0.813914, acc: 0.515625]\n",
      "8640: [discriminator loss: 0.582428, acc: 0.726562] [adversarial loss: 1.476169, acc: 0.078125]\n",
      "8641: [discriminator loss: 0.533255, acc: 0.703125] [adversarial loss: 0.947953, acc: 0.421875]\n",
      "8642: [discriminator loss: 0.580184, acc: 0.679688] [adversarial loss: 1.122664, acc: 0.234375]\n",
      "8643: [discriminator loss: 0.614393, acc: 0.664062] [adversarial loss: 1.232498, acc: 0.296875]\n",
      "8644: [discriminator loss: 0.518655, acc: 0.718750] [adversarial loss: 1.060734, acc: 0.312500]\n",
      "8645: [discriminator loss: 0.598473, acc: 0.703125] [adversarial loss: 1.154033, acc: 0.140625]\n",
      "8646: [discriminator loss: 0.574176, acc: 0.664062] [adversarial loss: 0.998701, acc: 0.328125]\n",
      "8647: [discriminator loss: 0.500688, acc: 0.718750] [adversarial loss: 1.077052, acc: 0.296875]\n",
      "8648: [discriminator loss: 0.563518, acc: 0.656250] [adversarial loss: 1.561201, acc: 0.046875]\n",
      "8649: [discriminator loss: 0.602590, acc: 0.625000] [adversarial loss: 1.023656, acc: 0.250000]\n",
      "8650: [discriminator loss: 0.535558, acc: 0.718750] [adversarial loss: 1.182075, acc: 0.187500]\n",
      "8651: [discriminator loss: 0.571598, acc: 0.679688] [adversarial loss: 1.164002, acc: 0.265625]\n",
      "8652: [discriminator loss: 0.562229, acc: 0.718750] [adversarial loss: 1.182837, acc: 0.187500]\n",
      "8653: [discriminator loss: 0.575464, acc: 0.695312] [adversarial loss: 1.005840, acc: 0.296875]\n",
      "8654: [discriminator loss: 0.564735, acc: 0.695312] [adversarial loss: 1.069973, acc: 0.250000]\n",
      "8655: [discriminator loss: 0.536180, acc: 0.765625] [adversarial loss: 0.948555, acc: 0.343750]\n",
      "8656: [discriminator loss: 0.527690, acc: 0.718750] [adversarial loss: 1.227080, acc: 0.203125]\n",
      "8657: [discriminator loss: 0.568464, acc: 0.718750] [adversarial loss: 0.962652, acc: 0.343750]\n",
      "8658: [discriminator loss: 0.555698, acc: 0.750000] [adversarial loss: 1.316039, acc: 0.140625]\n",
      "8659: [discriminator loss: 0.455343, acc: 0.820312] [adversarial loss: 1.228798, acc: 0.218750]\n",
      "8660: [discriminator loss: 0.567198, acc: 0.671875] [adversarial loss: 1.420050, acc: 0.187500]\n",
      "8661: [discriminator loss: 0.547421, acc: 0.703125] [adversarial loss: 0.900301, acc: 0.421875]\n",
      "8662: [discriminator loss: 0.530193, acc: 0.710938] [adversarial loss: 1.570930, acc: 0.109375]\n",
      "8663: [discriminator loss: 0.488095, acc: 0.757812] [adversarial loss: 1.038536, acc: 0.281250]\n",
      "8664: [discriminator loss: 0.613282, acc: 0.656250] [adversarial loss: 1.450215, acc: 0.125000]\n",
      "8665: [discriminator loss: 0.567738, acc: 0.656250] [adversarial loss: 0.853903, acc: 0.421875]\n",
      "8666: [discriminator loss: 0.535864, acc: 0.726562] [adversarial loss: 1.514552, acc: 0.109375]\n",
      "8667: [discriminator loss: 0.539816, acc: 0.734375] [adversarial loss: 1.172910, acc: 0.234375]\n",
      "8668: [discriminator loss: 0.543723, acc: 0.703125] [adversarial loss: 1.574529, acc: 0.046875]\n",
      "8669: [discriminator loss: 0.585052, acc: 0.695312] [adversarial loss: 0.954091, acc: 0.390625]\n",
      "8670: [discriminator loss: 0.535543, acc: 0.734375] [adversarial loss: 1.372139, acc: 0.171875]\n",
      "8671: [discriminator loss: 0.528198, acc: 0.734375] [adversarial loss: 1.017609, acc: 0.312500]\n",
      "8672: [discriminator loss: 0.600137, acc: 0.601562] [adversarial loss: 1.589759, acc: 0.140625]\n",
      "8673: [discriminator loss: 0.571470, acc: 0.687500] [adversarial loss: 0.900822, acc: 0.390625]\n",
      "8674: [discriminator loss: 0.495280, acc: 0.750000] [adversarial loss: 1.234361, acc: 0.125000]\n",
      "8675: [discriminator loss: 0.570424, acc: 0.695312] [adversarial loss: 0.971095, acc: 0.312500]\n",
      "8676: [discriminator loss: 0.585385, acc: 0.648438] [adversarial loss: 1.200720, acc: 0.203125]\n",
      "8677: [discriminator loss: 0.585310, acc: 0.679688] [adversarial loss: 0.895213, acc: 0.406250]\n",
      "8678: [discriminator loss: 0.519182, acc: 0.750000] [adversarial loss: 1.370902, acc: 0.125000]\n",
      "8679: [discriminator loss: 0.537566, acc: 0.718750] [adversarial loss: 0.969362, acc: 0.328125]\n",
      "8680: [discriminator loss: 0.573993, acc: 0.695312] [adversarial loss: 1.342733, acc: 0.156250]\n",
      "8681: [discriminator loss: 0.521514, acc: 0.726562] [adversarial loss: 1.099578, acc: 0.359375]\n",
      "8682: [discriminator loss: 0.563730, acc: 0.687500] [adversarial loss: 1.385335, acc: 0.156250]\n",
      "8683: [discriminator loss: 0.446349, acc: 0.781250] [adversarial loss: 1.335947, acc: 0.187500]\n",
      "8684: [discriminator loss: 0.554242, acc: 0.750000] [adversarial loss: 1.188881, acc: 0.218750]\n",
      "8685: [discriminator loss: 0.547407, acc: 0.710938] [adversarial loss: 0.999953, acc: 0.296875]\n",
      "8686: [discriminator loss: 0.548477, acc: 0.703125] [adversarial loss: 1.479076, acc: 0.125000]\n",
      "8687: [discriminator loss: 0.510480, acc: 0.726562] [adversarial loss: 0.917247, acc: 0.375000]\n",
      "8688: [discriminator loss: 0.540698, acc: 0.695312] [adversarial loss: 0.976661, acc: 0.359375]\n",
      "8689: [discriminator loss: 0.585683, acc: 0.695312] [adversarial loss: 1.189496, acc: 0.234375]\n",
      "8690: [discriminator loss: 0.553059, acc: 0.695312] [adversarial loss: 1.128971, acc: 0.328125]\n",
      "8691: [discriminator loss: 0.666975, acc: 0.601562] [adversarial loss: 1.037471, acc: 0.234375]\n",
      "8692: [discriminator loss: 0.579283, acc: 0.656250] [adversarial loss: 1.575050, acc: 0.109375]\n",
      "8693: [discriminator loss: 0.593060, acc: 0.648438] [adversarial loss: 0.737675, acc: 0.531250]\n",
      "8694: [discriminator loss: 0.599045, acc: 0.664062] [adversarial loss: 1.547997, acc: 0.062500]\n",
      "8695: [discriminator loss: 0.552812, acc: 0.726562] [adversarial loss: 0.948517, acc: 0.421875]\n",
      "8696: [discriminator loss: 0.564085, acc: 0.742188] [adversarial loss: 0.990301, acc: 0.281250]\n",
      "8697: [discriminator loss: 0.521041, acc: 0.734375] [adversarial loss: 1.050049, acc: 0.390625]\n",
      "8698: [discriminator loss: 0.548120, acc: 0.750000] [adversarial loss: 1.362861, acc: 0.125000]\n",
      "8699: [discriminator loss: 0.586909, acc: 0.625000] [adversarial loss: 1.220580, acc: 0.203125]\n",
      "8700: [discriminator loss: 0.517322, acc: 0.726562] [adversarial loss: 1.011875, acc: 0.265625]\n",
      "8701: [discriminator loss: 0.528158, acc: 0.742188] [adversarial loss: 0.905762, acc: 0.359375]\n",
      "8702: [discriminator loss: 0.508614, acc: 0.742188] [adversarial loss: 1.382864, acc: 0.093750]\n",
      "8703: [discriminator loss: 0.545702, acc: 0.742188] [adversarial loss: 0.964813, acc: 0.437500]\n",
      "8704: [discriminator loss: 0.564888, acc: 0.718750] [adversarial loss: 1.159708, acc: 0.265625]\n",
      "8705: [discriminator loss: 0.544646, acc: 0.718750] [adversarial loss: 1.040944, acc: 0.343750]\n",
      "8706: [discriminator loss: 0.615643, acc: 0.640625] [adversarial loss: 1.197985, acc: 0.234375]\n",
      "8707: [discriminator loss: 0.520883, acc: 0.726562] [adversarial loss: 1.138181, acc: 0.218750]\n",
      "8708: [discriminator loss: 0.536515, acc: 0.710938] [adversarial loss: 1.234118, acc: 0.265625]\n",
      "8709: [discriminator loss: 0.543436, acc: 0.742188] [adversarial loss: 1.279321, acc: 0.218750]\n",
      "8710: [discriminator loss: 0.530502, acc: 0.750000] [adversarial loss: 1.308311, acc: 0.171875]\n",
      "8711: [discriminator loss: 0.579378, acc: 0.710938] [adversarial loss: 1.055374, acc: 0.359375]\n",
      "8712: [discriminator loss: 0.571532, acc: 0.703125] [adversarial loss: 1.228641, acc: 0.234375]\n",
      "8713: [discriminator loss: 0.520019, acc: 0.726562] [adversarial loss: 1.022953, acc: 0.281250]\n",
      "8714: [discriminator loss: 0.521682, acc: 0.687500] [adversarial loss: 1.363582, acc: 0.187500]\n",
      "8715: [discriminator loss: 0.553333, acc: 0.664062] [adversarial loss: 1.025448, acc: 0.375000]\n",
      "8716: [discriminator loss: 0.514346, acc: 0.757812] [adversarial loss: 1.393505, acc: 0.156250]\n",
      "8717: [discriminator loss: 0.576369, acc: 0.742188] [adversarial loss: 1.010259, acc: 0.359375]\n",
      "8718: [discriminator loss: 0.556257, acc: 0.703125] [adversarial loss: 1.213040, acc: 0.218750]\n",
      "8719: [discriminator loss: 0.586905, acc: 0.687500] [adversarial loss: 0.886902, acc: 0.468750]\n",
      "8720: [discriminator loss: 0.570338, acc: 0.710938] [adversarial loss: 1.185298, acc: 0.296875]\n",
      "8721: [discriminator loss: 0.543735, acc: 0.757812] [adversarial loss: 1.003013, acc: 0.343750]\n",
      "8722: [discriminator loss: 0.618337, acc: 0.664062] [adversarial loss: 1.637665, acc: 0.093750]\n",
      "8723: [discriminator loss: 0.616330, acc: 0.687500] [adversarial loss: 0.919162, acc: 0.406250]\n",
      "8724: [discriminator loss: 0.554797, acc: 0.703125] [adversarial loss: 0.998227, acc: 0.312500]\n",
      "8725: [discriminator loss: 0.589367, acc: 0.625000] [adversarial loss: 1.345445, acc: 0.125000]\n",
      "8726: [discriminator loss: 0.534279, acc: 0.671875] [adversarial loss: 0.967417, acc: 0.296875]\n",
      "8727: [discriminator loss: 0.607474, acc: 0.656250] [adversarial loss: 1.142166, acc: 0.218750]\n",
      "8728: [discriminator loss: 0.595280, acc: 0.671875] [adversarial loss: 0.955696, acc: 0.406250]\n",
      "8729: [discriminator loss: 0.614694, acc: 0.679688] [adversarial loss: 1.090524, acc: 0.203125]\n",
      "8730: [discriminator loss: 0.496629, acc: 0.750000] [adversarial loss: 1.196184, acc: 0.234375]\n",
      "8731: [discriminator loss: 0.539393, acc: 0.703125] [adversarial loss: 1.178371, acc: 0.234375]\n",
      "8732: [discriminator loss: 0.581194, acc: 0.703125] [adversarial loss: 1.152002, acc: 0.234375]\n",
      "8733: [discriminator loss: 0.572069, acc: 0.687500] [adversarial loss: 1.125631, acc: 0.265625]\n",
      "8734: [discriminator loss: 0.542177, acc: 0.726562] [adversarial loss: 0.888684, acc: 0.406250]\n",
      "8735: [discriminator loss: 0.504632, acc: 0.750000] [adversarial loss: 1.396421, acc: 0.109375]\n",
      "8736: [discriminator loss: 0.531347, acc: 0.765625] [adversarial loss: 0.790354, acc: 0.515625]\n",
      "8737: [discriminator loss: 0.583275, acc: 0.679688] [adversarial loss: 1.504070, acc: 0.171875]\n",
      "8738: [discriminator loss: 0.555121, acc: 0.679688] [adversarial loss: 0.838478, acc: 0.437500]\n",
      "8739: [discriminator loss: 0.519309, acc: 0.703125] [adversarial loss: 1.157789, acc: 0.265625]\n",
      "8740: [discriminator loss: 0.524548, acc: 0.703125] [adversarial loss: 1.078761, acc: 0.265625]\n",
      "8741: [discriminator loss: 0.571195, acc: 0.664062] [adversarial loss: 1.364626, acc: 0.125000]\n",
      "8742: [discriminator loss: 0.593871, acc: 0.640625] [adversarial loss: 0.947954, acc: 0.328125]\n",
      "8743: [discriminator loss: 0.619817, acc: 0.617188] [adversarial loss: 1.077899, acc: 0.250000]\n",
      "8744: [discriminator loss: 0.531285, acc: 0.679688] [adversarial loss: 1.058344, acc: 0.281250]\n",
      "8745: [discriminator loss: 0.576397, acc: 0.679688] [adversarial loss: 1.147236, acc: 0.234375]\n",
      "8746: [discriminator loss: 0.584070, acc: 0.609375] [adversarial loss: 1.127176, acc: 0.234375]\n",
      "8747: [discriminator loss: 0.562451, acc: 0.710938] [adversarial loss: 1.246804, acc: 0.156250]\n",
      "8748: [discriminator loss: 0.521477, acc: 0.781250] [adversarial loss: 1.130117, acc: 0.265625]\n",
      "8749: [discriminator loss: 0.571433, acc: 0.679688] [adversarial loss: 1.049972, acc: 0.250000]\n",
      "8750: [discriminator loss: 0.589011, acc: 0.679688] [adversarial loss: 1.164023, acc: 0.203125]\n",
      "8751: [discriminator loss: 0.576369, acc: 0.695312] [adversarial loss: 0.958938, acc: 0.328125]\n",
      "8752: [discriminator loss: 0.506411, acc: 0.781250] [adversarial loss: 1.392483, acc: 0.109375]\n",
      "8753: [discriminator loss: 0.581046, acc: 0.710938] [adversarial loss: 0.981219, acc: 0.359375]\n",
      "8754: [discriminator loss: 0.553071, acc: 0.710938] [adversarial loss: 1.454635, acc: 0.140625]\n",
      "8755: [discriminator loss: 0.560945, acc: 0.687500] [adversarial loss: 1.140361, acc: 0.234375]\n",
      "8756: [discriminator loss: 0.492606, acc: 0.742188] [adversarial loss: 1.263972, acc: 0.109375]\n",
      "8757: [discriminator loss: 0.548368, acc: 0.710938] [adversarial loss: 0.974321, acc: 0.296875]\n",
      "8758: [discriminator loss: 0.563653, acc: 0.710938] [adversarial loss: 1.283231, acc: 0.171875]\n",
      "8759: [discriminator loss: 0.602504, acc: 0.648438] [adversarial loss: 0.915431, acc: 0.421875]\n",
      "8760: [discriminator loss: 0.566430, acc: 0.671875] [adversarial loss: 1.207651, acc: 0.171875]\n",
      "8761: [discriminator loss: 0.509639, acc: 0.742188] [adversarial loss: 1.377518, acc: 0.125000]\n",
      "8762: [discriminator loss: 0.532106, acc: 0.734375] [adversarial loss: 1.133486, acc: 0.218750]\n",
      "8763: [discriminator loss: 0.523080, acc: 0.734375] [adversarial loss: 1.191170, acc: 0.234375]\n",
      "8764: [discriminator loss: 0.514966, acc: 0.789062] [adversarial loss: 1.123452, acc: 0.234375]\n",
      "8765: [discriminator loss: 0.564648, acc: 0.742188] [adversarial loss: 1.505053, acc: 0.078125]\n",
      "8766: [discriminator loss: 0.555890, acc: 0.671875] [adversarial loss: 0.776835, acc: 0.468750]\n",
      "8767: [discriminator loss: 0.576881, acc: 0.710938] [adversarial loss: 1.443846, acc: 0.078125]\n",
      "8768: [discriminator loss: 0.578226, acc: 0.656250] [adversarial loss: 0.786806, acc: 0.578125]\n",
      "8769: [discriminator loss: 0.624048, acc: 0.656250] [adversarial loss: 1.246223, acc: 0.187500]\n",
      "8770: [discriminator loss: 0.580545, acc: 0.710938] [adversarial loss: 1.153898, acc: 0.218750]\n",
      "8771: [discriminator loss: 0.560185, acc: 0.710938] [adversarial loss: 1.389970, acc: 0.218750]\n",
      "8772: [discriminator loss: 0.510731, acc: 0.750000] [adversarial loss: 1.045206, acc: 0.359375]\n",
      "8773: [discriminator loss: 0.579895, acc: 0.679688] [adversarial loss: 1.132991, acc: 0.234375]\n",
      "8774: [discriminator loss: 0.549655, acc: 0.710938] [adversarial loss: 1.105116, acc: 0.265625]\n",
      "8775: [discriminator loss: 0.503538, acc: 0.734375] [adversarial loss: 0.953443, acc: 0.375000]\n",
      "8776: [discriminator loss: 0.563228, acc: 0.710938] [adversarial loss: 1.374605, acc: 0.171875]\n",
      "8777: [discriminator loss: 0.586408, acc: 0.679688] [adversarial loss: 1.223882, acc: 0.187500]\n",
      "8778: [discriminator loss: 0.587683, acc: 0.671875] [adversarial loss: 1.209068, acc: 0.218750]\n",
      "8779: [discriminator loss: 0.573217, acc: 0.671875] [adversarial loss: 1.089471, acc: 0.296875]\n",
      "8780: [discriminator loss: 0.515752, acc: 0.742188] [adversarial loss: 1.349540, acc: 0.156250]\n",
      "8781: [discriminator loss: 0.527759, acc: 0.679688] [adversarial loss: 1.058018, acc: 0.296875]\n",
      "8782: [discriminator loss: 0.511581, acc: 0.710938] [adversarial loss: 1.095680, acc: 0.281250]\n",
      "8783: [discriminator loss: 0.568066, acc: 0.710938] [adversarial loss: 1.224570, acc: 0.281250]\n",
      "8784: [discriminator loss: 0.574250, acc: 0.664062] [adversarial loss: 0.925701, acc: 0.437500]\n",
      "8785: [discriminator loss: 0.566049, acc: 0.632812] [adversarial loss: 1.506653, acc: 0.140625]\n",
      "8786: [discriminator loss: 0.549045, acc: 0.703125] [adversarial loss: 1.258498, acc: 0.171875]\n",
      "8787: [discriminator loss: 0.584820, acc: 0.679688] [adversarial loss: 1.079636, acc: 0.218750]\n",
      "8788: [discriminator loss: 0.627823, acc: 0.648438] [adversarial loss: 1.111200, acc: 0.250000]\n",
      "8789: [discriminator loss: 0.537452, acc: 0.742188] [adversarial loss: 0.996882, acc: 0.328125]\n",
      "8790: [discriminator loss: 0.522403, acc: 0.703125] [adversarial loss: 1.254719, acc: 0.156250]\n",
      "8791: [discriminator loss: 0.535123, acc: 0.710938] [adversarial loss: 1.030992, acc: 0.328125]\n",
      "8792: [discriminator loss: 0.503263, acc: 0.726562] [adversarial loss: 1.268758, acc: 0.171875]\n",
      "8793: [discriminator loss: 0.575527, acc: 0.703125] [adversarial loss: 1.170848, acc: 0.187500]\n",
      "8794: [discriminator loss: 0.628287, acc: 0.632812] [adversarial loss: 1.279211, acc: 0.218750]\n",
      "8795: [discriminator loss: 0.551097, acc: 0.664062] [adversarial loss: 0.942354, acc: 0.359375]\n",
      "8796: [discriminator loss: 0.560083, acc: 0.710938] [adversarial loss: 1.405993, acc: 0.140625]\n",
      "8797: [discriminator loss: 0.515215, acc: 0.726562] [adversarial loss: 0.946887, acc: 0.359375]\n",
      "8798: [discriminator loss: 0.580064, acc: 0.648438] [adversarial loss: 1.363853, acc: 0.062500]\n",
      "8799: [discriminator loss: 0.555515, acc: 0.765625] [adversarial loss: 0.850038, acc: 0.421875]\n",
      "8800: [discriminator loss: 0.549018, acc: 0.742188] [adversarial loss: 1.394139, acc: 0.140625]\n",
      "8801: [discriminator loss: 0.632614, acc: 0.609375] [adversarial loss: 0.677616, acc: 0.578125]\n",
      "8802: [discriminator loss: 0.606216, acc: 0.632812] [adversarial loss: 1.425344, acc: 0.093750]\n",
      "8803: [discriminator loss: 0.594441, acc: 0.648438] [adversarial loss: 1.089355, acc: 0.281250]\n",
      "8804: [discriminator loss: 0.606952, acc: 0.679688] [adversarial loss: 1.293287, acc: 0.187500]\n",
      "8805: [discriminator loss: 0.556689, acc: 0.687500] [adversarial loss: 1.136317, acc: 0.187500]\n",
      "8806: [discriminator loss: 0.499429, acc: 0.781250] [adversarial loss: 1.087177, acc: 0.343750]\n",
      "8807: [discriminator loss: 0.565492, acc: 0.710938] [adversarial loss: 1.064066, acc: 0.250000]\n",
      "8808: [discriminator loss: 0.598643, acc: 0.703125] [adversarial loss: 1.159183, acc: 0.203125]\n",
      "8809: [discriminator loss: 0.593560, acc: 0.664062] [adversarial loss: 1.154064, acc: 0.234375]\n",
      "8810: [discriminator loss: 0.486258, acc: 0.765625] [adversarial loss: 1.538899, acc: 0.093750]\n",
      "8811: [discriminator loss: 0.507011, acc: 0.750000] [adversarial loss: 0.843042, acc: 0.500000]\n",
      "8812: [discriminator loss: 0.567227, acc: 0.679688] [adversarial loss: 1.270484, acc: 0.140625]\n",
      "8813: [discriminator loss: 0.516852, acc: 0.773438] [adversarial loss: 1.131703, acc: 0.250000]\n",
      "8814: [discriminator loss: 0.536498, acc: 0.750000] [adversarial loss: 0.920053, acc: 0.343750]\n",
      "8815: [discriminator loss: 0.538843, acc: 0.703125] [adversarial loss: 1.241353, acc: 0.328125]\n",
      "8816: [discriminator loss: 0.542846, acc: 0.718750] [adversarial loss: 0.891803, acc: 0.375000]\n",
      "8817: [discriminator loss: 0.512375, acc: 0.765625] [adversarial loss: 1.548187, acc: 0.125000]\n",
      "8818: [discriminator loss: 0.681609, acc: 0.609375] [adversarial loss: 0.961347, acc: 0.359375]\n",
      "8819: [discriminator loss: 0.568682, acc: 0.703125] [adversarial loss: 1.067213, acc: 0.281250]\n",
      "8820: [discriminator loss: 0.640528, acc: 0.593750] [adversarial loss: 1.076221, acc: 0.328125]\n",
      "8821: [discriminator loss: 0.623109, acc: 0.703125] [adversarial loss: 1.037466, acc: 0.312500]\n",
      "8822: [discriminator loss: 0.516956, acc: 0.710938] [adversarial loss: 1.150144, acc: 0.187500]\n",
      "8823: [discriminator loss: 0.540851, acc: 0.726562] [adversarial loss: 1.180277, acc: 0.265625]\n",
      "8824: [discriminator loss: 0.556848, acc: 0.687500] [adversarial loss: 1.007601, acc: 0.406250]\n",
      "8825: [discriminator loss: 0.535850, acc: 0.750000] [adversarial loss: 1.408327, acc: 0.140625]\n",
      "8826: [discriminator loss: 0.562708, acc: 0.664062] [adversarial loss: 1.011528, acc: 0.312500]\n",
      "8827: [discriminator loss: 0.559885, acc: 0.718750] [adversarial loss: 1.509905, acc: 0.109375]\n",
      "8828: [discriminator loss: 0.576732, acc: 0.656250] [adversarial loss: 0.855525, acc: 0.484375]\n",
      "8829: [discriminator loss: 0.544034, acc: 0.757812] [adversarial loss: 1.267235, acc: 0.218750]\n",
      "8830: [discriminator loss: 0.508533, acc: 0.718750] [adversarial loss: 1.112349, acc: 0.234375]\n",
      "8831: [discriminator loss: 0.496835, acc: 0.757812] [adversarial loss: 1.312295, acc: 0.171875]\n",
      "8832: [discriminator loss: 0.522878, acc: 0.710938] [adversarial loss: 0.735507, acc: 0.484375]\n",
      "8833: [discriminator loss: 0.573206, acc: 0.703125] [adversarial loss: 1.338658, acc: 0.218750]\n",
      "8834: [discriminator loss: 0.566254, acc: 0.695312] [adversarial loss: 1.071838, acc: 0.328125]\n",
      "8835: [discriminator loss: 0.540272, acc: 0.750000] [adversarial loss: 1.126521, acc: 0.265625]\n",
      "8836: [discriminator loss: 0.503690, acc: 0.781250] [adversarial loss: 1.108448, acc: 0.156250]\n",
      "8837: [discriminator loss: 0.522265, acc: 0.710938] [adversarial loss: 1.155441, acc: 0.218750]\n",
      "8838: [discriminator loss: 0.503557, acc: 0.734375] [adversarial loss: 1.172851, acc: 0.203125]\n",
      "8839: [discriminator loss: 0.548359, acc: 0.703125] [adversarial loss: 0.860520, acc: 0.468750]\n",
      "8840: [discriminator loss: 0.574885, acc: 0.710938] [adversarial loss: 1.526364, acc: 0.125000]\n",
      "8841: [discriminator loss: 0.521282, acc: 0.734375] [adversarial loss: 0.899939, acc: 0.375000]\n",
      "8842: [discriminator loss: 0.540826, acc: 0.718750] [adversarial loss: 1.324251, acc: 0.187500]\n",
      "8843: [discriminator loss: 0.496328, acc: 0.773438] [adversarial loss: 1.203505, acc: 0.265625]\n",
      "8844: [discriminator loss: 0.520564, acc: 0.757812] [adversarial loss: 1.088382, acc: 0.281250]\n",
      "8845: [discriminator loss: 0.550350, acc: 0.687500] [adversarial loss: 1.155066, acc: 0.218750]\n",
      "8846: [discriminator loss: 0.514204, acc: 0.718750] [adversarial loss: 1.372527, acc: 0.156250]\n",
      "8847: [discriminator loss: 0.548753, acc: 0.742188] [adversarial loss: 1.093210, acc: 0.296875]\n",
      "8848: [discriminator loss: 0.580713, acc: 0.679688] [adversarial loss: 1.334424, acc: 0.156250]\n",
      "8849: [discriminator loss: 0.498229, acc: 0.773438] [adversarial loss: 1.161227, acc: 0.203125]\n",
      "8850: [discriminator loss: 0.561394, acc: 0.734375] [adversarial loss: 1.160882, acc: 0.265625]\n",
      "8851: [discriminator loss: 0.653843, acc: 0.601562] [adversarial loss: 1.325123, acc: 0.187500]\n",
      "8852: [discriminator loss: 0.540128, acc: 0.734375] [adversarial loss: 0.989946, acc: 0.375000]\n",
      "8853: [discriminator loss: 0.623109, acc: 0.664062] [adversarial loss: 1.123380, acc: 0.328125]\n",
      "8854: [discriminator loss: 0.481734, acc: 0.781250] [adversarial loss: 1.433869, acc: 0.078125]\n",
      "8855: [discriminator loss: 0.549843, acc: 0.703125] [adversarial loss: 0.990184, acc: 0.328125]\n",
      "8856: [discriminator loss: 0.529130, acc: 0.765625] [adversarial loss: 1.680508, acc: 0.093750]\n",
      "8857: [discriminator loss: 0.616066, acc: 0.679688] [adversarial loss: 0.861010, acc: 0.500000]\n",
      "8858: [discriminator loss: 0.538650, acc: 0.726562] [adversarial loss: 1.662166, acc: 0.078125]\n",
      "8859: [discriminator loss: 0.564773, acc: 0.671875] [adversarial loss: 0.699235, acc: 0.546875]\n",
      "8860: [discriminator loss: 0.614481, acc: 0.664062] [adversarial loss: 1.302113, acc: 0.265625]\n",
      "8861: [discriminator loss: 0.546136, acc: 0.679688] [adversarial loss: 0.906452, acc: 0.390625]\n",
      "8862: [discriminator loss: 0.518087, acc: 0.710938] [adversarial loss: 1.446135, acc: 0.156250]\n",
      "8863: [discriminator loss: 0.589643, acc: 0.695312] [adversarial loss: 0.811555, acc: 0.468750]\n",
      "8864: [discriminator loss: 0.503803, acc: 0.781250] [adversarial loss: 1.260926, acc: 0.203125]\n",
      "8865: [discriminator loss: 0.549400, acc: 0.671875] [adversarial loss: 1.044354, acc: 0.296875]\n",
      "8866: [discriminator loss: 0.560681, acc: 0.695312] [adversarial loss: 1.250628, acc: 0.140625]\n",
      "8867: [discriminator loss: 0.543399, acc: 0.718750] [adversarial loss: 1.032028, acc: 0.312500]\n",
      "8868: [discriminator loss: 0.577836, acc: 0.726562] [adversarial loss: 1.249018, acc: 0.250000]\n",
      "8869: [discriminator loss: 0.555566, acc: 0.687500] [adversarial loss: 0.987310, acc: 0.359375]\n",
      "8870: [discriminator loss: 0.624374, acc: 0.648438] [adversarial loss: 1.494177, acc: 0.109375]\n",
      "8871: [discriminator loss: 0.517853, acc: 0.726562] [adversarial loss: 1.101874, acc: 0.250000]\n",
      "8872: [discriminator loss: 0.543683, acc: 0.734375] [adversarial loss: 1.200729, acc: 0.250000]\n",
      "8873: [discriminator loss: 0.568100, acc: 0.710938] [adversarial loss: 1.044468, acc: 0.296875]\n",
      "8874: [discriminator loss: 0.491939, acc: 0.804688] [adversarial loss: 1.578564, acc: 0.140625]\n",
      "8875: [discriminator loss: 0.587060, acc: 0.703125] [adversarial loss: 1.266064, acc: 0.187500]\n",
      "8876: [discriminator loss: 0.514605, acc: 0.757812] [adversarial loss: 1.087007, acc: 0.218750]\n",
      "8877: [discriminator loss: 0.539948, acc: 0.687500] [adversarial loss: 1.027554, acc: 0.390625]\n",
      "8878: [discriminator loss: 0.483087, acc: 0.796875] [adversarial loss: 1.070467, acc: 0.296875]\n",
      "8879: [discriminator loss: 0.533688, acc: 0.734375] [adversarial loss: 0.910558, acc: 0.406250]\n",
      "8880: [discriminator loss: 0.585797, acc: 0.687500] [adversarial loss: 1.407611, acc: 0.234375]\n",
      "8881: [discriminator loss: 0.600329, acc: 0.664062] [adversarial loss: 0.943868, acc: 0.406250]\n",
      "8882: [discriminator loss: 0.554085, acc: 0.679688] [adversarial loss: 1.584836, acc: 0.046875]\n",
      "8883: [discriminator loss: 0.528190, acc: 0.695312] [adversarial loss: 0.903925, acc: 0.390625]\n",
      "8884: [discriminator loss: 0.597617, acc: 0.664062] [adversarial loss: 1.284512, acc: 0.171875]\n",
      "8885: [discriminator loss: 0.515559, acc: 0.750000] [adversarial loss: 0.810943, acc: 0.484375]\n",
      "8886: [discriminator loss: 0.582410, acc: 0.695312] [adversarial loss: 1.492616, acc: 0.062500]\n",
      "8887: [discriminator loss: 0.552365, acc: 0.664062] [adversarial loss: 0.962752, acc: 0.359375]\n",
      "8888: [discriminator loss: 0.556114, acc: 0.679688] [adversarial loss: 1.457960, acc: 0.093750]\n",
      "8889: [discriminator loss: 0.633913, acc: 0.593750] [adversarial loss: 0.911162, acc: 0.406250]\n",
      "8890: [discriminator loss: 0.629714, acc: 0.671875] [adversarial loss: 1.280567, acc: 0.109375]\n",
      "8891: [discriminator loss: 0.544004, acc: 0.742188] [adversarial loss: 1.016829, acc: 0.359375]\n",
      "8892: [discriminator loss: 0.536882, acc: 0.765625] [adversarial loss: 1.276736, acc: 0.234375]\n",
      "8893: [discriminator loss: 0.563368, acc: 0.679688] [adversarial loss: 1.150408, acc: 0.250000]\n",
      "8894: [discriminator loss: 0.539487, acc: 0.750000] [adversarial loss: 1.324788, acc: 0.203125]\n",
      "8895: [discriminator loss: 0.627957, acc: 0.632812] [adversarial loss: 0.979671, acc: 0.375000]\n",
      "8896: [discriminator loss: 0.607171, acc: 0.601562] [adversarial loss: 1.007240, acc: 0.343750]\n",
      "8897: [discriminator loss: 0.565657, acc: 0.679688] [adversarial loss: 1.241190, acc: 0.250000]\n",
      "8898: [discriminator loss: 0.552675, acc: 0.726562] [adversarial loss: 1.224230, acc: 0.250000]\n",
      "8899: [discriminator loss: 0.548082, acc: 0.765625] [adversarial loss: 1.130600, acc: 0.171875]\n",
      "8900: [discriminator loss: 0.568074, acc: 0.710938] [adversarial loss: 1.120819, acc: 0.296875]\n",
      "8901: [discriminator loss: 0.569477, acc: 0.687500] [adversarial loss: 1.125693, acc: 0.234375]\n",
      "8902: [discriminator loss: 0.555841, acc: 0.703125] [adversarial loss: 1.210500, acc: 0.234375]\n",
      "8903: [discriminator loss: 0.544877, acc: 0.710938] [adversarial loss: 1.076724, acc: 0.328125]\n",
      "8904: [discriminator loss: 0.488890, acc: 0.757812] [adversarial loss: 1.367305, acc: 0.156250]\n",
      "8905: [discriminator loss: 0.561258, acc: 0.664062] [adversarial loss: 0.845252, acc: 0.437500]\n",
      "8906: [discriminator loss: 0.636860, acc: 0.625000] [adversarial loss: 1.622197, acc: 0.078125]\n",
      "8907: [discriminator loss: 0.617502, acc: 0.656250] [adversarial loss: 0.852964, acc: 0.421875]\n",
      "8908: [discriminator loss: 0.602164, acc: 0.718750] [adversarial loss: 1.494726, acc: 0.078125]\n",
      "8909: [discriminator loss: 0.534549, acc: 0.726562] [adversarial loss: 1.094344, acc: 0.218750]\n",
      "8910: [discriminator loss: 0.543877, acc: 0.703125] [adversarial loss: 1.392339, acc: 0.109375]\n",
      "8911: [discriminator loss: 0.522491, acc: 0.710938] [adversarial loss: 1.070533, acc: 0.218750]\n",
      "8912: [discriminator loss: 0.576940, acc: 0.664062] [adversarial loss: 0.969402, acc: 0.328125]\n",
      "8913: [discriminator loss: 0.505014, acc: 0.750000] [adversarial loss: 1.075837, acc: 0.265625]\n",
      "8914: [discriminator loss: 0.572424, acc: 0.640625] [adversarial loss: 1.269326, acc: 0.218750]\n",
      "8915: [discriminator loss: 0.537140, acc: 0.734375] [adversarial loss: 0.959466, acc: 0.343750]\n",
      "8916: [discriminator loss: 0.598423, acc: 0.695312] [adversarial loss: 1.428590, acc: 0.125000]\n",
      "8917: [discriminator loss: 0.548537, acc: 0.695312] [adversarial loss: 0.929105, acc: 0.421875]\n",
      "8918: [discriminator loss: 0.568152, acc: 0.687500] [adversarial loss: 1.053566, acc: 0.343750]\n",
      "8919: [discriminator loss: 0.552639, acc: 0.718750] [adversarial loss: 1.242102, acc: 0.156250]\n",
      "8920: [discriminator loss: 0.485208, acc: 0.718750] [adversarial loss: 1.391923, acc: 0.125000]\n",
      "8921: [discriminator loss: 0.591724, acc: 0.640625] [adversarial loss: 0.914209, acc: 0.390625]\n",
      "8922: [discriminator loss: 0.528893, acc: 0.750000] [adversarial loss: 1.226174, acc: 0.187500]\n",
      "8923: [discriminator loss: 0.566967, acc: 0.703125] [adversarial loss: 0.967229, acc: 0.390625]\n",
      "8924: [discriminator loss: 0.569513, acc: 0.695312] [adversarial loss: 1.563401, acc: 0.125000]\n",
      "8925: [discriminator loss: 0.584775, acc: 0.695312] [adversarial loss: 0.806659, acc: 0.484375]\n",
      "8926: [discriminator loss: 0.517459, acc: 0.750000] [adversarial loss: 1.219680, acc: 0.250000]\n",
      "8927: [discriminator loss: 0.511625, acc: 0.742188] [adversarial loss: 1.389969, acc: 0.171875]\n",
      "8928: [discriminator loss: 0.530171, acc: 0.710938] [adversarial loss: 1.162905, acc: 0.218750]\n",
      "8929: [discriminator loss: 0.568183, acc: 0.718750] [adversarial loss: 1.207550, acc: 0.218750]\n",
      "8930: [discriminator loss: 0.650457, acc: 0.609375] [adversarial loss: 0.984187, acc: 0.390625]\n",
      "8931: [discriminator loss: 0.587390, acc: 0.703125] [adversarial loss: 1.413987, acc: 0.187500]\n",
      "8932: [discriminator loss: 0.539312, acc: 0.695312] [adversarial loss: 0.958826, acc: 0.343750]\n",
      "8933: [discriminator loss: 0.528083, acc: 0.695312] [adversarial loss: 1.504134, acc: 0.109375]\n",
      "8934: [discriminator loss: 0.652035, acc: 0.640625] [adversarial loss: 0.736544, acc: 0.546875]\n",
      "8935: [discriminator loss: 0.524599, acc: 0.679688] [adversarial loss: 1.203232, acc: 0.187500]\n",
      "8936: [discriminator loss: 0.548918, acc: 0.695312] [adversarial loss: 1.024097, acc: 0.359375]\n",
      "8937: [discriminator loss: 0.499159, acc: 0.742188] [adversarial loss: 1.296173, acc: 0.187500]\n",
      "8938: [discriminator loss: 0.546850, acc: 0.742188] [adversarial loss: 1.152868, acc: 0.187500]\n",
      "8939: [discriminator loss: 0.493461, acc: 0.750000] [adversarial loss: 1.263632, acc: 0.156250]\n",
      "8940: [discriminator loss: 0.558385, acc: 0.718750] [adversarial loss: 1.083030, acc: 0.265625]\n",
      "8941: [discriminator loss: 0.545949, acc: 0.718750] [adversarial loss: 1.185990, acc: 0.171875]\n",
      "8942: [discriminator loss: 0.540031, acc: 0.757812] [adversarial loss: 0.912829, acc: 0.343750]\n",
      "8943: [discriminator loss: 0.511285, acc: 0.750000] [adversarial loss: 1.230525, acc: 0.156250]\n",
      "8944: [discriminator loss: 0.585923, acc: 0.687500] [adversarial loss: 0.843595, acc: 0.437500]\n",
      "8945: [discriminator loss: 0.613709, acc: 0.640625] [adversarial loss: 1.469891, acc: 0.156250]\n",
      "8946: [discriminator loss: 0.547400, acc: 0.726562] [adversarial loss: 0.905880, acc: 0.375000]\n",
      "8947: [discriminator loss: 0.598938, acc: 0.648438] [adversarial loss: 1.174760, acc: 0.140625]\n",
      "8948: [discriminator loss: 0.606592, acc: 0.656250] [adversarial loss: 1.014749, acc: 0.343750]\n",
      "8949: [discriminator loss: 0.586763, acc: 0.664062] [adversarial loss: 1.183431, acc: 0.250000]\n",
      "8950: [discriminator loss: 0.518781, acc: 0.695312] [adversarial loss: 1.247025, acc: 0.140625]\n",
      "8951: [discriminator loss: 0.508051, acc: 0.789062] [adversarial loss: 1.023358, acc: 0.359375]\n",
      "8952: [discriminator loss: 0.558538, acc: 0.718750] [adversarial loss: 1.213085, acc: 0.250000]\n",
      "8953: [discriminator loss: 0.512093, acc: 0.710938] [adversarial loss: 0.971688, acc: 0.296875]\n",
      "8954: [discriminator loss: 0.504669, acc: 0.742188] [adversarial loss: 1.267882, acc: 0.203125]\n",
      "8955: [discriminator loss: 0.515740, acc: 0.687500] [adversarial loss: 0.906429, acc: 0.328125]\n",
      "8956: [discriminator loss: 0.505157, acc: 0.750000] [adversarial loss: 1.266177, acc: 0.203125]\n",
      "8957: [discriminator loss: 0.583464, acc: 0.734375] [adversarial loss: 1.114520, acc: 0.265625]\n",
      "8958: [discriminator loss: 0.541426, acc: 0.703125] [adversarial loss: 1.064555, acc: 0.281250]\n",
      "8959: [discriminator loss: 0.541158, acc: 0.695312] [adversarial loss: 1.126401, acc: 0.250000]\n",
      "8960: [discriminator loss: 0.579356, acc: 0.664062] [adversarial loss: 0.944663, acc: 0.281250]\n",
      "8961: [discriminator loss: 0.556162, acc: 0.710938] [adversarial loss: 1.110770, acc: 0.296875]\n",
      "8962: [discriminator loss: 0.485449, acc: 0.750000] [adversarial loss: 0.969801, acc: 0.406250]\n",
      "8963: [discriminator loss: 0.573030, acc: 0.734375] [adversarial loss: 1.503844, acc: 0.109375]\n",
      "8964: [discriminator loss: 0.678509, acc: 0.554688] [adversarial loss: 0.784243, acc: 0.531250]\n",
      "8965: [discriminator loss: 0.604214, acc: 0.632812] [adversarial loss: 1.583347, acc: 0.078125]\n",
      "8966: [discriminator loss: 0.627473, acc: 0.679688] [adversarial loss: 0.716379, acc: 0.562500]\n",
      "8967: [discriminator loss: 0.577018, acc: 0.703125] [adversarial loss: 1.457571, acc: 0.093750]\n",
      "8968: [discriminator loss: 0.520355, acc: 0.710938] [adversarial loss: 1.026138, acc: 0.265625]\n",
      "8969: [discriminator loss: 0.506408, acc: 0.750000] [adversarial loss: 1.140081, acc: 0.265625]\n",
      "8970: [discriminator loss: 0.438427, acc: 0.773438] [adversarial loss: 0.944532, acc: 0.437500]\n",
      "8971: [discriminator loss: 0.524606, acc: 0.703125] [adversarial loss: 1.284250, acc: 0.171875]\n",
      "8972: [discriminator loss: 0.558859, acc: 0.664062] [adversarial loss: 1.140558, acc: 0.312500]\n",
      "8973: [discriminator loss: 0.595150, acc: 0.664062] [adversarial loss: 1.145476, acc: 0.281250]\n",
      "8974: [discriminator loss: 0.553739, acc: 0.695312] [adversarial loss: 1.158206, acc: 0.140625]\n",
      "8975: [discriminator loss: 0.640579, acc: 0.679688] [adversarial loss: 1.057953, acc: 0.265625]\n",
      "8976: [discriminator loss: 0.538547, acc: 0.726562] [adversarial loss: 1.095165, acc: 0.218750]\n",
      "8977: [discriminator loss: 0.534531, acc: 0.710938] [adversarial loss: 1.496695, acc: 0.062500]\n",
      "8978: [discriminator loss: 0.567570, acc: 0.695312] [adversarial loss: 1.358213, acc: 0.156250]\n",
      "8979: [discriminator loss: 0.542291, acc: 0.734375] [adversarial loss: 1.165941, acc: 0.234375]\n",
      "8980: [discriminator loss: 0.644825, acc: 0.710938] [adversarial loss: 1.160340, acc: 0.250000]\n",
      "8981: [discriminator loss: 0.511632, acc: 0.757812] [adversarial loss: 1.013663, acc: 0.328125]\n",
      "8982: [discriminator loss: 0.523016, acc: 0.695312] [adversarial loss: 1.252769, acc: 0.203125]\n",
      "8983: [discriminator loss: 0.566512, acc: 0.695312] [adversarial loss: 1.080070, acc: 0.218750]\n",
      "8984: [discriminator loss: 0.608570, acc: 0.687500] [adversarial loss: 1.087417, acc: 0.234375]\n",
      "8985: [discriminator loss: 0.501081, acc: 0.773438] [adversarial loss: 1.162576, acc: 0.250000]\n",
      "8986: [discriminator loss: 0.456075, acc: 0.781250] [adversarial loss: 1.311399, acc: 0.125000]\n",
      "8987: [discriminator loss: 0.586449, acc: 0.664062] [adversarial loss: 1.328522, acc: 0.109375]\n",
      "8988: [discriminator loss: 0.544491, acc: 0.726562] [adversarial loss: 1.082573, acc: 0.250000]\n",
      "8989: [discriminator loss: 0.509437, acc: 0.781250] [adversarial loss: 1.474344, acc: 0.156250]\n",
      "8990: [discriminator loss: 0.506631, acc: 0.757812] [adversarial loss: 0.855913, acc: 0.359375]\n",
      "8991: [discriminator loss: 0.502728, acc: 0.757812] [adversarial loss: 1.667105, acc: 0.046875]\n",
      "8992: [discriminator loss: 0.558800, acc: 0.671875] [adversarial loss: 0.903848, acc: 0.437500]\n",
      "8993: [discriminator loss: 0.570360, acc: 0.703125] [adversarial loss: 1.500717, acc: 0.062500]\n",
      "8994: [discriminator loss: 0.568149, acc: 0.695312] [adversarial loss: 0.810984, acc: 0.406250]\n",
      "8995: [discriminator loss: 0.584473, acc: 0.671875] [adversarial loss: 1.778610, acc: 0.062500]\n",
      "8996: [discriminator loss: 0.534813, acc: 0.742188] [adversarial loss: 0.825584, acc: 0.421875]\n",
      "8997: [discriminator loss: 0.559583, acc: 0.718750] [adversarial loss: 0.924734, acc: 0.437500]\n",
      "8998: [discriminator loss: 0.555125, acc: 0.710938] [adversarial loss: 1.409974, acc: 0.156250]\n",
      "8999: [discriminator loss: 0.541150, acc: 0.726562] [adversarial loss: 0.891489, acc: 0.343750]\n",
      "9000: [discriminator loss: 0.535949, acc: 0.710938] [adversarial loss: 1.311214, acc: 0.187500]\n",
      "9001: [discriminator loss: 0.573398, acc: 0.656250] [adversarial loss: 0.999612, acc: 0.328125]\n",
      "9002: [discriminator loss: 0.593044, acc: 0.695312] [adversarial loss: 1.502406, acc: 0.109375]\n",
      "9003: [discriminator loss: 0.586974, acc: 0.679688] [adversarial loss: 0.914745, acc: 0.359375]\n",
      "9004: [discriminator loss: 0.652104, acc: 0.671875] [adversarial loss: 1.305557, acc: 0.140625]\n",
      "9005: [discriminator loss: 0.522808, acc: 0.695312] [adversarial loss: 0.896880, acc: 0.484375]\n",
      "9006: [discriminator loss: 0.520280, acc: 0.718750] [adversarial loss: 1.301701, acc: 0.234375]\n",
      "9007: [discriminator loss: 0.584378, acc: 0.664062] [adversarial loss: 1.033174, acc: 0.312500]\n",
      "9008: [discriminator loss: 0.435285, acc: 0.828125] [adversarial loss: 1.301580, acc: 0.218750]\n",
      "9009: [discriminator loss: 0.577566, acc: 0.656250] [adversarial loss: 1.093265, acc: 0.156250]\n",
      "9010: [discriminator loss: 0.509167, acc: 0.750000] [adversarial loss: 1.401484, acc: 0.109375]\n",
      "9011: [discriminator loss: 0.501377, acc: 0.773438] [adversarial loss: 0.858259, acc: 0.406250]\n",
      "9012: [discriminator loss: 0.518073, acc: 0.765625] [adversarial loss: 1.370832, acc: 0.125000]\n",
      "9013: [discriminator loss: 0.533483, acc: 0.710938] [adversarial loss: 1.117911, acc: 0.156250]\n",
      "9014: [discriminator loss: 0.542039, acc: 0.726562] [adversarial loss: 1.414242, acc: 0.156250]\n",
      "9015: [discriminator loss: 0.617322, acc: 0.656250] [adversarial loss: 1.116899, acc: 0.296875]\n",
      "9016: [discriminator loss: 0.509865, acc: 0.726562] [adversarial loss: 1.434293, acc: 0.156250]\n",
      "9017: [discriminator loss: 0.542832, acc: 0.742188] [adversarial loss: 1.004259, acc: 0.234375]\n",
      "9018: [discriminator loss: 0.510825, acc: 0.750000] [adversarial loss: 1.329060, acc: 0.171875]\n",
      "9019: [discriminator loss: 0.506087, acc: 0.695312] [adversarial loss: 0.959758, acc: 0.312500]\n",
      "9020: [discriminator loss: 0.534753, acc: 0.718750] [adversarial loss: 1.242405, acc: 0.203125]\n",
      "9021: [discriminator loss: 0.545150, acc: 0.703125] [adversarial loss: 0.959052, acc: 0.390625]\n",
      "9022: [discriminator loss: 0.551962, acc: 0.742188] [adversarial loss: 1.325017, acc: 0.125000]\n",
      "9023: [discriminator loss: 0.597828, acc: 0.671875] [adversarial loss: 1.046266, acc: 0.281250]\n",
      "9024: [discriminator loss: 0.588639, acc: 0.632812] [adversarial loss: 1.318847, acc: 0.140625]\n",
      "9025: [discriminator loss: 0.489511, acc: 0.757812] [adversarial loss: 1.086006, acc: 0.250000]\n",
      "9026: [discriminator loss: 0.520473, acc: 0.703125] [adversarial loss: 1.146998, acc: 0.218750]\n",
      "9027: [discriminator loss: 0.565034, acc: 0.710938] [adversarial loss: 1.322497, acc: 0.203125]\n",
      "9028: [discriminator loss: 0.605114, acc: 0.671875] [adversarial loss: 0.924949, acc: 0.406250]\n",
      "9029: [discriminator loss: 0.534213, acc: 0.734375] [adversarial loss: 1.467094, acc: 0.125000]\n",
      "9030: [discriminator loss: 0.580075, acc: 0.695312] [adversarial loss: 1.136642, acc: 0.234375]\n",
      "9031: [discriminator loss: 0.614300, acc: 0.656250] [adversarial loss: 1.244891, acc: 0.171875]\n",
      "9032: [discriminator loss: 0.518011, acc: 0.710938] [adversarial loss: 1.412169, acc: 0.093750]\n",
      "9033: [discriminator loss: 0.628065, acc: 0.617188] [adversarial loss: 0.736832, acc: 0.515625]\n",
      "9034: [discriminator loss: 0.607006, acc: 0.687500] [adversarial loss: 1.344660, acc: 0.109375]\n",
      "9035: [discriminator loss: 0.597382, acc: 0.664062] [adversarial loss: 0.998496, acc: 0.328125]\n",
      "9036: [discriminator loss: 0.513216, acc: 0.710938] [adversarial loss: 1.420549, acc: 0.156250]\n",
      "9037: [discriminator loss: 0.539754, acc: 0.726562] [adversarial loss: 1.078710, acc: 0.328125]\n",
      "9038: [discriminator loss: 0.578744, acc: 0.695312] [adversarial loss: 1.160391, acc: 0.218750]\n",
      "9039: [discriminator loss: 0.516919, acc: 0.726562] [adversarial loss: 1.072257, acc: 0.250000]\n",
      "9040: [discriminator loss: 0.526687, acc: 0.718750] [adversarial loss: 1.531475, acc: 0.062500]\n",
      "9041: [discriminator loss: 0.567108, acc: 0.710938] [adversarial loss: 1.091750, acc: 0.312500]\n",
      "9042: [discriminator loss: 0.522127, acc: 0.742188] [adversarial loss: 1.013429, acc: 0.296875]\n",
      "9043: [discriminator loss: 0.634418, acc: 0.671875] [adversarial loss: 1.005887, acc: 0.359375]\n",
      "9044: [discriminator loss: 0.574744, acc: 0.703125] [adversarial loss: 1.243626, acc: 0.250000]\n",
      "9045: [discriminator loss: 0.539799, acc: 0.710938] [adversarial loss: 1.073968, acc: 0.359375]\n",
      "9046: [discriminator loss: 0.513071, acc: 0.750000] [adversarial loss: 1.591363, acc: 0.109375]\n",
      "9047: [discriminator loss: 0.561906, acc: 0.679688] [adversarial loss: 0.686984, acc: 0.531250]\n",
      "9048: [discriminator loss: 0.567855, acc: 0.687500] [adversarial loss: 1.556535, acc: 0.078125]\n",
      "9049: [discriminator loss: 0.667235, acc: 0.609375] [adversarial loss: 0.810025, acc: 0.468750]\n",
      "9050: [discriminator loss: 0.554464, acc: 0.664062] [adversarial loss: 1.240649, acc: 0.171875]\n",
      "9051: [discriminator loss: 0.566329, acc: 0.671875] [adversarial loss: 0.857768, acc: 0.484375]\n",
      "9052: [discriminator loss: 0.509825, acc: 0.757812] [adversarial loss: 1.200613, acc: 0.250000]\n",
      "9053: [discriminator loss: 0.554601, acc: 0.726562] [adversarial loss: 1.419072, acc: 0.109375]\n",
      "9054: [discriminator loss: 0.480335, acc: 0.773438] [adversarial loss: 0.995140, acc: 0.406250]\n",
      "9055: [discriminator loss: 0.515553, acc: 0.757812] [adversarial loss: 1.360755, acc: 0.234375]\n",
      "9056: [discriminator loss: 0.558139, acc: 0.703125] [adversarial loss: 1.024793, acc: 0.296875]\n",
      "9057: [discriminator loss: 0.622978, acc: 0.679688] [adversarial loss: 1.399655, acc: 0.125000]\n",
      "9058: [discriminator loss: 0.623572, acc: 0.640625] [adversarial loss: 0.920254, acc: 0.421875]\n",
      "9059: [discriminator loss: 0.597138, acc: 0.671875] [adversarial loss: 1.319336, acc: 0.125000]\n",
      "9060: [discriminator loss: 0.556617, acc: 0.726562] [adversarial loss: 1.016123, acc: 0.281250]\n",
      "9061: [discriminator loss: 0.533517, acc: 0.726562] [adversarial loss: 1.330902, acc: 0.218750]\n",
      "9062: [discriminator loss: 0.561105, acc: 0.679688] [adversarial loss: 1.189035, acc: 0.140625]\n",
      "9063: [discriminator loss: 0.532393, acc: 0.718750] [adversarial loss: 1.248533, acc: 0.125000]\n",
      "9064: [discriminator loss: 0.512257, acc: 0.789062] [adversarial loss: 1.099107, acc: 0.234375]\n",
      "9065: [discriminator loss: 0.597855, acc: 0.640625] [adversarial loss: 1.252220, acc: 0.187500]\n",
      "9066: [discriminator loss: 0.549528, acc: 0.710938] [adversarial loss: 1.086791, acc: 0.296875]\n",
      "9067: [discriminator loss: 0.535911, acc: 0.710938] [adversarial loss: 0.997789, acc: 0.359375]\n",
      "9068: [discriminator loss: 0.547526, acc: 0.718750] [adversarial loss: 1.377121, acc: 0.234375]\n",
      "9069: [discriminator loss: 0.567675, acc: 0.687500] [adversarial loss: 0.966535, acc: 0.375000]\n",
      "9070: [discriminator loss: 0.550822, acc: 0.750000] [adversarial loss: 1.329179, acc: 0.171875]\n",
      "9071: [discriminator loss: 0.573037, acc: 0.703125] [adversarial loss: 0.935602, acc: 0.343750]\n",
      "9072: [discriminator loss: 0.551867, acc: 0.718750] [adversarial loss: 1.207594, acc: 0.234375]\n",
      "9073: [discriminator loss: 0.548738, acc: 0.734375] [adversarial loss: 0.872537, acc: 0.437500]\n",
      "9074: [discriminator loss: 0.507618, acc: 0.765625] [adversarial loss: 1.435435, acc: 0.156250]\n",
      "9075: [discriminator loss: 0.505176, acc: 0.742188] [adversarial loss: 1.137620, acc: 0.234375]\n",
      "9076: [discriminator loss: 0.581605, acc: 0.718750] [adversarial loss: 1.260006, acc: 0.265625]\n",
      "9077: [discriminator loss: 0.512313, acc: 0.773438] [adversarial loss: 0.978068, acc: 0.343750]\n",
      "9078: [discriminator loss: 0.559908, acc: 0.695312] [adversarial loss: 1.423840, acc: 0.125000]\n",
      "9079: [discriminator loss: 0.598399, acc: 0.664062] [adversarial loss: 0.934310, acc: 0.328125]\n",
      "9080: [discriminator loss: 0.590946, acc: 0.687500] [adversarial loss: 1.269819, acc: 0.234375]\n",
      "9081: [discriminator loss: 0.661601, acc: 0.609375] [adversarial loss: 1.355589, acc: 0.171875]\n",
      "9082: [discriminator loss: 0.522908, acc: 0.734375] [adversarial loss: 1.308215, acc: 0.250000]\n",
      "9083: [discriminator loss: 0.498240, acc: 0.765625] [adversarial loss: 1.137155, acc: 0.250000]\n",
      "9084: [discriminator loss: 0.478264, acc: 0.781250] [adversarial loss: 1.043839, acc: 0.359375]\n",
      "9085: [discriminator loss: 0.585582, acc: 0.695312] [adversarial loss: 1.117520, acc: 0.250000]\n",
      "9086: [discriminator loss: 0.528612, acc: 0.757812] [adversarial loss: 0.973539, acc: 0.359375]\n",
      "9087: [discriminator loss: 0.555557, acc: 0.703125] [adversarial loss: 1.466158, acc: 0.093750]\n",
      "9088: [discriminator loss: 0.504479, acc: 0.710938] [adversarial loss: 1.007295, acc: 0.359375]\n",
      "9089: [discriminator loss: 0.589388, acc: 0.656250] [adversarial loss: 1.204659, acc: 0.250000]\n",
      "9090: [discriminator loss: 0.575133, acc: 0.656250] [adversarial loss: 1.622403, acc: 0.109375]\n",
      "9091: [discriminator loss: 0.637573, acc: 0.648438] [adversarial loss: 0.650127, acc: 0.562500]\n",
      "9092: [discriminator loss: 0.584400, acc: 0.703125] [adversarial loss: 1.412782, acc: 0.125000]\n",
      "9093: [discriminator loss: 0.572781, acc: 0.671875] [adversarial loss: 0.887186, acc: 0.437500]\n",
      "9094: [discriminator loss: 0.543441, acc: 0.703125] [adversarial loss: 1.209788, acc: 0.156250]\n",
      "9095: [discriminator loss: 0.561745, acc: 0.710938] [adversarial loss: 1.211716, acc: 0.140625]\n",
      "9096: [discriminator loss: 0.526184, acc: 0.703125] [adversarial loss: 1.217474, acc: 0.265625]\n",
      "9097: [discriminator loss: 0.517454, acc: 0.726562] [adversarial loss: 1.433577, acc: 0.062500]\n",
      "9098: [discriminator loss: 0.547110, acc: 0.687500] [adversarial loss: 1.012783, acc: 0.296875]\n",
      "9099: [discriminator loss: 0.589924, acc: 0.726562] [adversarial loss: 1.269175, acc: 0.250000]\n",
      "9100: [discriminator loss: 0.526483, acc: 0.750000] [adversarial loss: 0.977227, acc: 0.406250]\n",
      "9101: [discriminator loss: 0.496119, acc: 0.750000] [adversarial loss: 1.264819, acc: 0.234375]\n",
      "9102: [discriminator loss: 0.570110, acc: 0.703125] [adversarial loss: 1.123379, acc: 0.187500]\n",
      "9103: [discriminator loss: 0.532281, acc: 0.703125] [adversarial loss: 1.153118, acc: 0.250000]\n",
      "9104: [discriminator loss: 0.547601, acc: 0.757812] [adversarial loss: 1.247357, acc: 0.187500]\n",
      "9105: [discriminator loss: 0.532161, acc: 0.703125] [adversarial loss: 1.127686, acc: 0.234375]\n",
      "9106: [discriminator loss: 0.489091, acc: 0.804688] [adversarial loss: 1.354614, acc: 0.187500]\n",
      "9107: [discriminator loss: 0.558530, acc: 0.710938] [adversarial loss: 0.998198, acc: 0.328125]\n",
      "9108: [discriminator loss: 0.538767, acc: 0.710938] [adversarial loss: 1.070669, acc: 0.359375]\n",
      "9109: [discriminator loss: 0.574100, acc: 0.718750] [adversarial loss: 1.257737, acc: 0.218750]\n",
      "9110: [discriminator loss: 0.515365, acc: 0.718750] [adversarial loss: 1.099467, acc: 0.281250]\n",
      "9111: [discriminator loss: 0.551694, acc: 0.710938] [adversarial loss: 1.358014, acc: 0.140625]\n",
      "9112: [discriminator loss: 0.614152, acc: 0.656250] [adversarial loss: 0.848133, acc: 0.468750]\n",
      "9113: [discriminator loss: 0.584817, acc: 0.726562] [adversarial loss: 1.334714, acc: 0.093750]\n",
      "9114: [discriminator loss: 0.602496, acc: 0.617188] [adversarial loss: 0.835416, acc: 0.562500]\n",
      "9115: [discriminator loss: 0.569217, acc: 0.679688] [adversarial loss: 1.507455, acc: 0.109375]\n",
      "9116: [discriminator loss: 0.510451, acc: 0.757812] [adversarial loss: 0.986467, acc: 0.265625]\n",
      "9117: [discriminator loss: 0.533536, acc: 0.742188] [adversarial loss: 1.416239, acc: 0.171875]\n",
      "9118: [discriminator loss: 0.460973, acc: 0.742188] [adversarial loss: 1.099355, acc: 0.187500]\n",
      "9119: [discriminator loss: 0.546867, acc: 0.671875] [adversarial loss: 1.316039, acc: 0.125000]\n",
      "9120: [discriminator loss: 0.523285, acc: 0.703125] [adversarial loss: 1.113105, acc: 0.250000]\n",
      "9121: [discriminator loss: 0.538649, acc: 0.734375] [adversarial loss: 1.172074, acc: 0.187500]\n",
      "9122: [discriminator loss: 0.518254, acc: 0.718750] [adversarial loss: 1.051983, acc: 0.328125]\n",
      "9123: [discriminator loss: 0.569211, acc: 0.664062] [adversarial loss: 1.296942, acc: 0.109375]\n",
      "9124: [discriminator loss: 0.577112, acc: 0.656250] [adversarial loss: 0.874743, acc: 0.484375]\n",
      "9125: [discriminator loss: 0.563281, acc: 0.710938] [adversarial loss: 1.233468, acc: 0.234375]\n",
      "9126: [discriminator loss: 0.511254, acc: 0.726562] [adversarial loss: 1.345441, acc: 0.109375]\n",
      "9127: [discriminator loss: 0.608041, acc: 0.648438] [adversarial loss: 0.816942, acc: 0.437500]\n",
      "9128: [discriminator loss: 0.556282, acc: 0.710938] [adversarial loss: 1.200368, acc: 0.203125]\n",
      "9129: [discriminator loss: 0.516082, acc: 0.742188] [adversarial loss: 0.988823, acc: 0.375000]\n",
      "9130: [discriminator loss: 0.497142, acc: 0.750000] [adversarial loss: 1.481545, acc: 0.140625]\n",
      "9131: [discriminator loss: 0.565729, acc: 0.703125] [adversarial loss: 0.986904, acc: 0.312500]\n",
      "9132: [discriminator loss: 0.595716, acc: 0.671875] [adversarial loss: 1.388719, acc: 0.187500]\n",
      "9133: [discriminator loss: 0.523944, acc: 0.742188] [adversarial loss: 0.987797, acc: 0.406250]\n",
      "9134: [discriminator loss: 0.573303, acc: 0.687500] [adversarial loss: 1.606071, acc: 0.062500]\n",
      "9135: [discriminator loss: 0.576522, acc: 0.687500] [adversarial loss: 0.982366, acc: 0.312500]\n",
      "9136: [discriminator loss: 0.511161, acc: 0.757812] [adversarial loss: 1.292696, acc: 0.187500]\n",
      "9137: [discriminator loss: 0.583001, acc: 0.703125] [adversarial loss: 1.089802, acc: 0.250000]\n",
      "9138: [discriminator loss: 0.497625, acc: 0.734375] [adversarial loss: 1.379727, acc: 0.093750]\n",
      "9139: [discriminator loss: 0.549493, acc: 0.687500] [adversarial loss: 0.918275, acc: 0.390625]\n",
      "9140: [discriminator loss: 0.532530, acc: 0.695312] [adversarial loss: 1.715625, acc: 0.062500]\n",
      "9141: [discriminator loss: 0.566397, acc: 0.687500] [adversarial loss: 0.976143, acc: 0.343750]\n",
      "9142: [discriminator loss: 0.557595, acc: 0.695312] [adversarial loss: 1.084985, acc: 0.250000]\n",
      "9143: [discriminator loss: 0.627450, acc: 0.640625] [adversarial loss: 1.386582, acc: 0.093750]\n",
      "9144: [discriminator loss: 0.493905, acc: 0.742188] [adversarial loss: 1.105873, acc: 0.281250]\n",
      "9145: [discriminator loss: 0.533953, acc: 0.734375] [adversarial loss: 1.199552, acc: 0.250000]\n",
      "9146: [discriminator loss: 0.513058, acc: 0.750000] [adversarial loss: 1.277319, acc: 0.171875]\n",
      "9147: [discriminator loss: 0.492479, acc: 0.734375] [adversarial loss: 1.129757, acc: 0.203125]\n",
      "9148: [discriminator loss: 0.545501, acc: 0.679688] [adversarial loss: 1.061532, acc: 0.296875]\n",
      "9149: [discriminator loss: 0.553695, acc: 0.695312] [adversarial loss: 1.359640, acc: 0.078125]\n",
      "9150: [discriminator loss: 0.625515, acc: 0.640625] [adversarial loss: 0.995052, acc: 0.437500]\n",
      "9151: [discriminator loss: 0.494071, acc: 0.750000] [adversarial loss: 0.961389, acc: 0.375000]\n",
      "9152: [discriminator loss: 0.616511, acc: 0.609375] [adversarial loss: 1.359113, acc: 0.187500]\n",
      "9153: [discriminator loss: 0.469956, acc: 0.765625] [adversarial loss: 1.055557, acc: 0.265625]\n",
      "9154: [discriminator loss: 0.501535, acc: 0.734375] [adversarial loss: 1.342687, acc: 0.109375]\n",
      "9155: [discriminator loss: 0.503708, acc: 0.765625] [adversarial loss: 1.104138, acc: 0.250000]\n",
      "9156: [discriminator loss: 0.561499, acc: 0.679688] [adversarial loss: 1.085915, acc: 0.312500]\n",
      "9157: [discriminator loss: 0.633003, acc: 0.679688] [adversarial loss: 1.284157, acc: 0.187500]\n",
      "9158: [discriminator loss: 0.509552, acc: 0.703125] [adversarial loss: 1.335543, acc: 0.218750]\n",
      "9159: [discriminator loss: 0.525300, acc: 0.726562] [adversarial loss: 0.926906, acc: 0.421875]\n",
      "9160: [discriminator loss: 0.534799, acc: 0.710938] [adversarial loss: 1.102868, acc: 0.281250]\n",
      "9161: [discriminator loss: 0.565924, acc: 0.664062] [adversarial loss: 1.192207, acc: 0.234375]\n",
      "9162: [discriminator loss: 0.561940, acc: 0.679688] [adversarial loss: 1.044734, acc: 0.296875]\n",
      "9163: [discriminator loss: 0.634544, acc: 0.656250] [adversarial loss: 1.263248, acc: 0.203125]\n",
      "9164: [discriminator loss: 0.532955, acc: 0.703125] [adversarial loss: 0.751968, acc: 0.500000]\n",
      "9165: [discriminator loss: 0.590756, acc: 0.695312] [adversarial loss: 1.709506, acc: 0.093750]\n",
      "9166: [discriminator loss: 0.672372, acc: 0.578125] [adversarial loss: 0.982637, acc: 0.359375]\n",
      "9167: [discriminator loss: 0.559848, acc: 0.679688] [adversarial loss: 1.314010, acc: 0.156250]\n",
      "9168: [discriminator loss: 0.538783, acc: 0.718750] [adversarial loss: 1.063314, acc: 0.250000]\n",
      "9169: [discriminator loss: 0.524969, acc: 0.710938] [adversarial loss: 1.346519, acc: 0.093750]\n",
      "9170: [discriminator loss: 0.555428, acc: 0.648438] [adversarial loss: 0.977195, acc: 0.328125]\n",
      "9171: [discriminator loss: 0.438589, acc: 0.789062] [adversarial loss: 1.224119, acc: 0.234375]\n",
      "9172: [discriminator loss: 0.586135, acc: 0.687500] [adversarial loss: 1.374407, acc: 0.140625]\n",
      "9173: [discriminator loss: 0.527401, acc: 0.710938] [adversarial loss: 1.242073, acc: 0.140625]\n",
      "9174: [discriminator loss: 0.478663, acc: 0.773438] [adversarial loss: 1.220961, acc: 0.250000]\n",
      "9175: [discriminator loss: 0.630040, acc: 0.625000] [adversarial loss: 1.076700, acc: 0.359375]\n",
      "9176: [discriminator loss: 0.604864, acc: 0.703125] [adversarial loss: 1.083411, acc: 0.281250]\n",
      "9177: [discriminator loss: 0.543592, acc: 0.671875] [adversarial loss: 1.449908, acc: 0.093750]\n",
      "9178: [discriminator loss: 0.549312, acc: 0.734375] [adversarial loss: 1.036814, acc: 0.312500]\n",
      "9179: [discriminator loss: 0.578921, acc: 0.695312] [adversarial loss: 1.329868, acc: 0.156250]\n",
      "9180: [discriminator loss: 0.471425, acc: 0.796875] [adversarial loss: 1.078897, acc: 0.281250]\n",
      "9181: [discriminator loss: 0.522406, acc: 0.726562] [adversarial loss: 1.092535, acc: 0.328125]\n",
      "9182: [discriminator loss: 0.552631, acc: 0.718750] [adversarial loss: 1.110724, acc: 0.343750]\n",
      "9183: [discriminator loss: 0.508071, acc: 0.757812] [adversarial loss: 1.202436, acc: 0.312500]\n",
      "9184: [discriminator loss: 0.551016, acc: 0.695312] [adversarial loss: 0.970451, acc: 0.296875]\n",
      "9185: [discriminator loss: 0.540977, acc: 0.726562] [adversarial loss: 1.601095, acc: 0.078125]\n",
      "9186: [discriminator loss: 0.587162, acc: 0.671875] [adversarial loss: 0.740922, acc: 0.500000]\n",
      "9187: [discriminator loss: 0.595848, acc: 0.664062] [adversarial loss: 1.254611, acc: 0.218750]\n",
      "9188: [discriminator loss: 0.531406, acc: 0.765625] [adversarial loss: 0.982808, acc: 0.390625]\n",
      "9189: [discriminator loss: 0.528350, acc: 0.710938] [adversarial loss: 1.397905, acc: 0.078125]\n",
      "9190: [discriminator loss: 0.538003, acc: 0.695312] [adversarial loss: 0.883019, acc: 0.468750]\n",
      "9191: [discriminator loss: 0.564606, acc: 0.679688] [adversarial loss: 1.272369, acc: 0.156250]\n",
      "9192: [discriminator loss: 0.494116, acc: 0.765625] [adversarial loss: 1.067344, acc: 0.296875]\n",
      "9193: [discriminator loss: 0.561271, acc: 0.710938] [adversarial loss: 1.450536, acc: 0.109375]\n",
      "9194: [discriminator loss: 0.551190, acc: 0.695312] [adversarial loss: 0.978009, acc: 0.281250]\n",
      "9195: [discriminator loss: 0.562618, acc: 0.703125] [adversarial loss: 1.352359, acc: 0.171875]\n",
      "9196: [discriminator loss: 0.554499, acc: 0.703125] [adversarial loss: 1.072061, acc: 0.312500]\n",
      "9197: [discriminator loss: 0.553729, acc: 0.640625] [adversarial loss: 1.663755, acc: 0.078125]\n",
      "9198: [discriminator loss: 0.528471, acc: 0.710938] [adversarial loss: 0.833946, acc: 0.437500]\n",
      "9199: [discriminator loss: 0.558899, acc: 0.742188] [adversarial loss: 1.412153, acc: 0.234375]\n",
      "9200: [discriminator loss: 0.598918, acc: 0.679688] [adversarial loss: 1.109433, acc: 0.218750]\n",
      "9201: [discriminator loss: 0.542944, acc: 0.726562] [adversarial loss: 1.133508, acc: 0.250000]\n",
      "9202: [discriminator loss: 0.538181, acc: 0.695312] [adversarial loss: 1.155772, acc: 0.234375]\n",
      "9203: [discriminator loss: 0.559514, acc: 0.703125] [adversarial loss: 1.175274, acc: 0.281250]\n",
      "9204: [discriminator loss: 0.520884, acc: 0.750000] [adversarial loss: 1.061459, acc: 0.265625]\n",
      "9205: [discriminator loss: 0.548585, acc: 0.664062] [adversarial loss: 1.148751, acc: 0.328125]\n",
      "9206: [discriminator loss: 0.570815, acc: 0.726562] [adversarial loss: 1.367963, acc: 0.109375]\n",
      "9207: [discriminator loss: 0.583609, acc: 0.687500] [adversarial loss: 0.812121, acc: 0.515625]\n",
      "9208: [discriminator loss: 0.526303, acc: 0.695312] [adversarial loss: 1.512281, acc: 0.125000]\n",
      "9209: [discriminator loss: 0.630821, acc: 0.632812] [adversarial loss: 0.981113, acc: 0.296875]\n",
      "9210: [discriminator loss: 0.557195, acc: 0.710938] [adversarial loss: 1.513918, acc: 0.109375]\n",
      "9211: [discriminator loss: 0.566084, acc: 0.750000] [adversarial loss: 0.926356, acc: 0.390625]\n",
      "9212: [discriminator loss: 0.573659, acc: 0.703125] [adversarial loss: 1.340315, acc: 0.187500]\n",
      "9213: [discriminator loss: 0.525568, acc: 0.726562] [adversarial loss: 0.968381, acc: 0.437500]\n",
      "9214: [discriminator loss: 0.596626, acc: 0.648438] [adversarial loss: 1.258110, acc: 0.234375]\n",
      "9215: [discriminator loss: 0.511889, acc: 0.742188] [adversarial loss: 1.398468, acc: 0.156250]\n",
      "9216: [discriminator loss: 0.501863, acc: 0.742188] [adversarial loss: 1.033717, acc: 0.281250]\n",
      "9217: [discriminator loss: 0.513782, acc: 0.750000] [adversarial loss: 1.134890, acc: 0.250000]\n",
      "9218: [discriminator loss: 0.524466, acc: 0.757812] [adversarial loss: 1.291620, acc: 0.218750]\n",
      "9219: [discriminator loss: 0.527936, acc: 0.726562] [adversarial loss: 0.894364, acc: 0.406250]\n",
      "9220: [discriminator loss: 0.512593, acc: 0.757812] [adversarial loss: 1.241218, acc: 0.187500]\n",
      "9221: [discriminator loss: 0.575633, acc: 0.671875] [adversarial loss: 0.889756, acc: 0.437500]\n",
      "9222: [discriminator loss: 0.559513, acc: 0.710938] [adversarial loss: 1.418316, acc: 0.093750]\n",
      "9223: [discriminator loss: 0.512242, acc: 0.757812] [adversarial loss: 1.149725, acc: 0.234375]\n",
      "9224: [discriminator loss: 0.599455, acc: 0.671875] [adversarial loss: 1.009529, acc: 0.406250]\n",
      "9225: [discriminator loss: 0.499247, acc: 0.718750] [adversarial loss: 0.937013, acc: 0.375000]\n",
      "9226: [discriminator loss: 0.490715, acc: 0.703125] [adversarial loss: 1.226238, acc: 0.250000]\n",
      "9227: [discriminator loss: 0.621475, acc: 0.648438] [adversarial loss: 1.302348, acc: 0.218750]\n",
      "9228: [discriminator loss: 0.557534, acc: 0.687500] [adversarial loss: 1.175074, acc: 0.187500]\n",
      "9229: [discriminator loss: 0.530327, acc: 0.679688] [adversarial loss: 1.231076, acc: 0.156250]\n",
      "9230: [discriminator loss: 0.478140, acc: 0.765625] [adversarial loss: 1.325625, acc: 0.093750]\n",
      "9231: [discriminator loss: 0.542461, acc: 0.726562] [adversarial loss: 1.016505, acc: 0.312500]\n",
      "9232: [discriminator loss: 0.563442, acc: 0.671875] [adversarial loss: 1.257092, acc: 0.140625]\n",
      "9233: [discriminator loss: 0.579917, acc: 0.640625] [adversarial loss: 0.863990, acc: 0.468750]\n",
      "9234: [discriminator loss: 0.555707, acc: 0.703125] [adversarial loss: 1.395148, acc: 0.109375]\n",
      "9235: [discriminator loss: 0.570326, acc: 0.718750] [adversarial loss: 1.009403, acc: 0.359375]\n",
      "9236: [discriminator loss: 0.576677, acc: 0.710938] [adversarial loss: 1.153460, acc: 0.234375]\n",
      "9237: [discriminator loss: 0.531510, acc: 0.695312] [adversarial loss: 1.187955, acc: 0.250000]\n",
      "9238: [discriminator loss: 0.549361, acc: 0.679688] [adversarial loss: 0.932963, acc: 0.421875]\n",
      "9239: [discriminator loss: 0.524886, acc: 0.734375] [adversarial loss: 1.262365, acc: 0.234375]\n",
      "9240: [discriminator loss: 0.572533, acc: 0.703125] [adversarial loss: 1.030485, acc: 0.343750]\n",
      "9241: [discriminator loss: 0.561565, acc: 0.726562] [adversarial loss: 1.461392, acc: 0.156250]\n",
      "9242: [discriminator loss: 0.521404, acc: 0.742188] [adversarial loss: 0.960064, acc: 0.375000]\n",
      "9243: [discriminator loss: 0.568424, acc: 0.679688] [adversarial loss: 1.445914, acc: 0.140625]\n",
      "9244: [discriminator loss: 0.524794, acc: 0.726562] [adversarial loss: 0.888496, acc: 0.390625]\n",
      "9245: [discriminator loss: 0.578292, acc: 0.679688] [adversarial loss: 1.412786, acc: 0.171875]\n",
      "9246: [discriminator loss: 0.582266, acc: 0.656250] [adversarial loss: 1.024923, acc: 0.421875]\n",
      "9247: [discriminator loss: 0.620312, acc: 0.648438] [adversarial loss: 0.989478, acc: 0.375000]\n",
      "9248: [discriminator loss: 0.526649, acc: 0.781250] [adversarial loss: 1.472809, acc: 0.203125]\n",
      "9249: [discriminator loss: 0.503936, acc: 0.718750] [adversarial loss: 1.157412, acc: 0.359375]\n",
      "9250: [discriminator loss: 0.557445, acc: 0.734375] [adversarial loss: 1.431793, acc: 0.156250]\n",
      "9251: [discriminator loss: 0.541949, acc: 0.687500] [adversarial loss: 0.743042, acc: 0.515625]\n",
      "9252: [discriminator loss: 0.528077, acc: 0.742188] [adversarial loss: 1.532517, acc: 0.093750]\n",
      "9253: [discriminator loss: 0.542226, acc: 0.687500] [adversarial loss: 1.136154, acc: 0.203125]\n",
      "9254: [discriminator loss: 0.494729, acc: 0.750000] [adversarial loss: 1.284023, acc: 0.140625]\n",
      "9255: [discriminator loss: 0.440234, acc: 0.796875] [adversarial loss: 1.252104, acc: 0.218750]\n",
      "9256: [discriminator loss: 0.589904, acc: 0.640625] [adversarial loss: 1.418776, acc: 0.187500]\n",
      "9257: [discriminator loss: 0.448759, acc: 0.796875] [adversarial loss: 1.100195, acc: 0.250000]\n",
      "9258: [discriminator loss: 0.495878, acc: 0.765625] [adversarial loss: 1.617194, acc: 0.093750]\n",
      "9259: [discriminator loss: 0.545398, acc: 0.703125] [adversarial loss: 0.955666, acc: 0.453125]\n",
      "9260: [discriminator loss: 0.583219, acc: 0.687500] [adversarial loss: 1.718451, acc: 0.125000]\n",
      "9261: [discriminator loss: 0.666091, acc: 0.625000] [adversarial loss: 0.825047, acc: 0.453125]\n",
      "9262: [discriminator loss: 0.593299, acc: 0.687500] [adversarial loss: 1.309951, acc: 0.187500]\n",
      "9263: [discriminator loss: 0.644727, acc: 0.695312] [adversarial loss: 0.944122, acc: 0.437500]\n",
      "9264: [discriminator loss: 0.580712, acc: 0.671875] [adversarial loss: 1.494286, acc: 0.125000]\n",
      "9265: [discriminator loss: 0.473611, acc: 0.757812] [adversarial loss: 0.982402, acc: 0.281250]\n",
      "9266: [discriminator loss: 0.452450, acc: 0.789062] [adversarial loss: 1.521036, acc: 0.140625]\n",
      "9267: [discriminator loss: 0.538345, acc: 0.726562] [adversarial loss: 0.889702, acc: 0.531250]\n",
      "9268: [discriminator loss: 0.587801, acc: 0.679688] [adversarial loss: 1.626284, acc: 0.078125]\n",
      "9269: [discriminator loss: 0.562601, acc: 0.671875] [adversarial loss: 1.147540, acc: 0.234375]\n",
      "9270: [discriminator loss: 0.571160, acc: 0.710938] [adversarial loss: 1.251223, acc: 0.109375]\n",
      "9271: [discriminator loss: 0.599512, acc: 0.718750] [adversarial loss: 1.075297, acc: 0.312500]\n",
      "9272: [discriminator loss: 0.570531, acc: 0.687500] [adversarial loss: 1.372617, acc: 0.156250]\n",
      "9273: [discriminator loss: 0.566627, acc: 0.671875] [adversarial loss: 0.955275, acc: 0.390625]\n",
      "9274: [discriminator loss: 0.556173, acc: 0.710938] [adversarial loss: 1.296778, acc: 0.171875]\n",
      "9275: [discriminator loss: 0.587052, acc: 0.656250] [adversarial loss: 1.239914, acc: 0.234375]\n",
      "9276: [discriminator loss: 0.504794, acc: 0.726562] [adversarial loss: 1.114130, acc: 0.250000]\n",
      "9277: [discriminator loss: 0.567420, acc: 0.656250] [adversarial loss: 1.128399, acc: 0.359375]\n",
      "9278: [discriminator loss: 0.554071, acc: 0.718750] [adversarial loss: 1.141631, acc: 0.296875]\n",
      "9279: [discriminator loss: 0.483016, acc: 0.757812] [adversarial loss: 1.264553, acc: 0.218750]\n",
      "9280: [discriminator loss: 0.518335, acc: 0.765625] [adversarial loss: 0.857092, acc: 0.500000]\n",
      "9281: [discriminator loss: 0.580832, acc: 0.726562] [adversarial loss: 1.787085, acc: 0.093750]\n",
      "9282: [discriminator loss: 0.548115, acc: 0.742188] [adversarial loss: 1.068739, acc: 0.281250]\n",
      "9283: [discriminator loss: 0.522258, acc: 0.687500] [adversarial loss: 1.123165, acc: 0.156250]\n",
      "9284: [discriminator loss: 0.523489, acc: 0.757812] [adversarial loss: 1.325568, acc: 0.187500]\n",
      "9285: [discriminator loss: 0.547149, acc: 0.671875] [adversarial loss: 0.882732, acc: 0.484375]\n",
      "9286: [discriminator loss: 0.574932, acc: 0.679688] [adversarial loss: 1.069567, acc: 0.281250]\n",
      "9287: [discriminator loss: 0.504973, acc: 0.750000] [adversarial loss: 1.312713, acc: 0.250000]\n",
      "9288: [discriminator loss: 0.507502, acc: 0.765625] [adversarial loss: 1.219013, acc: 0.187500]\n",
      "9289: [discriminator loss: 0.567330, acc: 0.703125] [adversarial loss: 1.332960, acc: 0.125000]\n",
      "9290: [discriminator loss: 0.512859, acc: 0.718750] [adversarial loss: 1.010011, acc: 0.359375]\n",
      "9291: [discriminator loss: 0.615153, acc: 0.648438] [adversarial loss: 1.133145, acc: 0.234375]\n",
      "9292: [discriminator loss: 0.528394, acc: 0.726562] [adversarial loss: 0.881809, acc: 0.437500]\n",
      "9293: [discriminator loss: 0.552296, acc: 0.726562] [adversarial loss: 1.580364, acc: 0.109375]\n",
      "9294: [discriminator loss: 0.551876, acc: 0.710938] [adversarial loss: 1.147348, acc: 0.265625]\n",
      "9295: [discriminator loss: 0.568329, acc: 0.695312] [adversarial loss: 1.596186, acc: 0.109375]\n",
      "9296: [discriminator loss: 0.553941, acc: 0.718750] [adversarial loss: 0.912839, acc: 0.390625]\n",
      "9297: [discriminator loss: 0.612110, acc: 0.562500] [adversarial loss: 1.277367, acc: 0.234375]\n",
      "9298: [discriminator loss: 0.535599, acc: 0.703125] [adversarial loss: 1.037411, acc: 0.281250]\n",
      "9299: [discriminator loss: 0.611668, acc: 0.648438] [adversarial loss: 1.216278, acc: 0.265625]\n",
      "9300: [discriminator loss: 0.518851, acc: 0.750000] [adversarial loss: 1.317717, acc: 0.187500]\n",
      "9301: [discriminator loss: 0.548078, acc: 0.703125] [adversarial loss: 1.500652, acc: 0.140625]\n",
      "9302: [discriminator loss: 0.547796, acc: 0.664062] [adversarial loss: 0.845788, acc: 0.437500]\n",
      "9303: [discriminator loss: 0.518895, acc: 0.757812] [adversarial loss: 1.674378, acc: 0.093750]\n",
      "9304: [discriminator loss: 0.555488, acc: 0.687500] [adversarial loss: 0.893901, acc: 0.406250]\n",
      "9305: [discriminator loss: 0.565318, acc: 0.710938] [adversarial loss: 1.307761, acc: 0.171875]\n",
      "9306: [discriminator loss: 0.591548, acc: 0.703125] [adversarial loss: 1.211064, acc: 0.312500]\n",
      "9307: [discriminator loss: 0.606176, acc: 0.679688] [adversarial loss: 1.101933, acc: 0.265625]\n",
      "9308: [discriminator loss: 0.592020, acc: 0.679688] [adversarial loss: 1.070573, acc: 0.265625]\n",
      "9309: [discriminator loss: 0.511995, acc: 0.765625] [adversarial loss: 1.187833, acc: 0.234375]\n",
      "9310: [discriminator loss: 0.507012, acc: 0.718750] [adversarial loss: 1.122806, acc: 0.265625]\n",
      "9311: [discriminator loss: 0.484476, acc: 0.781250] [adversarial loss: 1.134634, acc: 0.281250]\n",
      "9312: [discriminator loss: 0.583814, acc: 0.687500] [adversarial loss: 1.111451, acc: 0.343750]\n",
      "9313: [discriminator loss: 0.529517, acc: 0.726562] [adversarial loss: 1.095146, acc: 0.234375]\n",
      "9314: [discriminator loss: 0.524901, acc: 0.765625] [adversarial loss: 1.147583, acc: 0.234375]\n",
      "9315: [discriminator loss: 0.530228, acc: 0.726562] [adversarial loss: 1.151412, acc: 0.234375]\n",
      "9316: [discriminator loss: 0.590397, acc: 0.656250] [adversarial loss: 1.145254, acc: 0.187500]\n",
      "9317: [discriminator loss: 0.482780, acc: 0.796875] [adversarial loss: 1.278414, acc: 0.187500]\n",
      "9318: [discriminator loss: 0.524679, acc: 0.718750] [adversarial loss: 1.036032, acc: 0.343750]\n",
      "9319: [discriminator loss: 0.530354, acc: 0.703125] [adversarial loss: 1.486979, acc: 0.093750]\n",
      "9320: [discriminator loss: 0.566571, acc: 0.695312] [adversarial loss: 0.927254, acc: 0.437500]\n",
      "9321: [discriminator loss: 0.577721, acc: 0.710938] [adversarial loss: 1.892588, acc: 0.093750]\n",
      "9322: [discriminator loss: 0.656413, acc: 0.609375] [adversarial loss: 0.780805, acc: 0.546875]\n",
      "9323: [discriminator loss: 0.560592, acc: 0.695312] [adversarial loss: 1.286455, acc: 0.218750]\n",
      "9324: [discriminator loss: 0.511629, acc: 0.757812] [adversarial loss: 1.047611, acc: 0.375000]\n",
      "9325: [discriminator loss: 0.550104, acc: 0.695312] [adversarial loss: 1.156418, acc: 0.265625]\n",
      "9326: [discriminator loss: 0.544260, acc: 0.687500] [adversarial loss: 1.074364, acc: 0.296875]\n",
      "9327: [discriminator loss: 0.492935, acc: 0.789062] [adversarial loss: 1.154333, acc: 0.203125]\n",
      "9328: [discriminator loss: 0.613131, acc: 0.625000] [adversarial loss: 1.080155, acc: 0.234375]\n",
      "9329: [discriminator loss: 0.552323, acc: 0.710938] [adversarial loss: 1.256044, acc: 0.171875]\n",
      "9330: [discriminator loss: 0.519727, acc: 0.710938] [adversarial loss: 1.170695, acc: 0.187500]\n",
      "9331: [discriminator loss: 0.531629, acc: 0.734375] [adversarial loss: 1.107834, acc: 0.218750]\n",
      "9332: [discriminator loss: 0.509378, acc: 0.789062] [adversarial loss: 1.471105, acc: 0.203125]\n",
      "9333: [discriminator loss: 0.555652, acc: 0.632812] [adversarial loss: 0.842382, acc: 0.375000]\n",
      "9334: [discriminator loss: 0.615927, acc: 0.687500] [adversarial loss: 1.521809, acc: 0.062500]\n",
      "9335: [discriminator loss: 0.577288, acc: 0.687500] [adversarial loss: 0.886918, acc: 0.421875]\n",
      "9336: [discriminator loss: 0.538974, acc: 0.718750] [adversarial loss: 1.571502, acc: 0.140625]\n",
      "9337: [discriminator loss: 0.541423, acc: 0.687500] [adversarial loss: 0.892202, acc: 0.343750]\n",
      "9338: [discriminator loss: 0.577065, acc: 0.695312] [adversarial loss: 1.277213, acc: 0.234375]\n",
      "9339: [discriminator loss: 0.551048, acc: 0.687500] [adversarial loss: 1.124129, acc: 0.265625]\n",
      "9340: [discriminator loss: 0.498633, acc: 0.789062] [adversarial loss: 1.087658, acc: 0.343750]\n",
      "9341: [discriminator loss: 0.575964, acc: 0.695312] [adversarial loss: 1.091080, acc: 0.250000]\n",
      "9342: [discriminator loss: 0.546386, acc: 0.695312] [adversarial loss: 1.416670, acc: 0.140625]\n",
      "9343: [discriminator loss: 0.512558, acc: 0.710938] [adversarial loss: 1.067974, acc: 0.250000]\n",
      "9344: [discriminator loss: 0.495973, acc: 0.726562] [adversarial loss: 1.064866, acc: 0.218750]\n",
      "9345: [discriminator loss: 0.543290, acc: 0.726562] [adversarial loss: 1.309592, acc: 0.140625]\n",
      "9346: [discriminator loss: 0.586168, acc: 0.640625] [adversarial loss: 1.045945, acc: 0.296875]\n",
      "9347: [discriminator loss: 0.547178, acc: 0.734375] [adversarial loss: 1.406984, acc: 0.156250]\n",
      "9348: [discriminator loss: 0.538096, acc: 0.703125] [adversarial loss: 0.923730, acc: 0.406250]\n",
      "9349: [discriminator loss: 0.534950, acc: 0.710938] [adversarial loss: 1.519388, acc: 0.156250]\n",
      "9350: [discriminator loss: 0.593748, acc: 0.703125] [adversarial loss: 0.968055, acc: 0.406250]\n",
      "9351: [discriminator loss: 0.529361, acc: 0.773438] [adversarial loss: 1.333130, acc: 0.171875]\n",
      "9352: [discriminator loss: 0.611026, acc: 0.695312] [adversarial loss: 0.836309, acc: 0.468750]\n",
      "9353: [discriminator loss: 0.572778, acc: 0.687500] [adversarial loss: 1.404544, acc: 0.187500]\n",
      "9354: [discriminator loss: 0.552307, acc: 0.679688] [adversarial loss: 0.804597, acc: 0.375000]\n",
      "9355: [discriminator loss: 0.531220, acc: 0.718750] [adversarial loss: 1.281636, acc: 0.218750]\n",
      "9356: [discriminator loss: 0.611926, acc: 0.648438] [adversarial loss: 1.287335, acc: 0.281250]\n",
      "9357: [discriminator loss: 0.525540, acc: 0.750000] [adversarial loss: 1.353463, acc: 0.234375]\n",
      "9358: [discriminator loss: 0.540412, acc: 0.695312] [adversarial loss: 1.178255, acc: 0.265625]\n",
      "9359: [discriminator loss: 0.571260, acc: 0.671875] [adversarial loss: 1.065483, acc: 0.203125]\n",
      "9360: [discriminator loss: 0.471516, acc: 0.781250] [adversarial loss: 1.021355, acc: 0.281250]\n",
      "9361: [discriminator loss: 0.523353, acc: 0.718750] [adversarial loss: 1.353419, acc: 0.171875]\n",
      "9362: [discriminator loss: 0.505052, acc: 0.718750] [adversarial loss: 0.846251, acc: 0.406250]\n",
      "9363: [discriminator loss: 0.552354, acc: 0.695312] [adversarial loss: 1.485217, acc: 0.218750]\n",
      "9364: [discriminator loss: 0.582075, acc: 0.617188] [adversarial loss: 0.773140, acc: 0.500000]\n",
      "9365: [discriminator loss: 0.593632, acc: 0.726562] [adversarial loss: 1.377383, acc: 0.171875]\n",
      "9366: [discriminator loss: 0.567211, acc: 0.664062] [adversarial loss: 0.962882, acc: 0.328125]\n",
      "9367: [discriminator loss: 0.565701, acc: 0.750000] [adversarial loss: 1.518413, acc: 0.078125]\n",
      "9368: [discriminator loss: 0.527291, acc: 0.734375] [adversarial loss: 0.818778, acc: 0.453125]\n",
      "9369: [discriminator loss: 0.676242, acc: 0.679688] [adversarial loss: 1.506794, acc: 0.093750]\n",
      "9370: [discriminator loss: 0.571318, acc: 0.726562] [adversarial loss: 0.938383, acc: 0.375000]\n",
      "9371: [discriminator loss: 0.567242, acc: 0.703125] [adversarial loss: 1.373932, acc: 0.125000]\n",
      "9372: [discriminator loss: 0.591782, acc: 0.679688] [adversarial loss: 1.154001, acc: 0.250000]\n",
      "9373: [discriminator loss: 0.522427, acc: 0.750000] [adversarial loss: 1.022521, acc: 0.343750]\n",
      "9374: [discriminator loss: 0.558426, acc: 0.710938] [adversarial loss: 1.232394, acc: 0.203125]\n",
      "9375: [discriminator loss: 0.593718, acc: 0.726562] [adversarial loss: 1.105538, acc: 0.312500]\n",
      "9376: [discriminator loss: 0.564865, acc: 0.703125] [adversarial loss: 1.144459, acc: 0.281250]\n",
      "9377: [discriminator loss: 0.501719, acc: 0.757812] [adversarial loss: 1.128253, acc: 0.328125]\n",
      "9378: [discriminator loss: 0.461262, acc: 0.781250] [adversarial loss: 1.208125, acc: 0.265625]\n",
      "9379: [discriminator loss: 0.570925, acc: 0.687500] [adversarial loss: 0.932739, acc: 0.390625]\n",
      "9380: [discriminator loss: 0.523037, acc: 0.750000] [adversarial loss: 1.589099, acc: 0.078125]\n",
      "9381: [discriminator loss: 0.596686, acc: 0.656250] [adversarial loss: 0.748893, acc: 0.578125]\n",
      "9382: [discriminator loss: 0.556924, acc: 0.687500] [adversarial loss: 1.518610, acc: 0.109375]\n",
      "9383: [discriminator loss: 0.571472, acc: 0.726562] [adversarial loss: 1.145475, acc: 0.265625]\n",
      "9384: [discriminator loss: 0.578185, acc: 0.648438] [adversarial loss: 1.355727, acc: 0.203125]\n",
      "9385: [discriminator loss: 0.560675, acc: 0.710938] [adversarial loss: 0.983188, acc: 0.437500]\n",
      "9386: [discriminator loss: 0.522403, acc: 0.757812] [adversarial loss: 1.169760, acc: 0.203125]\n",
      "9387: [discriminator loss: 0.514140, acc: 0.718750] [adversarial loss: 1.026811, acc: 0.250000]\n",
      "9388: [discriminator loss: 0.503390, acc: 0.726562] [adversarial loss: 1.024823, acc: 0.296875]\n",
      "9389: [discriminator loss: 0.633588, acc: 0.687500] [adversarial loss: 1.271380, acc: 0.171875]\n",
      "9390: [discriminator loss: 0.525133, acc: 0.710938] [adversarial loss: 0.914587, acc: 0.390625]\n",
      "9391: [discriminator loss: 0.530909, acc: 0.718750] [adversarial loss: 1.379068, acc: 0.125000]\n",
      "9392: [discriminator loss: 0.601415, acc: 0.679688] [adversarial loss: 1.180516, acc: 0.234375]\n",
      "9393: [discriminator loss: 0.461240, acc: 0.781250] [adversarial loss: 1.175012, acc: 0.250000]\n",
      "9394: [discriminator loss: 0.608617, acc: 0.632812] [adversarial loss: 1.151887, acc: 0.218750]\n",
      "9395: [discriminator loss: 0.633326, acc: 0.625000] [adversarial loss: 1.224970, acc: 0.250000]\n",
      "9396: [discriminator loss: 0.547450, acc: 0.703125] [adversarial loss: 0.960325, acc: 0.343750]\n",
      "9397: [discriminator loss: 0.578570, acc: 0.695312] [adversarial loss: 1.103110, acc: 0.328125]\n",
      "9398: [discriminator loss: 0.537595, acc: 0.734375] [adversarial loss: 1.406269, acc: 0.093750]\n",
      "9399: [discriminator loss: 0.556942, acc: 0.703125] [adversarial loss: 1.124405, acc: 0.312500]\n",
      "9400: [discriminator loss: 0.648987, acc: 0.593750] [adversarial loss: 1.264732, acc: 0.171875]\n",
      "9401: [discriminator loss: 0.569353, acc: 0.710938] [adversarial loss: 0.957915, acc: 0.312500]\n",
      "9402: [discriminator loss: 0.594681, acc: 0.695312] [adversarial loss: 1.416862, acc: 0.109375]\n",
      "9403: [discriminator loss: 0.548449, acc: 0.734375] [adversarial loss: 0.873174, acc: 0.375000]\n",
      "9404: [discriminator loss: 0.540458, acc: 0.671875] [adversarial loss: 1.299457, acc: 0.187500]\n",
      "9405: [discriminator loss: 0.600602, acc: 0.671875] [adversarial loss: 0.800671, acc: 0.484375]\n",
      "9406: [discriminator loss: 0.552807, acc: 0.726562] [adversarial loss: 1.335331, acc: 0.156250]\n",
      "9407: [discriminator loss: 0.518389, acc: 0.703125] [adversarial loss: 1.289786, acc: 0.171875]\n",
      "9408: [discriminator loss: 0.477876, acc: 0.828125] [adversarial loss: 1.080745, acc: 0.359375]\n",
      "9409: [discriminator loss: 0.533451, acc: 0.703125] [adversarial loss: 1.293476, acc: 0.156250]\n",
      "9410: [discriminator loss: 0.510041, acc: 0.742188] [adversarial loss: 1.178169, acc: 0.250000]\n",
      "9411: [discriminator loss: 0.573469, acc: 0.679688] [adversarial loss: 1.283007, acc: 0.203125]\n",
      "9412: [discriminator loss: 0.537738, acc: 0.710938] [adversarial loss: 1.074154, acc: 0.203125]\n",
      "9413: [discriminator loss: 0.546148, acc: 0.734375] [adversarial loss: 1.374615, acc: 0.093750]\n",
      "9414: [discriminator loss: 0.505735, acc: 0.781250] [adversarial loss: 1.056562, acc: 0.265625]\n",
      "9415: [discriminator loss: 0.572125, acc: 0.734375] [adversarial loss: 1.496313, acc: 0.156250]\n",
      "9416: [discriminator loss: 0.546275, acc: 0.703125] [adversarial loss: 1.047075, acc: 0.328125]\n",
      "9417: [discriminator loss: 0.542595, acc: 0.710938] [adversarial loss: 1.421612, acc: 0.156250]\n",
      "9418: [discriminator loss: 0.516017, acc: 0.726562] [adversarial loss: 0.811322, acc: 0.406250]\n",
      "9419: [discriminator loss: 0.600089, acc: 0.671875] [adversarial loss: 1.578476, acc: 0.109375]\n",
      "9420: [discriminator loss: 0.570464, acc: 0.679688] [adversarial loss: 1.001339, acc: 0.343750]\n",
      "9421: [discriminator loss: 0.502923, acc: 0.757812] [adversarial loss: 1.417283, acc: 0.140625]\n",
      "9422: [discriminator loss: 0.504264, acc: 0.742188] [adversarial loss: 1.038549, acc: 0.265625]\n",
      "9423: [discriminator loss: 0.565485, acc: 0.726562] [adversarial loss: 1.356698, acc: 0.140625]\n",
      "9424: [discriminator loss: 0.555305, acc: 0.679688] [adversarial loss: 1.322312, acc: 0.156250]\n",
      "9425: [discriminator loss: 0.598032, acc: 0.664062] [adversarial loss: 1.080418, acc: 0.250000]\n",
      "9426: [discriminator loss: 0.578859, acc: 0.695312] [adversarial loss: 1.266038, acc: 0.125000]\n",
      "9427: [discriminator loss: 0.535554, acc: 0.781250] [adversarial loss: 0.978926, acc: 0.312500]\n",
      "9428: [discriminator loss: 0.576629, acc: 0.679688] [adversarial loss: 1.156814, acc: 0.187500]\n",
      "9429: [discriminator loss: 0.531520, acc: 0.757812] [adversarial loss: 1.144857, acc: 0.265625]\n",
      "9430: [discriminator loss: 0.543568, acc: 0.726562] [adversarial loss: 1.282054, acc: 0.171875]\n",
      "9431: [discriminator loss: 0.562283, acc: 0.710938] [adversarial loss: 0.977965, acc: 0.328125]\n",
      "9432: [discriminator loss: 0.543724, acc: 0.734375] [adversarial loss: 1.411625, acc: 0.125000]\n",
      "9433: [discriminator loss: 0.480965, acc: 0.773438] [adversarial loss: 1.021048, acc: 0.312500]\n",
      "9434: [discriminator loss: 0.573785, acc: 0.742188] [adversarial loss: 1.249265, acc: 0.250000]\n",
      "9435: [discriminator loss: 0.567802, acc: 0.710938] [adversarial loss: 0.894368, acc: 0.390625]\n",
      "9436: [discriminator loss: 0.584607, acc: 0.710938] [adversarial loss: 1.718432, acc: 0.062500]\n",
      "9437: [discriminator loss: 0.594445, acc: 0.679688] [adversarial loss: 0.872200, acc: 0.406250]\n",
      "9438: [discriminator loss: 0.607808, acc: 0.687500] [adversarial loss: 1.723181, acc: 0.000000]\n",
      "9439: [discriminator loss: 0.531239, acc: 0.726562] [adversarial loss: 1.141278, acc: 0.281250]\n",
      "9440: [discriminator loss: 0.505735, acc: 0.718750] [adversarial loss: 1.306389, acc: 0.171875]\n",
      "9441: [discriminator loss: 0.526377, acc: 0.718750] [adversarial loss: 1.037333, acc: 0.281250]\n",
      "9442: [discriminator loss: 0.597345, acc: 0.687500] [adversarial loss: 1.214745, acc: 0.250000]\n",
      "9443: [discriminator loss: 0.546255, acc: 0.703125] [adversarial loss: 1.132182, acc: 0.234375]\n",
      "9444: [discriminator loss: 0.539422, acc: 0.726562] [adversarial loss: 1.147622, acc: 0.171875]\n",
      "9445: [discriminator loss: 0.501455, acc: 0.742188] [adversarial loss: 1.178890, acc: 0.187500]\n",
      "9446: [discriminator loss: 0.558045, acc: 0.703125] [adversarial loss: 1.050779, acc: 0.343750]\n",
      "9447: [discriminator loss: 0.487125, acc: 0.757812] [adversarial loss: 1.429092, acc: 0.156250]\n",
      "9448: [discriminator loss: 0.535320, acc: 0.734375] [adversarial loss: 1.106233, acc: 0.281250]\n",
      "9449: [discriminator loss: 0.504330, acc: 0.804688] [adversarial loss: 1.291733, acc: 0.125000]\n",
      "9450: [discriminator loss: 0.545550, acc: 0.671875] [adversarial loss: 0.920741, acc: 0.390625]\n",
      "9451: [discriminator loss: 0.523281, acc: 0.765625] [adversarial loss: 1.650631, acc: 0.031250]\n",
      "9452: [discriminator loss: 0.563618, acc: 0.671875] [adversarial loss: 0.741258, acc: 0.531250]\n",
      "9453: [discriminator loss: 0.622886, acc: 0.664062] [adversarial loss: 1.483690, acc: 0.125000]\n",
      "9454: [discriminator loss: 0.581110, acc: 0.671875] [adversarial loss: 0.865567, acc: 0.390625]\n",
      "9455: [discriminator loss: 0.700586, acc: 0.570312] [adversarial loss: 1.440254, acc: 0.125000]\n",
      "9456: [discriminator loss: 0.642812, acc: 0.656250] [adversarial loss: 1.173401, acc: 0.234375]\n",
      "9457: [discriminator loss: 0.522006, acc: 0.742188] [adversarial loss: 0.992720, acc: 0.390625]\n",
      "9458: [discriminator loss: 0.551605, acc: 0.671875] [adversarial loss: 1.331675, acc: 0.203125]\n",
      "9459: [discriminator loss: 0.511773, acc: 0.734375] [adversarial loss: 1.104618, acc: 0.281250]\n",
      "9460: [discriminator loss: 0.559076, acc: 0.726562] [adversarial loss: 1.099607, acc: 0.328125]\n",
      "9461: [discriminator loss: 0.560253, acc: 0.710938] [adversarial loss: 1.046724, acc: 0.328125]\n",
      "9462: [discriminator loss: 0.494557, acc: 0.726562] [adversarial loss: 1.063064, acc: 0.250000]\n",
      "9463: [discriminator loss: 0.537430, acc: 0.718750] [adversarial loss: 1.253630, acc: 0.218750]\n",
      "9464: [discriminator loss: 0.553775, acc: 0.703125] [adversarial loss: 1.055786, acc: 0.328125]\n",
      "9465: [discriminator loss: 0.612494, acc: 0.640625] [adversarial loss: 1.213768, acc: 0.171875]\n",
      "9466: [discriminator loss: 0.502417, acc: 0.773438] [adversarial loss: 1.175151, acc: 0.265625]\n",
      "9467: [discriminator loss: 0.586707, acc: 0.703125] [adversarial loss: 0.999300, acc: 0.343750]\n",
      "9468: [discriminator loss: 0.591076, acc: 0.671875] [adversarial loss: 1.635233, acc: 0.062500]\n",
      "9469: [discriminator loss: 0.592323, acc: 0.679688] [adversarial loss: 0.694865, acc: 0.562500]\n",
      "9470: [discriminator loss: 0.570132, acc: 0.656250] [adversarial loss: 1.531337, acc: 0.125000]\n",
      "9471: [discriminator loss: 0.583680, acc: 0.734375] [adversarial loss: 1.060283, acc: 0.250000]\n",
      "9472: [discriminator loss: 0.542682, acc: 0.695312] [adversarial loss: 1.318624, acc: 0.156250]\n",
      "9473: [discriminator loss: 0.504356, acc: 0.796875] [adversarial loss: 0.957276, acc: 0.343750]\n",
      "9474: [discriminator loss: 0.551643, acc: 0.687500] [adversarial loss: 1.210854, acc: 0.187500]\n",
      "9475: [discriminator loss: 0.547124, acc: 0.718750] [adversarial loss: 0.940057, acc: 0.406250]\n",
      "9476: [discriminator loss: 0.569701, acc: 0.703125] [adversarial loss: 1.176621, acc: 0.281250]\n",
      "9477: [discriminator loss: 0.525269, acc: 0.726562] [adversarial loss: 1.341339, acc: 0.171875]\n",
      "9478: [discriminator loss: 0.480697, acc: 0.789062] [adversarial loss: 1.021569, acc: 0.328125]\n",
      "9479: [discriminator loss: 0.505148, acc: 0.796875] [adversarial loss: 1.487271, acc: 0.093750]\n",
      "9480: [discriminator loss: 0.651927, acc: 0.648438] [adversarial loss: 0.756648, acc: 0.437500]\n",
      "9481: [discriminator loss: 0.541254, acc: 0.703125] [adversarial loss: 1.408067, acc: 0.140625]\n",
      "9482: [discriminator loss: 0.555257, acc: 0.695312] [adversarial loss: 0.948554, acc: 0.343750]\n",
      "9483: [discriminator loss: 0.534878, acc: 0.742188] [adversarial loss: 1.195203, acc: 0.171875]\n",
      "9484: [discriminator loss: 0.516747, acc: 0.718750] [adversarial loss: 0.897487, acc: 0.359375]\n",
      "9485: [discriminator loss: 0.529823, acc: 0.718750] [adversarial loss: 1.208846, acc: 0.171875]\n",
      "9486: [discriminator loss: 0.615498, acc: 0.640625] [adversarial loss: 1.040431, acc: 0.265625]\n",
      "9487: [discriminator loss: 0.512834, acc: 0.734375] [adversarial loss: 1.395732, acc: 0.156250]\n",
      "9488: [discriminator loss: 0.702765, acc: 0.585938] [adversarial loss: 0.758677, acc: 0.500000]\n",
      "9489: [discriminator loss: 0.615268, acc: 0.671875] [adversarial loss: 1.539443, acc: 0.140625]\n",
      "9490: [discriminator loss: 0.530020, acc: 0.695312] [adversarial loss: 1.092712, acc: 0.281250]\n",
      "9491: [discriminator loss: 0.490387, acc: 0.757812] [adversarial loss: 1.304396, acc: 0.125000]\n",
      "9492: [discriminator loss: 0.455147, acc: 0.835938] [adversarial loss: 1.278223, acc: 0.156250]\n",
      "9493: [discriminator loss: 0.551807, acc: 0.687500] [adversarial loss: 0.836792, acc: 0.453125]\n",
      "9494: [discriminator loss: 0.520166, acc: 0.742188] [adversarial loss: 1.375399, acc: 0.156250]\n",
      "9495: [discriminator loss: 0.556620, acc: 0.734375] [adversarial loss: 1.147673, acc: 0.171875]\n",
      "9496: [discriminator loss: 0.549058, acc: 0.742188] [adversarial loss: 1.080781, acc: 0.281250]\n",
      "9497: [discriminator loss: 0.498278, acc: 0.773438] [adversarial loss: 1.477146, acc: 0.093750]\n",
      "9498: [discriminator loss: 0.500090, acc: 0.750000] [adversarial loss: 0.922972, acc: 0.453125]\n",
      "9499: [discriminator loss: 0.531810, acc: 0.695312] [adversarial loss: 1.419397, acc: 0.109375]\n",
      "9500: [discriminator loss: 0.552153, acc: 0.679688] [adversarial loss: 0.888345, acc: 0.406250]\n",
      "9501: [discriminator loss: 0.508335, acc: 0.703125] [adversarial loss: 1.716379, acc: 0.109375]\n",
      "9502: [discriminator loss: 0.489266, acc: 0.734375] [adversarial loss: 0.986409, acc: 0.328125]\n",
      "9503: [discriminator loss: 0.524036, acc: 0.710938] [adversarial loss: 1.152863, acc: 0.234375]\n",
      "9504: [discriminator loss: 0.521319, acc: 0.664062] [adversarial loss: 1.105481, acc: 0.203125]\n",
      "9505: [discriminator loss: 0.504468, acc: 0.703125] [adversarial loss: 1.258867, acc: 0.203125]\n",
      "9506: [discriminator loss: 0.613877, acc: 0.640625] [adversarial loss: 0.967238, acc: 0.359375]\n",
      "9507: [discriminator loss: 0.609905, acc: 0.664062] [adversarial loss: 1.399047, acc: 0.171875]\n",
      "9508: [discriminator loss: 0.566737, acc: 0.679688] [adversarial loss: 1.082591, acc: 0.281250]\n",
      "9509: [discriminator loss: 0.446149, acc: 0.820312] [adversarial loss: 1.607643, acc: 0.046875]\n",
      "9510: [discriminator loss: 0.552989, acc: 0.671875] [adversarial loss: 0.925295, acc: 0.453125]\n",
      "9511: [discriminator loss: 0.527388, acc: 0.734375] [adversarial loss: 1.485227, acc: 0.093750]\n",
      "9512: [discriminator loss: 0.492869, acc: 0.734375] [adversarial loss: 1.171788, acc: 0.187500]\n",
      "9513: [discriminator loss: 0.496726, acc: 0.789062] [adversarial loss: 1.262906, acc: 0.171875]\n",
      "9514: [discriminator loss: 0.654948, acc: 0.679688] [adversarial loss: 1.240195, acc: 0.218750]\n",
      "9515: [discriminator loss: 0.546676, acc: 0.757812] [adversarial loss: 0.932761, acc: 0.437500]\n",
      "9516: [discriminator loss: 0.540881, acc: 0.718750] [adversarial loss: 1.307612, acc: 0.187500]\n",
      "9517: [discriminator loss: 0.483533, acc: 0.718750] [adversarial loss: 1.112558, acc: 0.281250]\n",
      "9518: [discriminator loss: 0.548982, acc: 0.703125] [adversarial loss: 1.058205, acc: 0.265625]\n",
      "9519: [discriminator loss: 0.543848, acc: 0.687500] [adversarial loss: 1.278085, acc: 0.187500]\n",
      "9520: [discriminator loss: 0.566911, acc: 0.726562] [adversarial loss: 1.141029, acc: 0.234375]\n",
      "9521: [discriminator loss: 0.607562, acc: 0.671875] [adversarial loss: 1.042834, acc: 0.328125]\n",
      "9522: [discriminator loss: 0.589096, acc: 0.687500] [adversarial loss: 1.084944, acc: 0.203125]\n",
      "9523: [discriminator loss: 0.503864, acc: 0.726562] [adversarial loss: 1.163316, acc: 0.171875]\n",
      "9524: [discriminator loss: 0.535648, acc: 0.742188] [adversarial loss: 1.194393, acc: 0.218750]\n",
      "9525: [discriminator loss: 0.531995, acc: 0.710938] [adversarial loss: 0.825382, acc: 0.484375]\n",
      "9526: [discriminator loss: 0.534573, acc: 0.718750] [adversarial loss: 1.337941, acc: 0.125000]\n",
      "9527: [discriminator loss: 0.522274, acc: 0.734375] [adversarial loss: 0.911349, acc: 0.359375]\n",
      "9528: [discriminator loss: 0.592603, acc: 0.695312] [adversarial loss: 1.009666, acc: 0.281250]\n",
      "9529: [discriminator loss: 0.639187, acc: 0.656250] [adversarial loss: 1.285261, acc: 0.171875]\n",
      "9530: [discriminator loss: 0.570924, acc: 0.695312] [adversarial loss: 1.119971, acc: 0.203125]\n",
      "9531: [discriminator loss: 0.562080, acc: 0.679688] [adversarial loss: 1.179385, acc: 0.281250]\n",
      "9532: [discriminator loss: 0.562870, acc: 0.695312] [adversarial loss: 1.106352, acc: 0.234375]\n",
      "9533: [discriminator loss: 0.562358, acc: 0.687500] [adversarial loss: 0.946736, acc: 0.343750]\n",
      "9534: [discriminator loss: 0.548027, acc: 0.718750] [adversarial loss: 1.073171, acc: 0.218750]\n",
      "9535: [discriminator loss: 0.664195, acc: 0.632812] [adversarial loss: 1.202837, acc: 0.171875]\n",
      "9536: [discriminator loss: 0.581465, acc: 0.687500] [adversarial loss: 1.132051, acc: 0.296875]\n",
      "9537: [discriminator loss: 0.634031, acc: 0.695312] [adversarial loss: 1.441391, acc: 0.093750]\n",
      "9538: [discriminator loss: 0.516906, acc: 0.742188] [adversarial loss: 1.024136, acc: 0.312500]\n",
      "9539: [discriminator loss: 0.608274, acc: 0.640625] [adversarial loss: 1.271649, acc: 0.203125]\n",
      "9540: [discriminator loss: 0.516748, acc: 0.750000] [adversarial loss: 1.162985, acc: 0.218750]\n",
      "9541: [discriminator loss: 0.549527, acc: 0.718750] [adversarial loss: 1.166867, acc: 0.234375]\n",
      "9542: [discriminator loss: 0.565410, acc: 0.703125] [adversarial loss: 1.606427, acc: 0.093750]\n",
      "9543: [discriminator loss: 0.623611, acc: 0.640625] [adversarial loss: 0.852533, acc: 0.484375]\n",
      "9544: [discriminator loss: 0.601251, acc: 0.703125] [adversarial loss: 1.615623, acc: 0.062500]\n",
      "9545: [discriminator loss: 0.572898, acc: 0.664062] [adversarial loss: 0.780255, acc: 0.468750]\n",
      "9546: [discriminator loss: 0.556026, acc: 0.734375] [adversarial loss: 1.477857, acc: 0.171875]\n",
      "9547: [discriminator loss: 0.523607, acc: 0.726562] [adversarial loss: 1.212062, acc: 0.218750]\n",
      "9548: [discriminator loss: 0.519113, acc: 0.726562] [adversarial loss: 1.209875, acc: 0.187500]\n",
      "9549: [discriminator loss: 0.557334, acc: 0.664062] [adversarial loss: 0.959457, acc: 0.296875]\n",
      "9550: [discriminator loss: 0.584959, acc: 0.718750] [adversarial loss: 1.644814, acc: 0.078125]\n",
      "9551: [discriminator loss: 0.577936, acc: 0.664062] [adversarial loss: 0.708528, acc: 0.546875]\n",
      "9552: [discriminator loss: 0.563810, acc: 0.734375] [adversarial loss: 1.431689, acc: 0.109375]\n",
      "9553: [discriminator loss: 0.539010, acc: 0.734375] [adversarial loss: 0.835594, acc: 0.453125]\n",
      "9554: [discriminator loss: 0.580084, acc: 0.703125] [adversarial loss: 1.481063, acc: 0.203125]\n",
      "9555: [discriminator loss: 0.620341, acc: 0.656250] [adversarial loss: 1.049487, acc: 0.250000]\n",
      "9556: [discriminator loss: 0.588264, acc: 0.664062] [adversarial loss: 1.427832, acc: 0.171875]\n",
      "9557: [discriminator loss: 0.575826, acc: 0.703125] [adversarial loss: 0.924461, acc: 0.375000]\n",
      "9558: [discriminator loss: 0.581771, acc: 0.703125] [adversarial loss: 1.092521, acc: 0.234375]\n",
      "9559: [discriminator loss: 0.592068, acc: 0.695312] [adversarial loss: 1.211940, acc: 0.312500]\n",
      "9560: [discriminator loss: 0.585054, acc: 0.632812] [adversarial loss: 1.155693, acc: 0.312500]\n",
      "9561: [discriminator loss: 0.560892, acc: 0.734375] [adversarial loss: 1.186102, acc: 0.234375]\n",
      "9562: [discriminator loss: 0.571546, acc: 0.664062] [adversarial loss: 1.108756, acc: 0.296875]\n",
      "9563: [discriminator loss: 0.483612, acc: 0.804688] [adversarial loss: 1.008139, acc: 0.343750]\n",
      "9564: [discriminator loss: 0.484526, acc: 0.757812] [adversarial loss: 1.108239, acc: 0.281250]\n",
      "9565: [discriminator loss: 0.540331, acc: 0.710938] [adversarial loss: 1.040741, acc: 0.281250]\n",
      "9566: [discriminator loss: 0.545080, acc: 0.734375] [adversarial loss: 0.989521, acc: 0.375000]\n",
      "9567: [discriminator loss: 0.545782, acc: 0.718750] [adversarial loss: 1.370354, acc: 0.156250]\n",
      "9568: [discriminator loss: 0.542086, acc: 0.687500] [adversarial loss: 0.885048, acc: 0.406250]\n",
      "9569: [discriminator loss: 0.642826, acc: 0.609375] [adversarial loss: 1.297282, acc: 0.234375]\n",
      "9570: [discriminator loss: 0.597095, acc: 0.671875] [adversarial loss: 0.952222, acc: 0.296875]\n",
      "9571: [discriminator loss: 0.506107, acc: 0.781250] [adversarial loss: 1.097712, acc: 0.218750]\n",
      "9572: [discriminator loss: 0.579869, acc: 0.671875] [adversarial loss: 1.179330, acc: 0.218750]\n",
      "9573: [discriminator loss: 0.546112, acc: 0.718750] [adversarial loss: 1.036437, acc: 0.296875]\n",
      "9574: [discriminator loss: 0.505033, acc: 0.750000] [adversarial loss: 1.039874, acc: 0.406250]\n",
      "9575: [discriminator loss: 0.530722, acc: 0.750000] [adversarial loss: 1.009390, acc: 0.343750]\n",
      "9576: [discriminator loss: 0.565113, acc: 0.640625] [adversarial loss: 1.213283, acc: 0.218750]\n",
      "9577: [discriminator loss: 0.526740, acc: 0.773438] [adversarial loss: 1.154653, acc: 0.203125]\n",
      "9578: [discriminator loss: 0.516728, acc: 0.718750] [adversarial loss: 0.994323, acc: 0.328125]\n",
      "9579: [discriminator loss: 0.587867, acc: 0.718750] [adversarial loss: 1.354306, acc: 0.125000]\n",
      "9580: [discriminator loss: 0.499706, acc: 0.718750] [adversarial loss: 1.041146, acc: 0.343750]\n",
      "9581: [discriminator loss: 0.557145, acc: 0.734375] [adversarial loss: 1.423880, acc: 0.125000]\n",
      "9582: [discriminator loss: 0.516238, acc: 0.734375] [adversarial loss: 0.789447, acc: 0.515625]\n",
      "9583: [discriminator loss: 0.546511, acc: 0.750000] [adversarial loss: 1.482321, acc: 0.093750]\n",
      "9584: [discriminator loss: 0.541673, acc: 0.710938] [adversarial loss: 0.876667, acc: 0.406250]\n",
      "9585: [discriminator loss: 0.580663, acc: 0.726562] [adversarial loss: 1.321567, acc: 0.156250]\n",
      "9586: [discriminator loss: 0.565589, acc: 0.671875] [adversarial loss: 1.108830, acc: 0.265625]\n",
      "9587: [discriminator loss: 0.505428, acc: 0.757812] [adversarial loss: 1.452100, acc: 0.109375]\n",
      "9588: [discriminator loss: 0.469374, acc: 0.781250] [adversarial loss: 1.375382, acc: 0.125000]\n",
      "9589: [discriminator loss: 0.597992, acc: 0.648438] [adversarial loss: 1.279399, acc: 0.187500]\n",
      "9590: [discriminator loss: 0.604226, acc: 0.695312] [adversarial loss: 1.421264, acc: 0.171875]\n",
      "9591: [discriminator loss: 0.609336, acc: 0.640625] [adversarial loss: 0.896859, acc: 0.421875]\n",
      "9592: [discriminator loss: 0.530869, acc: 0.710938] [adversarial loss: 1.287715, acc: 0.234375]\n",
      "9593: [discriminator loss: 0.566739, acc: 0.687500] [adversarial loss: 1.068938, acc: 0.343750]\n",
      "9594: [discriminator loss: 0.552409, acc: 0.695312] [adversarial loss: 1.248443, acc: 0.187500]\n",
      "9595: [discriminator loss: 0.537707, acc: 0.710938] [adversarial loss: 1.020365, acc: 0.359375]\n",
      "9596: [discriminator loss: 0.644599, acc: 0.601562] [adversarial loss: 1.136996, acc: 0.187500]\n",
      "9597: [discriminator loss: 0.538278, acc: 0.742188] [adversarial loss: 1.025557, acc: 0.359375]\n",
      "9598: [discriminator loss: 0.541288, acc: 0.695312] [adversarial loss: 1.275309, acc: 0.218750]\n",
      "9599: [discriminator loss: 0.574522, acc: 0.671875] [adversarial loss: 1.010343, acc: 0.296875]\n",
      "9600: [discriminator loss: 0.500620, acc: 0.757812] [adversarial loss: 1.157278, acc: 0.234375]\n",
      "9601: [discriminator loss: 0.551631, acc: 0.734375] [adversarial loss: 1.212103, acc: 0.203125]\n",
      "9602: [discriminator loss: 0.555997, acc: 0.726562] [adversarial loss: 1.330832, acc: 0.109375]\n",
      "9603: [discriminator loss: 0.558169, acc: 0.726562] [adversarial loss: 1.092083, acc: 0.312500]\n",
      "9604: [discriminator loss: 0.536671, acc: 0.773438] [adversarial loss: 1.318261, acc: 0.140625]\n",
      "9605: [discriminator loss: 0.587551, acc: 0.671875] [adversarial loss: 1.102731, acc: 0.281250]\n",
      "9606: [discriminator loss: 0.495173, acc: 0.718750] [adversarial loss: 1.192184, acc: 0.312500]\n",
      "9607: [discriminator loss: 0.493205, acc: 0.750000] [adversarial loss: 0.943991, acc: 0.375000]\n",
      "9608: [discriminator loss: 0.660969, acc: 0.617188] [adversarial loss: 1.177977, acc: 0.250000]\n",
      "9609: [discriminator loss: 0.508830, acc: 0.773438] [adversarial loss: 1.101011, acc: 0.265625]\n",
      "9610: [discriminator loss: 0.533017, acc: 0.789062] [adversarial loss: 1.141472, acc: 0.250000]\n",
      "9611: [discriminator loss: 0.453346, acc: 0.757812] [adversarial loss: 0.982626, acc: 0.406250]\n",
      "9612: [discriminator loss: 0.583974, acc: 0.718750] [adversarial loss: 0.983707, acc: 0.312500]\n",
      "9613: [discriminator loss: 0.587721, acc: 0.687500] [adversarial loss: 1.578296, acc: 0.093750]\n",
      "9614: [discriminator loss: 0.510019, acc: 0.804688] [adversarial loss: 0.852869, acc: 0.406250]\n",
      "9615: [discriminator loss: 0.588688, acc: 0.671875] [adversarial loss: 1.343220, acc: 0.171875]\n",
      "9616: [discriminator loss: 0.590656, acc: 0.695312] [adversarial loss: 0.964682, acc: 0.343750]\n",
      "9617: [discriminator loss: 0.582361, acc: 0.679688] [adversarial loss: 1.461363, acc: 0.109375]\n",
      "9618: [discriminator loss: 0.542977, acc: 0.726562] [adversarial loss: 1.123574, acc: 0.234375]\n",
      "9619: [discriminator loss: 0.592456, acc: 0.687500] [adversarial loss: 1.229813, acc: 0.125000]\n",
      "9620: [discriminator loss: 0.568638, acc: 0.687500] [adversarial loss: 1.235365, acc: 0.171875]\n",
      "9621: [discriminator loss: 0.553498, acc: 0.710938] [adversarial loss: 1.135431, acc: 0.203125]\n",
      "9622: [discriminator loss: 0.504494, acc: 0.757812] [adversarial loss: 1.130228, acc: 0.250000]\n",
      "9623: [discriminator loss: 0.500566, acc: 0.750000] [adversarial loss: 1.174085, acc: 0.265625]\n",
      "9624: [discriminator loss: 0.526661, acc: 0.710938] [adversarial loss: 1.351238, acc: 0.156250]\n",
      "9625: [discriminator loss: 0.620359, acc: 0.671875] [adversarial loss: 1.030335, acc: 0.296875]\n",
      "9626: [discriminator loss: 0.580817, acc: 0.664062] [adversarial loss: 1.747193, acc: 0.156250]\n",
      "9627: [discriminator loss: 0.579891, acc: 0.703125] [adversarial loss: 0.988153, acc: 0.390625]\n",
      "9628: [discriminator loss: 0.603951, acc: 0.687500] [adversarial loss: 1.598936, acc: 0.078125]\n",
      "9629: [discriminator loss: 0.635639, acc: 0.656250] [adversarial loss: 0.773198, acc: 0.515625]\n",
      "9630: [discriminator loss: 0.564809, acc: 0.718750] [adversarial loss: 1.303887, acc: 0.156250]\n",
      "9631: [discriminator loss: 0.527408, acc: 0.734375] [adversarial loss: 1.257274, acc: 0.187500]\n",
      "9632: [discriminator loss: 0.586880, acc: 0.656250] [adversarial loss: 1.045863, acc: 0.250000]\n",
      "9633: [discriminator loss: 0.563413, acc: 0.687500] [adversarial loss: 1.322517, acc: 0.187500]\n",
      "9634: [discriminator loss: 0.459041, acc: 0.773438] [adversarial loss: 1.232309, acc: 0.218750]\n",
      "9635: [discriminator loss: 0.591289, acc: 0.750000] [adversarial loss: 1.391146, acc: 0.187500]\n",
      "9636: [discriminator loss: 0.598830, acc: 0.695312] [adversarial loss: 1.099172, acc: 0.218750]\n",
      "9637: [discriminator loss: 0.492371, acc: 0.781250] [adversarial loss: 1.305022, acc: 0.140625]\n",
      "9638: [discriminator loss: 0.612679, acc: 0.617188] [adversarial loss: 1.048902, acc: 0.328125]\n",
      "9639: [discriminator loss: 0.601718, acc: 0.632812] [adversarial loss: 1.050448, acc: 0.281250]\n",
      "9640: [discriminator loss: 0.508695, acc: 0.750000] [adversarial loss: 1.350264, acc: 0.171875]\n",
      "9641: [discriminator loss: 0.583120, acc: 0.703125] [adversarial loss: 1.198230, acc: 0.187500]\n",
      "9642: [discriminator loss: 0.581776, acc: 0.656250] [adversarial loss: 1.045444, acc: 0.203125]\n",
      "9643: [discriminator loss: 0.594391, acc: 0.671875] [adversarial loss: 1.210971, acc: 0.250000]\n",
      "9644: [discriminator loss: 0.553262, acc: 0.703125] [adversarial loss: 0.902509, acc: 0.406250]\n",
      "9645: [discriminator loss: 0.565111, acc: 0.695312] [adversarial loss: 1.080359, acc: 0.218750]\n",
      "9646: [discriminator loss: 0.588999, acc: 0.648438] [adversarial loss: 1.212476, acc: 0.234375]\n",
      "9647: [discriminator loss: 0.606348, acc: 0.679688] [adversarial loss: 1.046886, acc: 0.218750]\n",
      "9648: [discriminator loss: 0.510625, acc: 0.718750] [adversarial loss: 1.054084, acc: 0.281250]\n",
      "9649: [discriminator loss: 0.588540, acc: 0.703125] [adversarial loss: 1.098591, acc: 0.281250]\n",
      "9650: [discriminator loss: 0.524357, acc: 0.742188] [adversarial loss: 1.212751, acc: 0.125000]\n",
      "9651: [discriminator loss: 0.526716, acc: 0.718750] [adversarial loss: 0.967286, acc: 0.328125]\n",
      "9652: [discriminator loss: 0.552823, acc: 0.671875] [adversarial loss: 1.316261, acc: 0.140625]\n",
      "9653: [discriminator loss: 0.624048, acc: 0.703125] [adversarial loss: 0.974820, acc: 0.375000]\n",
      "9654: [discriminator loss: 0.522308, acc: 0.750000] [adversarial loss: 1.237892, acc: 0.250000]\n",
      "9655: [discriminator loss: 0.583774, acc: 0.679688] [adversarial loss: 1.304919, acc: 0.187500]\n",
      "9656: [discriminator loss: 0.531962, acc: 0.695312] [adversarial loss: 1.213285, acc: 0.218750]\n",
      "9657: [discriminator loss: 0.549074, acc: 0.664062] [adversarial loss: 1.159029, acc: 0.281250]\n",
      "9658: [discriminator loss: 0.587457, acc: 0.664062] [adversarial loss: 1.026217, acc: 0.343750]\n",
      "9659: [discriminator loss: 0.537559, acc: 0.773438] [adversarial loss: 1.097443, acc: 0.203125]\n",
      "9660: [discriminator loss: 0.478659, acc: 0.789062] [adversarial loss: 1.120901, acc: 0.250000]\n",
      "9661: [discriminator loss: 0.575477, acc: 0.671875] [adversarial loss: 1.203543, acc: 0.296875]\n",
      "9662: [discriminator loss: 0.549373, acc: 0.664062] [adversarial loss: 1.358148, acc: 0.156250]\n",
      "9663: [discriminator loss: 0.641307, acc: 0.625000] [adversarial loss: 0.912926, acc: 0.390625]\n",
      "9664: [discriminator loss: 0.590511, acc: 0.656250] [adversarial loss: 1.414889, acc: 0.078125]\n",
      "9665: [discriminator loss: 0.534765, acc: 0.757812] [adversarial loss: 0.909612, acc: 0.500000]\n",
      "9666: [discriminator loss: 0.543867, acc: 0.734375] [adversarial loss: 1.472775, acc: 0.156250]\n",
      "9667: [discriminator loss: 0.562033, acc: 0.679688] [adversarial loss: 1.051176, acc: 0.328125]\n",
      "9668: [discriminator loss: 0.664498, acc: 0.648438] [adversarial loss: 1.391122, acc: 0.125000]\n",
      "9669: [discriminator loss: 0.567525, acc: 0.656250] [adversarial loss: 0.749519, acc: 0.609375]\n",
      "9670: [discriminator loss: 0.571955, acc: 0.695312] [adversarial loss: 1.377448, acc: 0.125000]\n",
      "9671: [discriminator loss: 0.613115, acc: 0.664062] [adversarial loss: 0.682938, acc: 0.640625]\n",
      "9672: [discriminator loss: 0.572741, acc: 0.656250] [adversarial loss: 1.442350, acc: 0.093750]\n",
      "9673: [discriminator loss: 0.572791, acc: 0.703125] [adversarial loss: 0.961575, acc: 0.390625]\n",
      "9674: [discriminator loss: 0.520042, acc: 0.726562] [adversarial loss: 1.074067, acc: 0.359375]\n",
      "9675: [discriminator loss: 0.487032, acc: 0.734375] [adversarial loss: 1.554941, acc: 0.140625]\n",
      "9676: [discriminator loss: 0.540835, acc: 0.726562] [adversarial loss: 0.875381, acc: 0.453125]\n",
      "9677: [discriminator loss: 0.642404, acc: 0.648438] [adversarial loss: 1.207070, acc: 0.281250]\n",
      "9678: [discriminator loss: 0.588518, acc: 0.687500] [adversarial loss: 1.113112, acc: 0.328125]\n",
      "9679: [discriminator loss: 0.531756, acc: 0.718750] [adversarial loss: 1.230518, acc: 0.187500]\n",
      "9680: [discriminator loss: 0.542869, acc: 0.742188] [adversarial loss: 1.378142, acc: 0.156250]\n",
      "9681: [discriminator loss: 0.542780, acc: 0.695312] [adversarial loss: 1.233791, acc: 0.156250]\n",
      "9682: [discriminator loss: 0.523391, acc: 0.718750] [adversarial loss: 1.269825, acc: 0.203125]\n",
      "9683: [discriminator loss: 0.541699, acc: 0.750000] [adversarial loss: 0.938195, acc: 0.390625]\n",
      "9684: [discriminator loss: 0.573659, acc: 0.679688] [adversarial loss: 1.295486, acc: 0.281250]\n",
      "9685: [discriminator loss: 0.603995, acc: 0.671875] [adversarial loss: 1.072479, acc: 0.343750]\n",
      "9686: [discriminator loss: 0.509061, acc: 0.726562] [adversarial loss: 1.336435, acc: 0.187500]\n",
      "9687: [discriminator loss: 0.605237, acc: 0.625000] [adversarial loss: 0.958177, acc: 0.296875]\n",
      "9688: [discriminator loss: 0.541908, acc: 0.718750] [adversarial loss: 1.651145, acc: 0.125000]\n",
      "9689: [discriminator loss: 0.504613, acc: 0.742188] [adversarial loss: 1.080996, acc: 0.296875]\n",
      "9690: [discriminator loss: 0.501865, acc: 0.710938] [adversarial loss: 1.245912, acc: 0.187500]\n",
      "9691: [discriminator loss: 0.479454, acc: 0.789062] [adversarial loss: 1.190645, acc: 0.343750]\n",
      "9692: [discriminator loss: 0.576821, acc: 0.734375] [adversarial loss: 1.049615, acc: 0.312500]\n",
      "9693: [discriminator loss: 0.557484, acc: 0.710938] [adversarial loss: 1.251398, acc: 0.203125]\n",
      "9694: [discriminator loss: 0.527426, acc: 0.757812] [adversarial loss: 1.145295, acc: 0.234375]\n",
      "9695: [discriminator loss: 0.524522, acc: 0.734375] [adversarial loss: 1.268907, acc: 0.218750]\n",
      "9696: [discriminator loss: 0.521152, acc: 0.765625] [adversarial loss: 0.904411, acc: 0.406250]\n",
      "9697: [discriminator loss: 0.644003, acc: 0.648438] [adversarial loss: 1.095120, acc: 0.296875]\n",
      "9698: [discriminator loss: 0.554363, acc: 0.703125] [adversarial loss: 1.104170, acc: 0.234375]\n",
      "9699: [discriminator loss: 0.535178, acc: 0.750000] [adversarial loss: 1.142692, acc: 0.281250]\n",
      "9700: [discriminator loss: 0.527888, acc: 0.750000] [adversarial loss: 0.929149, acc: 0.375000]\n",
      "9701: [discriminator loss: 0.476425, acc: 0.796875] [adversarial loss: 1.327914, acc: 0.187500]\n",
      "9702: [discriminator loss: 0.606065, acc: 0.648438] [adversarial loss: 1.049128, acc: 0.375000]\n",
      "9703: [discriminator loss: 0.632925, acc: 0.679688] [adversarial loss: 1.442606, acc: 0.203125]\n",
      "9704: [discriminator loss: 0.590365, acc: 0.703125] [adversarial loss: 0.963246, acc: 0.375000]\n",
      "9705: [discriminator loss: 0.499180, acc: 0.742188] [adversarial loss: 1.427176, acc: 0.156250]\n",
      "9706: [discriminator loss: 0.545212, acc: 0.710938] [adversarial loss: 1.136969, acc: 0.218750]\n",
      "9707: [discriminator loss: 0.589255, acc: 0.656250] [adversarial loss: 1.466163, acc: 0.109375]\n",
      "9708: [discriminator loss: 0.511206, acc: 0.718750] [adversarial loss: 0.925882, acc: 0.312500]\n",
      "9709: [discriminator loss: 0.606173, acc: 0.695312] [adversarial loss: 1.306571, acc: 0.125000]\n",
      "9710: [discriminator loss: 0.544287, acc: 0.703125] [adversarial loss: 1.021024, acc: 0.281250]\n",
      "9711: [discriminator loss: 0.520889, acc: 0.726562] [adversarial loss: 1.272250, acc: 0.187500]\n",
      "9712: [discriminator loss: 0.558755, acc: 0.687500] [adversarial loss: 1.138971, acc: 0.265625]\n",
      "9713: [discriminator loss: 0.568670, acc: 0.695312] [adversarial loss: 1.071555, acc: 0.265625]\n",
      "9714: [discriminator loss: 0.523588, acc: 0.734375] [adversarial loss: 1.121114, acc: 0.234375]\n",
      "9715: [discriminator loss: 0.525476, acc: 0.757812] [adversarial loss: 1.309692, acc: 0.156250]\n",
      "9716: [discriminator loss: 0.508596, acc: 0.750000] [adversarial loss: 1.113807, acc: 0.281250]\n",
      "9717: [discriminator loss: 0.651384, acc: 0.640625] [adversarial loss: 1.220959, acc: 0.234375]\n",
      "9718: [discriminator loss: 0.547060, acc: 0.664062] [adversarial loss: 1.026217, acc: 0.390625]\n",
      "9719: [discriminator loss: 0.509452, acc: 0.734375] [adversarial loss: 1.352881, acc: 0.140625]\n",
      "9720: [discriminator loss: 0.544030, acc: 0.718750] [adversarial loss: 0.991168, acc: 0.328125]\n",
      "9721: [discriminator loss: 0.604644, acc: 0.679688] [adversarial loss: 1.425704, acc: 0.125000]\n",
      "9722: [discriminator loss: 0.551207, acc: 0.695312] [adversarial loss: 1.038686, acc: 0.359375]\n",
      "9723: [discriminator loss: 0.544943, acc: 0.687500] [adversarial loss: 1.195535, acc: 0.156250]\n",
      "9724: [discriminator loss: 0.521564, acc: 0.726562] [adversarial loss: 1.297038, acc: 0.140625]\n",
      "9725: [discriminator loss: 0.547180, acc: 0.773438] [adversarial loss: 1.108071, acc: 0.250000]\n",
      "9726: [discriminator loss: 0.525664, acc: 0.718750] [adversarial loss: 1.085179, acc: 0.203125]\n",
      "9727: [discriminator loss: 0.567025, acc: 0.718750] [adversarial loss: 1.604574, acc: 0.125000]\n",
      "9728: [discriminator loss: 0.614923, acc: 0.671875] [adversarial loss: 0.817754, acc: 0.484375]\n",
      "9729: [discriminator loss: 0.565804, acc: 0.656250] [adversarial loss: 1.703686, acc: 0.046875]\n",
      "9730: [discriminator loss: 0.582030, acc: 0.726562] [adversarial loss: 0.790827, acc: 0.500000]\n",
      "9731: [discriminator loss: 0.535390, acc: 0.710938] [adversarial loss: 1.416669, acc: 0.093750]\n",
      "9732: [discriminator loss: 0.573640, acc: 0.632812] [adversarial loss: 0.836205, acc: 0.406250]\n",
      "9733: [discriminator loss: 0.523574, acc: 0.703125] [adversarial loss: 1.153821, acc: 0.187500]\n",
      "9734: [discriminator loss: 0.565567, acc: 0.703125] [adversarial loss: 1.185393, acc: 0.234375]\n",
      "9735: [discriminator loss: 0.510727, acc: 0.757812] [adversarial loss: 1.067967, acc: 0.265625]\n",
      "9736: [discriminator loss: 0.541705, acc: 0.664062] [adversarial loss: 1.381505, acc: 0.156250]\n",
      "9737: [discriminator loss: 0.622568, acc: 0.695312] [adversarial loss: 1.104448, acc: 0.343750]\n",
      "9738: [discriminator loss: 0.660541, acc: 0.593750] [adversarial loss: 1.435195, acc: 0.156250]\n",
      "9739: [discriminator loss: 0.496068, acc: 0.773438] [adversarial loss: 1.031987, acc: 0.328125]\n",
      "9740: [discriminator loss: 0.557729, acc: 0.695312] [adversarial loss: 1.052051, acc: 0.359375]\n",
      "9741: [discriminator loss: 0.474863, acc: 0.781250] [adversarial loss: 0.993623, acc: 0.421875]\n",
      "9742: [discriminator loss: 0.537652, acc: 0.710938] [adversarial loss: 1.364192, acc: 0.140625]\n",
      "9743: [discriminator loss: 0.590417, acc: 0.640625] [adversarial loss: 1.106219, acc: 0.250000]\n",
      "9744: [discriminator loss: 0.523739, acc: 0.718750] [adversarial loss: 1.414638, acc: 0.156250]\n",
      "9745: [discriminator loss: 0.574868, acc: 0.656250] [adversarial loss: 0.723250, acc: 0.609375]\n",
      "9746: [discriminator loss: 0.553286, acc: 0.710938] [adversarial loss: 1.370007, acc: 0.140625]\n",
      "9747: [discriminator loss: 0.516800, acc: 0.718750] [adversarial loss: 0.952910, acc: 0.359375]\n",
      "9748: [discriminator loss: 0.503457, acc: 0.765625] [adversarial loss: 1.119816, acc: 0.328125]\n",
      "9749: [discriminator loss: 0.585156, acc: 0.671875] [adversarial loss: 1.282408, acc: 0.156250]\n",
      "9750: [discriminator loss: 0.570695, acc: 0.656250] [adversarial loss: 1.098536, acc: 0.234375]\n",
      "9751: [discriminator loss: 0.585210, acc: 0.656250] [adversarial loss: 0.884743, acc: 0.531250]\n",
      "9752: [discriminator loss: 0.512885, acc: 0.773438] [adversarial loss: 1.203545, acc: 0.265625]\n",
      "9753: [discriminator loss: 0.558493, acc: 0.726562] [adversarial loss: 1.240380, acc: 0.109375]\n",
      "9754: [discriminator loss: 0.492547, acc: 0.742188] [adversarial loss: 1.327341, acc: 0.062500]\n",
      "9755: [discriminator loss: 0.547176, acc: 0.718750] [adversarial loss: 1.159311, acc: 0.203125]\n",
      "9756: [discriminator loss: 0.521860, acc: 0.750000] [adversarial loss: 1.134320, acc: 0.250000]\n",
      "9757: [discriminator loss: 0.521785, acc: 0.703125] [adversarial loss: 1.306440, acc: 0.140625]\n",
      "9758: [discriminator loss: 0.561799, acc: 0.750000] [adversarial loss: 1.018494, acc: 0.328125]\n",
      "9759: [discriminator loss: 0.510021, acc: 0.773438] [adversarial loss: 1.100535, acc: 0.281250]\n",
      "9760: [discriminator loss: 0.523659, acc: 0.742188] [adversarial loss: 1.064541, acc: 0.328125]\n",
      "9761: [discriminator loss: 0.592038, acc: 0.648438] [adversarial loss: 1.360128, acc: 0.187500]\n",
      "9762: [discriminator loss: 0.524044, acc: 0.750000] [adversarial loss: 1.032459, acc: 0.281250]\n",
      "9763: [discriminator loss: 0.569446, acc: 0.726562] [adversarial loss: 1.361943, acc: 0.125000]\n",
      "9764: [discriminator loss: 0.574793, acc: 0.718750] [adversarial loss: 0.837155, acc: 0.421875]\n",
      "9765: [discriminator loss: 0.544328, acc: 0.710938] [adversarial loss: 1.264120, acc: 0.125000]\n",
      "9766: [discriminator loss: 0.612271, acc: 0.640625] [adversarial loss: 0.864276, acc: 0.437500]\n",
      "9767: [discriminator loss: 0.567400, acc: 0.664062] [adversarial loss: 1.671138, acc: 0.046875]\n",
      "9768: [discriminator loss: 0.584573, acc: 0.687500] [adversarial loss: 0.915751, acc: 0.421875]\n",
      "9769: [discriminator loss: 0.532279, acc: 0.734375] [adversarial loss: 1.661871, acc: 0.078125]\n",
      "9770: [discriminator loss: 0.571324, acc: 0.695312] [adversarial loss: 1.046541, acc: 0.265625]\n",
      "9771: [discriminator loss: 0.608768, acc: 0.671875] [adversarial loss: 1.230997, acc: 0.234375]\n",
      "9772: [discriminator loss: 0.538076, acc: 0.710938] [adversarial loss: 0.988935, acc: 0.359375]\n",
      "9773: [discriminator loss: 0.507771, acc: 0.765625] [adversarial loss: 1.040926, acc: 0.265625]\n",
      "9774: [discriminator loss: 0.564192, acc: 0.695312] [adversarial loss: 1.133661, acc: 0.359375]\n",
      "9775: [discriminator loss: 0.527448, acc: 0.742188] [adversarial loss: 1.375166, acc: 0.203125]\n",
      "9776: [discriminator loss: 0.530176, acc: 0.726562] [adversarial loss: 0.960034, acc: 0.390625]\n",
      "9777: [discriminator loss: 0.600920, acc: 0.695312] [adversarial loss: 1.509922, acc: 0.125000]\n",
      "9778: [discriminator loss: 0.577251, acc: 0.671875] [adversarial loss: 0.896597, acc: 0.437500]\n",
      "9779: [discriminator loss: 0.604069, acc: 0.656250] [adversarial loss: 1.367903, acc: 0.250000]\n",
      "9780: [discriminator loss: 0.646825, acc: 0.687500] [adversarial loss: 1.037090, acc: 0.250000]\n",
      "9781: [discriminator loss: 0.560517, acc: 0.703125] [adversarial loss: 1.520347, acc: 0.031250]\n",
      "9782: [discriminator loss: 0.582857, acc: 0.656250] [adversarial loss: 0.892609, acc: 0.437500]\n",
      "9783: [discriminator loss: 0.565535, acc: 0.734375] [adversarial loss: 1.394573, acc: 0.156250]\n",
      "9784: [discriminator loss: 0.559885, acc: 0.687500] [adversarial loss: 1.066921, acc: 0.312500]\n",
      "9785: [discriminator loss: 0.522476, acc: 0.703125] [adversarial loss: 1.258409, acc: 0.203125]\n",
      "9786: [discriminator loss: 0.492965, acc: 0.757812] [adversarial loss: 1.153439, acc: 0.265625]\n",
      "9787: [discriminator loss: 0.502332, acc: 0.757812] [adversarial loss: 1.015323, acc: 0.343750]\n",
      "9788: [discriminator loss: 0.556415, acc: 0.687500] [adversarial loss: 1.086255, acc: 0.265625]\n",
      "9789: [discriminator loss: 0.610391, acc: 0.632812] [adversarial loss: 1.041792, acc: 0.312500]\n",
      "9790: [discriminator loss: 0.547408, acc: 0.703125] [adversarial loss: 1.178912, acc: 0.203125]\n",
      "9791: [discriminator loss: 0.509114, acc: 0.734375] [adversarial loss: 1.048551, acc: 0.328125]\n",
      "9792: [discriminator loss: 0.541587, acc: 0.734375] [adversarial loss: 1.087854, acc: 0.312500]\n",
      "9793: [discriminator loss: 0.501104, acc: 0.703125] [adversarial loss: 1.284089, acc: 0.218750]\n",
      "9794: [discriminator loss: 0.523830, acc: 0.742188] [adversarial loss: 1.095798, acc: 0.296875]\n",
      "9795: [discriminator loss: 0.480685, acc: 0.804688] [adversarial loss: 1.420482, acc: 0.109375]\n",
      "9796: [discriminator loss: 0.596500, acc: 0.687500] [adversarial loss: 0.817181, acc: 0.484375]\n",
      "9797: [discriminator loss: 0.586778, acc: 0.679688] [adversarial loss: 1.334022, acc: 0.140625]\n",
      "9798: [discriminator loss: 0.479563, acc: 0.773438] [adversarial loss: 0.972520, acc: 0.359375]\n",
      "9799: [discriminator loss: 0.559113, acc: 0.703125] [adversarial loss: 1.362269, acc: 0.203125]\n",
      "9800: [discriminator loss: 0.576036, acc: 0.664062] [adversarial loss: 0.992166, acc: 0.343750]\n",
      "9801: [discriminator loss: 0.599739, acc: 0.632812] [adversarial loss: 1.305846, acc: 0.125000]\n",
      "9802: [discriminator loss: 0.535741, acc: 0.703125] [adversarial loss: 1.214410, acc: 0.250000]\n",
      "9803: [discriminator loss: 0.546109, acc: 0.726562] [adversarial loss: 1.191973, acc: 0.203125]\n",
      "9804: [discriminator loss: 0.524822, acc: 0.734375] [adversarial loss: 1.366771, acc: 0.171875]\n",
      "9805: [discriminator loss: 0.529333, acc: 0.734375] [adversarial loss: 0.976515, acc: 0.390625]\n",
      "9806: [discriminator loss: 0.538713, acc: 0.742188] [adversarial loss: 1.170680, acc: 0.218750]\n",
      "9807: [discriminator loss: 0.582597, acc: 0.664062] [adversarial loss: 1.005564, acc: 0.328125]\n",
      "9808: [discriminator loss: 0.529368, acc: 0.757812] [adversarial loss: 1.143510, acc: 0.250000]\n",
      "9809: [discriminator loss: 0.528109, acc: 0.750000] [adversarial loss: 1.289400, acc: 0.125000]\n",
      "9810: [discriminator loss: 0.471549, acc: 0.765625] [adversarial loss: 0.940891, acc: 0.312500]\n",
      "9811: [discriminator loss: 0.562793, acc: 0.671875] [adversarial loss: 1.333344, acc: 0.187500]\n",
      "9812: [discriminator loss: 0.544241, acc: 0.703125] [adversarial loss: 0.904567, acc: 0.421875]\n",
      "9813: [discriminator loss: 0.571101, acc: 0.656250] [adversarial loss: 1.382194, acc: 0.187500]\n",
      "9814: [discriminator loss: 0.526308, acc: 0.726562] [adversarial loss: 0.799053, acc: 0.546875]\n",
      "9815: [discriminator loss: 0.584466, acc: 0.687500] [adversarial loss: 1.341790, acc: 0.156250]\n",
      "9816: [discriminator loss: 0.554708, acc: 0.734375] [adversarial loss: 0.859553, acc: 0.468750]\n",
      "9817: [discriminator loss: 0.574214, acc: 0.695312] [adversarial loss: 1.149249, acc: 0.203125]\n",
      "9818: [discriminator loss: 0.549571, acc: 0.679688] [adversarial loss: 1.295183, acc: 0.125000]\n",
      "9819: [discriminator loss: 0.524226, acc: 0.750000] [adversarial loss: 1.055018, acc: 0.250000]\n",
      "9820: [discriminator loss: 0.581152, acc: 0.695312] [adversarial loss: 0.944219, acc: 0.375000]\n",
      "9821: [discriminator loss: 0.562496, acc: 0.718750] [adversarial loss: 1.353521, acc: 0.125000]\n",
      "9822: [discriminator loss: 0.522313, acc: 0.742188] [adversarial loss: 1.191809, acc: 0.281250]\n",
      "9823: [discriminator loss: 0.511905, acc: 0.726562] [adversarial loss: 1.218152, acc: 0.218750]\n",
      "9824: [discriminator loss: 0.522445, acc: 0.789062] [adversarial loss: 1.286951, acc: 0.125000]\n",
      "9825: [discriminator loss: 0.573019, acc: 0.710938] [adversarial loss: 1.076296, acc: 0.296875]\n",
      "9826: [discriminator loss: 0.539347, acc: 0.750000] [adversarial loss: 1.297008, acc: 0.187500]\n",
      "9827: [discriminator loss: 0.548792, acc: 0.687500] [adversarial loss: 0.918832, acc: 0.390625]\n",
      "9828: [discriminator loss: 0.569225, acc: 0.679688] [adversarial loss: 1.446832, acc: 0.156250]\n",
      "9829: [discriminator loss: 0.603528, acc: 0.703125] [adversarial loss: 0.838113, acc: 0.406250]\n",
      "9830: [discriminator loss: 0.554365, acc: 0.656250] [adversarial loss: 1.531355, acc: 0.156250]\n",
      "9831: [discriminator loss: 0.614914, acc: 0.687500] [adversarial loss: 0.859303, acc: 0.453125]\n",
      "9832: [discriminator loss: 0.561405, acc: 0.703125] [adversarial loss: 1.538629, acc: 0.062500]\n",
      "9833: [discriminator loss: 0.599400, acc: 0.726562] [adversarial loss: 0.970022, acc: 0.296875]\n",
      "9834: [discriminator loss: 0.524439, acc: 0.734375] [adversarial loss: 1.364559, acc: 0.156250]\n",
      "9835: [discriminator loss: 0.520829, acc: 0.734375] [adversarial loss: 1.217778, acc: 0.234375]\n",
      "9836: [discriminator loss: 0.585526, acc: 0.679688] [adversarial loss: 1.095325, acc: 0.296875]\n",
      "9837: [discriminator loss: 0.565836, acc: 0.687500] [adversarial loss: 1.504473, acc: 0.109375]\n",
      "9838: [discriminator loss: 0.676211, acc: 0.625000] [adversarial loss: 0.848682, acc: 0.375000]\n",
      "9839: [discriminator loss: 0.558630, acc: 0.710938] [adversarial loss: 1.199872, acc: 0.187500]\n",
      "9840: [discriminator loss: 0.574426, acc: 0.703125] [adversarial loss: 0.981231, acc: 0.281250]\n",
      "9841: [discriminator loss: 0.501580, acc: 0.781250] [adversarial loss: 1.519824, acc: 0.125000]\n",
      "9842: [discriminator loss: 0.546057, acc: 0.726562] [adversarial loss: 0.806031, acc: 0.375000]\n",
      "9843: [discriminator loss: 0.547654, acc: 0.718750] [adversarial loss: 1.297020, acc: 0.218750]\n",
      "9844: [discriminator loss: 0.521040, acc: 0.726562] [adversarial loss: 1.021012, acc: 0.250000]\n",
      "9845: [discriminator loss: 0.550639, acc: 0.718750] [adversarial loss: 1.370526, acc: 0.171875]\n",
      "9846: [discriminator loss: 0.583871, acc: 0.726562] [adversarial loss: 1.026426, acc: 0.312500]\n",
      "9847: [discriminator loss: 0.523395, acc: 0.734375] [adversarial loss: 1.295798, acc: 0.171875]\n",
      "9848: [discriminator loss: 0.541206, acc: 0.734375] [adversarial loss: 1.053094, acc: 0.296875]\n",
      "9849: [discriminator loss: 0.510794, acc: 0.718750] [adversarial loss: 1.310864, acc: 0.187500]\n",
      "9850: [discriminator loss: 0.528070, acc: 0.757812] [adversarial loss: 1.020118, acc: 0.250000]\n",
      "9851: [discriminator loss: 0.534565, acc: 0.734375] [adversarial loss: 1.273459, acc: 0.171875]\n",
      "9852: [discriminator loss: 0.574624, acc: 0.671875] [adversarial loss: 1.217017, acc: 0.250000]\n",
      "9853: [discriminator loss: 0.439485, acc: 0.820312] [adversarial loss: 1.229082, acc: 0.234375]\n",
      "9854: [discriminator loss: 0.552385, acc: 0.703125] [adversarial loss: 1.098236, acc: 0.296875]\n",
      "9855: [discriminator loss: 0.502572, acc: 0.789062] [adversarial loss: 1.070507, acc: 0.265625]\n",
      "9856: [discriminator loss: 0.551895, acc: 0.703125] [adversarial loss: 1.168876, acc: 0.375000]\n",
      "9857: [discriminator loss: 0.504936, acc: 0.804688] [adversarial loss: 1.337114, acc: 0.171875]\n",
      "9858: [discriminator loss: 0.526393, acc: 0.734375] [adversarial loss: 1.464840, acc: 0.140625]\n",
      "9859: [discriminator loss: 0.565844, acc: 0.710938] [adversarial loss: 0.939322, acc: 0.328125]\n",
      "9860: [discriminator loss: 0.594016, acc: 0.687500] [adversarial loss: 1.383563, acc: 0.062500]\n",
      "9861: [discriminator loss: 0.550630, acc: 0.695312] [adversarial loss: 0.826901, acc: 0.453125]\n",
      "9862: [discriminator loss: 0.586097, acc: 0.695312] [adversarial loss: 1.230437, acc: 0.234375]\n",
      "9863: [discriminator loss: 0.551326, acc: 0.695312] [adversarial loss: 0.837049, acc: 0.421875]\n",
      "9864: [discriminator loss: 0.556481, acc: 0.695312] [adversarial loss: 1.418961, acc: 0.109375]\n",
      "9865: [discriminator loss: 0.545235, acc: 0.703125] [adversarial loss: 1.280046, acc: 0.218750]\n",
      "9866: [discriminator loss: 0.468558, acc: 0.773438] [adversarial loss: 1.262782, acc: 0.203125]\n",
      "9867: [discriminator loss: 0.485226, acc: 0.789062] [adversarial loss: 1.182769, acc: 0.218750]\n",
      "9868: [discriminator loss: 0.539428, acc: 0.710938] [adversarial loss: 1.165492, acc: 0.250000]\n",
      "9869: [discriminator loss: 0.527520, acc: 0.718750] [adversarial loss: 1.177500, acc: 0.328125]\n",
      "9870: [discriminator loss: 0.584023, acc: 0.695312] [adversarial loss: 1.275778, acc: 0.140625]\n",
      "9871: [discriminator loss: 0.497330, acc: 0.750000] [adversarial loss: 1.220576, acc: 0.296875]\n",
      "9872: [discriminator loss: 0.589425, acc: 0.687500] [adversarial loss: 1.114445, acc: 0.265625]\n",
      "9873: [discriminator loss: 0.490316, acc: 0.773438] [adversarial loss: 1.348620, acc: 0.203125]\n",
      "9874: [discriminator loss: 0.560244, acc: 0.695312] [adversarial loss: 0.924458, acc: 0.437500]\n",
      "9875: [discriminator loss: 0.493405, acc: 0.757812] [adversarial loss: 1.290860, acc: 0.171875]\n",
      "9876: [discriminator loss: 0.551977, acc: 0.695312] [adversarial loss: 1.034109, acc: 0.312500]\n",
      "9877: [discriminator loss: 0.499801, acc: 0.718750] [adversarial loss: 1.216740, acc: 0.234375]\n",
      "9878: [discriminator loss: 0.546645, acc: 0.750000] [adversarial loss: 1.035346, acc: 0.265625]\n",
      "9879: [discriminator loss: 0.527319, acc: 0.710938] [adversarial loss: 1.447397, acc: 0.109375]\n",
      "9880: [discriminator loss: 0.512772, acc: 0.718750] [adversarial loss: 0.924849, acc: 0.359375]\n",
      "9881: [discriminator loss: 0.567663, acc: 0.632812] [adversarial loss: 1.185518, acc: 0.234375]\n",
      "9882: [discriminator loss: 0.563709, acc: 0.703125] [adversarial loss: 1.061878, acc: 0.250000]\n",
      "9883: [discriminator loss: 0.565729, acc: 0.695312] [adversarial loss: 1.321868, acc: 0.203125]\n",
      "9884: [discriminator loss: 0.585210, acc: 0.695312] [adversarial loss: 1.130790, acc: 0.250000]\n",
      "9885: [discriminator loss: 0.486254, acc: 0.789062] [adversarial loss: 1.250902, acc: 0.156250]\n",
      "9886: [discriminator loss: 0.501523, acc: 0.773438] [adversarial loss: 1.214677, acc: 0.187500]\n",
      "9887: [discriminator loss: 0.618468, acc: 0.648438] [adversarial loss: 1.003607, acc: 0.359375]\n",
      "9888: [discriminator loss: 0.570699, acc: 0.734375] [adversarial loss: 1.262393, acc: 0.218750]\n",
      "9889: [discriminator loss: 0.556522, acc: 0.687500] [adversarial loss: 1.134023, acc: 0.187500]\n",
      "9890: [discriminator loss: 0.570548, acc: 0.671875] [adversarial loss: 1.224718, acc: 0.203125]\n",
      "9891: [discriminator loss: 0.594177, acc: 0.632812] [adversarial loss: 1.213847, acc: 0.234375]\n",
      "9892: [discriminator loss: 0.492330, acc: 0.742188] [adversarial loss: 1.085300, acc: 0.203125]\n",
      "9893: [discriminator loss: 0.522263, acc: 0.773438] [adversarial loss: 1.575687, acc: 0.140625]\n",
      "9894: [discriminator loss: 0.642263, acc: 0.656250] [adversarial loss: 0.818000, acc: 0.531250]\n",
      "9895: [discriminator loss: 0.595662, acc: 0.679688] [adversarial loss: 1.626811, acc: 0.093750]\n",
      "9896: [discriminator loss: 0.587765, acc: 0.640625] [adversarial loss: 0.713832, acc: 0.593750]\n",
      "9897: [discriminator loss: 0.594126, acc: 0.679688] [adversarial loss: 1.447402, acc: 0.093750]\n",
      "9898: [discriminator loss: 0.614694, acc: 0.617188] [adversarial loss: 0.931571, acc: 0.406250]\n",
      "9899: [discriminator loss: 0.602454, acc: 0.671875] [adversarial loss: 1.336241, acc: 0.156250]\n",
      "9900: [discriminator loss: 0.523181, acc: 0.757812] [adversarial loss: 1.178179, acc: 0.265625]\n",
      "9901: [discriminator loss: 0.623209, acc: 0.656250] [adversarial loss: 1.228728, acc: 0.203125]\n",
      "9902: [discriminator loss: 0.498383, acc: 0.750000] [adversarial loss: 1.106234, acc: 0.328125]\n",
      "9903: [discriminator loss: 0.480579, acc: 0.773438] [adversarial loss: 1.234786, acc: 0.281250]\n",
      "9904: [discriminator loss: 0.584647, acc: 0.679688] [adversarial loss: 1.301855, acc: 0.234375]\n",
      "9905: [discriminator loss: 0.482238, acc: 0.765625] [adversarial loss: 1.331870, acc: 0.156250]\n",
      "9906: [discriminator loss: 0.544284, acc: 0.726562] [adversarial loss: 1.114872, acc: 0.171875]\n",
      "9907: [discriminator loss: 0.556750, acc: 0.695312] [adversarial loss: 1.135298, acc: 0.343750]\n",
      "9908: [discriminator loss: 0.559018, acc: 0.656250] [adversarial loss: 1.085983, acc: 0.265625]\n",
      "9909: [discriminator loss: 0.487815, acc: 0.789062] [adversarial loss: 1.465042, acc: 0.140625]\n",
      "9910: [discriminator loss: 0.641976, acc: 0.648438] [adversarial loss: 1.134557, acc: 0.328125]\n",
      "9911: [discriminator loss: 0.513295, acc: 0.710938] [adversarial loss: 1.290805, acc: 0.218750]\n",
      "9912: [discriminator loss: 0.562337, acc: 0.726562] [adversarial loss: 1.179105, acc: 0.187500]\n",
      "9913: [discriminator loss: 0.541583, acc: 0.734375] [adversarial loss: 1.213709, acc: 0.281250]\n",
      "9914: [discriminator loss: 0.559795, acc: 0.734375] [adversarial loss: 1.483991, acc: 0.125000]\n",
      "9915: [discriminator loss: 0.567968, acc: 0.671875] [adversarial loss: 0.923563, acc: 0.390625]\n",
      "9916: [discriminator loss: 0.531958, acc: 0.757812] [adversarial loss: 1.120873, acc: 0.328125]\n",
      "9917: [discriminator loss: 0.581714, acc: 0.703125] [adversarial loss: 1.244396, acc: 0.187500]\n",
      "9918: [discriminator loss: 0.537129, acc: 0.734375] [adversarial loss: 1.127581, acc: 0.265625]\n",
      "9919: [discriminator loss: 0.589466, acc: 0.695312] [adversarial loss: 1.098548, acc: 0.312500]\n",
      "9920: [discriminator loss: 0.495746, acc: 0.757812] [adversarial loss: 1.437068, acc: 0.078125]\n",
      "9921: [discriminator loss: 0.558395, acc: 0.710938] [adversarial loss: 0.848278, acc: 0.531250]\n",
      "9922: [discriminator loss: 0.617766, acc: 0.664062] [adversarial loss: 1.502958, acc: 0.093750]\n",
      "9923: [discriminator loss: 0.586128, acc: 0.687500] [adversarial loss: 0.657890, acc: 0.625000]\n",
      "9924: [discriminator loss: 0.628020, acc: 0.632812] [adversarial loss: 1.358516, acc: 0.156250]\n",
      "9925: [discriminator loss: 0.535759, acc: 0.726562] [adversarial loss: 1.016001, acc: 0.281250]\n",
      "9926: [discriminator loss: 0.550313, acc: 0.687500] [adversarial loss: 1.445906, acc: 0.140625]\n",
      "9927: [discriminator loss: 0.576591, acc: 0.656250] [adversarial loss: 1.100996, acc: 0.250000]\n",
      "9928: [discriminator loss: 0.588352, acc: 0.679688] [adversarial loss: 1.084597, acc: 0.281250]\n",
      "9929: [discriminator loss: 0.545728, acc: 0.703125] [adversarial loss: 1.127442, acc: 0.265625]\n",
      "9930: [discriminator loss: 0.546002, acc: 0.710938] [adversarial loss: 1.409220, acc: 0.109375]\n",
      "9931: [discriminator loss: 0.523310, acc: 0.703125] [adversarial loss: 0.937683, acc: 0.437500]\n",
      "9932: [discriminator loss: 0.522495, acc: 0.726562] [adversarial loss: 1.326705, acc: 0.187500]\n",
      "9933: [discriminator loss: 0.528581, acc: 0.703125] [adversarial loss: 1.053998, acc: 0.328125]\n",
      "9934: [discriminator loss: 0.564199, acc: 0.718750] [adversarial loss: 1.328423, acc: 0.171875]\n",
      "9935: [discriminator loss: 0.636424, acc: 0.640625] [adversarial loss: 1.252989, acc: 0.218750]\n",
      "9936: [discriminator loss: 0.541803, acc: 0.718750] [adversarial loss: 1.283282, acc: 0.171875]\n",
      "9937: [discriminator loss: 0.563469, acc: 0.703125] [adversarial loss: 1.292914, acc: 0.187500]\n",
      "9938: [discriminator loss: 0.547398, acc: 0.734375] [adversarial loss: 1.116247, acc: 0.234375]\n",
      "9939: [discriminator loss: 0.582574, acc: 0.648438] [adversarial loss: 0.976067, acc: 0.343750]\n",
      "9940: [discriminator loss: 0.580402, acc: 0.687500] [adversarial loss: 1.311970, acc: 0.125000]\n",
      "9941: [discriminator loss: 0.573221, acc: 0.695312] [adversarial loss: 1.092426, acc: 0.234375]\n",
      "9942: [discriminator loss: 0.520274, acc: 0.765625] [adversarial loss: 1.246955, acc: 0.171875]\n",
      "9943: [discriminator loss: 0.497523, acc: 0.781250] [adversarial loss: 0.988717, acc: 0.375000]\n",
      "9944: [discriminator loss: 0.554183, acc: 0.726562] [adversarial loss: 1.403715, acc: 0.109375]\n",
      "9945: [discriminator loss: 0.546520, acc: 0.742188] [adversarial loss: 0.783008, acc: 0.453125]\n",
      "9946: [discriminator loss: 0.616530, acc: 0.632812] [adversarial loss: 1.566952, acc: 0.109375]\n",
      "9947: [discriminator loss: 0.550090, acc: 0.687500] [adversarial loss: 0.839828, acc: 0.437500]\n",
      "9948: [discriminator loss: 0.543592, acc: 0.671875] [adversarial loss: 1.514863, acc: 0.109375]\n",
      "9949: [discriminator loss: 0.542427, acc: 0.695312] [adversarial loss: 0.899112, acc: 0.421875]\n",
      "9950: [discriminator loss: 0.537275, acc: 0.726562] [adversarial loss: 1.312826, acc: 0.187500]\n",
      "9951: [discriminator loss: 0.531685, acc: 0.742188] [adversarial loss: 0.981211, acc: 0.421875]\n",
      "9952: [discriminator loss: 0.562759, acc: 0.695312] [adversarial loss: 1.110765, acc: 0.250000]\n",
      "9953: [discriminator loss: 0.485483, acc: 0.757812] [adversarial loss: 1.214265, acc: 0.218750]\n",
      "9954: [discriminator loss: 0.546919, acc: 0.734375] [adversarial loss: 0.958263, acc: 0.406250]\n",
      "9955: [discriminator loss: 0.521981, acc: 0.757812] [adversarial loss: 1.299512, acc: 0.234375]\n",
      "9956: [discriminator loss: 0.591781, acc: 0.695312] [adversarial loss: 0.967895, acc: 0.359375]\n",
      "9957: [discriminator loss: 0.527190, acc: 0.734375] [adversarial loss: 1.433158, acc: 0.125000]\n",
      "9958: [discriminator loss: 0.564183, acc: 0.687500] [adversarial loss: 0.786894, acc: 0.500000]\n",
      "9959: [discriminator loss: 0.557992, acc: 0.695312] [adversarial loss: 1.096761, acc: 0.265625]\n",
      "9960: [discriminator loss: 0.563086, acc: 0.648438] [adversarial loss: 0.982696, acc: 0.265625]\n",
      "9961: [discriminator loss: 0.554272, acc: 0.734375] [adversarial loss: 1.374655, acc: 0.156250]\n",
      "9962: [discriminator loss: 0.528901, acc: 0.742188] [adversarial loss: 1.150994, acc: 0.265625]\n",
      "9963: [discriminator loss: 0.602982, acc: 0.671875] [adversarial loss: 1.280286, acc: 0.250000]\n",
      "9964: [discriminator loss: 0.618034, acc: 0.695312] [adversarial loss: 0.826757, acc: 0.437500]\n",
      "9965: [discriminator loss: 0.568014, acc: 0.710938] [adversarial loss: 1.340347, acc: 0.187500]\n",
      "9966: [discriminator loss: 0.485052, acc: 0.750000] [adversarial loss: 1.050214, acc: 0.312500]\n",
      "9967: [discriminator loss: 0.552674, acc: 0.679688] [adversarial loss: 1.212133, acc: 0.281250]\n",
      "9968: [discriminator loss: 0.513540, acc: 0.742188] [adversarial loss: 0.927362, acc: 0.343750]\n",
      "9969: [discriminator loss: 0.564201, acc: 0.687500] [adversarial loss: 1.019551, acc: 0.218750]\n",
      "9970: [discriminator loss: 0.532250, acc: 0.750000] [adversarial loss: 1.231990, acc: 0.171875]\n",
      "9971: [discriminator loss: 0.535510, acc: 0.695312] [adversarial loss: 0.933304, acc: 0.390625]\n",
      "9972: [discriminator loss: 0.561406, acc: 0.726562] [adversarial loss: 1.184980, acc: 0.203125]\n",
      "9973: [discriminator loss: 0.468486, acc: 0.804688] [adversarial loss: 0.969869, acc: 0.343750]\n",
      "9974: [discriminator loss: 0.576559, acc: 0.687500] [adversarial loss: 1.446988, acc: 0.140625]\n",
      "9975: [discriminator loss: 0.523185, acc: 0.726562] [adversarial loss: 0.980893, acc: 0.375000]\n",
      "9976: [discriminator loss: 0.579401, acc: 0.710938] [adversarial loss: 1.676962, acc: 0.125000]\n",
      "9977: [discriminator loss: 0.538951, acc: 0.710938] [adversarial loss: 0.848801, acc: 0.484375]\n",
      "9978: [discriminator loss: 0.606726, acc: 0.679688] [adversarial loss: 1.104135, acc: 0.328125]\n",
      "9979: [discriminator loss: 0.615942, acc: 0.617188] [adversarial loss: 1.016092, acc: 0.312500]\n",
      "9980: [discriminator loss: 0.610573, acc: 0.671875] [adversarial loss: 1.068657, acc: 0.265625]\n",
      "9981: [discriminator loss: 0.514385, acc: 0.742188] [adversarial loss: 1.256232, acc: 0.218750]\n",
      "9982: [discriminator loss: 0.506064, acc: 0.718750] [adversarial loss: 1.126590, acc: 0.187500]\n",
      "9983: [discriminator loss: 0.533357, acc: 0.703125] [adversarial loss: 1.066591, acc: 0.281250]\n",
      "9984: [discriminator loss: 0.587638, acc: 0.687500] [adversarial loss: 1.154023, acc: 0.203125]\n",
      "9985: [discriminator loss: 0.534207, acc: 0.718750] [adversarial loss: 0.971741, acc: 0.359375]\n",
      "9986: [discriminator loss: 0.552126, acc: 0.695312] [adversarial loss: 1.406307, acc: 0.140625]\n",
      "9987: [discriminator loss: 0.574004, acc: 0.703125] [adversarial loss: 0.924780, acc: 0.437500]\n",
      "9988: [discriminator loss: 0.480801, acc: 0.781250] [adversarial loss: 1.259309, acc: 0.218750]\n",
      "9989: [discriminator loss: 0.612052, acc: 0.640625] [adversarial loss: 1.579131, acc: 0.125000]\n",
      "9990: [discriminator loss: 0.540353, acc: 0.718750] [adversarial loss: 0.975407, acc: 0.390625]\n",
      "9991: [discriminator loss: 0.528100, acc: 0.734375] [adversarial loss: 1.125638, acc: 0.296875]\n",
      "9992: [discriminator loss: 0.553796, acc: 0.750000] [adversarial loss: 1.216912, acc: 0.234375]\n",
      "9993: [discriminator loss: 0.536926, acc: 0.734375] [adversarial loss: 1.135625, acc: 0.265625]\n",
      "9994: [discriminator loss: 0.499633, acc: 0.734375] [adversarial loss: 1.065887, acc: 0.312500]\n",
      "9995: [discriminator loss: 0.569713, acc: 0.710938] [adversarial loss: 1.294031, acc: 0.187500]\n",
      "9996: [discriminator loss: 0.530884, acc: 0.820312] [adversarial loss: 1.067371, acc: 0.218750]\n",
      "9997: [discriminator loss: 0.596402, acc: 0.679688] [adversarial loss: 1.217482, acc: 0.234375]\n",
      "9998: [discriminator loss: 0.565628, acc: 0.710938] [adversarial loss: 1.170819, acc: 0.234375]\n",
      "9999: [discriminator loss: 0.476532, acc: 0.773438] [adversarial loss: 1.065920, acc: 0.343750]\n",
      "10000: [discriminator loss: 0.563668, acc: 0.687500] [adversarial loss: 1.349620, acc: 0.234375]\n",
      "10001: [discriminator loss: 0.571466, acc: 0.679688] [adversarial loss: 0.942826, acc: 0.375000]\n",
      "10002: [discriminator loss: 0.559818, acc: 0.679688] [adversarial loss: 1.356229, acc: 0.171875]\n",
      "10003: [discriminator loss: 0.521512, acc: 0.718750] [adversarial loss: 0.765512, acc: 0.515625]\n",
      "10004: [discriminator loss: 0.574654, acc: 0.726562] [adversarial loss: 1.286278, acc: 0.281250]\n",
      "10005: [discriminator loss: 0.561668, acc: 0.664062] [adversarial loss: 1.238107, acc: 0.296875]\n",
      "10006: [discriminator loss: 0.534102, acc: 0.710938] [adversarial loss: 1.160555, acc: 0.312500]\n",
      "10007: [discriminator loss: 0.582005, acc: 0.664062] [adversarial loss: 1.382982, acc: 0.171875]\n",
      "10008: [discriminator loss: 0.572662, acc: 0.703125] [adversarial loss: 0.736393, acc: 0.531250]\n",
      "10009: [discriminator loss: 0.589972, acc: 0.656250] [adversarial loss: 1.689936, acc: 0.062500]\n",
      "10010: [discriminator loss: 0.569149, acc: 0.679688] [adversarial loss: 0.721455, acc: 0.562500]\n",
      "10011: [discriminator loss: 0.575222, acc: 0.710938] [adversarial loss: 1.386208, acc: 0.046875]\n",
      "10012: [discriminator loss: 0.518546, acc: 0.718750] [adversarial loss: 1.214614, acc: 0.250000]\n",
      "10013: [discriminator loss: 0.544897, acc: 0.734375] [adversarial loss: 1.058263, acc: 0.375000]\n",
      "10014: [discriminator loss: 0.481193, acc: 0.773438] [adversarial loss: 1.336748, acc: 0.265625]\n",
      "10015: [discriminator loss: 0.584970, acc: 0.695312] [adversarial loss: 1.226335, acc: 0.187500]\n",
      "10016: [discriminator loss: 0.591403, acc: 0.648438] [adversarial loss: 1.328246, acc: 0.156250]\n",
      "10017: [discriminator loss: 0.509064, acc: 0.773438] [adversarial loss: 0.967828, acc: 0.390625]\n",
      "10018: [discriminator loss: 0.527676, acc: 0.703125] [adversarial loss: 1.331490, acc: 0.156250]\n",
      "10019: [discriminator loss: 0.504088, acc: 0.812500] [adversarial loss: 1.196262, acc: 0.250000]\n",
      "10020: [discriminator loss: 0.593369, acc: 0.679688] [adversarial loss: 1.338591, acc: 0.187500]\n",
      "10021: [discriminator loss: 0.540796, acc: 0.703125] [adversarial loss: 0.943833, acc: 0.468750]\n",
      "10022: [discriminator loss: 0.542588, acc: 0.734375] [adversarial loss: 1.293564, acc: 0.234375]\n",
      "10023: [discriminator loss: 0.658893, acc: 0.609375] [adversarial loss: 1.053982, acc: 0.312500]\n",
      "10024: [discriminator loss: 0.587147, acc: 0.648438] [adversarial loss: 1.408603, acc: 0.125000]\n",
      "10025: [discriminator loss: 0.540464, acc: 0.734375] [adversarial loss: 1.103745, acc: 0.234375]\n",
      "10026: [discriminator loss: 0.485654, acc: 0.765625] [adversarial loss: 1.087575, acc: 0.296875]\n",
      "10027: [discriminator loss: 0.529353, acc: 0.750000] [adversarial loss: 1.045697, acc: 0.343750]\n",
      "10028: [discriminator loss: 0.558866, acc: 0.718750] [adversarial loss: 1.185271, acc: 0.234375]\n",
      "10029: [discriminator loss: 0.548782, acc: 0.710938] [adversarial loss: 1.191365, acc: 0.265625]\n",
      "10030: [discriminator loss: 0.587925, acc: 0.656250] [adversarial loss: 0.931439, acc: 0.421875]\n",
      "10031: [discriminator loss: 0.533160, acc: 0.695312] [adversarial loss: 1.235850, acc: 0.156250]\n",
      "10032: [discriminator loss: 0.559325, acc: 0.671875] [adversarial loss: 1.105074, acc: 0.250000]\n",
      "10033: [discriminator loss: 0.523003, acc: 0.703125] [adversarial loss: 1.150624, acc: 0.265625]\n",
      "10034: [discriminator loss: 0.587938, acc: 0.695312] [adversarial loss: 1.259944, acc: 0.250000]\n",
      "10035: [discriminator loss: 0.490619, acc: 0.742188] [adversarial loss: 1.604365, acc: 0.062500]\n",
      "10036: [discriminator loss: 0.588437, acc: 0.679688] [adversarial loss: 0.815251, acc: 0.484375]\n",
      "10037: [discriminator loss: 0.529720, acc: 0.695312] [adversarial loss: 1.356653, acc: 0.234375]\n",
      "10038: [discriminator loss: 0.527953, acc: 0.757812] [adversarial loss: 1.036273, acc: 0.343750]\n",
      "10039: [discriminator loss: 0.563469, acc: 0.718750] [adversarial loss: 1.444626, acc: 0.125000]\n",
      "10040: [discriminator loss: 0.507305, acc: 0.742188] [adversarial loss: 1.024590, acc: 0.343750]\n",
      "10041: [discriminator loss: 0.539642, acc: 0.687500] [adversarial loss: 1.658789, acc: 0.062500]\n",
      "10042: [discriminator loss: 0.523443, acc: 0.734375] [adversarial loss: 1.117601, acc: 0.281250]\n",
      "10043: [discriminator loss: 0.532946, acc: 0.726562] [adversarial loss: 1.292299, acc: 0.218750]\n",
      "10044: [discriminator loss: 0.542161, acc: 0.695312] [adversarial loss: 0.879013, acc: 0.406250]\n",
      "10045: [discriminator loss: 0.566108, acc: 0.726562] [adversarial loss: 1.297594, acc: 0.187500]\n",
      "10046: [discriminator loss: 0.529240, acc: 0.703125] [adversarial loss: 0.967578, acc: 0.328125]\n",
      "10047: [discriminator loss: 0.506785, acc: 0.750000] [adversarial loss: 1.205075, acc: 0.203125]\n",
      "10048: [discriminator loss: 0.574559, acc: 0.648438] [adversarial loss: 0.940424, acc: 0.359375]\n",
      "10049: [discriminator loss: 0.524842, acc: 0.765625] [adversarial loss: 1.374868, acc: 0.140625]\n",
      "10050: [discriminator loss: 0.567387, acc: 0.710938] [adversarial loss: 0.981187, acc: 0.390625]\n",
      "10051: [discriminator loss: 0.579394, acc: 0.734375] [adversarial loss: 1.431264, acc: 0.125000]\n",
      "10052: [discriminator loss: 0.485589, acc: 0.765625] [adversarial loss: 0.963074, acc: 0.343750]\n",
      "10053: [discriminator loss: 0.528413, acc: 0.734375] [adversarial loss: 1.347725, acc: 0.156250]\n",
      "10054: [discriminator loss: 0.572215, acc: 0.695312] [adversarial loss: 1.006158, acc: 0.375000]\n",
      "10055: [discriminator loss: 0.598592, acc: 0.656250] [adversarial loss: 1.282045, acc: 0.187500]\n",
      "10056: [discriminator loss: 0.515663, acc: 0.710938] [adversarial loss: 0.862647, acc: 0.546875]\n",
      "10057: [discriminator loss: 0.549945, acc: 0.757812] [adversarial loss: 1.403076, acc: 0.140625]\n",
      "10058: [discriminator loss: 0.500324, acc: 0.750000] [adversarial loss: 1.153329, acc: 0.312500]\n",
      "10059: [discriminator loss: 0.508169, acc: 0.750000] [adversarial loss: 1.015851, acc: 0.312500]\n",
      "10060: [discriminator loss: 0.593420, acc: 0.726562] [adversarial loss: 1.207971, acc: 0.218750]\n",
      "10061: [discriminator loss: 0.555010, acc: 0.664062] [adversarial loss: 1.255521, acc: 0.203125]\n",
      "10062: [discriminator loss: 0.598420, acc: 0.656250] [adversarial loss: 0.867517, acc: 0.406250]\n",
      "10063: [discriminator loss: 0.583792, acc: 0.656250] [adversarial loss: 1.284075, acc: 0.265625]\n",
      "10064: [discriminator loss: 0.590845, acc: 0.656250] [adversarial loss: 1.174806, acc: 0.265625]\n",
      "10065: [discriminator loss: 0.538815, acc: 0.750000] [adversarial loss: 1.061749, acc: 0.359375]\n",
      "10066: [discriminator loss: 0.506479, acc: 0.773438] [adversarial loss: 1.213085, acc: 0.171875]\n",
      "10067: [discriminator loss: 0.597198, acc: 0.695312] [adversarial loss: 0.955085, acc: 0.406250]\n",
      "10068: [discriminator loss: 0.496121, acc: 0.757812] [adversarial loss: 1.430093, acc: 0.125000]\n",
      "10069: [discriminator loss: 0.546575, acc: 0.742188] [adversarial loss: 0.873142, acc: 0.453125]\n",
      "10070: [discriminator loss: 0.626563, acc: 0.695312] [adversarial loss: 1.386090, acc: 0.171875]\n",
      "10071: [discriminator loss: 0.597076, acc: 0.656250] [adversarial loss: 0.915563, acc: 0.390625]\n",
      "10072: [discriminator loss: 0.553471, acc: 0.695312] [adversarial loss: 1.338224, acc: 0.140625]\n",
      "10073: [discriminator loss: 0.595481, acc: 0.664062] [adversarial loss: 0.751192, acc: 0.531250]\n",
      "10074: [discriminator loss: 0.564356, acc: 0.695312] [adversarial loss: 1.204309, acc: 0.156250]\n",
      "10075: [discriminator loss: 0.540735, acc: 0.742188] [adversarial loss: 1.198199, acc: 0.234375]\n",
      "10076: [discriminator loss: 0.568618, acc: 0.671875] [adversarial loss: 1.298215, acc: 0.171875]\n",
      "10077: [discriminator loss: 0.578632, acc: 0.695312] [adversarial loss: 0.951302, acc: 0.328125]\n",
      "10078: [discriminator loss: 0.597883, acc: 0.664062] [adversarial loss: 1.292827, acc: 0.203125]\n",
      "10079: [discriminator loss: 0.610558, acc: 0.703125] [adversarial loss: 0.948462, acc: 0.359375]\n",
      "10080: [discriminator loss: 0.570970, acc: 0.664062] [adversarial loss: 1.536828, acc: 0.125000]\n",
      "10081: [discriminator loss: 0.533914, acc: 0.726562] [adversarial loss: 0.792046, acc: 0.593750]\n",
      "10082: [discriminator loss: 0.615260, acc: 0.656250] [adversarial loss: 1.450212, acc: 0.140625]\n",
      "10083: [discriminator loss: 0.550112, acc: 0.695312] [adversarial loss: 0.981813, acc: 0.265625]\n",
      "10084: [discriminator loss: 0.543551, acc: 0.710938] [adversarial loss: 1.192337, acc: 0.203125]\n",
      "10085: [discriminator loss: 0.576101, acc: 0.679688] [adversarial loss: 0.873581, acc: 0.406250]\n",
      "10086: [discriminator loss: 0.544884, acc: 0.718750] [adversarial loss: 1.489405, acc: 0.156250]\n",
      "10087: [discriminator loss: 0.519096, acc: 0.718750] [adversarial loss: 1.041711, acc: 0.328125]\n",
      "10088: [discriminator loss: 0.566342, acc: 0.710938] [adversarial loss: 1.076663, acc: 0.312500]\n",
      "10089: [discriminator loss: 0.558115, acc: 0.710938] [adversarial loss: 1.185924, acc: 0.203125]\n",
      "10090: [discriminator loss: 0.562749, acc: 0.687500] [adversarial loss: 1.156180, acc: 0.312500]\n",
      "10091: [discriminator loss: 0.559574, acc: 0.703125] [adversarial loss: 1.105951, acc: 0.234375]\n",
      "10092: [discriminator loss: 0.517935, acc: 0.742188] [adversarial loss: 1.072441, acc: 0.328125]\n",
      "10093: [discriminator loss: 0.539336, acc: 0.750000] [adversarial loss: 1.316353, acc: 0.218750]\n",
      "10094: [discriminator loss: 0.512632, acc: 0.687500] [adversarial loss: 1.307031, acc: 0.109375]\n",
      "10095: [discriminator loss: 0.571790, acc: 0.703125] [adversarial loss: 1.400446, acc: 0.171875]\n",
      "10096: [discriminator loss: 0.616244, acc: 0.695312] [adversarial loss: 0.807242, acc: 0.468750]\n",
      "10097: [discriminator loss: 0.664454, acc: 0.648438] [adversarial loss: 1.810219, acc: 0.015625]\n",
      "10098: [discriminator loss: 0.673751, acc: 0.617188] [adversarial loss: 1.039501, acc: 0.312500]\n",
      "10099: [discriminator loss: 0.550181, acc: 0.773438] [adversarial loss: 1.139512, acc: 0.281250]\n",
      "10100: [discriminator loss: 0.526901, acc: 0.718750] [adversarial loss: 1.068550, acc: 0.265625]\n",
      "10101: [discriminator loss: 0.630064, acc: 0.617188] [adversarial loss: 1.074830, acc: 0.265625]\n",
      "10102: [discriminator loss: 0.546649, acc: 0.734375] [adversarial loss: 1.068119, acc: 0.296875]\n",
      "10103: [discriminator loss: 0.558847, acc: 0.695312] [adversarial loss: 1.130891, acc: 0.296875]\n",
      "10104: [discriminator loss: 0.551929, acc: 0.726562] [adversarial loss: 1.239224, acc: 0.140625]\n",
      "10105: [discriminator loss: 0.551177, acc: 0.671875] [adversarial loss: 1.016651, acc: 0.328125]\n",
      "10106: [discriminator loss: 0.478692, acc: 0.796875] [adversarial loss: 1.225447, acc: 0.203125]\n",
      "10107: [discriminator loss: 0.522974, acc: 0.726562] [adversarial loss: 1.381066, acc: 0.187500]\n",
      "10108: [discriminator loss: 0.532062, acc: 0.679688] [adversarial loss: 0.841727, acc: 0.375000]\n",
      "10109: [discriminator loss: 0.599332, acc: 0.664062] [adversarial loss: 1.303231, acc: 0.140625]\n",
      "10110: [discriminator loss: 0.587056, acc: 0.640625] [adversarial loss: 0.924321, acc: 0.421875]\n",
      "10111: [discriminator loss: 0.531862, acc: 0.765625] [adversarial loss: 1.188048, acc: 0.140625]\n",
      "10112: [discriminator loss: 0.524362, acc: 0.695312] [adversarial loss: 1.156437, acc: 0.265625]\n",
      "10113: [discriminator loss: 0.511793, acc: 0.757812] [adversarial loss: 1.419077, acc: 0.187500]\n",
      "10114: [discriminator loss: 0.456403, acc: 0.796875] [adversarial loss: 1.070678, acc: 0.265625]\n",
      "10115: [discriminator loss: 0.581382, acc: 0.695312] [adversarial loss: 1.575307, acc: 0.109375]\n",
      "10116: [discriminator loss: 0.578614, acc: 0.695312] [adversarial loss: 1.044060, acc: 0.328125]\n",
      "10117: [discriminator loss: 0.555750, acc: 0.671875] [adversarial loss: 1.217092, acc: 0.265625]\n",
      "10118: [discriminator loss: 0.519702, acc: 0.718750] [adversarial loss: 1.473905, acc: 0.187500]\n",
      "10119: [discriminator loss: 0.549551, acc: 0.679688] [adversarial loss: 1.028558, acc: 0.375000]\n",
      "10120: [discriminator loss: 0.593113, acc: 0.671875] [adversarial loss: 1.458223, acc: 0.156250]\n",
      "10121: [discriminator loss: 0.600374, acc: 0.710938] [adversarial loss: 0.814981, acc: 0.421875]\n",
      "10122: [discriminator loss: 0.505183, acc: 0.726562] [adversarial loss: 1.283748, acc: 0.156250]\n",
      "10123: [discriminator loss: 0.540227, acc: 0.726562] [adversarial loss: 1.275845, acc: 0.125000]\n",
      "10124: [discriminator loss: 0.540296, acc: 0.750000] [adversarial loss: 1.213838, acc: 0.234375]\n",
      "10125: [discriminator loss: 0.528797, acc: 0.750000] [adversarial loss: 1.033147, acc: 0.343750]\n",
      "10126: [discriminator loss: 0.578067, acc: 0.664062] [adversarial loss: 1.252652, acc: 0.281250]\n",
      "10127: [discriminator loss: 0.548625, acc: 0.664062] [adversarial loss: 1.198605, acc: 0.218750]\n",
      "10128: [discriminator loss: 0.513347, acc: 0.718750] [adversarial loss: 0.952948, acc: 0.375000]\n",
      "10129: [discriminator loss: 0.537997, acc: 0.742188] [adversarial loss: 1.497245, acc: 0.125000]\n",
      "10130: [discriminator loss: 0.633415, acc: 0.625000] [adversarial loss: 0.974984, acc: 0.281250]\n",
      "10131: [discriminator loss: 0.570654, acc: 0.687500] [adversarial loss: 1.377682, acc: 0.187500]\n",
      "10132: [discriminator loss: 0.529516, acc: 0.710938] [adversarial loss: 0.958502, acc: 0.328125]\n",
      "10133: [discriminator loss: 0.585321, acc: 0.687500] [adversarial loss: 1.483255, acc: 0.046875]\n",
      "10134: [discriminator loss: 0.605438, acc: 0.632812] [adversarial loss: 0.931236, acc: 0.406250]\n",
      "10135: [discriminator loss: 0.477445, acc: 0.765625] [adversarial loss: 1.483422, acc: 0.109375]\n",
      "10136: [discriminator loss: 0.627262, acc: 0.640625] [adversarial loss: 0.842917, acc: 0.390625]\n",
      "10137: [discriminator loss: 0.563537, acc: 0.664062] [adversarial loss: 1.209955, acc: 0.234375]\n",
      "10138: [discriminator loss: 0.527418, acc: 0.734375] [adversarial loss: 0.819302, acc: 0.359375]\n",
      "10139: [discriminator loss: 0.526507, acc: 0.687500] [adversarial loss: 1.125657, acc: 0.187500]\n",
      "10140: [discriminator loss: 0.561186, acc: 0.687500] [adversarial loss: 1.102190, acc: 0.203125]\n",
      "10141: [discriminator loss: 0.527894, acc: 0.718750] [adversarial loss: 1.227511, acc: 0.234375]\n",
      "10142: [discriminator loss: 0.570972, acc: 0.679688] [adversarial loss: 1.180070, acc: 0.265625]\n",
      "10143: [discriminator loss: 0.525808, acc: 0.734375] [adversarial loss: 1.139143, acc: 0.250000]\n",
      "10144: [discriminator loss: 0.505864, acc: 0.757812] [adversarial loss: 1.189969, acc: 0.281250]\n",
      "10145: [discriminator loss: 0.535118, acc: 0.742188] [adversarial loss: 1.087745, acc: 0.359375]\n",
      "10146: [discriminator loss: 0.595124, acc: 0.656250] [adversarial loss: 1.373900, acc: 0.109375]\n",
      "10147: [discriminator loss: 0.603953, acc: 0.656250] [adversarial loss: 1.045089, acc: 0.328125]\n",
      "10148: [discriminator loss: 0.577080, acc: 0.625000] [adversarial loss: 1.572048, acc: 0.140625]\n",
      "10149: [discriminator loss: 0.612902, acc: 0.703125] [adversarial loss: 0.713401, acc: 0.625000]\n",
      "10150: [discriminator loss: 0.566042, acc: 0.703125] [adversarial loss: 1.334219, acc: 0.218750]\n",
      "10151: [discriminator loss: 0.526174, acc: 0.726562] [adversarial loss: 0.931206, acc: 0.328125]\n",
      "10152: [discriminator loss: 0.522718, acc: 0.781250] [adversarial loss: 1.266448, acc: 0.203125]\n",
      "10153: [discriminator loss: 0.556582, acc: 0.664062] [adversarial loss: 0.993756, acc: 0.296875]\n",
      "10154: [discriminator loss: 0.531235, acc: 0.695312] [adversarial loss: 1.094555, acc: 0.187500]\n",
      "10155: [discriminator loss: 0.545708, acc: 0.710938] [adversarial loss: 1.197001, acc: 0.203125]\n",
      "10156: [discriminator loss: 0.514260, acc: 0.750000] [adversarial loss: 1.017365, acc: 0.265625]\n",
      "10157: [discriminator loss: 0.538935, acc: 0.734375] [adversarial loss: 1.334345, acc: 0.140625]\n",
      "10158: [discriminator loss: 0.627771, acc: 0.617188] [adversarial loss: 0.804692, acc: 0.468750]\n",
      "10159: [discriminator loss: 0.535444, acc: 0.718750] [adversarial loss: 1.351910, acc: 0.171875]\n",
      "10160: [discriminator loss: 0.560410, acc: 0.687500] [adversarial loss: 1.200362, acc: 0.140625]\n",
      "10161: [discriminator loss: 0.535480, acc: 0.742188] [adversarial loss: 1.251618, acc: 0.281250]\n",
      "10162: [discriminator loss: 0.566087, acc: 0.687500] [adversarial loss: 1.094926, acc: 0.265625]\n",
      "10163: [discriminator loss: 0.586212, acc: 0.679688] [adversarial loss: 1.211872, acc: 0.140625]\n",
      "10164: [discriminator loss: 0.475481, acc: 0.773438] [adversarial loss: 1.192740, acc: 0.234375]\n",
      "10165: [discriminator loss: 0.553916, acc: 0.718750] [adversarial loss: 1.123693, acc: 0.234375]\n",
      "10166: [discriminator loss: 0.570879, acc: 0.648438] [adversarial loss: 1.286880, acc: 0.281250]\n",
      "10167: [discriminator loss: 0.531587, acc: 0.718750] [adversarial loss: 1.092698, acc: 0.328125]\n",
      "10168: [discriminator loss: 0.502707, acc: 0.796875] [adversarial loss: 1.339166, acc: 0.078125]\n",
      "10169: [discriminator loss: 0.497775, acc: 0.742188] [adversarial loss: 0.982745, acc: 0.375000]\n",
      "10170: [discriminator loss: 0.516502, acc: 0.742188] [adversarial loss: 1.102020, acc: 0.281250]\n",
      "10171: [discriminator loss: 0.558369, acc: 0.718750] [adversarial loss: 1.476132, acc: 0.156250]\n",
      "10172: [discriminator loss: 0.533066, acc: 0.687500] [adversarial loss: 1.027677, acc: 0.375000]\n",
      "10173: [discriminator loss: 0.569543, acc: 0.710938] [adversarial loss: 1.346034, acc: 0.156250]\n",
      "10174: [discriminator loss: 0.494082, acc: 0.750000] [adversarial loss: 0.852325, acc: 0.468750]\n",
      "10175: [discriminator loss: 0.599370, acc: 0.726562] [adversarial loss: 1.571387, acc: 0.093750]\n",
      "10176: [discriminator loss: 0.545140, acc: 0.726562] [adversarial loss: 0.933309, acc: 0.343750]\n",
      "10177: [discriminator loss: 0.593271, acc: 0.656250] [adversarial loss: 1.450175, acc: 0.140625]\n",
      "10178: [discriminator loss: 0.591921, acc: 0.656250] [adversarial loss: 0.954481, acc: 0.437500]\n",
      "10179: [discriminator loss: 0.634015, acc: 0.617188] [adversarial loss: 1.240608, acc: 0.171875]\n",
      "10180: [discriminator loss: 0.521365, acc: 0.773438] [adversarial loss: 1.069725, acc: 0.250000]\n",
      "10181: [discriminator loss: 0.586325, acc: 0.695312] [adversarial loss: 1.237325, acc: 0.250000]\n",
      "10182: [discriminator loss: 0.573747, acc: 0.695312] [adversarial loss: 1.257751, acc: 0.203125]\n",
      "10183: [discriminator loss: 0.486913, acc: 0.781250] [adversarial loss: 1.396028, acc: 0.171875]\n",
      "10184: [discriminator loss: 0.612566, acc: 0.687500] [adversarial loss: 0.929224, acc: 0.453125]\n",
      "10185: [discriminator loss: 0.544347, acc: 0.726562] [adversarial loss: 1.248943, acc: 0.171875]\n",
      "10186: [discriminator loss: 0.574682, acc: 0.710938] [adversarial loss: 1.080725, acc: 0.328125]\n",
      "10187: [discriminator loss: 0.585689, acc: 0.679688] [adversarial loss: 1.346079, acc: 0.171875]\n",
      "10188: [discriminator loss: 0.527917, acc: 0.718750] [adversarial loss: 1.173820, acc: 0.203125]\n",
      "10189: [discriminator loss: 0.520738, acc: 0.789062] [adversarial loss: 1.431618, acc: 0.156250]\n",
      "10190: [discriminator loss: 0.538773, acc: 0.718750] [adversarial loss: 1.001540, acc: 0.265625]\n",
      "10191: [discriminator loss: 0.581591, acc: 0.710938] [adversarial loss: 1.203611, acc: 0.187500]\n",
      "10192: [discriminator loss: 0.574166, acc: 0.710938] [adversarial loss: 1.076252, acc: 0.187500]\n",
      "10193: [discriminator loss: 0.555993, acc: 0.718750] [adversarial loss: 1.342931, acc: 0.109375]\n",
      "10194: [discriminator loss: 0.488917, acc: 0.757812] [adversarial loss: 0.837960, acc: 0.390625]\n",
      "10195: [discriminator loss: 0.574667, acc: 0.703125] [adversarial loss: 1.557783, acc: 0.093750]\n",
      "10196: [discriminator loss: 0.561899, acc: 0.703125] [adversarial loss: 0.891938, acc: 0.390625]\n",
      "10197: [discriminator loss: 0.570361, acc: 0.664062] [adversarial loss: 1.261532, acc: 0.187500]\n",
      "10198: [discriminator loss: 0.524908, acc: 0.710938] [adversarial loss: 1.006855, acc: 0.281250]\n",
      "10199: [discriminator loss: 0.529292, acc: 0.757812] [adversarial loss: 1.045601, acc: 0.281250]\n",
      "10200: [discriminator loss: 0.508554, acc: 0.765625] [adversarial loss: 1.312727, acc: 0.125000]\n",
      "10201: [discriminator loss: 0.527727, acc: 0.734375] [adversarial loss: 1.133576, acc: 0.234375]\n",
      "10202: [discriminator loss: 0.546496, acc: 0.687500] [adversarial loss: 1.360632, acc: 0.125000]\n",
      "10203: [discriminator loss: 0.584132, acc: 0.718750] [adversarial loss: 0.715564, acc: 0.531250]\n",
      "10204: [discriminator loss: 0.564345, acc: 0.695312] [adversarial loss: 1.627454, acc: 0.093750]\n",
      "10205: [discriminator loss: 0.655090, acc: 0.625000] [adversarial loss: 0.753437, acc: 0.531250]\n",
      "10206: [discriminator loss: 0.649177, acc: 0.664062] [adversarial loss: 1.320699, acc: 0.187500]\n",
      "10207: [discriminator loss: 0.568636, acc: 0.679688] [adversarial loss: 1.137165, acc: 0.234375]\n",
      "10208: [discriminator loss: 0.531582, acc: 0.773438] [adversarial loss: 1.230655, acc: 0.203125]\n",
      "10209: [discriminator loss: 0.568584, acc: 0.679688] [adversarial loss: 1.010358, acc: 0.281250]\n",
      "10210: [discriminator loss: 0.525793, acc: 0.734375] [adversarial loss: 1.211289, acc: 0.156250]\n",
      "10211: [discriminator loss: 0.494714, acc: 0.765625] [adversarial loss: 1.228707, acc: 0.187500]\n",
      "10212: [discriminator loss: 0.512446, acc: 0.750000] [adversarial loss: 1.060763, acc: 0.296875]\n",
      "10213: [discriminator loss: 0.552655, acc: 0.671875] [adversarial loss: 1.037913, acc: 0.359375]\n",
      "10214: [discriminator loss: 0.576481, acc: 0.640625] [adversarial loss: 0.923719, acc: 0.437500]\n",
      "10215: [discriminator loss: 0.587389, acc: 0.671875] [adversarial loss: 0.990815, acc: 0.312500]\n",
      "10216: [discriminator loss: 0.533836, acc: 0.757812] [adversarial loss: 1.356689, acc: 0.218750]\n",
      "10217: [discriminator loss: 0.471302, acc: 0.757812] [adversarial loss: 1.300082, acc: 0.265625]\n",
      "10218: [discriminator loss: 0.581116, acc: 0.679688] [adversarial loss: 1.524458, acc: 0.109375]\n",
      "10219: [discriminator loss: 0.565920, acc: 0.679688] [adversarial loss: 0.880110, acc: 0.453125]\n",
      "10220: [discriminator loss: 0.585756, acc: 0.718750] [adversarial loss: 1.299710, acc: 0.171875]\n",
      "10221: [discriminator loss: 0.602748, acc: 0.679688] [adversarial loss: 1.075676, acc: 0.281250]\n",
      "10222: [discriminator loss: 0.553594, acc: 0.703125] [adversarial loss: 1.002990, acc: 0.281250]\n",
      "10223: [discriminator loss: 0.554926, acc: 0.718750] [adversarial loss: 1.200073, acc: 0.203125]\n",
      "10224: [discriminator loss: 0.481184, acc: 0.750000] [adversarial loss: 0.958794, acc: 0.328125]\n",
      "10225: [discriminator loss: 0.554503, acc: 0.687500] [adversarial loss: 1.089756, acc: 0.281250]\n",
      "10226: [discriminator loss: 0.516783, acc: 0.742188] [adversarial loss: 1.064529, acc: 0.312500]\n",
      "10227: [discriminator loss: 0.580392, acc: 0.695312] [adversarial loss: 0.924851, acc: 0.421875]\n",
      "10228: [discriminator loss: 0.572517, acc: 0.679688] [adversarial loss: 1.809712, acc: 0.031250]\n",
      "10229: [discriminator loss: 0.597767, acc: 0.679688] [adversarial loss: 0.946582, acc: 0.359375]\n",
      "10230: [discriminator loss: 0.581749, acc: 0.656250] [adversarial loss: 1.604742, acc: 0.031250]\n",
      "10231: [discriminator loss: 0.507884, acc: 0.726562] [adversarial loss: 0.908873, acc: 0.390625]\n",
      "10232: [discriminator loss: 0.584409, acc: 0.671875] [adversarial loss: 1.458692, acc: 0.171875]\n",
      "10233: [discriminator loss: 0.629919, acc: 0.656250] [adversarial loss: 1.064679, acc: 0.359375]\n",
      "10234: [discriminator loss: 0.514606, acc: 0.757812] [adversarial loss: 1.036845, acc: 0.296875]\n",
      "10235: [discriminator loss: 0.608192, acc: 0.617188] [adversarial loss: 0.861223, acc: 0.453125]\n",
      "10236: [discriminator loss: 0.533473, acc: 0.718750] [adversarial loss: 1.496098, acc: 0.140625]\n",
      "10237: [discriminator loss: 0.610181, acc: 0.679688] [adversarial loss: 1.028890, acc: 0.296875]\n",
      "10238: [discriminator loss: 0.533499, acc: 0.726562] [adversarial loss: 1.272760, acc: 0.140625]\n",
      "10239: [discriminator loss: 0.584457, acc: 0.679688] [adversarial loss: 0.810086, acc: 0.531250]\n",
      "10240: [discriminator loss: 0.573982, acc: 0.687500] [adversarial loss: 1.343059, acc: 0.187500]\n",
      "10241: [discriminator loss: 0.530060, acc: 0.734375] [adversarial loss: 0.955483, acc: 0.375000]\n",
      "10242: [discriminator loss: 0.548519, acc: 0.726562] [adversarial loss: 1.299067, acc: 0.156250]\n",
      "10243: [discriminator loss: 0.530439, acc: 0.742188] [adversarial loss: 1.003041, acc: 0.328125]\n",
      "10244: [discriminator loss: 0.500846, acc: 0.765625] [adversarial loss: 1.203593, acc: 0.218750]\n",
      "10245: [discriminator loss: 0.507047, acc: 0.750000] [adversarial loss: 1.051975, acc: 0.312500]\n",
      "10246: [discriminator loss: 0.518230, acc: 0.742188] [adversarial loss: 1.299181, acc: 0.218750]\n",
      "10247: [discriminator loss: 0.545381, acc: 0.687500] [adversarial loss: 0.998214, acc: 0.312500]\n",
      "10248: [discriminator loss: 0.601348, acc: 0.648438] [adversarial loss: 1.340165, acc: 0.156250]\n",
      "10249: [discriminator loss: 0.596400, acc: 0.625000] [adversarial loss: 0.936093, acc: 0.390625]\n",
      "10250: [discriminator loss: 0.504565, acc: 0.781250] [adversarial loss: 1.113784, acc: 0.265625]\n",
      "10251: [discriminator loss: 0.495979, acc: 0.726562] [adversarial loss: 1.161386, acc: 0.343750]\n",
      "10252: [discriminator loss: 0.570811, acc: 0.703125] [adversarial loss: 1.167505, acc: 0.265625]\n",
      "10253: [discriminator loss: 0.507135, acc: 0.757812] [adversarial loss: 1.091533, acc: 0.328125]\n",
      "10254: [discriminator loss: 0.586068, acc: 0.679688] [adversarial loss: 0.902117, acc: 0.343750]\n",
      "10255: [discriminator loss: 0.465072, acc: 0.804688] [adversarial loss: 1.355779, acc: 0.156250]\n",
      "10256: [discriminator loss: 0.583279, acc: 0.703125] [adversarial loss: 1.414687, acc: 0.125000]\n",
      "10257: [discriminator loss: 0.596939, acc: 0.664062] [adversarial loss: 0.947829, acc: 0.328125]\n",
      "10258: [discriminator loss: 0.577781, acc: 0.664062] [adversarial loss: 1.595283, acc: 0.062500]\n",
      "10259: [discriminator loss: 0.547019, acc: 0.734375] [adversarial loss: 1.201322, acc: 0.203125]\n",
      "10260: [discriminator loss: 0.548254, acc: 0.703125] [adversarial loss: 1.236374, acc: 0.265625]\n",
      "10261: [discriminator loss: 0.579039, acc: 0.718750] [adversarial loss: 0.911676, acc: 0.515625]\n",
      "10262: [discriminator loss: 0.526853, acc: 0.765625] [adversarial loss: 1.409521, acc: 0.234375]\n",
      "10263: [discriminator loss: 0.565990, acc: 0.687500] [adversarial loss: 0.997012, acc: 0.281250]\n",
      "10264: [discriminator loss: 0.492248, acc: 0.742188] [adversarial loss: 0.998936, acc: 0.328125]\n",
      "10265: [discriminator loss: 0.498868, acc: 0.781250] [adversarial loss: 1.477463, acc: 0.218750]\n",
      "10266: [discriminator loss: 0.488503, acc: 0.726562] [adversarial loss: 1.013929, acc: 0.281250]\n",
      "10267: [discriminator loss: 0.526170, acc: 0.750000] [adversarial loss: 1.174795, acc: 0.281250]\n",
      "10268: [discriminator loss: 0.491244, acc: 0.750000] [adversarial loss: 0.957000, acc: 0.390625]\n",
      "10269: [discriminator loss: 0.551681, acc: 0.710938] [adversarial loss: 1.658998, acc: 0.093750]\n",
      "10270: [discriminator loss: 0.584682, acc: 0.664062] [adversarial loss: 0.860739, acc: 0.437500]\n",
      "10271: [discriminator loss: 0.518281, acc: 0.742188] [adversarial loss: 1.405044, acc: 0.156250]\n",
      "10272: [discriminator loss: 0.728851, acc: 0.554688] [adversarial loss: 0.996143, acc: 0.359375]\n",
      "10273: [discriminator loss: 0.533120, acc: 0.765625] [adversarial loss: 1.478011, acc: 0.187500]\n",
      "10274: [discriminator loss: 0.558180, acc: 0.671875] [adversarial loss: 0.884261, acc: 0.421875]\n",
      "10275: [discriminator loss: 0.561608, acc: 0.726562] [adversarial loss: 1.240486, acc: 0.203125]\n",
      "10276: [discriminator loss: 0.521772, acc: 0.742188] [adversarial loss: 1.181571, acc: 0.250000]\n",
      "10277: [discriminator loss: 0.580412, acc: 0.656250] [adversarial loss: 1.201055, acc: 0.218750]\n",
      "10278: [discriminator loss: 0.557159, acc: 0.718750] [adversarial loss: 1.346561, acc: 0.156250]\n",
      "10279: [discriminator loss: 0.567172, acc: 0.687500] [adversarial loss: 0.984979, acc: 0.343750]\n",
      "10280: [discriminator loss: 0.574234, acc: 0.679688] [adversarial loss: 1.225694, acc: 0.187500]\n",
      "10281: [discriminator loss: 0.512395, acc: 0.773438] [adversarial loss: 0.873429, acc: 0.406250]\n",
      "10282: [discriminator loss: 0.550151, acc: 0.703125] [adversarial loss: 1.432397, acc: 0.156250]\n",
      "10283: [discriminator loss: 0.515527, acc: 0.765625] [adversarial loss: 0.995065, acc: 0.281250]\n",
      "10284: [discriminator loss: 0.608176, acc: 0.656250] [adversarial loss: 1.711706, acc: 0.046875]\n",
      "10285: [discriminator loss: 0.590575, acc: 0.640625] [adversarial loss: 0.857217, acc: 0.437500]\n",
      "10286: [discriminator loss: 0.535109, acc: 0.781250] [adversarial loss: 1.414174, acc: 0.140625]\n",
      "10287: [discriminator loss: 0.598361, acc: 0.695312] [adversarial loss: 0.954211, acc: 0.375000]\n",
      "10288: [discriminator loss: 0.520361, acc: 0.703125] [adversarial loss: 1.440511, acc: 0.078125]\n",
      "10289: [discriminator loss: 0.525962, acc: 0.695312] [adversarial loss: 1.124981, acc: 0.296875]\n",
      "10290: [discriminator loss: 0.558799, acc: 0.703125] [adversarial loss: 1.139555, acc: 0.328125]\n",
      "10291: [discriminator loss: 0.509057, acc: 0.742188] [adversarial loss: 0.887896, acc: 0.437500]\n",
      "10292: [discriminator loss: 0.620022, acc: 0.687500] [adversarial loss: 1.428630, acc: 0.109375]\n",
      "10293: [discriminator loss: 0.592848, acc: 0.648438] [adversarial loss: 1.100180, acc: 0.218750]\n",
      "10294: [discriminator loss: 0.534638, acc: 0.718750] [adversarial loss: 1.028466, acc: 0.234375]\n",
      "10295: [discriminator loss: 0.468048, acc: 0.789062] [adversarial loss: 1.151134, acc: 0.328125]\n",
      "10296: [discriminator loss: 0.568552, acc: 0.710938] [adversarial loss: 1.094535, acc: 0.296875]\n",
      "10297: [discriminator loss: 0.580819, acc: 0.695312] [adversarial loss: 1.311427, acc: 0.156250]\n",
      "10298: [discriminator loss: 0.562059, acc: 0.726562] [adversarial loss: 0.929129, acc: 0.375000]\n",
      "10299: [discriminator loss: 0.540302, acc: 0.734375] [adversarial loss: 1.348909, acc: 0.171875]\n",
      "10300: [discriminator loss: 0.602736, acc: 0.671875] [adversarial loss: 1.104291, acc: 0.296875]\n",
      "10301: [discriminator loss: 0.461768, acc: 0.742188] [adversarial loss: 1.417721, acc: 0.171875]\n",
      "10302: [discriminator loss: 0.590679, acc: 0.671875] [adversarial loss: 0.961598, acc: 0.359375]\n",
      "10303: [discriminator loss: 0.562547, acc: 0.664062] [adversarial loss: 1.374181, acc: 0.218750]\n",
      "10304: [discriminator loss: 0.614128, acc: 0.664062] [adversarial loss: 1.135421, acc: 0.296875]\n",
      "10305: [discriminator loss: 0.507475, acc: 0.773438] [adversarial loss: 1.143335, acc: 0.218750]\n",
      "10306: [discriminator loss: 0.578041, acc: 0.695312] [adversarial loss: 1.373491, acc: 0.156250]\n",
      "10307: [discriminator loss: 0.490895, acc: 0.750000] [adversarial loss: 0.993914, acc: 0.328125]\n",
      "10308: [discriminator loss: 0.610538, acc: 0.679688] [adversarial loss: 1.138464, acc: 0.187500]\n",
      "10309: [discriminator loss: 0.549092, acc: 0.679688] [adversarial loss: 1.093494, acc: 0.281250]\n",
      "10310: [discriminator loss: 0.528592, acc: 0.781250] [adversarial loss: 1.282162, acc: 0.187500]\n",
      "10311: [discriminator loss: 0.546193, acc: 0.718750] [adversarial loss: 0.973079, acc: 0.375000]\n",
      "10312: [discriminator loss: 0.582661, acc: 0.671875] [adversarial loss: 1.391061, acc: 0.203125]\n",
      "10313: [discriminator loss: 0.541705, acc: 0.703125] [adversarial loss: 1.029506, acc: 0.359375]\n",
      "10314: [discriminator loss: 0.474414, acc: 0.773438] [adversarial loss: 1.436575, acc: 0.093750]\n",
      "10315: [discriminator loss: 0.610080, acc: 0.671875] [adversarial loss: 0.789447, acc: 0.453125]\n",
      "10316: [discriminator loss: 0.517394, acc: 0.726562] [adversarial loss: 1.383815, acc: 0.187500]\n",
      "10317: [discriminator loss: 0.542001, acc: 0.710938] [adversarial loss: 1.080977, acc: 0.296875]\n",
      "10318: [discriminator loss: 0.581768, acc: 0.671875] [adversarial loss: 1.190658, acc: 0.218750]\n",
      "10319: [discriminator loss: 0.522272, acc: 0.710938] [adversarial loss: 1.007273, acc: 0.312500]\n",
      "10320: [discriminator loss: 0.587047, acc: 0.664062] [adversarial loss: 1.411589, acc: 0.140625]\n",
      "10321: [discriminator loss: 0.593050, acc: 0.671875] [adversarial loss: 1.014579, acc: 0.328125]\n",
      "10322: [discriminator loss: 0.533163, acc: 0.695312] [adversarial loss: 1.323252, acc: 0.250000]\n",
      "10323: [discriminator loss: 0.530555, acc: 0.710938] [adversarial loss: 0.845039, acc: 0.468750]\n",
      "10324: [discriminator loss: 0.497557, acc: 0.789062] [adversarial loss: 1.483386, acc: 0.078125]\n",
      "10325: [discriminator loss: 0.569259, acc: 0.703125] [adversarial loss: 0.843865, acc: 0.484375]\n",
      "10326: [discriminator loss: 0.518731, acc: 0.703125] [adversarial loss: 1.402142, acc: 0.140625]\n",
      "10327: [discriminator loss: 0.585056, acc: 0.640625] [adversarial loss: 1.034281, acc: 0.265625]\n",
      "10328: [discriminator loss: 0.556850, acc: 0.742188] [adversarial loss: 1.123306, acc: 0.250000]\n",
      "10329: [discriminator loss: 0.485732, acc: 0.765625] [adversarial loss: 1.258313, acc: 0.218750]\n",
      "10330: [discriminator loss: 0.591438, acc: 0.656250] [adversarial loss: 1.090641, acc: 0.296875]\n",
      "10331: [discriminator loss: 0.488555, acc: 0.757812] [adversarial loss: 1.211831, acc: 0.218750]\n",
      "10332: [discriminator loss: 0.502424, acc: 0.750000] [adversarial loss: 0.903725, acc: 0.390625]\n",
      "10333: [discriminator loss: 0.515073, acc: 0.765625] [adversarial loss: 1.219601, acc: 0.187500]\n",
      "10334: [discriminator loss: 0.488910, acc: 0.781250] [adversarial loss: 1.131938, acc: 0.281250]\n",
      "10335: [discriminator loss: 0.483903, acc: 0.750000] [adversarial loss: 1.049765, acc: 0.328125]\n",
      "10336: [discriminator loss: 0.606410, acc: 0.687500] [adversarial loss: 1.279474, acc: 0.203125]\n",
      "10337: [discriminator loss: 0.569693, acc: 0.703125] [adversarial loss: 1.289703, acc: 0.156250]\n",
      "10338: [discriminator loss: 0.532987, acc: 0.718750] [adversarial loss: 1.205406, acc: 0.218750]\n",
      "10339: [discriminator loss: 0.513245, acc: 0.679688] [adversarial loss: 1.082230, acc: 0.328125]\n",
      "10340: [discriminator loss: 0.634117, acc: 0.648438] [adversarial loss: 1.353008, acc: 0.218750]\n",
      "10341: [discriminator loss: 0.530622, acc: 0.695312] [adversarial loss: 1.165701, acc: 0.125000]\n",
      "10342: [discriminator loss: 0.575757, acc: 0.703125] [adversarial loss: 1.421867, acc: 0.203125]\n",
      "10343: [discriminator loss: 0.584233, acc: 0.671875] [adversarial loss: 0.918828, acc: 0.343750]\n",
      "10344: [discriminator loss: 0.536191, acc: 0.734375] [adversarial loss: 1.495444, acc: 0.125000]\n",
      "10345: [discriminator loss: 0.547910, acc: 0.710938] [adversarial loss: 0.762802, acc: 0.484375]\n",
      "10346: [discriminator loss: 0.555527, acc: 0.781250] [adversarial loss: 1.347954, acc: 0.140625]\n",
      "10347: [discriminator loss: 0.492801, acc: 0.773438] [adversarial loss: 0.937217, acc: 0.421875]\n",
      "10348: [discriminator loss: 0.555413, acc: 0.710938] [adversarial loss: 1.389544, acc: 0.140625]\n",
      "10349: [discriminator loss: 0.523218, acc: 0.726562] [adversarial loss: 1.197555, acc: 0.250000]\n",
      "10350: [discriminator loss: 0.499332, acc: 0.765625] [adversarial loss: 1.349930, acc: 0.171875]\n",
      "10351: [discriminator loss: 0.588576, acc: 0.648438] [adversarial loss: 0.806009, acc: 0.484375]\n",
      "10352: [discriminator loss: 0.632987, acc: 0.625000] [adversarial loss: 1.344665, acc: 0.203125]\n",
      "10353: [discriminator loss: 0.549508, acc: 0.671875] [adversarial loss: 0.937941, acc: 0.421875]\n",
      "10354: [discriminator loss: 0.569606, acc: 0.703125] [adversarial loss: 1.211612, acc: 0.250000]\n",
      "10355: [discriminator loss: 0.588479, acc: 0.695312] [adversarial loss: 1.160734, acc: 0.265625]\n",
      "10356: [discriminator loss: 0.564399, acc: 0.695312] [adversarial loss: 1.067343, acc: 0.281250]\n",
      "10357: [discriminator loss: 0.594288, acc: 0.640625] [adversarial loss: 1.286403, acc: 0.218750]\n",
      "10358: [discriminator loss: 0.543964, acc: 0.710938] [adversarial loss: 1.019935, acc: 0.296875]\n",
      "10359: [discriminator loss: 0.535973, acc: 0.703125] [adversarial loss: 1.335330, acc: 0.218750]\n",
      "10360: [discriminator loss: 0.554635, acc: 0.703125] [adversarial loss: 1.098552, acc: 0.312500]\n",
      "10361: [discriminator loss: 0.534445, acc: 0.757812] [adversarial loss: 1.145434, acc: 0.328125]\n",
      "10362: [discriminator loss: 0.498219, acc: 0.765625] [adversarial loss: 1.164153, acc: 0.296875]\n",
      "10363: [discriminator loss: 0.498207, acc: 0.710938] [adversarial loss: 1.242249, acc: 0.265625]\n",
      "10364: [discriminator loss: 0.577358, acc: 0.703125] [adversarial loss: 1.092752, acc: 0.281250]\n",
      "10365: [discriminator loss: 0.558836, acc: 0.718750] [adversarial loss: 0.735994, acc: 0.468750]\n",
      "10366: [discriminator loss: 0.553444, acc: 0.679688] [adversarial loss: 1.837749, acc: 0.046875]\n",
      "10367: [discriminator loss: 0.601542, acc: 0.656250] [adversarial loss: 0.910494, acc: 0.437500]\n",
      "10368: [discriminator loss: 0.517223, acc: 0.742188] [adversarial loss: 1.277037, acc: 0.140625]\n",
      "10369: [discriminator loss: 0.614518, acc: 0.671875] [adversarial loss: 0.944003, acc: 0.421875]\n",
      "10370: [discriminator loss: 0.534550, acc: 0.718750] [adversarial loss: 1.231905, acc: 0.156250]\n",
      "10371: [discriminator loss: 0.519713, acc: 0.726562] [adversarial loss: 0.973192, acc: 0.390625]\n",
      "10372: [discriminator loss: 0.443381, acc: 0.812500] [adversarial loss: 1.516548, acc: 0.093750]\n",
      "10373: [discriminator loss: 0.492177, acc: 0.773438] [adversarial loss: 0.997921, acc: 0.281250]\n",
      "10374: [discriminator loss: 0.549330, acc: 0.679688] [adversarial loss: 1.298725, acc: 0.125000]\n",
      "10375: [discriminator loss: 0.599359, acc: 0.671875] [adversarial loss: 0.998191, acc: 0.281250]\n",
      "10376: [discriminator loss: 0.529232, acc: 0.757812] [adversarial loss: 1.180769, acc: 0.187500]\n",
      "10377: [discriminator loss: 0.461370, acc: 0.789062] [adversarial loss: 1.018306, acc: 0.375000]\n",
      "10378: [discriminator loss: 0.533631, acc: 0.726562] [adversarial loss: 1.200760, acc: 0.265625]\n",
      "10379: [discriminator loss: 0.582146, acc: 0.734375] [adversarial loss: 1.206294, acc: 0.234375]\n",
      "10380: [discriminator loss: 0.607299, acc: 0.679688] [adversarial loss: 0.932096, acc: 0.453125]\n",
      "10381: [discriminator loss: 0.568934, acc: 0.710938] [adversarial loss: 1.424019, acc: 0.187500]\n",
      "10382: [discriminator loss: 0.585410, acc: 0.664062] [adversarial loss: 1.016010, acc: 0.328125]\n",
      "10383: [discriminator loss: 0.588849, acc: 0.687500] [adversarial loss: 1.356458, acc: 0.140625]\n",
      "10384: [discriminator loss: 0.574783, acc: 0.671875] [adversarial loss: 0.910722, acc: 0.375000]\n",
      "10385: [discriminator loss: 0.620048, acc: 0.718750] [adversarial loss: 1.568993, acc: 0.078125]\n",
      "10386: [discriminator loss: 0.549754, acc: 0.703125] [adversarial loss: 0.994525, acc: 0.312500]\n",
      "10387: [discriminator loss: 0.548649, acc: 0.710938] [adversarial loss: 1.221422, acc: 0.203125]\n",
      "10388: [discriminator loss: 0.591674, acc: 0.601562] [adversarial loss: 1.246868, acc: 0.203125]\n",
      "10389: [discriminator loss: 0.525313, acc: 0.742188] [adversarial loss: 1.047391, acc: 0.234375]\n",
      "10390: [discriminator loss: 0.617305, acc: 0.648438] [adversarial loss: 1.152567, acc: 0.281250]\n",
      "10391: [discriminator loss: 0.518380, acc: 0.781250] [adversarial loss: 0.993102, acc: 0.312500]\n",
      "10392: [discriminator loss: 0.623147, acc: 0.687500] [adversarial loss: 1.344336, acc: 0.125000]\n",
      "10393: [discriminator loss: 0.556331, acc: 0.726562] [adversarial loss: 1.325274, acc: 0.187500]\n",
      "10394: [discriminator loss: 0.489195, acc: 0.773438] [adversarial loss: 1.031235, acc: 0.312500]\n",
      "10395: [discriminator loss: 0.562964, acc: 0.718750] [adversarial loss: 1.273163, acc: 0.156250]\n",
      "10396: [discriminator loss: 0.555351, acc: 0.703125] [adversarial loss: 0.869069, acc: 0.437500]\n",
      "10397: [discriminator loss: 0.578309, acc: 0.687500] [adversarial loss: 1.444800, acc: 0.078125]\n",
      "10398: [discriminator loss: 0.549393, acc: 0.742188] [adversarial loss: 0.761866, acc: 0.531250]\n",
      "10399: [discriminator loss: 0.604612, acc: 0.648438] [adversarial loss: 1.591969, acc: 0.046875]\n",
      "10400: [discriminator loss: 0.640053, acc: 0.632812] [adversarial loss: 1.005603, acc: 0.375000]\n",
      "10401: [discriminator loss: 0.558875, acc: 0.742188] [adversarial loss: 1.021288, acc: 0.312500]\n",
      "10402: [discriminator loss: 0.508078, acc: 0.757812] [adversarial loss: 1.224203, acc: 0.171875]\n",
      "10403: [discriminator loss: 0.529165, acc: 0.703125] [adversarial loss: 1.210131, acc: 0.203125]\n",
      "10404: [discriminator loss: 0.532526, acc: 0.757812] [adversarial loss: 1.207479, acc: 0.218750]\n",
      "10405: [discriminator loss: 0.520196, acc: 0.757812] [adversarial loss: 1.030385, acc: 0.328125]\n",
      "10406: [discriminator loss: 0.572772, acc: 0.718750] [adversarial loss: 1.145420, acc: 0.203125]\n",
      "10407: [discriminator loss: 0.480566, acc: 0.820312] [adversarial loss: 1.057064, acc: 0.250000]\n",
      "10408: [discriminator loss: 0.564190, acc: 0.726562] [adversarial loss: 1.243073, acc: 0.187500]\n",
      "10409: [discriminator loss: 0.600784, acc: 0.664062] [adversarial loss: 1.078048, acc: 0.296875]\n",
      "10410: [discriminator loss: 0.492311, acc: 0.820312] [adversarial loss: 1.305042, acc: 0.171875]\n",
      "10411: [discriminator loss: 0.507048, acc: 0.773438] [adversarial loss: 0.972430, acc: 0.375000]\n",
      "10412: [discriminator loss: 0.560216, acc: 0.687500] [adversarial loss: 1.168876, acc: 0.250000]\n",
      "10413: [discriminator loss: 0.536370, acc: 0.695312] [adversarial loss: 1.071814, acc: 0.359375]\n",
      "10414: [discriminator loss: 0.511129, acc: 0.695312] [adversarial loss: 1.247575, acc: 0.171875]\n",
      "10415: [discriminator loss: 0.549757, acc: 0.726562] [adversarial loss: 1.261555, acc: 0.125000]\n",
      "10416: [discriminator loss: 0.516786, acc: 0.742188] [adversarial loss: 0.900164, acc: 0.421875]\n",
      "10417: [discriminator loss: 0.519337, acc: 0.710938] [adversarial loss: 1.230572, acc: 0.250000]\n",
      "10418: [discriminator loss: 0.512951, acc: 0.742188] [adversarial loss: 1.177852, acc: 0.250000]\n",
      "10419: [discriminator loss: 0.478363, acc: 0.781250] [adversarial loss: 1.252224, acc: 0.218750]\n",
      "10420: [discriminator loss: 0.497739, acc: 0.773438] [adversarial loss: 0.933948, acc: 0.453125]\n",
      "10421: [discriminator loss: 0.544915, acc: 0.687500] [adversarial loss: 1.442245, acc: 0.109375]\n",
      "10422: [discriminator loss: 0.625863, acc: 0.664062] [adversarial loss: 0.691567, acc: 0.609375]\n",
      "10423: [discriminator loss: 0.559759, acc: 0.695312] [adversarial loss: 1.800704, acc: 0.046875]\n",
      "10424: [discriminator loss: 0.576171, acc: 0.710938] [adversarial loss: 0.884453, acc: 0.421875]\n",
      "10425: [discriminator loss: 0.586489, acc: 0.632812] [adversarial loss: 1.412978, acc: 0.156250]\n",
      "10426: [discriminator loss: 0.554561, acc: 0.664062] [adversarial loss: 0.868679, acc: 0.406250]\n",
      "10427: [discriminator loss: 0.519924, acc: 0.726562] [adversarial loss: 1.381623, acc: 0.171875]\n",
      "10428: [discriminator loss: 0.522510, acc: 0.750000] [adversarial loss: 1.078362, acc: 0.265625]\n",
      "10429: [discriminator loss: 0.535802, acc: 0.734375] [adversarial loss: 0.947256, acc: 0.390625]\n",
      "10430: [discriminator loss: 0.537964, acc: 0.726562] [adversarial loss: 1.160518, acc: 0.265625]\n",
      "10431: [discriminator loss: 0.559909, acc: 0.718750] [adversarial loss: 1.162564, acc: 0.234375]\n",
      "10432: [discriminator loss: 0.485760, acc: 0.796875] [adversarial loss: 1.223804, acc: 0.250000]\n",
      "10433: [discriminator loss: 0.589478, acc: 0.671875] [adversarial loss: 1.051556, acc: 0.359375]\n",
      "10434: [discriminator loss: 0.585276, acc: 0.664062] [adversarial loss: 1.203880, acc: 0.265625]\n",
      "10435: [discriminator loss: 0.529601, acc: 0.703125] [adversarial loss: 1.187675, acc: 0.312500]\n",
      "10436: [discriminator loss: 0.516257, acc: 0.718750] [adversarial loss: 1.185422, acc: 0.187500]\n",
      "10437: [discriminator loss: 0.570904, acc: 0.726562] [adversarial loss: 1.230603, acc: 0.187500]\n",
      "10438: [discriminator loss: 0.605974, acc: 0.656250] [adversarial loss: 1.199251, acc: 0.203125]\n",
      "10439: [discriminator loss: 0.522783, acc: 0.710938] [adversarial loss: 1.012376, acc: 0.375000]\n",
      "10440: [discriminator loss: 0.594215, acc: 0.625000] [adversarial loss: 1.524339, acc: 0.109375]\n",
      "10441: [discriminator loss: 0.560493, acc: 0.718750] [adversarial loss: 1.003200, acc: 0.343750]\n",
      "10442: [discriminator loss: 0.510752, acc: 0.765625] [adversarial loss: 1.365165, acc: 0.125000]\n",
      "10443: [discriminator loss: 0.596508, acc: 0.664062] [adversarial loss: 0.840635, acc: 0.359375]\n",
      "10444: [discriminator loss: 0.521365, acc: 0.726562] [adversarial loss: 1.312244, acc: 0.218750]\n",
      "10445: [discriminator loss: 0.531475, acc: 0.710938] [adversarial loss: 1.166208, acc: 0.156250]\n",
      "10446: [discriminator loss: 0.579024, acc: 0.703125] [adversarial loss: 1.087845, acc: 0.281250]\n",
      "10447: [discriminator loss: 0.514929, acc: 0.750000] [adversarial loss: 1.377444, acc: 0.171875]\n",
      "10448: [discriminator loss: 0.521698, acc: 0.726562] [adversarial loss: 1.051140, acc: 0.312500]\n",
      "10449: [discriminator loss: 0.563762, acc: 0.703125] [adversarial loss: 1.194574, acc: 0.187500]\n",
      "10450: [discriminator loss: 0.502751, acc: 0.757812] [adversarial loss: 0.856233, acc: 0.468750]\n",
      "10451: [discriminator loss: 0.655387, acc: 0.656250] [adversarial loss: 1.426164, acc: 0.109375]\n",
      "10452: [discriminator loss: 0.561908, acc: 0.742188] [adversarial loss: 1.177437, acc: 0.234375]\n",
      "10453: [discriminator loss: 0.578282, acc: 0.648438] [adversarial loss: 1.268417, acc: 0.171875]\n",
      "10454: [discriminator loss: 0.525600, acc: 0.742188] [adversarial loss: 0.920335, acc: 0.406250]\n",
      "10455: [discriminator loss: 0.548688, acc: 0.734375] [adversarial loss: 1.464150, acc: 0.203125]\n",
      "10456: [discriminator loss: 0.563753, acc: 0.671875] [adversarial loss: 1.053046, acc: 0.281250]\n",
      "10457: [discriminator loss: 0.574008, acc: 0.695312] [adversarial loss: 1.387298, acc: 0.125000]\n",
      "10458: [discriminator loss: 0.539733, acc: 0.718750] [adversarial loss: 0.931009, acc: 0.468750]\n",
      "10459: [discriminator loss: 0.622281, acc: 0.640625] [adversarial loss: 1.598957, acc: 0.140625]\n",
      "10460: [discriminator loss: 0.558532, acc: 0.734375] [adversarial loss: 1.204516, acc: 0.250000]\n",
      "10461: [discriminator loss: 0.447460, acc: 0.843750] [adversarial loss: 1.391004, acc: 0.265625]\n",
      "10462: [discriminator loss: 0.529144, acc: 0.726562] [adversarial loss: 1.014892, acc: 0.343750]\n",
      "10463: [discriminator loss: 0.553509, acc: 0.695312] [adversarial loss: 0.982563, acc: 0.250000]\n",
      "10464: [discriminator loss: 0.529263, acc: 0.765625] [adversarial loss: 1.206451, acc: 0.265625]\n",
      "10465: [discriminator loss: 0.585745, acc: 0.679688] [adversarial loss: 0.895696, acc: 0.406250]\n",
      "10466: [discriminator loss: 0.547519, acc: 0.718750] [adversarial loss: 1.134540, acc: 0.265625]\n",
      "10467: [discriminator loss: 0.574164, acc: 0.710938] [adversarial loss: 0.926972, acc: 0.390625]\n",
      "10468: [discriminator loss: 0.539413, acc: 0.703125] [adversarial loss: 1.164201, acc: 0.203125]\n",
      "10469: [discriminator loss: 0.470933, acc: 0.796875] [adversarial loss: 0.911484, acc: 0.421875]\n",
      "10470: [discriminator loss: 0.589188, acc: 0.710938] [adversarial loss: 1.405232, acc: 0.140625]\n",
      "10471: [discriminator loss: 0.523158, acc: 0.757812] [adversarial loss: 0.759934, acc: 0.500000]\n",
      "10472: [discriminator loss: 0.609500, acc: 0.625000] [adversarial loss: 1.518168, acc: 0.125000]\n",
      "10473: [discriminator loss: 0.502617, acc: 0.742188] [adversarial loss: 1.281843, acc: 0.140625]\n",
      "10474: [discriminator loss: 0.564097, acc: 0.695312] [adversarial loss: 1.097298, acc: 0.250000]\n",
      "10475: [discriminator loss: 0.542267, acc: 0.710938] [adversarial loss: 1.317831, acc: 0.218750]\n",
      "10476: [discriminator loss: 0.600364, acc: 0.671875] [adversarial loss: 0.978007, acc: 0.359375]\n",
      "10477: [discriminator loss: 0.564685, acc: 0.718750] [adversarial loss: 1.452060, acc: 0.218750]\n",
      "10478: [discriminator loss: 0.541477, acc: 0.726562] [adversarial loss: 1.068527, acc: 0.343750]\n",
      "10479: [discriminator loss: 0.510981, acc: 0.742188] [adversarial loss: 1.469473, acc: 0.109375]\n",
      "10480: [discriminator loss: 0.597225, acc: 0.726562] [adversarial loss: 1.070728, acc: 0.218750]\n",
      "10481: [discriminator loss: 0.518930, acc: 0.734375] [adversarial loss: 1.149719, acc: 0.359375]\n",
      "10482: [discriminator loss: 0.542789, acc: 0.765625] [adversarial loss: 1.218419, acc: 0.250000]\n",
      "10483: [discriminator loss: 0.619484, acc: 0.632812] [adversarial loss: 1.156160, acc: 0.250000]\n",
      "10484: [discriminator loss: 0.578019, acc: 0.703125] [adversarial loss: 0.912610, acc: 0.343750]\n",
      "10485: [discriminator loss: 0.566393, acc: 0.703125] [adversarial loss: 1.413560, acc: 0.125000]\n",
      "10486: [discriminator loss: 0.526849, acc: 0.703125] [adversarial loss: 0.773248, acc: 0.515625]\n",
      "10487: [discriminator loss: 0.576726, acc: 0.710938] [adversarial loss: 1.436773, acc: 0.171875]\n",
      "10488: [discriminator loss: 0.586762, acc: 0.718750] [adversarial loss: 0.891195, acc: 0.390625]\n",
      "10489: [discriminator loss: 0.611158, acc: 0.687500] [adversarial loss: 1.469757, acc: 0.140625]\n",
      "10490: [discriminator loss: 0.595679, acc: 0.664062] [adversarial loss: 1.062630, acc: 0.296875]\n",
      "10491: [discriminator loss: 0.544256, acc: 0.726562] [adversarial loss: 1.038846, acc: 0.234375]\n",
      "10492: [discriminator loss: 0.580763, acc: 0.671875] [adversarial loss: 1.088059, acc: 0.281250]\n",
      "10493: [discriminator loss: 0.491569, acc: 0.742188] [adversarial loss: 1.147776, acc: 0.218750]\n",
      "10494: [discriminator loss: 0.506154, acc: 0.710938] [adversarial loss: 0.998204, acc: 0.421875]\n",
      "10495: [discriminator loss: 0.593607, acc: 0.679688] [adversarial loss: 1.132238, acc: 0.250000]\n",
      "10496: [discriminator loss: 0.493268, acc: 0.726562] [adversarial loss: 1.055724, acc: 0.312500]\n",
      "10497: [discriminator loss: 0.527470, acc: 0.734375] [adversarial loss: 1.152348, acc: 0.250000]\n",
      "10498: [discriminator loss: 0.516613, acc: 0.718750] [adversarial loss: 1.072173, acc: 0.359375]\n",
      "10499: [discriminator loss: 0.586286, acc: 0.695312] [adversarial loss: 1.377828, acc: 0.140625]\n",
      "10500: [discriminator loss: 0.557599, acc: 0.687500] [adversarial loss: 0.997123, acc: 0.390625]\n",
      "10501: [discriminator loss: 0.580903, acc: 0.734375] [adversarial loss: 1.227399, acc: 0.156250]\n",
      "10502: [discriminator loss: 0.512843, acc: 0.718750] [adversarial loss: 0.907416, acc: 0.343750]\n",
      "10503: [discriminator loss: 0.530571, acc: 0.757812] [adversarial loss: 1.224979, acc: 0.281250]\n",
      "10504: [discriminator loss: 0.470540, acc: 0.742188] [adversarial loss: 1.031649, acc: 0.312500]\n",
      "10505: [discriminator loss: 0.550152, acc: 0.703125] [adversarial loss: 0.954937, acc: 0.390625]\n",
      "10506: [discriminator loss: 0.531057, acc: 0.695312] [adversarial loss: 1.345536, acc: 0.156250]\n",
      "10507: [discriminator loss: 0.507747, acc: 0.750000] [adversarial loss: 0.809771, acc: 0.484375]\n",
      "10508: [discriminator loss: 0.597121, acc: 0.648438] [adversarial loss: 1.325044, acc: 0.171875]\n",
      "10509: [discriminator loss: 0.524297, acc: 0.718750] [adversarial loss: 0.811775, acc: 0.500000]\n",
      "10510: [discriminator loss: 0.563366, acc: 0.664062] [adversarial loss: 1.533348, acc: 0.156250]\n",
      "10511: [discriminator loss: 0.595539, acc: 0.671875] [adversarial loss: 1.008083, acc: 0.328125]\n",
      "10512: [discriminator loss: 0.605949, acc: 0.671875] [adversarial loss: 0.971304, acc: 0.359375]\n",
      "10513: [discriminator loss: 0.551677, acc: 0.734375] [adversarial loss: 1.207141, acc: 0.218750]\n",
      "10514: [discriminator loss: 0.583912, acc: 0.656250] [adversarial loss: 1.239047, acc: 0.218750]\n",
      "10515: [discriminator loss: 0.520674, acc: 0.757812] [adversarial loss: 1.036540, acc: 0.312500]\n",
      "10516: [discriminator loss: 0.575231, acc: 0.664062] [adversarial loss: 1.501797, acc: 0.125000]\n",
      "10517: [discriminator loss: 0.579584, acc: 0.679688] [adversarial loss: 0.801069, acc: 0.500000]\n",
      "10518: [discriminator loss: 0.484710, acc: 0.789062] [adversarial loss: 1.096400, acc: 0.296875]\n",
      "10519: [discriminator loss: 0.540293, acc: 0.726562] [adversarial loss: 1.126050, acc: 0.296875]\n",
      "10520: [discriminator loss: 0.574886, acc: 0.695312] [adversarial loss: 1.346286, acc: 0.218750]\n",
      "10521: [discriminator loss: 0.531853, acc: 0.687500] [adversarial loss: 0.891161, acc: 0.406250]\n",
      "10522: [discriminator loss: 0.554488, acc: 0.703125] [adversarial loss: 1.667246, acc: 0.140625]\n",
      "10523: [discriminator loss: 0.587358, acc: 0.710938] [adversarial loss: 1.238812, acc: 0.171875]\n",
      "10524: [discriminator loss: 0.582670, acc: 0.679688] [adversarial loss: 0.987727, acc: 0.406250]\n",
      "10525: [discriminator loss: 0.559120, acc: 0.695312] [adversarial loss: 1.252794, acc: 0.156250]\n",
      "10526: [discriminator loss: 0.526384, acc: 0.781250] [adversarial loss: 1.087993, acc: 0.250000]\n",
      "10527: [discriminator loss: 0.593472, acc: 0.695312] [adversarial loss: 1.136194, acc: 0.250000]\n",
      "10528: [discriminator loss: 0.574730, acc: 0.734375] [adversarial loss: 1.097757, acc: 0.343750]\n",
      "10529: [discriminator loss: 0.550320, acc: 0.687500] [adversarial loss: 1.107611, acc: 0.296875]\n",
      "10530: [discriminator loss: 0.512083, acc: 0.773438] [adversarial loss: 1.210477, acc: 0.234375]\n",
      "10531: [discriminator loss: 0.533894, acc: 0.703125] [adversarial loss: 1.223078, acc: 0.343750]\n",
      "10532: [discriminator loss: 0.594842, acc: 0.703125] [adversarial loss: 1.184489, acc: 0.250000]\n",
      "10533: [discriminator loss: 0.497185, acc: 0.757812] [adversarial loss: 1.162047, acc: 0.250000]\n",
      "10534: [discriminator loss: 0.545847, acc: 0.687500] [adversarial loss: 1.041872, acc: 0.359375]\n",
      "10535: [discriminator loss: 0.560251, acc: 0.664062] [adversarial loss: 1.292998, acc: 0.171875]\n",
      "10536: [discriminator loss: 0.542137, acc: 0.734375] [adversarial loss: 0.808697, acc: 0.468750]\n",
      "10537: [discriminator loss: 0.521313, acc: 0.710938] [adversarial loss: 1.413611, acc: 0.109375]\n",
      "10538: [discriminator loss: 0.596335, acc: 0.671875] [adversarial loss: 0.713905, acc: 0.546875]\n",
      "10539: [discriminator loss: 0.563679, acc: 0.687500] [adversarial loss: 1.731217, acc: 0.078125]\n",
      "10540: [discriminator loss: 0.605343, acc: 0.671875] [adversarial loss: 0.894093, acc: 0.406250]\n",
      "10541: [discriminator loss: 0.572364, acc: 0.703125] [adversarial loss: 1.448968, acc: 0.125000]\n",
      "10542: [discriminator loss: 0.590740, acc: 0.648438] [adversarial loss: 0.929431, acc: 0.437500]\n",
      "10543: [discriminator loss: 0.546405, acc: 0.718750] [adversarial loss: 1.373203, acc: 0.187500]\n",
      "10544: [discriminator loss: 0.562766, acc: 0.718750] [adversarial loss: 0.904479, acc: 0.421875]\n",
      "10545: [discriminator loss: 0.542091, acc: 0.742188] [adversarial loss: 1.146378, acc: 0.250000]\n",
      "10546: [discriminator loss: 0.500002, acc: 0.757812] [adversarial loss: 1.092117, acc: 0.328125]\n",
      "10547: [discriminator loss: 0.455087, acc: 0.773438] [adversarial loss: 1.271011, acc: 0.156250]\n",
      "10548: [discriminator loss: 0.527413, acc: 0.765625] [adversarial loss: 1.135352, acc: 0.281250]\n",
      "10549: [discriminator loss: 0.531495, acc: 0.726562] [adversarial loss: 0.986676, acc: 0.421875]\n",
      "10550: [discriminator loss: 0.485469, acc: 0.781250] [adversarial loss: 1.210180, acc: 0.218750]\n",
      "10551: [discriminator loss: 0.591322, acc: 0.640625] [adversarial loss: 1.000522, acc: 0.343750]\n",
      "10552: [discriminator loss: 0.532277, acc: 0.726562] [adversarial loss: 1.353386, acc: 0.140625]\n",
      "10553: [discriminator loss: 0.548192, acc: 0.710938] [adversarial loss: 1.090981, acc: 0.312500]\n",
      "10554: [discriminator loss: 0.549320, acc: 0.757812] [adversarial loss: 1.557297, acc: 0.078125]\n",
      "10555: [discriminator loss: 0.533766, acc: 0.710938] [adversarial loss: 0.934365, acc: 0.375000]\n",
      "10556: [discriminator loss: 0.530428, acc: 0.703125] [adversarial loss: 1.449792, acc: 0.078125]\n",
      "10557: [discriminator loss: 0.575741, acc: 0.679688] [adversarial loss: 0.959889, acc: 0.343750]\n",
      "10558: [discriminator loss: 0.521582, acc: 0.742188] [adversarial loss: 1.367175, acc: 0.140625]\n",
      "10559: [discriminator loss: 0.532123, acc: 0.765625] [adversarial loss: 0.800777, acc: 0.468750]\n",
      "10560: [discriminator loss: 0.613548, acc: 0.664062] [adversarial loss: 1.176731, acc: 0.203125]\n",
      "10561: [discriminator loss: 0.548666, acc: 0.671875] [adversarial loss: 1.052461, acc: 0.343750]\n",
      "10562: [discriminator loss: 0.575131, acc: 0.703125] [adversarial loss: 1.339640, acc: 0.171875]\n",
      "10563: [discriminator loss: 0.608723, acc: 0.664062] [adversarial loss: 0.899332, acc: 0.406250]\n",
      "10564: [discriminator loss: 0.552725, acc: 0.710938] [adversarial loss: 1.179907, acc: 0.250000]\n",
      "10565: [discriminator loss: 0.543295, acc: 0.726562] [adversarial loss: 1.201877, acc: 0.218750]\n",
      "10566: [discriminator loss: 0.562382, acc: 0.703125] [adversarial loss: 1.206991, acc: 0.171875]\n",
      "10567: [discriminator loss: 0.554192, acc: 0.679688] [adversarial loss: 1.038036, acc: 0.328125]\n",
      "10568: [discriminator loss: 0.551456, acc: 0.726562] [adversarial loss: 1.365820, acc: 0.093750]\n",
      "10569: [discriminator loss: 0.602080, acc: 0.695312] [adversarial loss: 0.883257, acc: 0.437500]\n",
      "10570: [discriminator loss: 0.536785, acc: 0.734375] [adversarial loss: 1.606179, acc: 0.109375]\n",
      "10571: [discriminator loss: 0.510796, acc: 0.773438] [adversarial loss: 1.085955, acc: 0.312500]\n",
      "10572: [discriminator loss: 0.505823, acc: 0.734375] [adversarial loss: 1.234179, acc: 0.218750]\n",
      "10573: [discriminator loss: 0.494293, acc: 0.718750] [adversarial loss: 0.970804, acc: 0.421875]\n",
      "10574: [discriminator loss: 0.616324, acc: 0.695312] [adversarial loss: 1.322475, acc: 0.203125]\n",
      "10575: [discriminator loss: 0.487348, acc: 0.773438] [adversarial loss: 1.075355, acc: 0.375000]\n",
      "10576: [discriminator loss: 0.578858, acc: 0.664062] [adversarial loss: 1.341208, acc: 0.203125]\n",
      "10577: [discriminator loss: 0.525996, acc: 0.726562] [adversarial loss: 1.084370, acc: 0.359375]\n",
      "10578: [discriminator loss: 0.573029, acc: 0.679688] [adversarial loss: 1.193993, acc: 0.281250]\n",
      "10579: [discriminator loss: 0.584151, acc: 0.664062] [adversarial loss: 1.253415, acc: 0.156250]\n",
      "10580: [discriminator loss: 0.600315, acc: 0.648438] [adversarial loss: 1.161210, acc: 0.312500]\n",
      "10581: [discriminator loss: 0.600040, acc: 0.632812] [adversarial loss: 1.077847, acc: 0.265625]\n",
      "10582: [discriminator loss: 0.529810, acc: 0.703125] [adversarial loss: 1.024603, acc: 0.359375]\n",
      "10583: [discriminator loss: 0.584485, acc: 0.664062] [adversarial loss: 1.236284, acc: 0.171875]\n",
      "10584: [discriminator loss: 0.511842, acc: 0.750000] [adversarial loss: 1.044098, acc: 0.343750]\n",
      "10585: [discriminator loss: 0.462627, acc: 0.796875] [adversarial loss: 1.280764, acc: 0.187500]\n",
      "10586: [discriminator loss: 0.578918, acc: 0.710938] [adversarial loss: 1.008000, acc: 0.328125]\n",
      "10587: [discriminator loss: 0.629668, acc: 0.671875] [adversarial loss: 1.384586, acc: 0.171875]\n",
      "10588: [discriminator loss: 0.531241, acc: 0.710938] [adversarial loss: 1.079741, acc: 0.203125]\n",
      "10589: [discriminator loss: 0.543834, acc: 0.750000] [adversarial loss: 1.281260, acc: 0.171875]\n",
      "10590: [discriminator loss: 0.608157, acc: 0.671875] [adversarial loss: 0.830173, acc: 0.484375]\n",
      "10591: [discriminator loss: 0.636112, acc: 0.718750] [adversarial loss: 1.440436, acc: 0.109375]\n",
      "10592: [discriminator loss: 0.570811, acc: 0.695312] [adversarial loss: 0.854837, acc: 0.468750]\n",
      "10593: [discriminator loss: 0.613232, acc: 0.734375] [adversarial loss: 1.531839, acc: 0.093750]\n",
      "10594: [discriminator loss: 0.625486, acc: 0.679688] [adversarial loss: 0.821086, acc: 0.453125]\n",
      "10595: [discriminator loss: 0.510399, acc: 0.742188] [adversarial loss: 1.305874, acc: 0.156250]\n",
      "10596: [discriminator loss: 0.557725, acc: 0.679688] [adversarial loss: 1.159767, acc: 0.281250]\n",
      "10597: [discriminator loss: 0.513778, acc: 0.750000] [adversarial loss: 1.358343, acc: 0.218750]\n",
      "10598: [discriminator loss: 0.562721, acc: 0.687500] [adversarial loss: 1.061513, acc: 0.312500]\n",
      "10599: [discriminator loss: 0.550464, acc: 0.695312] [adversarial loss: 1.371465, acc: 0.140625]\n",
      "10600: [discriminator loss: 0.509292, acc: 0.750000] [adversarial loss: 0.879085, acc: 0.375000]\n",
      "10601: [discriminator loss: 0.607919, acc: 0.656250] [adversarial loss: 1.225968, acc: 0.171875]\n",
      "10602: [discriminator loss: 0.606369, acc: 0.703125] [adversarial loss: 0.970082, acc: 0.312500]\n",
      "10603: [discriminator loss: 0.567126, acc: 0.710938] [adversarial loss: 1.429114, acc: 0.187500]\n",
      "10604: [discriminator loss: 0.533349, acc: 0.718750] [adversarial loss: 1.194000, acc: 0.203125]\n",
      "10605: [discriminator loss: 0.523750, acc: 0.726562] [adversarial loss: 1.204051, acc: 0.281250]\n",
      "10606: [discriminator loss: 0.489367, acc: 0.765625] [adversarial loss: 0.845460, acc: 0.437500]\n",
      "10607: [discriminator loss: 0.555184, acc: 0.734375] [adversarial loss: 1.418583, acc: 0.109375]\n",
      "10608: [discriminator loss: 0.598669, acc: 0.632812] [adversarial loss: 1.085377, acc: 0.296875]\n",
      "10609: [discriminator loss: 0.544423, acc: 0.710938] [adversarial loss: 1.111688, acc: 0.312500]\n",
      "10610: [discriminator loss: 0.531517, acc: 0.726562] [adversarial loss: 1.094115, acc: 0.296875]\n",
      "10611: [discriminator loss: 0.568610, acc: 0.695312] [adversarial loss: 1.528294, acc: 0.156250]\n",
      "10612: [discriminator loss: 0.579606, acc: 0.703125] [adversarial loss: 0.924978, acc: 0.390625]\n",
      "10613: [discriminator loss: 0.632925, acc: 0.640625] [adversarial loss: 1.216627, acc: 0.156250]\n",
      "10614: [discriminator loss: 0.542656, acc: 0.687500] [adversarial loss: 1.167789, acc: 0.250000]\n",
      "10615: [discriminator loss: 0.493820, acc: 0.742188] [adversarial loss: 1.171731, acc: 0.281250]\n",
      "10616: [discriminator loss: 0.537024, acc: 0.703125] [adversarial loss: 1.392368, acc: 0.171875]\n",
      "10617: [discriminator loss: 0.610611, acc: 0.664062] [adversarial loss: 0.889707, acc: 0.437500]\n",
      "10618: [discriminator loss: 0.598680, acc: 0.664062] [adversarial loss: 1.373358, acc: 0.171875]\n",
      "10619: [discriminator loss: 0.569894, acc: 0.750000] [adversarial loss: 1.322622, acc: 0.125000]\n",
      "10620: [discriminator loss: 0.562675, acc: 0.695312] [adversarial loss: 1.063959, acc: 0.218750]\n",
      "10621: [discriminator loss: 0.481835, acc: 0.765625] [adversarial loss: 1.251254, acc: 0.187500]\n",
      "10622: [discriminator loss: 0.511678, acc: 0.742188] [adversarial loss: 1.110814, acc: 0.218750]\n",
      "10623: [discriminator loss: 0.520964, acc: 0.726562] [adversarial loss: 1.207500, acc: 0.187500]\n",
      "10624: [discriminator loss: 0.535868, acc: 0.687500] [adversarial loss: 1.131272, acc: 0.203125]\n",
      "10625: [discriminator loss: 0.554196, acc: 0.734375] [adversarial loss: 1.280021, acc: 0.218750]\n",
      "10626: [discriminator loss: 0.580352, acc: 0.703125] [adversarial loss: 0.906760, acc: 0.406250]\n",
      "10627: [discriminator loss: 0.603147, acc: 0.687500] [adversarial loss: 1.548827, acc: 0.093750]\n",
      "10628: [discriminator loss: 0.605555, acc: 0.664062] [adversarial loss: 0.988790, acc: 0.312500]\n",
      "10629: [discriminator loss: 0.544334, acc: 0.710938] [adversarial loss: 1.303468, acc: 0.203125]\n",
      "10630: [discriminator loss: 0.577859, acc: 0.718750] [adversarial loss: 0.934234, acc: 0.375000]\n",
      "10631: [discriminator loss: 0.545271, acc: 0.710938] [adversarial loss: 1.461568, acc: 0.078125]\n",
      "10632: [discriminator loss: 0.540437, acc: 0.695312] [adversarial loss: 0.854500, acc: 0.421875]\n",
      "10633: [discriminator loss: 0.595042, acc: 0.617188] [adversarial loss: 1.358697, acc: 0.109375]\n",
      "10634: [discriminator loss: 0.512985, acc: 0.750000] [adversarial loss: 1.084000, acc: 0.343750]\n",
      "10635: [discriminator loss: 0.566581, acc: 0.671875] [adversarial loss: 1.427680, acc: 0.140625]\n",
      "10636: [discriminator loss: 0.520825, acc: 0.703125] [adversarial loss: 1.112480, acc: 0.218750]\n",
      "10637: [discriminator loss: 0.512086, acc: 0.773438] [adversarial loss: 1.262411, acc: 0.171875]\n",
      "10638: [discriminator loss: 0.605087, acc: 0.679688] [adversarial loss: 1.112881, acc: 0.250000]\n",
      "10639: [discriminator loss: 0.509089, acc: 0.757812] [adversarial loss: 1.253942, acc: 0.187500]\n",
      "10640: [discriminator loss: 0.583235, acc: 0.679688] [adversarial loss: 1.026592, acc: 0.328125]\n",
      "10641: [discriminator loss: 0.492326, acc: 0.765625] [adversarial loss: 1.328729, acc: 0.203125]\n",
      "10642: [discriminator loss: 0.552936, acc: 0.664062] [adversarial loss: 1.126642, acc: 0.250000]\n",
      "10643: [discriminator loss: 0.554040, acc: 0.703125] [adversarial loss: 1.361755, acc: 0.203125]\n",
      "10644: [discriminator loss: 0.514135, acc: 0.710938] [adversarial loss: 1.000046, acc: 0.375000]\n",
      "10645: [discriminator loss: 0.556517, acc: 0.734375] [adversarial loss: 1.384236, acc: 0.187500]\n",
      "10646: [discriminator loss: 0.558764, acc: 0.679688] [adversarial loss: 0.879641, acc: 0.437500]\n",
      "10647: [discriminator loss: 0.636205, acc: 0.679688] [adversarial loss: 1.414871, acc: 0.078125]\n",
      "10648: [discriminator loss: 0.542467, acc: 0.710938] [adversarial loss: 0.984732, acc: 0.375000]\n",
      "10649: [discriminator loss: 0.618316, acc: 0.640625] [adversarial loss: 1.211245, acc: 0.156250]\n",
      "10650: [discriminator loss: 0.578146, acc: 0.664062] [adversarial loss: 1.169029, acc: 0.218750]\n",
      "10651: [discriminator loss: 0.602881, acc: 0.671875] [adversarial loss: 1.341051, acc: 0.125000]\n",
      "10652: [discriminator loss: 0.552435, acc: 0.703125] [adversarial loss: 0.958309, acc: 0.406250]\n",
      "10653: [discriminator loss: 0.501123, acc: 0.718750] [adversarial loss: 1.403030, acc: 0.203125]\n",
      "10654: [discriminator loss: 0.548316, acc: 0.718750] [adversarial loss: 0.917346, acc: 0.359375]\n",
      "10655: [discriminator loss: 0.586178, acc: 0.710938] [adversarial loss: 1.213008, acc: 0.218750]\n",
      "10656: [discriminator loss: 0.581407, acc: 0.695312] [adversarial loss: 0.777074, acc: 0.468750]\n",
      "10657: [discriminator loss: 0.584755, acc: 0.695312] [adversarial loss: 1.419638, acc: 0.078125]\n",
      "10658: [discriminator loss: 0.534488, acc: 0.710938] [adversarial loss: 0.939860, acc: 0.390625]\n",
      "10659: [discriminator loss: 0.566026, acc: 0.703125] [adversarial loss: 1.245995, acc: 0.187500]\n",
      "10660: [discriminator loss: 0.625905, acc: 0.640625] [adversarial loss: 1.307009, acc: 0.125000]\n",
      "10661: [discriminator loss: 0.557542, acc: 0.726562] [adversarial loss: 0.853584, acc: 0.390625]\n",
      "10662: [discriminator loss: 0.528201, acc: 0.703125] [adversarial loss: 1.267118, acc: 0.187500]\n",
      "10663: [discriminator loss: 0.485742, acc: 0.750000] [adversarial loss: 1.079869, acc: 0.187500]\n",
      "10664: [discriminator loss: 0.485776, acc: 0.757812] [adversarial loss: 1.349294, acc: 0.125000]\n",
      "10665: [discriminator loss: 0.505555, acc: 0.726562] [adversarial loss: 1.220907, acc: 0.250000]\n",
      "10666: [discriminator loss: 0.488406, acc: 0.804688] [adversarial loss: 1.178852, acc: 0.296875]\n",
      "10667: [discriminator loss: 0.582856, acc: 0.625000] [adversarial loss: 1.457473, acc: 0.187500]\n",
      "10668: [discriminator loss: 0.565088, acc: 0.664062] [adversarial loss: 0.994707, acc: 0.296875]\n",
      "10669: [discriminator loss: 0.516188, acc: 0.757812] [adversarial loss: 1.099996, acc: 0.281250]\n",
      "10670: [discriminator loss: 0.480269, acc: 0.765625] [adversarial loss: 1.280599, acc: 0.250000]\n",
      "10671: [discriminator loss: 0.502603, acc: 0.765625] [adversarial loss: 0.881016, acc: 0.406250]\n",
      "10672: [discriminator loss: 0.531808, acc: 0.695312] [adversarial loss: 1.451152, acc: 0.125000]\n",
      "10673: [discriminator loss: 0.553277, acc: 0.632812] [adversarial loss: 0.668436, acc: 0.593750]\n",
      "10674: [discriminator loss: 0.684065, acc: 0.625000] [adversarial loss: 1.659972, acc: 0.046875]\n",
      "10675: [discriminator loss: 0.616438, acc: 0.656250] [adversarial loss: 0.806531, acc: 0.406250]\n",
      "10676: [discriminator loss: 0.511448, acc: 0.757812] [adversarial loss: 1.545326, acc: 0.093750]\n",
      "10677: [discriminator loss: 0.596390, acc: 0.687500] [adversarial loss: 0.978875, acc: 0.328125]\n",
      "10678: [discriminator loss: 0.600390, acc: 0.687500] [adversarial loss: 1.415868, acc: 0.171875]\n",
      "10679: [discriminator loss: 0.558994, acc: 0.671875] [adversarial loss: 1.127040, acc: 0.218750]\n",
      "10680: [discriminator loss: 0.492043, acc: 0.765625] [adversarial loss: 1.199118, acc: 0.343750]\n",
      "10681: [discriminator loss: 0.542300, acc: 0.664062] [adversarial loss: 1.150847, acc: 0.250000]\n",
      "10682: [discriminator loss: 0.535531, acc: 0.773438] [adversarial loss: 1.116850, acc: 0.218750]\n",
      "10683: [discriminator loss: 0.538851, acc: 0.710938] [adversarial loss: 1.028350, acc: 0.406250]\n",
      "10684: [discriminator loss: 0.605156, acc: 0.632812] [adversarial loss: 1.009650, acc: 0.265625]\n",
      "10685: [discriminator loss: 0.558161, acc: 0.726562] [adversarial loss: 1.278189, acc: 0.234375]\n",
      "10686: [discriminator loss: 0.620861, acc: 0.617188] [adversarial loss: 1.128717, acc: 0.234375]\n",
      "10687: [discriminator loss: 0.606083, acc: 0.671875] [adversarial loss: 1.418865, acc: 0.109375]\n",
      "10688: [discriminator loss: 0.544942, acc: 0.695312] [adversarial loss: 1.032916, acc: 0.187500]\n",
      "10689: [discriminator loss: 0.599731, acc: 0.664062] [adversarial loss: 1.143344, acc: 0.156250]\n",
      "10690: [discriminator loss: 0.469708, acc: 0.773438] [adversarial loss: 1.053550, acc: 0.359375]\n",
      "10691: [discriminator loss: 0.527655, acc: 0.726562] [adversarial loss: 1.302788, acc: 0.156250]\n",
      "10692: [discriminator loss: 0.519717, acc: 0.765625] [adversarial loss: 1.348237, acc: 0.140625]\n",
      "10693: [discriminator loss: 0.556508, acc: 0.710938] [adversarial loss: 0.955460, acc: 0.437500]\n",
      "10694: [discriminator loss: 0.585964, acc: 0.664062] [adversarial loss: 1.620434, acc: 0.109375]\n",
      "10695: [discriminator loss: 0.589721, acc: 0.687500] [adversarial loss: 0.879245, acc: 0.390625]\n",
      "10696: [discriminator loss: 0.532501, acc: 0.718750] [adversarial loss: 1.333339, acc: 0.125000]\n",
      "10697: [discriminator loss: 0.588254, acc: 0.687500] [adversarial loss: 0.831956, acc: 0.453125]\n",
      "10698: [discriminator loss: 0.628531, acc: 0.656250] [adversarial loss: 1.554122, acc: 0.093750]\n",
      "10699: [discriminator loss: 0.558574, acc: 0.710938] [adversarial loss: 1.232590, acc: 0.234375]\n",
      "10700: [discriminator loss: 0.573856, acc: 0.679688] [adversarial loss: 1.026069, acc: 0.312500]\n",
      "10701: [discriminator loss: 0.576426, acc: 0.734375] [adversarial loss: 1.309625, acc: 0.125000]\n",
      "10702: [discriminator loss: 0.536737, acc: 0.726562] [adversarial loss: 0.912340, acc: 0.421875]\n",
      "10703: [discriminator loss: 0.575173, acc: 0.710938] [adversarial loss: 1.295097, acc: 0.234375]\n",
      "10704: [discriminator loss: 0.570182, acc: 0.671875] [adversarial loss: 1.008070, acc: 0.312500]\n",
      "10705: [discriminator loss: 0.607009, acc: 0.625000] [adversarial loss: 1.126901, acc: 0.328125]\n",
      "10706: [discriminator loss: 0.622613, acc: 0.585938] [adversarial loss: 0.997000, acc: 0.375000]\n",
      "10707: [discriminator loss: 0.541704, acc: 0.718750] [adversarial loss: 1.023184, acc: 0.281250]\n",
      "10708: [discriminator loss: 0.495330, acc: 0.773438] [adversarial loss: 1.059752, acc: 0.312500]\n",
      "10709: [discriminator loss: 0.595174, acc: 0.695312] [adversarial loss: 1.270747, acc: 0.187500]\n",
      "10710: [discriminator loss: 0.578784, acc: 0.703125] [adversarial loss: 0.947567, acc: 0.312500]\n",
      "10711: [discriminator loss: 0.583017, acc: 0.617188] [adversarial loss: 1.272997, acc: 0.140625]\n",
      "10712: [discriminator loss: 0.586374, acc: 0.648438] [adversarial loss: 1.059132, acc: 0.265625]\n",
      "10713: [discriminator loss: 0.656272, acc: 0.640625] [adversarial loss: 1.229745, acc: 0.093750]\n",
      "10714: [discriminator loss: 0.520851, acc: 0.703125] [adversarial loss: 1.119219, acc: 0.265625]\n",
      "10715: [discriminator loss: 0.488354, acc: 0.765625] [adversarial loss: 1.125939, acc: 0.281250]\n",
      "10716: [discriminator loss: 0.522341, acc: 0.773438] [adversarial loss: 1.212125, acc: 0.250000]\n",
      "10717: [discriminator loss: 0.590743, acc: 0.742188] [adversarial loss: 1.355271, acc: 0.156250]\n",
      "10718: [discriminator loss: 0.590137, acc: 0.656250] [adversarial loss: 1.182309, acc: 0.203125]\n",
      "10719: [discriminator loss: 0.537886, acc: 0.710938] [adversarial loss: 1.143647, acc: 0.218750]\n",
      "10720: [discriminator loss: 0.525534, acc: 0.734375] [adversarial loss: 1.218088, acc: 0.171875]\n",
      "10721: [discriminator loss: 0.571276, acc: 0.671875] [adversarial loss: 0.948454, acc: 0.312500]\n",
      "10722: [discriminator loss: 0.559646, acc: 0.671875] [adversarial loss: 1.598842, acc: 0.093750]\n",
      "10723: [discriminator loss: 0.605563, acc: 0.656250] [adversarial loss: 0.788555, acc: 0.562500]\n",
      "10724: [discriminator loss: 0.611153, acc: 0.671875] [adversarial loss: 1.412374, acc: 0.046875]\n",
      "10725: [discriminator loss: 0.591534, acc: 0.632812] [adversarial loss: 0.955137, acc: 0.359375]\n",
      "10726: [discriminator loss: 0.539388, acc: 0.734375] [adversarial loss: 1.053101, acc: 0.343750]\n",
      "10727: [discriminator loss: 0.551183, acc: 0.718750] [adversarial loss: 1.236161, acc: 0.109375]\n",
      "10728: [discriminator loss: 0.525348, acc: 0.718750] [adversarial loss: 1.209547, acc: 0.171875]\n",
      "10729: [discriminator loss: 0.585284, acc: 0.671875] [adversarial loss: 1.485445, acc: 0.109375]\n",
      "10730: [discriminator loss: 0.489027, acc: 0.789062] [adversarial loss: 1.224596, acc: 0.171875]\n",
      "10731: [discriminator loss: 0.534705, acc: 0.718750] [adversarial loss: 1.169617, acc: 0.187500]\n",
      "10732: [discriminator loss: 0.575154, acc: 0.718750] [adversarial loss: 0.972706, acc: 0.421875]\n",
      "10733: [discriminator loss: 0.575076, acc: 0.710938] [adversarial loss: 1.518809, acc: 0.109375]\n",
      "10734: [discriminator loss: 0.580088, acc: 0.679688] [adversarial loss: 0.912884, acc: 0.328125]\n",
      "10735: [discriminator loss: 0.537909, acc: 0.765625] [adversarial loss: 1.375322, acc: 0.156250]\n",
      "10736: [discriminator loss: 0.520302, acc: 0.726562] [adversarial loss: 1.214486, acc: 0.187500]\n",
      "10737: [discriminator loss: 0.533376, acc: 0.679688] [adversarial loss: 0.881635, acc: 0.421875]\n",
      "10738: [discriminator loss: 0.589248, acc: 0.687500] [adversarial loss: 1.227555, acc: 0.171875]\n",
      "10739: [discriminator loss: 0.479600, acc: 0.726562] [adversarial loss: 1.066428, acc: 0.281250]\n",
      "10740: [discriminator loss: 0.562366, acc: 0.718750] [adversarial loss: 1.292853, acc: 0.203125]\n",
      "10741: [discriminator loss: 0.564661, acc: 0.734375] [adversarial loss: 0.971720, acc: 0.343750]\n",
      "10742: [discriminator loss: 0.564986, acc: 0.695312] [adversarial loss: 1.225538, acc: 0.218750]\n",
      "10743: [discriminator loss: 0.549789, acc: 0.710938] [adversarial loss: 0.901629, acc: 0.390625]\n",
      "10744: [discriminator loss: 0.480749, acc: 0.781250] [adversarial loss: 1.104110, acc: 0.343750]\n",
      "10745: [discriminator loss: 0.540684, acc: 0.710938] [adversarial loss: 0.909798, acc: 0.343750]\n",
      "10746: [discriminator loss: 0.526182, acc: 0.750000] [adversarial loss: 1.209989, acc: 0.171875]\n",
      "10747: [discriminator loss: 0.550965, acc: 0.726562] [adversarial loss: 0.915978, acc: 0.437500]\n",
      "10748: [discriminator loss: 0.553499, acc: 0.718750] [adversarial loss: 1.203232, acc: 0.156250]\n",
      "10749: [discriminator loss: 0.539313, acc: 0.718750] [adversarial loss: 0.982733, acc: 0.375000]\n",
      "10750: [discriminator loss: 0.594593, acc: 0.687500] [adversarial loss: 1.521708, acc: 0.187500]\n",
      "10751: [discriminator loss: 0.578712, acc: 0.687500] [adversarial loss: 0.646212, acc: 0.656250]\n",
      "10752: [discriminator loss: 0.598427, acc: 0.648438] [adversarial loss: 1.584604, acc: 0.031250]\n",
      "10753: [discriminator loss: 0.566267, acc: 0.703125] [adversarial loss: 0.837436, acc: 0.390625]\n",
      "10754: [discriminator loss: 0.525904, acc: 0.742188] [adversarial loss: 1.492229, acc: 0.109375]\n",
      "10755: [discriminator loss: 0.550753, acc: 0.726562] [adversarial loss: 1.087777, acc: 0.281250]\n",
      "10756: [discriminator loss: 0.549283, acc: 0.695312] [adversarial loss: 1.186847, acc: 0.312500]\n",
      "10757: [discriminator loss: 0.588244, acc: 0.671875] [adversarial loss: 1.137188, acc: 0.281250]\n",
      "10758: [discriminator loss: 0.554663, acc: 0.710938] [adversarial loss: 1.215834, acc: 0.218750]\n",
      "10759: [discriminator loss: 0.572305, acc: 0.648438] [adversarial loss: 1.076924, acc: 0.312500]\n",
      "10760: [discriminator loss: 0.515594, acc: 0.757812] [adversarial loss: 0.914290, acc: 0.421875]\n",
      "10761: [discriminator loss: 0.585337, acc: 0.664062] [adversarial loss: 1.401185, acc: 0.171875]\n",
      "10762: [discriminator loss: 0.588794, acc: 0.671875] [adversarial loss: 0.930613, acc: 0.375000]\n",
      "10763: [discriminator loss: 0.564226, acc: 0.726562] [adversarial loss: 1.283410, acc: 0.109375]\n",
      "10764: [discriminator loss: 0.561054, acc: 0.679688] [adversarial loss: 1.131844, acc: 0.250000]\n",
      "10765: [discriminator loss: 0.585262, acc: 0.664062] [adversarial loss: 1.191623, acc: 0.140625]\n",
      "10766: [discriminator loss: 0.593941, acc: 0.695312] [adversarial loss: 1.091657, acc: 0.343750]\n",
      "10767: [discriminator loss: 0.507222, acc: 0.742188] [adversarial loss: 1.273072, acc: 0.218750]\n",
      "10768: [discriminator loss: 0.557228, acc: 0.718750] [adversarial loss: 1.234013, acc: 0.250000]\n",
      "10769: [discriminator loss: 0.589152, acc: 0.695312] [adversarial loss: 1.109036, acc: 0.281250]\n",
      "10770: [discriminator loss: 0.557187, acc: 0.718750] [adversarial loss: 1.277719, acc: 0.171875]\n",
      "10771: [discriminator loss: 0.626219, acc: 0.625000] [adversarial loss: 0.976852, acc: 0.390625]\n",
      "10772: [discriminator loss: 0.557117, acc: 0.695312] [adversarial loss: 1.217358, acc: 0.218750]\n",
      "10773: [discriminator loss: 0.557132, acc: 0.726562] [adversarial loss: 0.964997, acc: 0.390625]\n",
      "10774: [discriminator loss: 0.558752, acc: 0.671875] [adversarial loss: 1.492857, acc: 0.125000]\n",
      "10775: [discriminator loss: 0.538154, acc: 0.703125] [adversarial loss: 0.941233, acc: 0.390625]\n",
      "10776: [discriminator loss: 0.624811, acc: 0.664062] [adversarial loss: 1.360938, acc: 0.156250]\n",
      "10777: [discriminator loss: 0.544144, acc: 0.773438] [adversarial loss: 1.244632, acc: 0.203125]\n",
      "10778: [discriminator loss: 0.553573, acc: 0.718750] [adversarial loss: 1.033041, acc: 0.296875]\n",
      "10779: [discriminator loss: 0.590467, acc: 0.703125] [adversarial loss: 0.985405, acc: 0.265625]\n",
      "10780: [discriminator loss: 0.481292, acc: 0.796875] [adversarial loss: 1.220343, acc: 0.218750]\n",
      "10781: [discriminator loss: 0.545054, acc: 0.695312] [adversarial loss: 0.990952, acc: 0.312500]\n",
      "10782: [discriminator loss: 0.524377, acc: 0.742188] [adversarial loss: 0.829960, acc: 0.406250]\n",
      "10783: [discriminator loss: 0.576793, acc: 0.687500] [adversarial loss: 1.339627, acc: 0.109375]\n",
      "10784: [discriminator loss: 0.538796, acc: 0.703125] [adversarial loss: 0.879410, acc: 0.453125]\n",
      "10785: [discriminator loss: 0.539665, acc: 0.742188] [adversarial loss: 1.543618, acc: 0.093750]\n",
      "10786: [discriminator loss: 0.610168, acc: 0.601562] [adversarial loss: 0.791319, acc: 0.531250]\n",
      "10787: [discriminator loss: 0.596742, acc: 0.726562] [adversarial loss: 1.492774, acc: 0.062500]\n",
      "10788: [discriminator loss: 0.566541, acc: 0.648438] [adversarial loss: 0.977597, acc: 0.421875]\n",
      "10789: [discriminator loss: 0.524235, acc: 0.765625] [adversarial loss: 1.118336, acc: 0.250000]\n",
      "10790: [discriminator loss: 0.547398, acc: 0.710938] [adversarial loss: 0.917757, acc: 0.312500]\n",
      "10791: [discriminator loss: 0.572983, acc: 0.687500] [adversarial loss: 1.339326, acc: 0.156250]\n",
      "10792: [discriminator loss: 0.507061, acc: 0.734375] [adversarial loss: 0.890147, acc: 0.390625]\n",
      "10793: [discriminator loss: 0.506416, acc: 0.757812] [adversarial loss: 1.564714, acc: 0.093750]\n",
      "10794: [discriminator loss: 0.511296, acc: 0.742188] [adversarial loss: 0.924840, acc: 0.390625]\n",
      "10795: [discriminator loss: 0.588887, acc: 0.710938] [adversarial loss: 1.472089, acc: 0.078125]\n",
      "10796: [discriminator loss: 0.651982, acc: 0.648438] [adversarial loss: 0.856212, acc: 0.437500]\n",
      "10797: [discriminator loss: 0.580221, acc: 0.718750] [adversarial loss: 1.274214, acc: 0.187500]\n",
      "10798: [discriminator loss: 0.502784, acc: 0.703125] [adversarial loss: 1.158961, acc: 0.281250]\n",
      "10799: [discriminator loss: 0.557812, acc: 0.687500] [adversarial loss: 0.915523, acc: 0.421875]\n",
      "10800: [discriminator loss: 0.609139, acc: 0.656250] [adversarial loss: 1.241869, acc: 0.218750]\n",
      "10801: [discriminator loss: 0.480587, acc: 0.781250] [adversarial loss: 1.057393, acc: 0.234375]\n",
      "10802: [discriminator loss: 0.573304, acc: 0.664062] [adversarial loss: 1.056609, acc: 0.328125]\n",
      "10803: [discriminator loss: 0.512106, acc: 0.765625] [adversarial loss: 1.126701, acc: 0.203125]\n",
      "10804: [discriminator loss: 0.524971, acc: 0.710938] [adversarial loss: 1.221285, acc: 0.234375]\n",
      "10805: [discriminator loss: 0.534057, acc: 0.757812] [adversarial loss: 1.147851, acc: 0.312500]\n",
      "10806: [discriminator loss: 0.516211, acc: 0.765625] [adversarial loss: 1.023098, acc: 0.296875]\n",
      "10807: [discriminator loss: 0.578264, acc: 0.671875] [adversarial loss: 1.329780, acc: 0.125000]\n",
      "10808: [discriminator loss: 0.526303, acc: 0.734375] [adversarial loss: 1.080228, acc: 0.218750]\n",
      "10809: [discriminator loss: 0.626553, acc: 0.656250] [adversarial loss: 1.197939, acc: 0.234375]\n",
      "10810: [discriminator loss: 0.518382, acc: 0.726562] [adversarial loss: 0.976008, acc: 0.328125]\n",
      "10811: [discriminator loss: 0.557870, acc: 0.710938] [adversarial loss: 1.519761, acc: 0.140625]\n",
      "10812: [discriminator loss: 0.491518, acc: 0.726562] [adversarial loss: 0.968554, acc: 0.328125]\n",
      "10813: [discriminator loss: 0.538078, acc: 0.750000] [adversarial loss: 1.046234, acc: 0.328125]\n",
      "10814: [discriminator loss: 0.578541, acc: 0.695312] [adversarial loss: 1.028389, acc: 0.281250]\n",
      "10815: [discriminator loss: 0.532120, acc: 0.757812] [adversarial loss: 1.239326, acc: 0.187500]\n",
      "10816: [discriminator loss: 0.578169, acc: 0.695312] [adversarial loss: 0.875779, acc: 0.421875]\n",
      "10817: [discriminator loss: 0.536773, acc: 0.710938] [adversarial loss: 1.420666, acc: 0.125000]\n",
      "10818: [discriminator loss: 0.524970, acc: 0.710938] [adversarial loss: 1.005072, acc: 0.343750]\n",
      "10819: [discriminator loss: 0.581751, acc: 0.679688] [adversarial loss: 0.996867, acc: 0.312500]\n",
      "10820: [discriminator loss: 0.509523, acc: 0.726562] [adversarial loss: 1.061249, acc: 0.250000]\n",
      "10821: [discriminator loss: 0.485165, acc: 0.742188] [adversarial loss: 1.037063, acc: 0.281250]\n",
      "10822: [discriminator loss: 0.592011, acc: 0.687500] [adversarial loss: 1.179722, acc: 0.234375]\n",
      "10823: [discriminator loss: 0.536086, acc: 0.710938] [adversarial loss: 1.015501, acc: 0.312500]\n",
      "10824: [discriminator loss: 0.552604, acc: 0.671875] [adversarial loss: 0.943747, acc: 0.359375]\n",
      "10825: [discriminator loss: 0.502108, acc: 0.718750] [adversarial loss: 1.295038, acc: 0.187500]\n",
      "10826: [discriminator loss: 0.534619, acc: 0.710938] [adversarial loss: 0.872210, acc: 0.500000]\n",
      "10827: [discriminator loss: 0.614625, acc: 0.687500] [adversarial loss: 1.412977, acc: 0.109375]\n",
      "10828: [discriminator loss: 0.515198, acc: 0.734375] [adversarial loss: 0.778063, acc: 0.546875]\n",
      "10829: [discriminator loss: 0.528754, acc: 0.664062] [adversarial loss: 1.459979, acc: 0.125000]\n",
      "10830: [discriminator loss: 0.531804, acc: 0.726562] [adversarial loss: 0.744857, acc: 0.515625]\n",
      "10831: [discriminator loss: 0.666282, acc: 0.593750] [adversarial loss: 1.503258, acc: 0.125000]\n",
      "10832: [discriminator loss: 0.572385, acc: 0.718750] [adversarial loss: 0.936649, acc: 0.375000]\n",
      "10833: [discriminator loss: 0.572657, acc: 0.656250] [adversarial loss: 1.478747, acc: 0.062500]\n",
      "10834: [discriminator loss: 0.621272, acc: 0.648438] [adversarial loss: 0.744373, acc: 0.593750]\n",
      "10835: [discriminator loss: 0.644130, acc: 0.617188] [adversarial loss: 1.285359, acc: 0.125000]\n",
      "10836: [discriminator loss: 0.536560, acc: 0.695312] [adversarial loss: 0.925212, acc: 0.343750]\n",
      "10837: [discriminator loss: 0.549331, acc: 0.734375] [adversarial loss: 1.423831, acc: 0.093750]\n",
      "10838: [discriminator loss: 0.619698, acc: 0.664062] [adversarial loss: 1.008927, acc: 0.312500]\n",
      "10839: [discriminator loss: 0.544124, acc: 0.742188] [adversarial loss: 1.530907, acc: 0.125000]\n",
      "10840: [discriminator loss: 0.515003, acc: 0.718750] [adversarial loss: 0.980609, acc: 0.390625]\n",
      "10841: [discriminator loss: 0.554904, acc: 0.726562] [adversarial loss: 1.389881, acc: 0.156250]\n",
      "10842: [discriminator loss: 0.542180, acc: 0.710938] [adversarial loss: 1.298043, acc: 0.234375]\n",
      "10843: [discriminator loss: 0.589260, acc: 0.656250] [adversarial loss: 0.956879, acc: 0.359375]\n",
      "10844: [discriminator loss: 0.523224, acc: 0.703125] [adversarial loss: 1.087429, acc: 0.390625]\n",
      "10845: [discriminator loss: 0.569862, acc: 0.687500] [adversarial loss: 0.966526, acc: 0.328125]\n",
      "10846: [discriminator loss: 0.567959, acc: 0.703125] [adversarial loss: 1.378735, acc: 0.156250]\n",
      "10847: [discriminator loss: 0.531600, acc: 0.695312] [adversarial loss: 0.951498, acc: 0.343750]\n",
      "10848: [discriminator loss: 0.508217, acc: 0.750000] [adversarial loss: 1.318927, acc: 0.140625]\n",
      "10849: [discriminator loss: 0.634020, acc: 0.656250] [adversarial loss: 1.024410, acc: 0.343750]\n",
      "10850: [discriminator loss: 0.539333, acc: 0.703125] [adversarial loss: 1.236141, acc: 0.265625]\n",
      "10851: [discriminator loss: 0.548805, acc: 0.703125] [adversarial loss: 1.232294, acc: 0.218750]\n",
      "10852: [discriminator loss: 0.590851, acc: 0.687500] [adversarial loss: 0.847518, acc: 0.390625]\n",
      "10853: [discriminator loss: 0.575768, acc: 0.695312] [adversarial loss: 1.232673, acc: 0.187500]\n",
      "10854: [discriminator loss: 0.642233, acc: 0.625000] [adversarial loss: 0.804522, acc: 0.437500]\n",
      "10855: [discriminator loss: 0.547840, acc: 0.726562] [adversarial loss: 1.276584, acc: 0.203125]\n",
      "10856: [discriminator loss: 0.500308, acc: 0.765625] [adversarial loss: 1.515126, acc: 0.156250]\n",
      "10857: [discriminator loss: 0.620257, acc: 0.656250] [adversarial loss: 0.951270, acc: 0.359375]\n",
      "10858: [discriminator loss: 0.559587, acc: 0.757812] [adversarial loss: 1.230301, acc: 0.171875]\n",
      "10859: [discriminator loss: 0.559536, acc: 0.687500] [adversarial loss: 1.102471, acc: 0.296875]\n",
      "10860: [discriminator loss: 0.521052, acc: 0.718750] [adversarial loss: 1.089894, acc: 0.281250]\n",
      "10861: [discriminator loss: 0.611972, acc: 0.632812] [adversarial loss: 1.020588, acc: 0.296875]\n",
      "10862: [discriminator loss: 0.520256, acc: 0.750000] [adversarial loss: 1.035626, acc: 0.281250]\n",
      "10863: [discriminator loss: 0.499747, acc: 0.726562] [adversarial loss: 1.267389, acc: 0.187500]\n",
      "10864: [discriminator loss: 0.532973, acc: 0.726562] [adversarial loss: 0.957691, acc: 0.390625]\n",
      "10865: [discriminator loss: 0.628125, acc: 0.617188] [adversarial loss: 1.394453, acc: 0.125000]\n",
      "10866: [discriminator loss: 0.555969, acc: 0.703125] [adversarial loss: 0.882563, acc: 0.421875]\n",
      "10867: [discriminator loss: 0.541676, acc: 0.750000] [adversarial loss: 1.441265, acc: 0.125000]\n",
      "10868: [discriminator loss: 0.569959, acc: 0.679688] [adversarial loss: 1.053135, acc: 0.343750]\n",
      "10869: [discriminator loss: 0.496216, acc: 0.734375] [adversarial loss: 1.379405, acc: 0.187500]\n",
      "10870: [discriminator loss: 0.475118, acc: 0.789062] [adversarial loss: 0.904252, acc: 0.328125]\n",
      "10871: [discriminator loss: 0.593135, acc: 0.648438] [adversarial loss: 1.379585, acc: 0.156250]\n",
      "10872: [discriminator loss: 0.525002, acc: 0.726562] [adversarial loss: 1.132037, acc: 0.218750]\n",
      "10873: [discriminator loss: 0.577910, acc: 0.640625] [adversarial loss: 1.095233, acc: 0.281250]\n",
      "10874: [discriminator loss: 0.554005, acc: 0.695312] [adversarial loss: 1.106863, acc: 0.312500]\n",
      "10875: [discriminator loss: 0.568943, acc: 0.710938] [adversarial loss: 1.094220, acc: 0.234375]\n",
      "10876: [discriminator loss: 0.517112, acc: 0.695312] [adversarial loss: 1.235979, acc: 0.265625]\n",
      "10877: [discriminator loss: 0.562555, acc: 0.679688] [adversarial loss: 1.047219, acc: 0.343750]\n",
      "10878: [discriminator loss: 0.507945, acc: 0.750000] [adversarial loss: 1.351128, acc: 0.140625]\n",
      "10879: [discriminator loss: 0.564660, acc: 0.656250] [adversarial loss: 1.373908, acc: 0.109375]\n",
      "10880: [discriminator loss: 0.515297, acc: 0.765625] [adversarial loss: 1.357128, acc: 0.187500]\n",
      "10881: [discriminator loss: 0.556040, acc: 0.695312] [adversarial loss: 1.318317, acc: 0.234375]\n",
      "10882: [discriminator loss: 0.648594, acc: 0.648438] [adversarial loss: 0.939750, acc: 0.312500]\n",
      "10883: [discriminator loss: 0.579930, acc: 0.695312] [adversarial loss: 1.731015, acc: 0.187500]\n",
      "10884: [discriminator loss: 0.724797, acc: 0.625000] [adversarial loss: 0.887100, acc: 0.421875]\n",
      "10885: [discriminator loss: 0.546910, acc: 0.726562] [adversarial loss: 1.364335, acc: 0.171875]\n",
      "10886: [discriminator loss: 0.501156, acc: 0.773438] [adversarial loss: 1.205789, acc: 0.265625]\n",
      "10887: [discriminator loss: 0.621233, acc: 0.648438] [adversarial loss: 1.283048, acc: 0.171875]\n",
      "10888: [discriminator loss: 0.529014, acc: 0.703125] [adversarial loss: 1.000586, acc: 0.281250]\n",
      "10889: [discriminator loss: 0.541679, acc: 0.726562] [adversarial loss: 1.280873, acc: 0.140625]\n",
      "10890: [discriminator loss: 0.557241, acc: 0.718750] [adversarial loss: 1.335907, acc: 0.109375]\n",
      "10891: [discriminator loss: 0.583416, acc: 0.671875] [adversarial loss: 0.927611, acc: 0.437500]\n",
      "10892: [discriminator loss: 0.589529, acc: 0.671875] [adversarial loss: 1.263126, acc: 0.234375]\n",
      "10893: [discriminator loss: 0.524138, acc: 0.734375] [adversarial loss: 1.261762, acc: 0.171875]\n",
      "10894: [discriminator loss: 0.590049, acc: 0.703125] [adversarial loss: 1.140533, acc: 0.328125]\n",
      "10895: [discriminator loss: 0.534687, acc: 0.750000] [adversarial loss: 1.088609, acc: 0.312500]\n",
      "10896: [discriminator loss: 0.499315, acc: 0.718750] [adversarial loss: 1.149059, acc: 0.250000]\n",
      "10897: [discriminator loss: 0.570922, acc: 0.664062] [adversarial loss: 1.296950, acc: 0.125000]\n",
      "10898: [discriminator loss: 0.545451, acc: 0.750000] [adversarial loss: 0.770091, acc: 0.546875]\n",
      "10899: [discriminator loss: 0.572021, acc: 0.671875] [adversarial loss: 1.393847, acc: 0.093750]\n",
      "10900: [discriminator loss: 0.653167, acc: 0.625000] [adversarial loss: 0.847240, acc: 0.390625]\n",
      "10901: [discriminator loss: 0.580432, acc: 0.734375] [adversarial loss: 1.475266, acc: 0.109375]\n",
      "10902: [discriminator loss: 0.540797, acc: 0.710938] [adversarial loss: 0.916774, acc: 0.421875]\n",
      "10903: [discriminator loss: 0.611548, acc: 0.664062] [adversarial loss: 1.521002, acc: 0.140625]\n",
      "10904: [discriminator loss: 0.605769, acc: 0.632812] [adversarial loss: 0.883286, acc: 0.406250]\n",
      "10905: [discriminator loss: 0.562034, acc: 0.687500] [adversarial loss: 1.546034, acc: 0.125000]\n",
      "10906: [discriminator loss: 0.564180, acc: 0.687500] [adversarial loss: 1.021605, acc: 0.359375]\n",
      "10907: [discriminator loss: 0.537225, acc: 0.687500] [adversarial loss: 1.448447, acc: 0.062500]\n",
      "10908: [discriminator loss: 0.548713, acc: 0.695312] [adversarial loss: 1.093606, acc: 0.234375]\n",
      "10909: [discriminator loss: 0.528512, acc: 0.710938] [adversarial loss: 1.248166, acc: 0.218750]\n",
      "10910: [discriminator loss: 0.520938, acc: 0.750000] [adversarial loss: 1.139698, acc: 0.250000]\n",
      "10911: [discriminator loss: 0.506066, acc: 0.742188] [adversarial loss: 0.982335, acc: 0.265625]\n",
      "10912: [discriminator loss: 0.513218, acc: 0.789062] [adversarial loss: 1.227432, acc: 0.281250]\n",
      "10913: [discriminator loss: 0.575565, acc: 0.664062] [adversarial loss: 0.976672, acc: 0.296875]\n",
      "10914: [discriminator loss: 0.591333, acc: 0.640625] [adversarial loss: 1.231155, acc: 0.203125]\n",
      "10915: [discriminator loss: 0.505095, acc: 0.757812] [adversarial loss: 1.171003, acc: 0.203125]\n",
      "10916: [discriminator loss: 0.534672, acc: 0.718750] [adversarial loss: 0.929459, acc: 0.343750]\n",
      "10917: [discriminator loss: 0.580372, acc: 0.656250] [adversarial loss: 1.529986, acc: 0.046875]\n",
      "10918: [discriminator loss: 0.519706, acc: 0.710938] [adversarial loss: 1.319725, acc: 0.203125]\n",
      "10919: [discriminator loss: 0.578624, acc: 0.695312] [adversarial loss: 0.856352, acc: 0.390625]\n",
      "10920: [discriminator loss: 0.563618, acc: 0.656250] [adversarial loss: 1.297785, acc: 0.156250]\n",
      "10921: [discriminator loss: 0.576374, acc: 0.640625] [adversarial loss: 0.960540, acc: 0.375000]\n",
      "10922: [discriminator loss: 0.529619, acc: 0.734375] [adversarial loss: 1.336736, acc: 0.171875]\n",
      "10923: [discriminator loss: 0.587511, acc: 0.671875] [adversarial loss: 1.035317, acc: 0.312500]\n",
      "10924: [discriminator loss: 0.520905, acc: 0.718750] [adversarial loss: 1.181380, acc: 0.218750]\n",
      "10925: [discriminator loss: 0.559437, acc: 0.718750] [adversarial loss: 1.144001, acc: 0.187500]\n",
      "10926: [discriminator loss: 0.561366, acc: 0.742188] [adversarial loss: 1.115592, acc: 0.281250]\n",
      "10927: [discriminator loss: 0.550417, acc: 0.718750] [adversarial loss: 1.279756, acc: 0.171875]\n",
      "10928: [discriminator loss: 0.558985, acc: 0.664062] [adversarial loss: 0.993648, acc: 0.343750]\n",
      "10929: [discriminator loss: 0.539311, acc: 0.718750] [adversarial loss: 1.205063, acc: 0.218750]\n",
      "10930: [discriminator loss: 0.561241, acc: 0.742188] [adversarial loss: 1.024343, acc: 0.281250]\n",
      "10931: [discriminator loss: 0.521672, acc: 0.734375] [adversarial loss: 1.500051, acc: 0.140625]\n",
      "10932: [discriminator loss: 0.470650, acc: 0.726562] [adversarial loss: 0.981182, acc: 0.312500]\n",
      "10933: [discriminator loss: 0.543453, acc: 0.695312] [adversarial loss: 1.560101, acc: 0.093750]\n",
      "10934: [discriminator loss: 0.645948, acc: 0.656250] [adversarial loss: 0.875980, acc: 0.406250]\n",
      "10935: [discriminator loss: 0.546545, acc: 0.671875] [adversarial loss: 1.742974, acc: 0.078125]\n",
      "10936: [discriminator loss: 0.574118, acc: 0.710938] [adversarial loss: 0.891944, acc: 0.453125]\n",
      "10937: [discriminator loss: 0.585026, acc: 0.695312] [adversarial loss: 1.220374, acc: 0.187500]\n",
      "10938: [discriminator loss: 0.628091, acc: 0.601562] [adversarial loss: 0.864158, acc: 0.500000]\n",
      "10939: [discriminator loss: 0.563623, acc: 0.718750] [adversarial loss: 1.129298, acc: 0.203125]\n",
      "10940: [discriminator loss: 0.512206, acc: 0.726562] [adversarial loss: 1.249738, acc: 0.265625]\n",
      "10941: [discriminator loss: 0.635387, acc: 0.625000] [adversarial loss: 1.070665, acc: 0.265625]\n",
      "10942: [discriminator loss: 0.587047, acc: 0.703125] [adversarial loss: 1.231939, acc: 0.250000]\n",
      "10943: [discriminator loss: 0.608169, acc: 0.640625] [adversarial loss: 1.013082, acc: 0.328125]\n",
      "10944: [discriminator loss: 0.497235, acc: 0.757812] [adversarial loss: 1.000527, acc: 0.375000]\n",
      "10945: [discriminator loss: 0.521810, acc: 0.750000] [adversarial loss: 1.076663, acc: 0.218750]\n",
      "10946: [discriminator loss: 0.545764, acc: 0.726562] [adversarial loss: 1.043811, acc: 0.265625]\n",
      "10947: [discriminator loss: 0.564259, acc: 0.679688] [adversarial loss: 1.115647, acc: 0.281250]\n",
      "10948: [discriminator loss: 0.503649, acc: 0.726562] [adversarial loss: 0.992193, acc: 0.328125]\n",
      "10949: [discriminator loss: 0.519482, acc: 0.718750] [adversarial loss: 1.029799, acc: 0.218750]\n",
      "10950: [discriminator loss: 0.560182, acc: 0.687500] [adversarial loss: 1.256734, acc: 0.234375]\n",
      "10951: [discriminator loss: 0.547630, acc: 0.687500] [adversarial loss: 1.048906, acc: 0.312500]\n",
      "10952: [discriminator loss: 0.542742, acc: 0.710938] [adversarial loss: 1.371418, acc: 0.140625]\n",
      "10953: [discriminator loss: 0.436237, acc: 0.820312] [adversarial loss: 1.052501, acc: 0.265625]\n",
      "10954: [discriminator loss: 0.528706, acc: 0.757812] [adversarial loss: 1.242509, acc: 0.187500]\n",
      "10955: [discriminator loss: 0.578941, acc: 0.648438] [adversarial loss: 1.127474, acc: 0.265625]\n",
      "10956: [discriminator loss: 0.509109, acc: 0.757812] [adversarial loss: 1.230161, acc: 0.250000]\n",
      "10957: [discriminator loss: 0.633778, acc: 0.671875] [adversarial loss: 1.155291, acc: 0.265625]\n",
      "10958: [discriminator loss: 0.467503, acc: 0.765625] [adversarial loss: 1.303520, acc: 0.187500]\n",
      "10959: [discriminator loss: 0.503420, acc: 0.773438] [adversarial loss: 1.100523, acc: 0.375000]\n",
      "10960: [discriminator loss: 0.531941, acc: 0.750000] [adversarial loss: 1.427176, acc: 0.125000]\n",
      "10961: [discriminator loss: 0.579711, acc: 0.687500] [adversarial loss: 1.061859, acc: 0.328125]\n",
      "10962: [discriminator loss: 0.500818, acc: 0.750000] [adversarial loss: 1.465066, acc: 0.140625]\n",
      "10963: [discriminator loss: 0.626795, acc: 0.625000] [adversarial loss: 1.052789, acc: 0.296875]\n",
      "10964: [discriminator loss: 0.546945, acc: 0.718750] [adversarial loss: 1.443889, acc: 0.171875]\n",
      "10965: [discriminator loss: 0.574827, acc: 0.703125] [adversarial loss: 0.813424, acc: 0.468750]\n",
      "10966: [discriminator loss: 0.577269, acc: 0.703125] [adversarial loss: 1.109563, acc: 0.203125]\n",
      "10967: [discriminator loss: 0.589710, acc: 0.687500] [adversarial loss: 0.984260, acc: 0.359375]\n",
      "10968: [discriminator loss: 0.549062, acc: 0.695312] [adversarial loss: 1.415911, acc: 0.218750]\n",
      "10969: [discriminator loss: 0.603735, acc: 0.656250] [adversarial loss: 0.798169, acc: 0.609375]\n",
      "10970: [discriminator loss: 0.570569, acc: 0.671875] [adversarial loss: 1.318227, acc: 0.187500]\n",
      "10971: [discriminator loss: 0.588163, acc: 0.664062] [adversarial loss: 1.256442, acc: 0.250000]\n",
      "10972: [discriminator loss: 0.586191, acc: 0.632812] [adversarial loss: 1.225604, acc: 0.218750]\n",
      "10973: [discriminator loss: 0.511211, acc: 0.757812] [adversarial loss: 0.914904, acc: 0.406250]\n",
      "10974: [discriminator loss: 0.529738, acc: 0.757812] [adversarial loss: 1.155903, acc: 0.250000]\n",
      "10975: [discriminator loss: 0.566731, acc: 0.687500] [adversarial loss: 1.466560, acc: 0.171875]\n",
      "10976: [discriminator loss: 0.683156, acc: 0.617188] [adversarial loss: 0.976681, acc: 0.343750]\n",
      "10977: [discriminator loss: 0.606127, acc: 0.664062] [adversarial loss: 1.107856, acc: 0.203125]\n",
      "10978: [discriminator loss: 0.530908, acc: 0.734375] [adversarial loss: 1.036609, acc: 0.343750]\n",
      "10979: [discriminator loss: 0.530042, acc: 0.773438] [adversarial loss: 1.038740, acc: 0.265625]\n",
      "10980: [discriminator loss: 0.576871, acc: 0.679688] [adversarial loss: 0.918774, acc: 0.437500]\n",
      "10981: [discriminator loss: 0.545345, acc: 0.695312] [adversarial loss: 1.136511, acc: 0.281250]\n",
      "10982: [discriminator loss: 0.549642, acc: 0.718750] [adversarial loss: 1.059378, acc: 0.343750]\n",
      "10983: [discriminator loss: 0.546743, acc: 0.734375] [adversarial loss: 1.201189, acc: 0.328125]\n",
      "10984: [discriminator loss: 0.542428, acc: 0.765625] [adversarial loss: 1.313022, acc: 0.187500]\n",
      "10985: [discriminator loss: 0.551704, acc: 0.703125] [adversarial loss: 0.993847, acc: 0.406250]\n",
      "10986: [discriminator loss: 0.491564, acc: 0.757812] [adversarial loss: 1.321371, acc: 0.218750]\n",
      "10987: [discriminator loss: 0.597928, acc: 0.679688] [adversarial loss: 1.168610, acc: 0.218750]\n",
      "10988: [discriminator loss: 0.488434, acc: 0.750000] [adversarial loss: 1.078438, acc: 0.359375]\n",
      "10989: [discriminator loss: 0.545552, acc: 0.734375] [adversarial loss: 1.186150, acc: 0.296875]\n",
      "10990: [discriminator loss: 0.651044, acc: 0.625000] [adversarial loss: 0.837503, acc: 0.484375]\n",
      "10991: [discriminator loss: 0.550661, acc: 0.687500] [adversarial loss: 1.439360, acc: 0.156250]\n",
      "10992: [discriminator loss: 0.559695, acc: 0.710938] [adversarial loss: 0.937890, acc: 0.406250]\n",
      "10993: [discriminator loss: 0.606587, acc: 0.656250] [adversarial loss: 1.692956, acc: 0.093750]\n",
      "10994: [discriminator loss: 0.570169, acc: 0.632812] [adversarial loss: 0.894202, acc: 0.406250]\n",
      "10995: [discriminator loss: 0.587031, acc: 0.640625] [adversarial loss: 1.564102, acc: 0.125000]\n",
      "10996: [discriminator loss: 0.589912, acc: 0.679688] [adversarial loss: 0.914843, acc: 0.406250]\n",
      "10997: [discriminator loss: 0.501620, acc: 0.781250] [adversarial loss: 1.455858, acc: 0.140625]\n",
      "10998: [discriminator loss: 0.585315, acc: 0.687500] [adversarial loss: 0.790542, acc: 0.500000]\n",
      "10999: [discriminator loss: 0.572257, acc: 0.671875] [adversarial loss: 1.861435, acc: 0.062500]\n",
      "11000: [discriminator loss: 0.651851, acc: 0.609375] [adversarial loss: 0.878574, acc: 0.375000]\n",
      "11001: [discriminator loss: 0.573762, acc: 0.734375] [adversarial loss: 1.178342, acc: 0.203125]\n",
      "11002: [discriminator loss: 0.562078, acc: 0.695312] [adversarial loss: 1.161802, acc: 0.218750]\n",
      "11003: [discriminator loss: 0.575835, acc: 0.695312] [adversarial loss: 1.294751, acc: 0.203125]\n",
      "11004: [discriminator loss: 0.598753, acc: 0.726562] [adversarial loss: 0.914523, acc: 0.421875]\n",
      "11005: [discriminator loss: 0.575007, acc: 0.734375] [adversarial loss: 1.348415, acc: 0.187500]\n",
      "11006: [discriminator loss: 0.564963, acc: 0.679688] [adversarial loss: 1.151029, acc: 0.187500]\n",
      "11007: [discriminator loss: 0.582870, acc: 0.671875] [adversarial loss: 1.160235, acc: 0.296875]\n",
      "11008: [discriminator loss: 0.518211, acc: 0.757812] [adversarial loss: 1.121712, acc: 0.203125]\n",
      "11009: [discriminator loss: 0.525534, acc: 0.750000] [adversarial loss: 1.147244, acc: 0.140625]\n",
      "11010: [discriminator loss: 0.618377, acc: 0.617188] [adversarial loss: 1.003380, acc: 0.375000]\n",
      "11011: [discriminator loss: 0.570456, acc: 0.710938] [adversarial loss: 1.024210, acc: 0.281250]\n",
      "11012: [discriminator loss: 0.531712, acc: 0.734375] [adversarial loss: 1.208166, acc: 0.234375]\n",
      "11013: [discriminator loss: 0.514789, acc: 0.726562] [adversarial loss: 1.179421, acc: 0.265625]\n",
      "11014: [discriminator loss: 0.568545, acc: 0.656250] [adversarial loss: 1.175368, acc: 0.156250]\n",
      "11015: [discriminator loss: 0.596992, acc: 0.671875] [adversarial loss: 1.076818, acc: 0.218750]\n",
      "11016: [discriminator loss: 0.610174, acc: 0.648438] [adversarial loss: 1.169707, acc: 0.281250]\n",
      "11017: [discriminator loss: 0.547937, acc: 0.703125] [adversarial loss: 1.116881, acc: 0.203125]\n",
      "11018: [discriminator loss: 0.529809, acc: 0.757812] [adversarial loss: 1.264690, acc: 0.171875]\n",
      "11019: [discriminator loss: 0.564702, acc: 0.718750] [adversarial loss: 0.744516, acc: 0.468750]\n",
      "11020: [discriminator loss: 0.581912, acc: 0.703125] [adversarial loss: 1.527936, acc: 0.062500]\n",
      "11021: [discriminator loss: 0.555963, acc: 0.703125] [adversarial loss: 0.787605, acc: 0.468750]\n",
      "11022: [discriminator loss: 0.516653, acc: 0.773438] [adversarial loss: 1.383947, acc: 0.171875]\n",
      "11023: [discriminator loss: 0.509812, acc: 0.726562] [adversarial loss: 0.997092, acc: 0.343750]\n",
      "11024: [discriminator loss: 0.506280, acc: 0.765625] [adversarial loss: 1.451419, acc: 0.171875]\n",
      "11025: [discriminator loss: 0.555231, acc: 0.671875] [adversarial loss: 1.202262, acc: 0.156250]\n",
      "11026: [discriminator loss: 0.538163, acc: 0.718750] [adversarial loss: 1.011761, acc: 0.281250]\n",
      "11027: [discriminator loss: 0.588258, acc: 0.648438] [adversarial loss: 0.981299, acc: 0.359375]\n",
      "11028: [discriminator loss: 0.547032, acc: 0.726562] [adversarial loss: 0.996085, acc: 0.343750]\n",
      "11029: [discriminator loss: 0.574914, acc: 0.710938] [adversarial loss: 1.352437, acc: 0.109375]\n",
      "11030: [discriminator loss: 0.527179, acc: 0.710938] [adversarial loss: 1.047043, acc: 0.265625]\n",
      "11031: [discriminator loss: 0.528551, acc: 0.734375] [adversarial loss: 1.275684, acc: 0.125000]\n",
      "11032: [discriminator loss: 0.531069, acc: 0.734375] [adversarial loss: 0.960773, acc: 0.281250]\n",
      "11033: [discriminator loss: 0.548494, acc: 0.742188] [adversarial loss: 1.426088, acc: 0.187500]\n",
      "11034: [discriminator loss: 0.570853, acc: 0.703125] [adversarial loss: 0.988382, acc: 0.359375]\n",
      "11035: [discriminator loss: 0.543099, acc: 0.703125] [adversarial loss: 1.201808, acc: 0.171875]\n",
      "11036: [discriminator loss: 0.533827, acc: 0.679688] [adversarial loss: 1.208576, acc: 0.265625]\n",
      "11037: [discriminator loss: 0.578126, acc: 0.687500] [adversarial loss: 1.225223, acc: 0.359375]\n",
      "11038: [discriminator loss: 0.567876, acc: 0.679688] [adversarial loss: 0.950194, acc: 0.359375]\n",
      "11039: [discriminator loss: 0.592225, acc: 0.695312] [adversarial loss: 1.374200, acc: 0.171875]\n",
      "11040: [discriminator loss: 0.613292, acc: 0.648438] [adversarial loss: 1.087064, acc: 0.359375]\n",
      "11041: [discriminator loss: 0.595958, acc: 0.703125] [adversarial loss: 1.279263, acc: 0.171875]\n",
      "11042: [discriminator loss: 0.565866, acc: 0.679688] [adversarial loss: 1.407354, acc: 0.109375]\n",
      "11043: [discriminator loss: 0.573447, acc: 0.679688] [adversarial loss: 0.863735, acc: 0.406250]\n",
      "11044: [discriminator loss: 0.615989, acc: 0.671875] [adversarial loss: 1.358760, acc: 0.109375]\n",
      "11045: [discriminator loss: 0.502506, acc: 0.742188] [adversarial loss: 0.843189, acc: 0.453125]\n",
      "11046: [discriminator loss: 0.533262, acc: 0.750000] [adversarial loss: 1.341334, acc: 0.187500]\n",
      "11047: [discriminator loss: 0.557202, acc: 0.679688] [adversarial loss: 1.047475, acc: 0.265625]\n",
      "11048: [discriminator loss: 0.562548, acc: 0.726562] [adversarial loss: 1.118232, acc: 0.218750]\n",
      "11049: [discriminator loss: 0.508353, acc: 0.726562] [adversarial loss: 0.976268, acc: 0.343750]\n",
      "11050: [discriminator loss: 0.498378, acc: 0.757812] [adversarial loss: 1.692083, acc: 0.062500]\n",
      "11051: [discriminator loss: 0.532009, acc: 0.742188] [adversarial loss: 0.808908, acc: 0.453125]\n",
      "11052: [discriminator loss: 0.515489, acc: 0.742188] [adversarial loss: 1.114895, acc: 0.296875]\n",
      "11053: [discriminator loss: 0.550497, acc: 0.710938] [adversarial loss: 1.193417, acc: 0.250000]\n",
      "11054: [discriminator loss: 0.512760, acc: 0.734375] [adversarial loss: 1.096281, acc: 0.296875]\n",
      "11055: [discriminator loss: 0.565677, acc: 0.726562] [adversarial loss: 1.105459, acc: 0.265625]\n",
      "11056: [discriminator loss: 0.581985, acc: 0.679688] [adversarial loss: 1.415468, acc: 0.140625]\n",
      "11057: [discriminator loss: 0.493637, acc: 0.750000] [adversarial loss: 1.106706, acc: 0.250000]\n",
      "11058: [discriminator loss: 0.587570, acc: 0.648438] [adversarial loss: 1.457832, acc: 0.156250]\n",
      "11059: [discriminator loss: 0.500726, acc: 0.734375] [adversarial loss: 1.140311, acc: 0.281250]\n",
      "11060: [discriminator loss: 0.518338, acc: 0.750000] [adversarial loss: 1.363258, acc: 0.156250]\n",
      "11061: [discriminator loss: 0.516681, acc: 0.703125] [adversarial loss: 1.134500, acc: 0.234375]\n",
      "11062: [discriminator loss: 0.585162, acc: 0.726562] [adversarial loss: 0.978627, acc: 0.296875]\n",
      "11063: [discriminator loss: 0.547088, acc: 0.718750] [adversarial loss: 1.138909, acc: 0.296875]\n",
      "11064: [discriminator loss: 0.512367, acc: 0.742188] [adversarial loss: 1.302184, acc: 0.265625]\n",
      "11065: [discriminator loss: 0.571655, acc: 0.726562] [adversarial loss: 1.224741, acc: 0.234375]\n",
      "11066: [discriminator loss: 0.490664, acc: 0.750000] [adversarial loss: 1.021513, acc: 0.312500]\n",
      "11067: [discriminator loss: 0.466048, acc: 0.796875] [adversarial loss: 1.346128, acc: 0.140625]\n",
      "11068: [discriminator loss: 0.612984, acc: 0.679688] [adversarial loss: 1.012134, acc: 0.312500]\n",
      "11069: [discriminator loss: 0.572517, acc: 0.695312] [adversarial loss: 1.226509, acc: 0.171875]\n",
      "11070: [discriminator loss: 0.575658, acc: 0.687500] [adversarial loss: 0.835779, acc: 0.406250]\n",
      "11071: [discriminator loss: 0.532522, acc: 0.726562] [adversarial loss: 1.337645, acc: 0.125000]\n",
      "11072: [discriminator loss: 0.535787, acc: 0.750000] [adversarial loss: 0.877950, acc: 0.343750]\n",
      "11073: [discriminator loss: 0.566117, acc: 0.703125] [adversarial loss: 1.550650, acc: 0.093750]\n",
      "11074: [discriminator loss: 0.641297, acc: 0.671875] [adversarial loss: 0.767573, acc: 0.468750]\n",
      "11075: [discriminator loss: 0.569356, acc: 0.695312] [adversarial loss: 1.679763, acc: 0.031250]\n",
      "11076: [discriminator loss: 0.578616, acc: 0.695312] [adversarial loss: 0.841575, acc: 0.421875]\n",
      "11077: [discriminator loss: 0.585485, acc: 0.695312] [adversarial loss: 1.328357, acc: 0.125000]\n",
      "11078: [discriminator loss: 0.550024, acc: 0.734375] [adversarial loss: 0.921005, acc: 0.343750]\n",
      "11079: [discriminator loss: 0.529453, acc: 0.734375] [adversarial loss: 1.282755, acc: 0.171875]\n",
      "11080: [discriminator loss: 0.561483, acc: 0.671875] [adversarial loss: 1.044106, acc: 0.281250]\n",
      "11081: [discriminator loss: 0.526404, acc: 0.718750] [adversarial loss: 1.360516, acc: 0.140625]\n",
      "11082: [discriminator loss: 0.491241, acc: 0.773438] [adversarial loss: 1.157213, acc: 0.234375]\n",
      "11083: [discriminator loss: 0.517612, acc: 0.742188] [adversarial loss: 1.299962, acc: 0.250000]\n",
      "11084: [discriminator loss: 0.595352, acc: 0.679688] [adversarial loss: 1.204465, acc: 0.250000]\n",
      "11085: [discriminator loss: 0.547155, acc: 0.757812] [adversarial loss: 1.208008, acc: 0.187500]\n",
      "11086: [discriminator loss: 0.595757, acc: 0.679688] [adversarial loss: 0.990569, acc: 0.375000]\n",
      "11087: [discriminator loss: 0.599913, acc: 0.656250] [adversarial loss: 1.632252, acc: 0.093750]\n",
      "11088: [discriminator loss: 0.507550, acc: 0.757812] [adversarial loss: 0.896077, acc: 0.421875]\n",
      "11089: [discriminator loss: 0.502622, acc: 0.742188] [adversarial loss: 1.308681, acc: 0.187500]\n",
      "11090: [discriminator loss: 0.645802, acc: 0.648438] [adversarial loss: 0.936543, acc: 0.437500]\n",
      "11091: [discriminator loss: 0.602370, acc: 0.679688] [adversarial loss: 1.336573, acc: 0.093750]\n",
      "11092: [discriminator loss: 0.537991, acc: 0.757812] [adversarial loss: 1.225173, acc: 0.265625]\n",
      "11093: [discriminator loss: 0.558832, acc: 0.726562] [adversarial loss: 1.318915, acc: 0.156250]\n",
      "11094: [discriminator loss: 0.588773, acc: 0.671875] [adversarial loss: 1.078621, acc: 0.312500]\n",
      "11095: [discriminator loss: 0.581267, acc: 0.679688] [adversarial loss: 1.267609, acc: 0.109375]\n",
      "11096: [discriminator loss: 0.481180, acc: 0.789062] [adversarial loss: 1.021739, acc: 0.281250]\n",
      "11097: [discriminator loss: 0.572963, acc: 0.656250] [adversarial loss: 1.115932, acc: 0.296875]\n",
      "11098: [discriminator loss: 0.547721, acc: 0.710938] [adversarial loss: 1.173049, acc: 0.234375]\n",
      "11099: [discriminator loss: 0.572400, acc: 0.695312] [adversarial loss: 0.913831, acc: 0.421875]\n",
      "11100: [discriminator loss: 0.485003, acc: 0.742188] [adversarial loss: 1.271500, acc: 0.171875]\n",
      "11101: [discriminator loss: 0.482327, acc: 0.765625] [adversarial loss: 1.154076, acc: 0.250000]\n",
      "11102: [discriminator loss: 0.559207, acc: 0.703125] [adversarial loss: 1.202080, acc: 0.125000]\n",
      "11103: [discriminator loss: 0.516594, acc: 0.773438] [adversarial loss: 1.122208, acc: 0.250000]\n",
      "11104: [discriminator loss: 0.557538, acc: 0.679688] [adversarial loss: 0.934661, acc: 0.421875]\n",
      "11105: [discriminator loss: 0.605803, acc: 0.664062] [adversarial loss: 1.446836, acc: 0.187500]\n",
      "11106: [discriminator loss: 0.574127, acc: 0.695312] [adversarial loss: 0.838669, acc: 0.468750]\n",
      "11107: [discriminator loss: 0.578855, acc: 0.695312] [adversarial loss: 1.232320, acc: 0.187500]\n",
      "11108: [discriminator loss: 0.542403, acc: 0.710938] [adversarial loss: 1.030674, acc: 0.343750]\n",
      "11109: [discriminator loss: 0.567223, acc: 0.726562] [adversarial loss: 1.022681, acc: 0.312500]\n",
      "11110: [discriminator loss: 0.540792, acc: 0.742188] [adversarial loss: 1.352936, acc: 0.156250]\n",
      "11111: [discriminator loss: 0.527409, acc: 0.726562] [adversarial loss: 0.963292, acc: 0.421875]\n",
      "11112: [discriminator loss: 0.599206, acc: 0.648438] [adversarial loss: 1.668150, acc: 0.156250]\n",
      "11113: [discriminator loss: 0.652040, acc: 0.601562] [adversarial loss: 0.845297, acc: 0.468750]\n",
      "11114: [discriminator loss: 0.532008, acc: 0.726562] [adversarial loss: 1.429450, acc: 0.093750]\n",
      "11115: [discriminator loss: 0.628560, acc: 0.640625] [adversarial loss: 0.846991, acc: 0.437500]\n",
      "11116: [discriminator loss: 0.556252, acc: 0.703125] [adversarial loss: 1.263941, acc: 0.093750]\n",
      "11117: [discriminator loss: 0.486444, acc: 0.789062] [adversarial loss: 1.119536, acc: 0.203125]\n",
      "11118: [discriminator loss: 0.519873, acc: 0.726562] [adversarial loss: 1.403020, acc: 0.140625]\n",
      "11119: [discriminator loss: 0.646403, acc: 0.656250] [adversarial loss: 0.974179, acc: 0.328125]\n",
      "11120: [discriminator loss: 0.573838, acc: 0.648438] [adversarial loss: 1.446671, acc: 0.062500]\n",
      "11121: [discriminator loss: 0.622055, acc: 0.648438] [adversarial loss: 0.900241, acc: 0.406250]\n",
      "11122: [discriminator loss: 0.593270, acc: 0.679688] [adversarial loss: 1.274255, acc: 0.187500]\n",
      "11123: [discriminator loss: 0.515819, acc: 0.734375] [adversarial loss: 0.799827, acc: 0.453125]\n",
      "11124: [discriminator loss: 0.469938, acc: 0.765625] [adversarial loss: 1.356502, acc: 0.218750]\n",
      "11125: [discriminator loss: 0.563748, acc: 0.679688] [adversarial loss: 1.001278, acc: 0.359375]\n",
      "11126: [discriminator loss: 0.577097, acc: 0.671875] [adversarial loss: 1.114950, acc: 0.281250]\n",
      "11127: [discriminator loss: 0.495325, acc: 0.765625] [adversarial loss: 1.170277, acc: 0.250000]\n",
      "11128: [discriminator loss: 0.584082, acc: 0.742188] [adversarial loss: 1.466386, acc: 0.203125]\n",
      "11129: [discriminator loss: 0.587379, acc: 0.703125] [adversarial loss: 0.866778, acc: 0.468750]\n",
      "11130: [discriminator loss: 0.565114, acc: 0.734375] [adversarial loss: 1.381139, acc: 0.171875]\n",
      "11131: [discriminator loss: 0.534901, acc: 0.734375] [adversarial loss: 0.996021, acc: 0.375000]\n",
      "11132: [discriminator loss: 0.595728, acc: 0.687500] [adversarial loss: 0.873078, acc: 0.406250]\n",
      "11133: [discriminator loss: 0.553289, acc: 0.664062] [adversarial loss: 1.394087, acc: 0.156250]\n",
      "11134: [discriminator loss: 0.504494, acc: 0.734375] [adversarial loss: 1.016227, acc: 0.296875]\n",
      "11135: [discriminator loss: 0.559083, acc: 0.664062] [adversarial loss: 1.206722, acc: 0.218750]\n",
      "11136: [discriminator loss: 0.549944, acc: 0.695312] [adversarial loss: 1.269225, acc: 0.234375]\n",
      "11137: [discriminator loss: 0.480601, acc: 0.773438] [adversarial loss: 1.200918, acc: 0.250000]\n",
      "11138: [discriminator loss: 0.569276, acc: 0.625000] [adversarial loss: 1.135354, acc: 0.328125]\n",
      "11139: [discriminator loss: 0.498649, acc: 0.718750] [adversarial loss: 1.274691, acc: 0.156250]\n",
      "11140: [discriminator loss: 0.606866, acc: 0.640625] [adversarial loss: 0.894086, acc: 0.468750]\n",
      "11141: [discriminator loss: 0.581029, acc: 0.710938] [adversarial loss: 1.802664, acc: 0.125000]\n",
      "11142: [discriminator loss: 0.660675, acc: 0.640625] [adversarial loss: 0.815856, acc: 0.421875]\n",
      "11143: [discriminator loss: 0.698934, acc: 0.617188] [adversarial loss: 1.498760, acc: 0.078125]\n",
      "11144: [discriminator loss: 0.632968, acc: 0.640625] [adversarial loss: 1.003080, acc: 0.312500]\n",
      "11145: [discriminator loss: 0.526366, acc: 0.734375] [adversarial loss: 1.225365, acc: 0.140625]\n",
      "11146: [discriminator loss: 0.563843, acc: 0.687500] [adversarial loss: 1.024477, acc: 0.234375]\n",
      "11147: [discriminator loss: 0.527403, acc: 0.726562] [adversarial loss: 1.212009, acc: 0.296875]\n",
      "11148: [discriminator loss: 0.521257, acc: 0.750000] [adversarial loss: 0.916497, acc: 0.468750]\n",
      "11149: [discriminator loss: 0.534324, acc: 0.750000] [adversarial loss: 1.282547, acc: 0.218750]\n",
      "11150: [discriminator loss: 0.500238, acc: 0.750000] [adversarial loss: 1.101779, acc: 0.312500]\n",
      "11151: [discriminator loss: 0.539417, acc: 0.671875] [adversarial loss: 1.387166, acc: 0.187500]\n",
      "11152: [discriminator loss: 0.507840, acc: 0.742188] [adversarial loss: 0.976293, acc: 0.390625]\n",
      "11153: [discriminator loss: 0.589933, acc: 0.687500] [adversarial loss: 1.349811, acc: 0.156250]\n",
      "11154: [discriminator loss: 0.543717, acc: 0.695312] [adversarial loss: 1.133754, acc: 0.265625]\n",
      "11155: [discriminator loss: 0.516834, acc: 0.742188] [adversarial loss: 1.224862, acc: 0.171875]\n",
      "11156: [discriminator loss: 0.528210, acc: 0.703125] [adversarial loss: 0.984934, acc: 0.250000]\n",
      "11157: [discriminator loss: 0.539456, acc: 0.710938] [adversarial loss: 1.322969, acc: 0.156250]\n",
      "11158: [discriminator loss: 0.545274, acc: 0.718750] [adversarial loss: 0.928833, acc: 0.328125]\n",
      "11159: [discriminator loss: 0.565230, acc: 0.679688] [adversarial loss: 1.351592, acc: 0.109375]\n",
      "11160: [discriminator loss: 0.561591, acc: 0.718750] [adversarial loss: 0.865424, acc: 0.468750]\n",
      "11161: [discriminator loss: 0.591709, acc: 0.664062] [adversarial loss: 1.216073, acc: 0.156250]\n",
      "11162: [discriminator loss: 0.556139, acc: 0.726562] [adversarial loss: 1.279092, acc: 0.218750]\n",
      "11163: [discriminator loss: 0.662005, acc: 0.617188] [adversarial loss: 1.274017, acc: 0.093750]\n",
      "11164: [discriminator loss: 0.628394, acc: 0.625000] [adversarial loss: 0.942132, acc: 0.328125]\n",
      "11165: [discriminator loss: 0.592661, acc: 0.703125] [adversarial loss: 1.356476, acc: 0.187500]\n",
      "11166: [discriminator loss: 0.561165, acc: 0.703125] [adversarial loss: 0.780868, acc: 0.562500]\n",
      "11167: [discriminator loss: 0.625050, acc: 0.648438] [adversarial loss: 1.451664, acc: 0.109375]\n",
      "11168: [discriminator loss: 0.625038, acc: 0.671875] [adversarial loss: 0.953439, acc: 0.359375]\n",
      "11169: [discriminator loss: 0.511630, acc: 0.773438] [adversarial loss: 1.144266, acc: 0.187500]\n",
      "11170: [discriminator loss: 0.513896, acc: 0.734375] [adversarial loss: 1.208747, acc: 0.265625]\n",
      "11171: [discriminator loss: 0.593071, acc: 0.671875] [adversarial loss: 1.037815, acc: 0.375000]\n",
      "11172: [discriminator loss: 0.570033, acc: 0.726562] [adversarial loss: 1.448904, acc: 0.093750]\n",
      "11173: [discriminator loss: 0.521257, acc: 0.625000] [adversarial loss: 0.847277, acc: 0.437500]\n",
      "11174: [discriminator loss: 0.588907, acc: 0.656250] [adversarial loss: 1.479949, acc: 0.125000]\n",
      "11175: [discriminator loss: 0.588853, acc: 0.671875] [adversarial loss: 0.823277, acc: 0.468750]\n",
      "11176: [discriminator loss: 0.570815, acc: 0.703125] [adversarial loss: 1.457268, acc: 0.156250]\n",
      "11177: [discriminator loss: 0.669610, acc: 0.671875] [adversarial loss: 0.735918, acc: 0.468750]\n",
      "11178: [discriminator loss: 0.536955, acc: 0.757812] [adversarial loss: 1.197370, acc: 0.265625]\n",
      "11179: [discriminator loss: 0.505687, acc: 0.781250] [adversarial loss: 1.146292, acc: 0.265625]\n",
      "11180: [discriminator loss: 0.536037, acc: 0.726562] [adversarial loss: 0.980020, acc: 0.359375]\n",
      "11181: [discriminator loss: 0.489675, acc: 0.742188] [adversarial loss: 1.102607, acc: 0.281250]\n",
      "11182: [discriminator loss: 0.547577, acc: 0.734375] [adversarial loss: 1.008906, acc: 0.328125]\n",
      "11183: [discriminator loss: 0.542924, acc: 0.750000] [adversarial loss: 1.350718, acc: 0.140625]\n",
      "11184: [discriminator loss: 0.539271, acc: 0.695312] [adversarial loss: 1.120549, acc: 0.296875]\n",
      "11185: [discriminator loss: 0.536109, acc: 0.710938] [adversarial loss: 0.977558, acc: 0.281250]\n",
      "11186: [discriminator loss: 0.589529, acc: 0.632812] [adversarial loss: 1.056844, acc: 0.218750]\n",
      "11187: [discriminator loss: 0.522059, acc: 0.773438] [adversarial loss: 1.163087, acc: 0.234375]\n",
      "11188: [discriminator loss: 0.473442, acc: 0.820312] [adversarial loss: 0.959948, acc: 0.312500]\n",
      "11189: [discriminator loss: 0.622351, acc: 0.695312] [adversarial loss: 1.171370, acc: 0.171875]\n",
      "11190: [discriminator loss: 0.505336, acc: 0.773438] [adversarial loss: 1.405403, acc: 0.093750]\n",
      "11191: [discriminator loss: 0.612313, acc: 0.687500] [adversarial loss: 1.086252, acc: 0.265625]\n",
      "11192: [discriminator loss: 0.590321, acc: 0.695312] [adversarial loss: 1.196209, acc: 0.203125]\n",
      "11193: [discriminator loss: 0.468832, acc: 0.796875] [adversarial loss: 1.209577, acc: 0.281250]\n",
      "11194: [discriminator loss: 0.577472, acc: 0.679688] [adversarial loss: 1.130004, acc: 0.218750]\n",
      "11195: [discriminator loss: 0.543980, acc: 0.679688] [adversarial loss: 0.960185, acc: 0.328125]\n",
      "11196: [discriminator loss: 0.533697, acc: 0.726562] [adversarial loss: 1.298283, acc: 0.203125]\n",
      "11197: [discriminator loss: 0.571622, acc: 0.710938] [adversarial loss: 0.829681, acc: 0.500000]\n",
      "11198: [discriminator loss: 0.539486, acc: 0.687500] [adversarial loss: 1.482004, acc: 0.140625]\n",
      "11199: [discriminator loss: 0.535313, acc: 0.726562] [adversarial loss: 0.852015, acc: 0.468750]\n",
      "11200: [discriminator loss: 0.618592, acc: 0.671875] [adversarial loss: 1.294229, acc: 0.156250]\n",
      "11201: [discriminator loss: 0.644524, acc: 0.617188] [adversarial loss: 1.014944, acc: 0.406250]\n",
      "11202: [discriminator loss: 0.582979, acc: 0.640625] [adversarial loss: 1.517622, acc: 0.109375]\n",
      "11203: [discriminator loss: 0.587827, acc: 0.679688] [adversarial loss: 0.940680, acc: 0.375000]\n",
      "11204: [discriminator loss: 0.568367, acc: 0.703125] [adversarial loss: 1.378008, acc: 0.234375]\n",
      "11205: [discriminator loss: 0.572893, acc: 0.671875] [adversarial loss: 1.011483, acc: 0.375000]\n",
      "11206: [discriminator loss: 0.519265, acc: 0.765625] [adversarial loss: 0.907677, acc: 0.453125]\n",
      "11207: [discriminator loss: 0.547830, acc: 0.695312] [adversarial loss: 1.430433, acc: 0.140625]\n",
      "11208: [discriminator loss: 0.581930, acc: 0.695312] [adversarial loss: 1.047913, acc: 0.265625]\n",
      "11209: [discriminator loss: 0.525776, acc: 0.750000] [adversarial loss: 1.079484, acc: 0.218750]\n",
      "11210: [discriminator loss: 0.511153, acc: 0.726562] [adversarial loss: 1.479614, acc: 0.093750]\n",
      "11211: [discriminator loss: 0.495310, acc: 0.750000] [adversarial loss: 0.902369, acc: 0.421875]\n",
      "11212: [discriminator loss: 0.556710, acc: 0.726562] [adversarial loss: 1.516657, acc: 0.171875]\n",
      "11213: [discriminator loss: 0.622310, acc: 0.671875] [adversarial loss: 0.780382, acc: 0.468750]\n",
      "11214: [discriminator loss: 0.556214, acc: 0.734375] [adversarial loss: 1.365646, acc: 0.171875]\n",
      "11215: [discriminator loss: 0.572344, acc: 0.687500] [adversarial loss: 1.060250, acc: 0.281250]\n",
      "11216: [discriminator loss: 0.531217, acc: 0.726562] [adversarial loss: 1.186062, acc: 0.250000]\n",
      "11217: [discriminator loss: 0.529052, acc: 0.718750] [adversarial loss: 1.082984, acc: 0.328125]\n",
      "11218: [discriminator loss: 0.528199, acc: 0.742188] [adversarial loss: 1.445950, acc: 0.171875]\n",
      "11219: [discriminator loss: 0.619358, acc: 0.648438] [adversarial loss: 0.839835, acc: 0.468750]\n",
      "11220: [discriminator loss: 0.547037, acc: 0.750000] [adversarial loss: 1.510787, acc: 0.140625]\n",
      "11221: [discriminator loss: 0.603074, acc: 0.687500] [adversarial loss: 0.853408, acc: 0.468750]\n",
      "11222: [discriminator loss: 0.620153, acc: 0.687500] [adversarial loss: 1.433314, acc: 0.109375]\n",
      "11223: [discriminator loss: 0.544481, acc: 0.687500] [adversarial loss: 1.067243, acc: 0.250000]\n",
      "11224: [discriminator loss: 0.553188, acc: 0.718750] [adversarial loss: 1.207257, acc: 0.218750]\n",
      "11225: [discriminator loss: 0.588623, acc: 0.632812] [adversarial loss: 0.807335, acc: 0.437500]\n",
      "11226: [discriminator loss: 0.556430, acc: 0.718750] [adversarial loss: 1.337170, acc: 0.203125]\n",
      "11227: [discriminator loss: 0.630221, acc: 0.632812] [adversarial loss: 0.872822, acc: 0.421875]\n",
      "11228: [discriminator loss: 0.589526, acc: 0.742188] [adversarial loss: 1.494909, acc: 0.140625]\n",
      "11229: [discriminator loss: 0.533597, acc: 0.710938] [adversarial loss: 1.107731, acc: 0.234375]\n",
      "11230: [discriminator loss: 0.477620, acc: 0.796875] [adversarial loss: 1.521373, acc: 0.093750]\n",
      "11231: [discriminator loss: 0.650134, acc: 0.632812] [adversarial loss: 0.931162, acc: 0.359375]\n",
      "11232: [discriminator loss: 0.505004, acc: 0.773438] [adversarial loss: 1.429242, acc: 0.140625]\n",
      "11233: [discriminator loss: 0.480753, acc: 0.773438] [adversarial loss: 1.178067, acc: 0.281250]\n",
      "11234: [discriminator loss: 0.522366, acc: 0.726562] [adversarial loss: 0.867671, acc: 0.421875]\n",
      "11235: [discriminator loss: 0.538156, acc: 0.703125] [adversarial loss: 1.314213, acc: 0.171875]\n",
      "11236: [discriminator loss: 0.527025, acc: 0.703125] [adversarial loss: 1.076513, acc: 0.296875]\n",
      "11237: [discriminator loss: 0.533674, acc: 0.703125] [adversarial loss: 1.194347, acc: 0.203125]\n",
      "11238: [discriminator loss: 0.519511, acc: 0.718750] [adversarial loss: 1.061787, acc: 0.281250]\n",
      "11239: [discriminator loss: 0.555228, acc: 0.687500] [adversarial loss: 1.245477, acc: 0.218750]\n",
      "11240: [discriminator loss: 0.559993, acc: 0.687500] [adversarial loss: 0.901538, acc: 0.437500]\n",
      "11241: [discriminator loss: 0.536077, acc: 0.765625] [adversarial loss: 1.232307, acc: 0.250000]\n",
      "11242: [discriminator loss: 0.611859, acc: 0.671875] [adversarial loss: 1.010857, acc: 0.343750]\n",
      "11243: [discriminator loss: 0.509704, acc: 0.734375] [adversarial loss: 1.822512, acc: 0.078125]\n",
      "11244: [discriminator loss: 0.559827, acc: 0.718750] [adversarial loss: 0.937365, acc: 0.359375]\n",
      "11245: [discriminator loss: 0.506014, acc: 0.726562] [adversarial loss: 1.433086, acc: 0.125000]\n",
      "11246: [discriminator loss: 0.581427, acc: 0.679688] [adversarial loss: 1.000509, acc: 0.281250]\n",
      "11247: [discriminator loss: 0.544808, acc: 0.726562] [adversarial loss: 1.071327, acc: 0.203125]\n",
      "11248: [discriminator loss: 0.576881, acc: 0.710938] [adversarial loss: 1.269304, acc: 0.203125]\n",
      "11249: [discriminator loss: 0.494246, acc: 0.789062] [adversarial loss: 0.916027, acc: 0.406250]\n",
      "11250: [discriminator loss: 0.582448, acc: 0.679688] [adversarial loss: 1.348424, acc: 0.125000]\n",
      "11251: [discriminator loss: 0.472158, acc: 0.820312] [adversarial loss: 0.749039, acc: 0.593750]\n",
      "11252: [discriminator loss: 0.509349, acc: 0.679688] [adversarial loss: 1.514736, acc: 0.140625]\n",
      "11253: [discriminator loss: 0.641718, acc: 0.640625] [adversarial loss: 0.961766, acc: 0.359375]\n",
      "11254: [discriminator loss: 0.541708, acc: 0.710938] [adversarial loss: 1.180766, acc: 0.250000]\n",
      "11255: [discriminator loss: 0.625471, acc: 0.617188] [adversarial loss: 1.111078, acc: 0.281250]\n",
      "11256: [discriminator loss: 0.549654, acc: 0.703125] [adversarial loss: 1.444999, acc: 0.187500]\n",
      "11257: [discriminator loss: 0.568467, acc: 0.703125] [adversarial loss: 1.186628, acc: 0.218750]\n",
      "11258: [discriminator loss: 0.601922, acc: 0.648438] [adversarial loss: 1.331619, acc: 0.171875]\n",
      "11259: [discriminator loss: 0.511753, acc: 0.757812] [adversarial loss: 0.899826, acc: 0.406250]\n",
      "11260: [discriminator loss: 0.517074, acc: 0.742188] [adversarial loss: 1.076678, acc: 0.296875]\n",
      "11261: [discriminator loss: 0.578250, acc: 0.726562] [adversarial loss: 1.226563, acc: 0.250000]\n",
      "11262: [discriminator loss: 0.624073, acc: 0.640625] [adversarial loss: 0.780167, acc: 0.500000]\n",
      "11263: [discriminator loss: 0.583713, acc: 0.671875] [adversarial loss: 1.634217, acc: 0.093750]\n",
      "11264: [discriminator loss: 0.610247, acc: 0.664062] [adversarial loss: 0.810800, acc: 0.468750]\n",
      "11265: [discriminator loss: 0.540696, acc: 0.734375] [adversarial loss: 1.538302, acc: 0.140625]\n",
      "11266: [discriminator loss: 0.554618, acc: 0.664062] [adversarial loss: 0.806462, acc: 0.546875]\n",
      "11267: [discriminator loss: 0.558875, acc: 0.726562] [adversarial loss: 1.337549, acc: 0.171875]\n",
      "11268: [discriminator loss: 0.550328, acc: 0.734375] [adversarial loss: 0.980734, acc: 0.359375]\n",
      "11269: [discriminator loss: 0.608553, acc: 0.687500] [adversarial loss: 1.321210, acc: 0.187500]\n",
      "11270: [discriminator loss: 0.513467, acc: 0.710938] [adversarial loss: 1.052549, acc: 0.359375]\n",
      "11271: [discriminator loss: 0.561184, acc: 0.687500] [adversarial loss: 1.109583, acc: 0.281250]\n",
      "11272: [discriminator loss: 0.540033, acc: 0.695312] [adversarial loss: 1.146563, acc: 0.250000]\n",
      "11273: [discriminator loss: 0.554698, acc: 0.703125] [adversarial loss: 1.062155, acc: 0.359375]\n",
      "11274: [discriminator loss: 0.528353, acc: 0.734375] [adversarial loss: 0.948071, acc: 0.390625]\n",
      "11275: [discriminator loss: 0.618427, acc: 0.648438] [adversarial loss: 1.089444, acc: 0.312500]\n",
      "11276: [discriminator loss: 0.535607, acc: 0.726562] [adversarial loss: 1.141733, acc: 0.265625]\n",
      "11277: [discriminator loss: 0.538779, acc: 0.710938] [adversarial loss: 1.393352, acc: 0.156250]\n",
      "11278: [discriminator loss: 0.566443, acc: 0.726562] [adversarial loss: 0.949133, acc: 0.375000]\n",
      "11279: [discriminator loss: 0.543507, acc: 0.734375] [adversarial loss: 1.229698, acc: 0.265625]\n",
      "11280: [discriminator loss: 0.574032, acc: 0.671875] [adversarial loss: 1.284315, acc: 0.203125]\n",
      "11281: [discriminator loss: 0.522316, acc: 0.796875] [adversarial loss: 1.200763, acc: 0.187500]\n",
      "11282: [discriminator loss: 0.535050, acc: 0.742188] [adversarial loss: 1.077697, acc: 0.250000]\n",
      "11283: [discriminator loss: 0.548236, acc: 0.726562] [adversarial loss: 1.256026, acc: 0.187500]\n",
      "11284: [discriminator loss: 0.638406, acc: 0.625000] [adversarial loss: 1.091756, acc: 0.250000]\n",
      "11285: [discriminator loss: 0.586360, acc: 0.640625] [adversarial loss: 1.135626, acc: 0.312500]\n",
      "11286: [discriminator loss: 0.508965, acc: 0.781250] [adversarial loss: 1.182230, acc: 0.265625]\n",
      "11287: [discriminator loss: 0.559726, acc: 0.703125] [adversarial loss: 0.895553, acc: 0.343750]\n",
      "11288: [discriminator loss: 0.620322, acc: 0.695312] [adversarial loss: 1.381007, acc: 0.187500]\n",
      "11289: [discriminator loss: 0.575828, acc: 0.726562] [adversarial loss: 0.812908, acc: 0.421875]\n",
      "11290: [discriminator loss: 0.550004, acc: 0.710938] [adversarial loss: 1.643174, acc: 0.078125]\n",
      "11291: [discriminator loss: 0.508750, acc: 0.726562] [adversarial loss: 0.964346, acc: 0.453125]\n",
      "11292: [discriminator loss: 0.556462, acc: 0.710938] [adversarial loss: 1.285070, acc: 0.234375]\n",
      "11293: [discriminator loss: 0.570971, acc: 0.710938] [adversarial loss: 0.955040, acc: 0.375000]\n",
      "11294: [discriminator loss: 0.489750, acc: 0.726562] [adversarial loss: 1.288342, acc: 0.203125]\n",
      "11295: [discriminator loss: 0.509936, acc: 0.718750] [adversarial loss: 1.095679, acc: 0.250000]\n",
      "11296: [discriminator loss: 0.537342, acc: 0.710938] [adversarial loss: 1.465655, acc: 0.156250]\n",
      "11297: [discriminator loss: 0.582169, acc: 0.664062] [adversarial loss: 1.052281, acc: 0.265625]\n",
      "11298: [discriminator loss: 0.589119, acc: 0.718750] [adversarial loss: 1.288316, acc: 0.171875]\n",
      "11299: [discriminator loss: 0.588568, acc: 0.671875] [adversarial loss: 0.908746, acc: 0.421875]\n",
      "11300: [discriminator loss: 0.539039, acc: 0.710938] [adversarial loss: 1.277209, acc: 0.140625]\n",
      "11301: [discriminator loss: 0.611902, acc: 0.664062] [adversarial loss: 0.917292, acc: 0.453125]\n",
      "11302: [discriminator loss: 0.622726, acc: 0.695312] [adversarial loss: 1.306636, acc: 0.250000]\n",
      "11303: [discriminator loss: 0.532203, acc: 0.742188] [adversarial loss: 1.062020, acc: 0.296875]\n",
      "11304: [discriminator loss: 0.532452, acc: 0.718750] [adversarial loss: 1.310017, acc: 0.218750]\n",
      "11305: [discriminator loss: 0.584475, acc: 0.679688] [adversarial loss: 1.037200, acc: 0.375000]\n",
      "11306: [discriminator loss: 0.528682, acc: 0.742188] [adversarial loss: 1.139111, acc: 0.265625]\n",
      "11307: [discriminator loss: 0.646313, acc: 0.648438] [adversarial loss: 1.119951, acc: 0.234375]\n",
      "11308: [discriminator loss: 0.520725, acc: 0.734375] [adversarial loss: 1.237148, acc: 0.234375]\n",
      "11309: [discriminator loss: 0.530881, acc: 0.726562] [adversarial loss: 1.164149, acc: 0.234375]\n",
      "11310: [discriminator loss: 0.616575, acc: 0.656250] [adversarial loss: 1.039810, acc: 0.312500]\n",
      "11311: [discriminator loss: 0.593338, acc: 0.679688] [adversarial loss: 1.277179, acc: 0.171875]\n",
      "11312: [discriminator loss: 0.490433, acc: 0.750000] [adversarial loss: 1.115630, acc: 0.250000]\n",
      "11313: [discriminator loss: 0.510224, acc: 0.750000] [adversarial loss: 1.230416, acc: 0.265625]\n",
      "11314: [discriminator loss: 0.572860, acc: 0.687500] [adversarial loss: 1.087745, acc: 0.281250]\n",
      "11315: [discriminator loss: 0.520902, acc: 0.789062] [adversarial loss: 1.115207, acc: 0.250000]\n",
      "11316: [discriminator loss: 0.553366, acc: 0.687500] [adversarial loss: 1.027209, acc: 0.312500]\n",
      "11317: [discriminator loss: 0.516996, acc: 0.703125] [adversarial loss: 1.006005, acc: 0.234375]\n",
      "11318: [discriminator loss: 0.610303, acc: 0.671875] [adversarial loss: 0.978960, acc: 0.359375]\n",
      "11319: [discriminator loss: 0.563076, acc: 0.687500] [adversarial loss: 1.653208, acc: 0.062500]\n",
      "11320: [discriminator loss: 0.585442, acc: 0.671875] [adversarial loss: 0.782473, acc: 0.484375]\n",
      "11321: [discriminator loss: 0.642949, acc: 0.648438] [adversarial loss: 1.482426, acc: 0.203125]\n",
      "11322: [discriminator loss: 0.614557, acc: 0.664062] [adversarial loss: 0.918559, acc: 0.406250]\n",
      "11323: [discriminator loss: 0.572774, acc: 0.679688] [adversarial loss: 1.226484, acc: 0.203125]\n",
      "11324: [discriminator loss: 0.517597, acc: 0.703125] [adversarial loss: 0.953578, acc: 0.296875]\n",
      "11325: [discriminator loss: 0.570444, acc: 0.664062] [adversarial loss: 1.237507, acc: 0.187500]\n",
      "11326: [discriminator loss: 0.609098, acc: 0.679688] [adversarial loss: 1.425879, acc: 0.109375]\n",
      "11327: [discriminator loss: 0.589622, acc: 0.687500] [adversarial loss: 0.895007, acc: 0.359375]\n",
      "11328: [discriminator loss: 0.593511, acc: 0.695312] [adversarial loss: 1.308174, acc: 0.171875]\n",
      "11329: [discriminator loss: 0.560852, acc: 0.656250] [adversarial loss: 1.053101, acc: 0.296875]\n",
      "11330: [discriminator loss: 0.633265, acc: 0.617188] [adversarial loss: 1.045598, acc: 0.234375]\n",
      "11331: [discriminator loss: 0.505869, acc: 0.726562] [adversarial loss: 1.004692, acc: 0.281250]\n",
      "11332: [discriminator loss: 0.561443, acc: 0.710938] [adversarial loss: 1.208295, acc: 0.250000]\n",
      "11333: [discriminator loss: 0.488398, acc: 0.757812] [adversarial loss: 1.142479, acc: 0.265625]\n",
      "11334: [discriminator loss: 0.569686, acc: 0.703125] [adversarial loss: 1.300774, acc: 0.218750]\n",
      "11335: [discriminator loss: 0.572282, acc: 0.726562] [adversarial loss: 1.025963, acc: 0.343750]\n",
      "11336: [discriminator loss: 0.563065, acc: 0.710938] [adversarial loss: 1.282518, acc: 0.250000]\n",
      "11337: [discriminator loss: 0.538426, acc: 0.750000] [adversarial loss: 0.975082, acc: 0.390625]\n",
      "11338: [discriminator loss: 0.555519, acc: 0.703125] [adversarial loss: 1.249976, acc: 0.156250]\n",
      "11339: [discriminator loss: 0.519784, acc: 0.718750] [adversarial loss: 0.897036, acc: 0.421875]\n",
      "11340: [discriminator loss: 0.656781, acc: 0.625000] [adversarial loss: 1.211501, acc: 0.203125]\n",
      "11341: [discriminator loss: 0.523218, acc: 0.734375] [adversarial loss: 0.984164, acc: 0.296875]\n",
      "11342: [discriminator loss: 0.468979, acc: 0.781250] [adversarial loss: 1.513911, acc: 0.187500]\n",
      "11343: [discriminator loss: 0.625560, acc: 0.664062] [adversarial loss: 1.241526, acc: 0.218750]\n",
      "11344: [discriminator loss: 0.521788, acc: 0.695312] [adversarial loss: 1.165468, acc: 0.250000]\n",
      "11345: [discriminator loss: 0.484687, acc: 0.781250] [adversarial loss: 1.073885, acc: 0.281250]\n",
      "11346: [discriminator loss: 0.553765, acc: 0.710938] [adversarial loss: 1.330511, acc: 0.218750]\n",
      "11347: [discriminator loss: 0.550467, acc: 0.710938] [adversarial loss: 1.065167, acc: 0.218750]\n",
      "11348: [discriminator loss: 0.575397, acc: 0.718750] [adversarial loss: 1.214574, acc: 0.265625]\n",
      "11349: [discriminator loss: 0.580882, acc: 0.664062] [adversarial loss: 1.070207, acc: 0.296875]\n",
      "11350: [discriminator loss: 0.611622, acc: 0.656250] [adversarial loss: 1.195681, acc: 0.187500]\n",
      "11351: [discriminator loss: 0.550629, acc: 0.695312] [adversarial loss: 1.047692, acc: 0.328125]\n",
      "11352: [discriminator loss: 0.622727, acc: 0.632812] [adversarial loss: 1.280174, acc: 0.187500]\n",
      "11353: [discriminator loss: 0.522798, acc: 0.742188] [adversarial loss: 1.044989, acc: 0.281250]\n",
      "11354: [discriminator loss: 0.534027, acc: 0.726562] [adversarial loss: 1.300066, acc: 0.125000]\n",
      "11355: [discriminator loss: 0.529555, acc: 0.734375] [adversarial loss: 0.921081, acc: 0.421875]\n",
      "11356: [discriminator loss: 0.548416, acc: 0.695312] [adversarial loss: 1.348337, acc: 0.125000]\n",
      "11357: [discriminator loss: 0.584923, acc: 0.648438] [adversarial loss: 1.058631, acc: 0.312500]\n",
      "11358: [discriminator loss: 0.593265, acc: 0.687500] [adversarial loss: 1.235375, acc: 0.218750]\n",
      "11359: [discriminator loss: 0.570592, acc: 0.640625] [adversarial loss: 1.000248, acc: 0.375000]\n",
      "11360: [discriminator loss: 0.575010, acc: 0.656250] [adversarial loss: 1.274817, acc: 0.109375]\n",
      "11361: [discriminator loss: 0.546953, acc: 0.671875] [adversarial loss: 1.099238, acc: 0.281250]\n",
      "11362: [discriminator loss: 0.554995, acc: 0.703125] [adversarial loss: 1.338329, acc: 0.187500]\n",
      "11363: [discriminator loss: 0.595122, acc: 0.632812] [adversarial loss: 0.779342, acc: 0.562500]\n",
      "11364: [discriminator loss: 0.656092, acc: 0.617188] [adversarial loss: 1.385394, acc: 0.156250]\n",
      "11365: [discriminator loss: 0.550279, acc: 0.695312] [adversarial loss: 0.884464, acc: 0.421875]\n",
      "11366: [discriminator loss: 0.526739, acc: 0.773438] [adversarial loss: 1.362969, acc: 0.171875]\n",
      "11367: [discriminator loss: 0.591880, acc: 0.648438] [adversarial loss: 0.960265, acc: 0.390625]\n",
      "11368: [discriminator loss: 0.534224, acc: 0.726562] [adversarial loss: 1.317137, acc: 0.203125]\n",
      "11369: [discriminator loss: 0.529174, acc: 0.726562] [adversarial loss: 1.163504, acc: 0.218750]\n",
      "11370: [discriminator loss: 0.612599, acc: 0.656250] [adversarial loss: 0.960607, acc: 0.359375]\n",
      "11371: [discriminator loss: 0.507649, acc: 0.773438] [adversarial loss: 1.299131, acc: 0.140625]\n",
      "11372: [discriminator loss: 0.565488, acc: 0.664062] [adversarial loss: 1.121049, acc: 0.265625]\n",
      "11373: [discriminator loss: 0.526950, acc: 0.742188] [adversarial loss: 1.348128, acc: 0.171875]\n",
      "11374: [discriminator loss: 0.594335, acc: 0.703125] [adversarial loss: 0.926406, acc: 0.328125]\n",
      "11375: [discriminator loss: 0.545911, acc: 0.757812] [adversarial loss: 1.243848, acc: 0.156250]\n",
      "11376: [discriminator loss: 0.527765, acc: 0.726562] [adversarial loss: 1.017916, acc: 0.343750]\n",
      "11377: [discriminator loss: 0.508346, acc: 0.789062] [adversarial loss: 1.023624, acc: 0.437500]\n",
      "11378: [discriminator loss: 0.537156, acc: 0.742188] [adversarial loss: 1.160604, acc: 0.265625]\n",
      "11379: [discriminator loss: 0.528762, acc: 0.718750] [adversarial loss: 1.031768, acc: 0.343750]\n",
      "11380: [discriminator loss: 0.542599, acc: 0.695312] [adversarial loss: 0.979324, acc: 0.359375]\n",
      "11381: [discriminator loss: 0.537694, acc: 0.726562] [adversarial loss: 1.587046, acc: 0.093750]\n",
      "11382: [discriminator loss: 0.537875, acc: 0.734375] [adversarial loss: 0.893716, acc: 0.406250]\n",
      "11383: [discriminator loss: 0.610327, acc: 0.632812] [adversarial loss: 1.545046, acc: 0.093750]\n",
      "11384: [discriminator loss: 0.595371, acc: 0.648438] [adversarial loss: 0.745041, acc: 0.515625]\n",
      "11385: [discriminator loss: 0.610422, acc: 0.679688] [adversarial loss: 1.282746, acc: 0.187500]\n",
      "11386: [discriminator loss: 0.521999, acc: 0.679688] [adversarial loss: 0.749675, acc: 0.500000]\n",
      "11387: [discriminator loss: 0.633689, acc: 0.625000] [adversarial loss: 1.587367, acc: 0.062500]\n",
      "11388: [discriminator loss: 0.603509, acc: 0.664062] [adversarial loss: 0.911891, acc: 0.390625]\n",
      "11389: [discriminator loss: 0.573660, acc: 0.656250] [adversarial loss: 1.519384, acc: 0.093750]\n",
      "11390: [discriminator loss: 0.517131, acc: 0.750000] [adversarial loss: 1.071504, acc: 0.296875]\n",
      "11391: [discriminator loss: 0.563454, acc: 0.703125] [adversarial loss: 1.278120, acc: 0.156250]\n",
      "11392: [discriminator loss: 0.529727, acc: 0.710938] [adversarial loss: 1.081165, acc: 0.265625]\n",
      "11393: [discriminator loss: 0.578346, acc: 0.710938] [adversarial loss: 1.009606, acc: 0.250000]\n",
      "11394: [discriminator loss: 0.574826, acc: 0.687500] [adversarial loss: 1.219326, acc: 0.171875]\n",
      "11395: [discriminator loss: 0.559447, acc: 0.695312] [adversarial loss: 1.102554, acc: 0.218750]\n",
      "11396: [discriminator loss: 0.540090, acc: 0.656250] [adversarial loss: 1.349142, acc: 0.171875]\n",
      "11397: [discriminator loss: 0.634008, acc: 0.656250] [adversarial loss: 0.915625, acc: 0.343750]\n",
      "11398: [discriminator loss: 0.650434, acc: 0.593750] [adversarial loss: 1.595954, acc: 0.031250]\n",
      "11399: [discriminator loss: 0.548179, acc: 0.710938] [adversarial loss: 0.830037, acc: 0.453125]\n",
      "11400: [discriminator loss: 0.587201, acc: 0.679688] [adversarial loss: 1.591712, acc: 0.031250]\n",
      "11401: [discriminator loss: 0.562490, acc: 0.703125] [adversarial loss: 0.922777, acc: 0.375000]\n",
      "11402: [discriminator loss: 0.556514, acc: 0.671875] [adversarial loss: 1.479533, acc: 0.093750]\n",
      "11403: [discriminator loss: 0.508761, acc: 0.742188] [adversarial loss: 1.013401, acc: 0.312500]\n",
      "11404: [discriminator loss: 0.558631, acc: 0.718750] [adversarial loss: 1.231734, acc: 0.203125]\n",
      "11405: [discriminator loss: 0.548486, acc: 0.695312] [adversarial loss: 1.299452, acc: 0.187500]\n",
      "11406: [discriminator loss: 0.539736, acc: 0.718750] [adversarial loss: 0.992121, acc: 0.296875]\n",
      "11407: [discriminator loss: 0.493564, acc: 0.757812] [adversarial loss: 1.171185, acc: 0.218750]\n",
      "11408: [discriminator loss: 0.519912, acc: 0.671875] [adversarial loss: 0.986675, acc: 0.328125]\n",
      "11409: [discriminator loss: 0.544936, acc: 0.703125] [adversarial loss: 1.165571, acc: 0.234375]\n",
      "11410: [discriminator loss: 0.579267, acc: 0.687500] [adversarial loss: 1.168561, acc: 0.187500]\n",
      "11411: [discriminator loss: 0.560129, acc: 0.687500] [adversarial loss: 1.065132, acc: 0.281250]\n",
      "11412: [discriminator loss: 0.566012, acc: 0.695312] [adversarial loss: 1.338619, acc: 0.109375]\n",
      "11413: [discriminator loss: 0.523061, acc: 0.742188] [adversarial loss: 0.890166, acc: 0.484375]\n",
      "11414: [discriminator loss: 0.503093, acc: 0.734375] [adversarial loss: 1.481250, acc: 0.140625]\n",
      "11415: [discriminator loss: 0.515038, acc: 0.718750] [adversarial loss: 0.971730, acc: 0.328125]\n",
      "11416: [discriminator loss: 0.604249, acc: 0.671875] [adversarial loss: 0.954098, acc: 0.328125]\n",
      "11417: [discriminator loss: 0.517992, acc: 0.734375] [adversarial loss: 1.023584, acc: 0.359375]\n",
      "11418: [discriminator loss: 0.572186, acc: 0.679688] [adversarial loss: 1.049905, acc: 0.250000]\n",
      "11419: [discriminator loss: 0.521421, acc: 0.781250] [adversarial loss: 1.048972, acc: 0.328125]\n",
      "11420: [discriminator loss: 0.534271, acc: 0.687500] [adversarial loss: 1.113552, acc: 0.296875]\n",
      "11421: [discriminator loss: 0.533178, acc: 0.710938] [adversarial loss: 1.215136, acc: 0.109375]\n",
      "11422: [discriminator loss: 0.605001, acc: 0.640625] [adversarial loss: 1.006212, acc: 0.343750]\n",
      "11423: [discriminator loss: 0.543881, acc: 0.734375] [adversarial loss: 0.899090, acc: 0.390625]\n",
      "11424: [discriminator loss: 0.550136, acc: 0.710938] [adversarial loss: 1.384465, acc: 0.109375]\n",
      "11425: [discriminator loss: 0.478486, acc: 0.757812] [adversarial loss: 0.998299, acc: 0.281250]\n",
      "11426: [discriminator loss: 0.655369, acc: 0.632812] [adversarial loss: 1.075951, acc: 0.281250]\n",
      "11427: [discriminator loss: 0.551934, acc: 0.718750] [adversarial loss: 1.006287, acc: 0.328125]\n",
      "11428: [discriminator loss: 0.595945, acc: 0.695312] [adversarial loss: 1.532362, acc: 0.171875]\n",
      "11429: [discriminator loss: 0.466560, acc: 0.804688] [adversarial loss: 1.192686, acc: 0.296875]\n",
      "11430: [discriminator loss: 0.572216, acc: 0.734375] [adversarial loss: 1.166906, acc: 0.218750]\n",
      "11431: [discriminator loss: 0.523956, acc: 0.687500] [adversarial loss: 1.249825, acc: 0.093750]\n",
      "11432: [discriminator loss: 0.627809, acc: 0.656250] [adversarial loss: 0.926613, acc: 0.328125]\n",
      "11433: [discriminator loss: 0.525438, acc: 0.750000] [adversarial loss: 1.336084, acc: 0.203125]\n",
      "11434: [discriminator loss: 0.556909, acc: 0.710938] [adversarial loss: 0.818493, acc: 0.546875]\n",
      "11435: [discriminator loss: 0.625789, acc: 0.632812] [adversarial loss: 1.488774, acc: 0.093750]\n",
      "11436: [discriminator loss: 0.605754, acc: 0.625000] [adversarial loss: 0.876356, acc: 0.375000]\n",
      "11437: [discriminator loss: 0.519466, acc: 0.742188] [adversarial loss: 1.360771, acc: 0.140625]\n",
      "11438: [discriminator loss: 0.679614, acc: 0.609375] [adversarial loss: 0.908506, acc: 0.406250]\n",
      "11439: [discriminator loss: 0.552422, acc: 0.679688] [adversarial loss: 1.280867, acc: 0.109375]\n",
      "11440: [discriminator loss: 0.519011, acc: 0.734375] [adversarial loss: 1.177098, acc: 0.203125]\n",
      "11441: [discriminator loss: 0.514238, acc: 0.710938] [adversarial loss: 1.175911, acc: 0.156250]\n",
      "11442: [discriminator loss: 0.494858, acc: 0.750000] [adversarial loss: 1.140849, acc: 0.187500]\n",
      "11443: [discriminator loss: 0.564752, acc: 0.664062] [adversarial loss: 1.211280, acc: 0.203125]\n",
      "11444: [discriminator loss: 0.615907, acc: 0.664062] [adversarial loss: 0.841233, acc: 0.437500]\n",
      "11445: [discriminator loss: 0.532199, acc: 0.695312] [adversarial loss: 1.151879, acc: 0.250000]\n",
      "11446: [discriminator loss: 0.544952, acc: 0.703125] [adversarial loss: 1.173644, acc: 0.281250]\n",
      "11447: [discriminator loss: 0.541921, acc: 0.710938] [adversarial loss: 1.320474, acc: 0.203125]\n",
      "11448: [discriminator loss: 0.545147, acc: 0.734375] [adversarial loss: 0.819770, acc: 0.453125]\n",
      "11449: [discriminator loss: 0.584027, acc: 0.671875] [adversarial loss: 1.401356, acc: 0.109375]\n",
      "11450: [discriminator loss: 0.514687, acc: 0.718750] [adversarial loss: 0.795979, acc: 0.468750]\n",
      "11451: [discriminator loss: 0.436198, acc: 0.781250] [adversarial loss: 1.212454, acc: 0.203125]\n",
      "11452: [discriminator loss: 0.548651, acc: 0.679688] [adversarial loss: 1.233160, acc: 0.187500]\n",
      "11453: [discriminator loss: 0.514212, acc: 0.695312] [adversarial loss: 1.227554, acc: 0.218750]\n",
      "11454: [discriminator loss: 0.507357, acc: 0.718750] [adversarial loss: 1.005257, acc: 0.359375]\n",
      "11455: [discriminator loss: 0.510010, acc: 0.781250] [adversarial loss: 1.314355, acc: 0.140625]\n",
      "11456: [discriminator loss: 0.540558, acc: 0.695312] [adversarial loss: 0.891198, acc: 0.437500]\n",
      "11457: [discriminator loss: 0.588750, acc: 0.609375] [adversarial loss: 1.362864, acc: 0.125000]\n",
      "11458: [discriminator loss: 0.522762, acc: 0.726562] [adversarial loss: 1.135838, acc: 0.203125]\n",
      "11459: [discriminator loss: 0.549009, acc: 0.695312] [adversarial loss: 1.425804, acc: 0.171875]\n",
      "11460: [discriminator loss: 0.496131, acc: 0.765625] [adversarial loss: 0.893211, acc: 0.390625]\n",
      "11461: [discriminator loss: 0.576582, acc: 0.687500] [adversarial loss: 1.404790, acc: 0.109375]\n",
      "11462: [discriminator loss: 0.534381, acc: 0.718750] [adversarial loss: 0.874767, acc: 0.453125]\n",
      "11463: [discriminator loss: 0.492200, acc: 0.734375] [adversarial loss: 1.350432, acc: 0.234375]\n",
      "11464: [discriminator loss: 0.572580, acc: 0.664062] [adversarial loss: 0.988736, acc: 0.406250]\n",
      "11465: [discriminator loss: 0.603238, acc: 0.664062] [adversarial loss: 1.290924, acc: 0.171875]\n",
      "11466: [discriminator loss: 0.562051, acc: 0.726562] [adversarial loss: 1.055885, acc: 0.359375]\n",
      "11467: [discriminator loss: 0.588030, acc: 0.687500] [adversarial loss: 1.288106, acc: 0.171875]\n",
      "11468: [discriminator loss: 0.545085, acc: 0.750000] [adversarial loss: 1.002847, acc: 0.250000]\n",
      "11469: [discriminator loss: 0.532192, acc: 0.726562] [adversarial loss: 1.170053, acc: 0.203125]\n",
      "11470: [discriminator loss: 0.595423, acc: 0.671875] [adversarial loss: 1.293051, acc: 0.218750]\n",
      "11471: [discriminator loss: 0.652180, acc: 0.640625] [adversarial loss: 0.961389, acc: 0.390625]\n",
      "11472: [discriminator loss: 0.593289, acc: 0.679688] [adversarial loss: 1.457102, acc: 0.109375]\n",
      "11473: [discriminator loss: 0.637271, acc: 0.648438] [adversarial loss: 0.988771, acc: 0.359375]\n",
      "11474: [discriminator loss: 0.552030, acc: 0.679688] [adversarial loss: 1.329566, acc: 0.203125]\n",
      "11475: [discriminator loss: 0.568319, acc: 0.718750] [adversarial loss: 0.878796, acc: 0.421875]\n",
      "11476: [discriminator loss: 0.453953, acc: 0.773438] [adversarial loss: 1.247471, acc: 0.156250]\n",
      "11477: [discriminator loss: 0.571180, acc: 0.671875] [adversarial loss: 1.036593, acc: 0.312500]\n",
      "11478: [discriminator loss: 0.545198, acc: 0.695312] [adversarial loss: 1.020554, acc: 0.312500]\n",
      "11479: [discriminator loss: 0.517745, acc: 0.734375] [adversarial loss: 1.482684, acc: 0.156250]\n",
      "11480: [discriminator loss: 0.613558, acc: 0.656250] [adversarial loss: 0.799470, acc: 0.531250]\n",
      "11481: [discriminator loss: 0.578940, acc: 0.687500] [adversarial loss: 1.489330, acc: 0.093750]\n",
      "11482: [discriminator loss: 0.622089, acc: 0.656250] [adversarial loss: 1.061751, acc: 0.343750]\n",
      "11483: [discriminator loss: 0.585248, acc: 0.679688] [adversarial loss: 1.254369, acc: 0.187500]\n",
      "11484: [discriminator loss: 0.586774, acc: 0.710938] [adversarial loss: 0.867017, acc: 0.437500]\n",
      "11485: [discriminator loss: 0.590870, acc: 0.695312] [adversarial loss: 1.246115, acc: 0.171875]\n",
      "11486: [discriminator loss: 0.676064, acc: 0.640625] [adversarial loss: 0.819873, acc: 0.484375]\n",
      "11487: [discriminator loss: 0.544038, acc: 0.695312] [adversarial loss: 1.433587, acc: 0.156250]\n",
      "11488: [discriminator loss: 0.553874, acc: 0.710938] [adversarial loss: 0.852845, acc: 0.421875]\n",
      "11489: [discriminator loss: 0.572904, acc: 0.632812] [adversarial loss: 1.360058, acc: 0.203125]\n",
      "11490: [discriminator loss: 0.542854, acc: 0.750000] [adversarial loss: 0.968281, acc: 0.359375]\n",
      "11491: [discriminator loss: 0.561673, acc: 0.710938] [adversarial loss: 1.072082, acc: 0.312500]\n",
      "11492: [discriminator loss: 0.519524, acc: 0.773438] [adversarial loss: 1.212344, acc: 0.218750]\n",
      "11493: [discriminator loss: 0.526791, acc: 0.718750] [adversarial loss: 1.428633, acc: 0.140625]\n",
      "11494: [discriminator loss: 0.564893, acc: 0.710938] [adversarial loss: 0.913553, acc: 0.390625]\n",
      "11495: [discriminator loss: 0.518604, acc: 0.734375] [adversarial loss: 0.955535, acc: 0.375000]\n",
      "11496: [discriminator loss: 0.558135, acc: 0.718750] [adversarial loss: 1.148115, acc: 0.328125]\n",
      "11497: [discriminator loss: 0.577984, acc: 0.664062] [adversarial loss: 1.032423, acc: 0.296875]\n",
      "11498: [discriminator loss: 0.622419, acc: 0.687500] [adversarial loss: 1.291292, acc: 0.140625]\n",
      "11499: [discriminator loss: 0.539308, acc: 0.718750] [adversarial loss: 1.108916, acc: 0.281250]\n",
      "11500: [discriminator loss: 0.611204, acc: 0.664062] [adversarial loss: 1.210797, acc: 0.203125]\n",
      "11501: [discriminator loss: 0.605535, acc: 0.679688] [adversarial loss: 1.208040, acc: 0.234375]\n",
      "11502: [discriminator loss: 0.525403, acc: 0.726562] [adversarial loss: 1.148444, acc: 0.234375]\n",
      "11503: [discriminator loss: 0.475635, acc: 0.765625] [adversarial loss: 1.154077, acc: 0.250000]\n",
      "11504: [discriminator loss: 0.543402, acc: 0.687500] [adversarial loss: 1.006766, acc: 0.281250]\n",
      "11505: [discriminator loss: 0.677152, acc: 0.562500] [adversarial loss: 1.355053, acc: 0.156250]\n",
      "11506: [discriminator loss: 0.523249, acc: 0.703125] [adversarial loss: 0.896037, acc: 0.406250]\n",
      "11507: [discriminator loss: 0.529736, acc: 0.703125] [adversarial loss: 0.906062, acc: 0.281250]\n",
      "11508: [discriminator loss: 0.542948, acc: 0.726562] [adversarial loss: 1.056442, acc: 0.250000]\n",
      "11509: [discriminator loss: 0.527172, acc: 0.726562] [adversarial loss: 1.103876, acc: 0.296875]\n",
      "11510: [discriminator loss: 0.479582, acc: 0.773438] [adversarial loss: 1.084237, acc: 0.343750]\n",
      "11511: [discriminator loss: 0.554439, acc: 0.734375] [adversarial loss: 1.094128, acc: 0.296875]\n",
      "11512: [discriminator loss: 0.497635, acc: 0.765625] [adversarial loss: 1.288429, acc: 0.156250]\n",
      "11513: [discriminator loss: 0.457686, acc: 0.781250] [adversarial loss: 1.288834, acc: 0.218750]\n",
      "11514: [discriminator loss: 0.485274, acc: 0.734375] [adversarial loss: 0.979631, acc: 0.312500]\n",
      "11515: [discriminator loss: 0.635870, acc: 0.664062] [adversarial loss: 1.362512, acc: 0.109375]\n",
      "11516: [discriminator loss: 0.635427, acc: 0.632812] [adversarial loss: 0.706289, acc: 0.562500]\n",
      "11517: [discriminator loss: 0.614469, acc: 0.679688] [adversarial loss: 1.531870, acc: 0.125000]\n",
      "11518: [discriminator loss: 0.588926, acc: 0.679688] [adversarial loss: 0.769794, acc: 0.531250]\n",
      "11519: [discriminator loss: 0.544059, acc: 0.687500] [adversarial loss: 1.336087, acc: 0.125000]\n",
      "11520: [discriminator loss: 0.508849, acc: 0.765625] [adversarial loss: 1.151399, acc: 0.265625]\n",
      "11521: [discriminator loss: 0.564431, acc: 0.742188] [adversarial loss: 1.106989, acc: 0.312500]\n",
      "11522: [discriminator loss: 0.548937, acc: 0.757812] [adversarial loss: 1.034262, acc: 0.250000]\n",
      "11523: [discriminator loss: 0.544811, acc: 0.718750] [adversarial loss: 1.175982, acc: 0.265625]\n",
      "11524: [discriminator loss: 0.520080, acc: 0.757812] [adversarial loss: 1.265930, acc: 0.156250]\n",
      "11525: [discriminator loss: 0.577608, acc: 0.710938] [adversarial loss: 1.001243, acc: 0.250000]\n",
      "11526: [discriminator loss: 0.560991, acc: 0.679688] [adversarial loss: 1.166500, acc: 0.250000]\n",
      "11527: [discriminator loss: 0.554901, acc: 0.734375] [adversarial loss: 1.022610, acc: 0.296875]\n",
      "11528: [discriminator loss: 0.517745, acc: 0.750000] [adversarial loss: 1.111647, acc: 0.250000]\n",
      "11529: [discriminator loss: 0.559834, acc: 0.742188] [adversarial loss: 1.234713, acc: 0.171875]\n",
      "11530: [discriminator loss: 0.529278, acc: 0.734375] [adversarial loss: 1.004256, acc: 0.328125]\n",
      "11531: [discriminator loss: 0.544419, acc: 0.750000] [adversarial loss: 1.375443, acc: 0.093750]\n",
      "11532: [discriminator loss: 0.496772, acc: 0.757812] [adversarial loss: 1.112755, acc: 0.265625]\n",
      "11533: [discriminator loss: 0.519948, acc: 0.718750] [adversarial loss: 1.317787, acc: 0.171875]\n",
      "11534: [discriminator loss: 0.569234, acc: 0.726562] [adversarial loss: 1.006943, acc: 0.312500]\n",
      "11535: [discriminator loss: 0.571081, acc: 0.679688] [adversarial loss: 1.547196, acc: 0.125000]\n",
      "11536: [discriminator loss: 0.493966, acc: 0.718750] [adversarial loss: 0.775762, acc: 0.546875]\n",
      "11537: [discriminator loss: 0.592676, acc: 0.679688] [adversarial loss: 1.516499, acc: 0.140625]\n",
      "11538: [discriminator loss: 0.479172, acc: 0.773438] [adversarial loss: 0.936885, acc: 0.390625]\n",
      "11539: [discriminator loss: 0.524553, acc: 0.742188] [adversarial loss: 1.624515, acc: 0.109375]\n",
      "11540: [discriminator loss: 0.534423, acc: 0.734375] [adversarial loss: 1.030268, acc: 0.359375]\n",
      "11541: [discriminator loss: 0.494928, acc: 0.734375] [adversarial loss: 1.236351, acc: 0.265625]\n",
      "11542: [discriminator loss: 0.567828, acc: 0.750000] [adversarial loss: 1.308047, acc: 0.234375]\n",
      "11543: [discriminator loss: 0.544435, acc: 0.703125] [adversarial loss: 1.224517, acc: 0.218750]\n",
      "11544: [discriminator loss: 0.485264, acc: 0.765625] [adversarial loss: 0.903843, acc: 0.406250]\n",
      "11545: [discriminator loss: 0.535243, acc: 0.789062] [adversarial loss: 1.469010, acc: 0.140625]\n",
      "11546: [discriminator loss: 0.538995, acc: 0.710938] [adversarial loss: 0.690094, acc: 0.593750]\n",
      "11547: [discriminator loss: 0.555990, acc: 0.703125] [adversarial loss: 1.532292, acc: 0.109375]\n",
      "11548: [discriminator loss: 0.635478, acc: 0.632812] [adversarial loss: 0.956438, acc: 0.421875]\n",
      "11549: [discriminator loss: 0.604334, acc: 0.687500] [adversarial loss: 1.057384, acc: 0.375000]\n",
      "11550: [discriminator loss: 0.497996, acc: 0.773438] [adversarial loss: 1.138467, acc: 0.250000]\n",
      "11551: [discriminator loss: 0.612193, acc: 0.671875] [adversarial loss: 1.135149, acc: 0.296875]\n",
      "11552: [discriminator loss: 0.546200, acc: 0.750000] [adversarial loss: 1.167205, acc: 0.218750]\n",
      "11553: [discriminator loss: 0.606135, acc: 0.656250] [adversarial loss: 0.766913, acc: 0.578125]\n",
      "11554: [discriminator loss: 0.664379, acc: 0.625000] [adversarial loss: 1.502822, acc: 0.109375]\n",
      "11555: [discriminator loss: 0.528735, acc: 0.726562] [adversarial loss: 1.179787, acc: 0.187500]\n",
      "11556: [discriminator loss: 0.488074, acc: 0.781250] [adversarial loss: 0.954668, acc: 0.343750]\n",
      "11557: [discriminator loss: 0.530230, acc: 0.695312] [adversarial loss: 0.885405, acc: 0.406250]\n",
      "11558: [discriminator loss: 0.553443, acc: 0.710938] [adversarial loss: 1.194253, acc: 0.187500]\n",
      "11559: [discriminator loss: 0.568478, acc: 0.679688] [adversarial loss: 1.139926, acc: 0.203125]\n",
      "11560: [discriminator loss: 0.501268, acc: 0.757812] [adversarial loss: 1.301274, acc: 0.171875]\n",
      "11561: [discriminator loss: 0.563550, acc: 0.671875] [adversarial loss: 0.951518, acc: 0.328125]\n",
      "11562: [discriminator loss: 0.558379, acc: 0.757812] [adversarial loss: 1.213298, acc: 0.218750]\n",
      "11563: [discriminator loss: 0.556266, acc: 0.687500] [adversarial loss: 0.866896, acc: 0.453125]\n",
      "11564: [discriminator loss: 0.528853, acc: 0.765625] [adversarial loss: 1.400154, acc: 0.203125]\n",
      "11565: [discriminator loss: 0.566681, acc: 0.679688] [adversarial loss: 1.097204, acc: 0.328125]\n",
      "11566: [discriminator loss: 0.502691, acc: 0.757812] [adversarial loss: 1.405821, acc: 0.125000]\n",
      "11567: [discriminator loss: 0.546733, acc: 0.718750] [adversarial loss: 0.894274, acc: 0.390625]\n",
      "11568: [discriminator loss: 0.589451, acc: 0.710938] [adversarial loss: 1.324768, acc: 0.140625]\n",
      "11569: [discriminator loss: 0.460273, acc: 0.812500] [adversarial loss: 1.070906, acc: 0.312500]\n",
      "11570: [discriminator loss: 0.612044, acc: 0.671875] [adversarial loss: 1.270766, acc: 0.234375]\n",
      "11571: [discriminator loss: 0.551707, acc: 0.664062] [adversarial loss: 1.217071, acc: 0.359375]\n",
      "11572: [discriminator loss: 0.521927, acc: 0.679688] [adversarial loss: 1.299563, acc: 0.140625]\n",
      "11573: [discriminator loss: 0.482343, acc: 0.750000] [adversarial loss: 0.774229, acc: 0.515625]\n",
      "11574: [discriminator loss: 0.635596, acc: 0.640625] [adversarial loss: 1.806957, acc: 0.031250]\n",
      "11575: [discriminator loss: 0.580439, acc: 0.687500] [adversarial loss: 0.903012, acc: 0.406250]\n",
      "11576: [discriminator loss: 0.563442, acc: 0.703125] [adversarial loss: 1.285169, acc: 0.140625]\n",
      "11577: [discriminator loss: 0.557164, acc: 0.695312] [adversarial loss: 1.109033, acc: 0.250000]\n",
      "11578: [discriminator loss: 0.517421, acc: 0.718750] [adversarial loss: 1.270899, acc: 0.203125]\n",
      "11579: [discriminator loss: 0.540700, acc: 0.750000] [adversarial loss: 1.083891, acc: 0.296875]\n",
      "11580: [discriminator loss: 0.541662, acc: 0.726562] [adversarial loss: 1.238731, acc: 0.234375]\n",
      "11581: [discriminator loss: 0.460549, acc: 0.789062] [adversarial loss: 1.075624, acc: 0.265625]\n",
      "11582: [discriminator loss: 0.656121, acc: 0.632812] [adversarial loss: 1.429589, acc: 0.140625]\n",
      "11583: [discriminator loss: 0.545906, acc: 0.726562] [adversarial loss: 1.070256, acc: 0.312500]\n",
      "11584: [discriminator loss: 0.577285, acc: 0.726562] [adversarial loss: 1.261701, acc: 0.140625]\n",
      "11585: [discriminator loss: 0.532105, acc: 0.757812] [adversarial loss: 1.028648, acc: 0.328125]\n",
      "11586: [discriminator loss: 0.475948, acc: 0.781250] [adversarial loss: 1.329256, acc: 0.187500]\n",
      "11587: [discriminator loss: 0.519503, acc: 0.679688] [adversarial loss: 1.148466, acc: 0.296875]\n",
      "11588: [discriminator loss: 0.637043, acc: 0.664062] [adversarial loss: 1.281729, acc: 0.218750]\n",
      "11589: [discriminator loss: 0.611157, acc: 0.664062] [adversarial loss: 1.275283, acc: 0.140625]\n",
      "11590: [discriminator loss: 0.527638, acc: 0.734375] [adversarial loss: 1.430701, acc: 0.140625]\n",
      "11591: [discriminator loss: 0.563866, acc: 0.687500] [adversarial loss: 0.870118, acc: 0.468750]\n",
      "11592: [discriminator loss: 0.481426, acc: 0.781250] [adversarial loss: 1.395842, acc: 0.140625]\n",
      "11593: [discriminator loss: 0.569084, acc: 0.703125] [adversarial loss: 0.890618, acc: 0.406250]\n",
      "11594: [discriminator loss: 0.547474, acc: 0.718750] [adversarial loss: 1.106025, acc: 0.328125]\n",
      "11595: [discriminator loss: 0.548138, acc: 0.734375] [adversarial loss: 0.983577, acc: 0.343750]\n",
      "11596: [discriminator loss: 0.511451, acc: 0.695312] [adversarial loss: 1.159265, acc: 0.234375]\n",
      "11597: [discriminator loss: 0.517816, acc: 0.757812] [adversarial loss: 1.216627, acc: 0.203125]\n",
      "11598: [discriminator loss: 0.547337, acc: 0.710938] [adversarial loss: 0.996021, acc: 0.312500]\n",
      "11599: [discriminator loss: 0.532070, acc: 0.742188] [adversarial loss: 1.719878, acc: 0.046875]\n",
      "11600: [discriminator loss: 0.496476, acc: 0.703125] [adversarial loss: 0.837528, acc: 0.484375]\n",
      "11601: [discriminator loss: 0.579216, acc: 0.710938] [adversarial loss: 1.513942, acc: 0.171875]\n",
      "11602: [discriminator loss: 0.623808, acc: 0.671875] [adversarial loss: 1.143471, acc: 0.296875]\n",
      "11603: [discriminator loss: 0.630910, acc: 0.648438] [adversarial loss: 1.069353, acc: 0.343750]\n",
      "11604: [discriminator loss: 0.500028, acc: 0.726562] [adversarial loss: 1.140935, acc: 0.234375]\n",
      "11605: [discriminator loss: 0.557210, acc: 0.726562] [adversarial loss: 0.922135, acc: 0.375000]\n",
      "11606: [discriminator loss: 0.548302, acc: 0.718750] [adversarial loss: 1.240724, acc: 0.171875]\n",
      "11607: [discriminator loss: 0.551560, acc: 0.703125] [adversarial loss: 0.945453, acc: 0.406250]\n",
      "11608: [discriminator loss: 0.555687, acc: 0.687500] [adversarial loss: 1.447554, acc: 0.125000]\n",
      "11609: [discriminator loss: 0.597160, acc: 0.687500] [adversarial loss: 0.818088, acc: 0.468750]\n",
      "11610: [discriminator loss: 0.586784, acc: 0.687500] [adversarial loss: 1.196029, acc: 0.265625]\n",
      "11611: [discriminator loss: 0.574990, acc: 0.671875] [adversarial loss: 1.154613, acc: 0.296875]\n",
      "11612: [discriminator loss: 0.524432, acc: 0.734375] [adversarial loss: 0.974214, acc: 0.343750]\n",
      "11613: [discriminator loss: 0.556271, acc: 0.687500] [adversarial loss: 1.417286, acc: 0.140625]\n",
      "11614: [discriminator loss: 0.562565, acc: 0.750000] [adversarial loss: 1.094108, acc: 0.265625]\n",
      "11615: [discriminator loss: 0.533660, acc: 0.742188] [adversarial loss: 1.094616, acc: 0.328125]\n",
      "11616: [discriminator loss: 0.602972, acc: 0.687500] [adversarial loss: 1.121148, acc: 0.281250]\n",
      "11617: [discriminator loss: 0.557292, acc: 0.718750] [adversarial loss: 1.308438, acc: 0.125000]\n",
      "11618: [discriminator loss: 0.541317, acc: 0.703125] [adversarial loss: 0.966888, acc: 0.375000]\n",
      "11619: [discriminator loss: 0.523832, acc: 0.789062] [adversarial loss: 1.263546, acc: 0.203125]\n",
      "11620: [discriminator loss: 0.607101, acc: 0.687500] [adversarial loss: 1.244943, acc: 0.218750]\n",
      "11621: [discriminator loss: 0.579048, acc: 0.671875] [adversarial loss: 0.997828, acc: 0.343750]\n",
      "11622: [discriminator loss: 0.623701, acc: 0.703125] [adversarial loss: 1.005431, acc: 0.312500]\n",
      "11623: [discriminator loss: 0.477223, acc: 0.742188] [adversarial loss: 1.483023, acc: 0.234375]\n",
      "11624: [discriminator loss: 0.618543, acc: 0.648438] [adversarial loss: 0.783631, acc: 0.500000]\n",
      "11625: [discriminator loss: 0.547387, acc: 0.710938] [adversarial loss: 1.974611, acc: 0.015625]\n",
      "11626: [discriminator loss: 0.653541, acc: 0.609375] [adversarial loss: 0.968886, acc: 0.328125]\n",
      "11627: [discriminator loss: 0.475219, acc: 0.750000] [adversarial loss: 1.346769, acc: 0.156250]\n",
      "11628: [discriminator loss: 0.579863, acc: 0.671875] [adversarial loss: 1.070894, acc: 0.359375]\n",
      "11629: [discriminator loss: 0.537749, acc: 0.757812] [adversarial loss: 1.199227, acc: 0.265625]\n",
      "11630: [discriminator loss: 0.514404, acc: 0.734375] [adversarial loss: 1.310668, acc: 0.265625]\n",
      "11631: [discriminator loss: 0.557067, acc: 0.734375] [adversarial loss: 1.114268, acc: 0.171875]\n",
      "11632: [discriminator loss: 0.500926, acc: 0.726562] [adversarial loss: 0.989820, acc: 0.296875]\n",
      "11633: [discriminator loss: 0.519030, acc: 0.757812] [adversarial loss: 1.162977, acc: 0.156250]\n",
      "11634: [discriminator loss: 0.517838, acc: 0.757812] [adversarial loss: 1.013299, acc: 0.343750]\n",
      "11635: [discriminator loss: 0.546978, acc: 0.703125] [adversarial loss: 1.375992, acc: 0.125000]\n",
      "11636: [discriminator loss: 0.530750, acc: 0.734375] [adversarial loss: 0.916271, acc: 0.390625]\n",
      "11637: [discriminator loss: 0.565960, acc: 0.648438] [adversarial loss: 1.208653, acc: 0.234375]\n",
      "11638: [discriminator loss: 0.572518, acc: 0.726562] [adversarial loss: 1.145704, acc: 0.218750]\n",
      "11639: [discriminator loss: 0.548154, acc: 0.671875] [adversarial loss: 1.090871, acc: 0.171875]\n",
      "11640: [discriminator loss: 0.597472, acc: 0.648438] [adversarial loss: 0.934072, acc: 0.375000]\n",
      "11641: [discriminator loss: 0.469581, acc: 0.789062] [adversarial loss: 1.206681, acc: 0.234375]\n",
      "11642: [discriminator loss: 0.584167, acc: 0.687500] [adversarial loss: 1.108640, acc: 0.250000]\n",
      "11643: [discriminator loss: 0.522392, acc: 0.679688] [adversarial loss: 1.325010, acc: 0.125000]\n",
      "11644: [discriminator loss: 0.532296, acc: 0.703125] [adversarial loss: 1.050177, acc: 0.312500]\n",
      "11645: [discriminator loss: 0.532724, acc: 0.742188] [adversarial loss: 1.016366, acc: 0.390625]\n",
      "11646: [discriminator loss: 0.622916, acc: 0.625000] [adversarial loss: 1.337915, acc: 0.109375]\n",
      "11647: [discriminator loss: 0.537436, acc: 0.734375] [adversarial loss: 1.121252, acc: 0.250000]\n",
      "11648: [discriminator loss: 0.551211, acc: 0.742188] [adversarial loss: 0.917374, acc: 0.375000]\n",
      "11649: [discriminator loss: 0.547956, acc: 0.742188] [adversarial loss: 1.101768, acc: 0.296875]\n",
      "11650: [discriminator loss: 0.537067, acc: 0.695312] [adversarial loss: 1.168398, acc: 0.187500]\n",
      "11651: [discriminator loss: 0.540634, acc: 0.695312] [adversarial loss: 1.162635, acc: 0.234375]\n",
      "11652: [discriminator loss: 0.503753, acc: 0.750000] [adversarial loss: 1.825964, acc: 0.031250]\n",
      "11653: [discriminator loss: 0.517772, acc: 0.750000] [adversarial loss: 1.111627, acc: 0.265625]\n",
      "11654: [discriminator loss: 0.489435, acc: 0.789062] [adversarial loss: 1.345240, acc: 0.187500]\n",
      "11655: [discriminator loss: 0.515229, acc: 0.718750] [adversarial loss: 0.901570, acc: 0.437500]\n",
      "11656: [discriminator loss: 0.509489, acc: 0.804688] [adversarial loss: 1.501901, acc: 0.078125]\n",
      "11657: [discriminator loss: 0.553734, acc: 0.703125] [adversarial loss: 0.735385, acc: 0.578125]\n",
      "11658: [discriminator loss: 0.568259, acc: 0.695312] [adversarial loss: 1.559704, acc: 0.109375]\n",
      "11659: [discriminator loss: 0.551259, acc: 0.687500] [adversarial loss: 1.087775, acc: 0.312500]\n",
      "11660: [discriminator loss: 0.543675, acc: 0.703125] [adversarial loss: 1.163984, acc: 0.281250]\n",
      "11661: [discriminator loss: 0.556952, acc: 0.695312] [adversarial loss: 1.297235, acc: 0.187500]\n",
      "11662: [discriminator loss: 0.504480, acc: 0.765625] [adversarial loss: 1.111453, acc: 0.312500]\n",
      "11663: [discriminator loss: 0.588198, acc: 0.679688] [adversarial loss: 1.198441, acc: 0.187500]\n",
      "11664: [discriminator loss: 0.500168, acc: 0.773438] [adversarial loss: 1.143669, acc: 0.203125]\n",
      "11665: [discriminator loss: 0.450566, acc: 0.812500] [adversarial loss: 1.397635, acc: 0.218750]\n",
      "11666: [discriminator loss: 0.533903, acc: 0.687500] [adversarial loss: 1.266694, acc: 0.203125]\n",
      "11667: [discriminator loss: 0.512130, acc: 0.734375] [adversarial loss: 1.346942, acc: 0.140625]\n",
      "11668: [discriminator loss: 0.552220, acc: 0.703125] [adversarial loss: 0.992280, acc: 0.328125]\n",
      "11669: [discriminator loss: 0.625038, acc: 0.664062] [adversarial loss: 1.354473, acc: 0.156250]\n",
      "11670: [discriminator loss: 0.588145, acc: 0.726562] [adversarial loss: 0.774114, acc: 0.484375]\n",
      "11671: [discriminator loss: 0.542339, acc: 0.695312] [adversarial loss: 1.445115, acc: 0.078125]\n",
      "11672: [discriminator loss: 0.520589, acc: 0.742188] [adversarial loss: 1.188375, acc: 0.281250]\n",
      "11673: [discriminator loss: 0.513319, acc: 0.781250] [adversarial loss: 1.284871, acc: 0.265625]\n",
      "11674: [discriminator loss: 0.539154, acc: 0.734375] [adversarial loss: 1.125962, acc: 0.234375]\n",
      "11675: [discriminator loss: 0.529088, acc: 0.695312] [adversarial loss: 1.265894, acc: 0.265625]\n",
      "11676: [discriminator loss: 0.575483, acc: 0.726562] [adversarial loss: 0.987667, acc: 0.390625]\n",
      "11677: [discriminator loss: 0.609926, acc: 0.695312] [adversarial loss: 1.539438, acc: 0.125000]\n",
      "11678: [discriminator loss: 0.556748, acc: 0.671875] [adversarial loss: 0.929066, acc: 0.390625]\n",
      "11679: [discriminator loss: 0.608944, acc: 0.617188] [adversarial loss: 1.435655, acc: 0.109375]\n",
      "11680: [discriminator loss: 0.519199, acc: 0.703125] [adversarial loss: 1.003481, acc: 0.343750]\n",
      "11681: [discriminator loss: 0.554592, acc: 0.710938] [adversarial loss: 1.313661, acc: 0.234375]\n",
      "11682: [discriminator loss: 0.541859, acc: 0.710938] [adversarial loss: 0.922745, acc: 0.406250]\n",
      "11683: [discriminator loss: 0.597610, acc: 0.671875] [adversarial loss: 1.324003, acc: 0.140625]\n",
      "11684: [discriminator loss: 0.560863, acc: 0.695312] [adversarial loss: 0.871451, acc: 0.375000]\n",
      "11685: [discriminator loss: 0.488567, acc: 0.765625] [adversarial loss: 1.593557, acc: 0.125000]\n",
      "11686: [discriminator loss: 0.713780, acc: 0.609375] [adversarial loss: 0.721503, acc: 0.593750]\n",
      "11687: [discriminator loss: 0.626631, acc: 0.703125] [adversarial loss: 1.539222, acc: 0.093750]\n",
      "11688: [discriminator loss: 0.583655, acc: 0.695312] [adversarial loss: 1.070195, acc: 0.265625]\n",
      "11689: [discriminator loss: 0.573884, acc: 0.687500] [adversarial loss: 1.220250, acc: 0.171875]\n",
      "11690: [discriminator loss: 0.464427, acc: 0.742188] [adversarial loss: 1.107417, acc: 0.234375]\n",
      "11691: [discriminator loss: 0.550835, acc: 0.757812] [adversarial loss: 1.326654, acc: 0.109375]\n",
      "11692: [discriminator loss: 0.554662, acc: 0.703125] [adversarial loss: 0.953030, acc: 0.328125]\n",
      "11693: [discriminator loss: 0.618705, acc: 0.671875] [adversarial loss: 1.387921, acc: 0.062500]\n",
      "11694: [discriminator loss: 0.517880, acc: 0.718750] [adversarial loss: 1.074329, acc: 0.328125]\n",
      "11695: [discriminator loss: 0.540775, acc: 0.718750] [adversarial loss: 1.207212, acc: 0.250000]\n",
      "11696: [discriminator loss: 0.517567, acc: 0.726562] [adversarial loss: 1.087677, acc: 0.296875]\n",
      "11697: [discriminator loss: 0.649974, acc: 0.640625] [adversarial loss: 0.967247, acc: 0.375000]\n",
      "11698: [discriminator loss: 0.575683, acc: 0.679688] [adversarial loss: 1.335014, acc: 0.140625]\n",
      "11699: [discriminator loss: 0.586638, acc: 0.710938] [adversarial loss: 0.940507, acc: 0.406250]\n",
      "11700: [discriminator loss: 0.553588, acc: 0.695312] [adversarial loss: 1.652162, acc: 0.062500]\n",
      "11701: [discriminator loss: 0.553355, acc: 0.664062] [adversarial loss: 0.960521, acc: 0.343750]\n",
      "11702: [discriminator loss: 0.495385, acc: 0.804688] [adversarial loss: 1.235912, acc: 0.265625]\n",
      "11703: [discriminator loss: 0.546538, acc: 0.734375] [adversarial loss: 1.073251, acc: 0.296875]\n",
      "11704: [discriminator loss: 0.555559, acc: 0.695312] [adversarial loss: 1.210680, acc: 0.203125]\n",
      "11705: [discriminator loss: 0.539045, acc: 0.726562] [adversarial loss: 0.954092, acc: 0.421875]\n",
      "11706: [discriminator loss: 0.553213, acc: 0.726562] [adversarial loss: 1.191331, acc: 0.203125]\n",
      "11707: [discriminator loss: 0.555990, acc: 0.710938] [adversarial loss: 1.179837, acc: 0.218750]\n",
      "11708: [discriminator loss: 0.500253, acc: 0.750000] [adversarial loss: 0.991586, acc: 0.359375]\n",
      "11709: [discriminator loss: 0.530655, acc: 0.734375] [adversarial loss: 1.141310, acc: 0.250000]\n",
      "11710: [discriminator loss: 0.542781, acc: 0.718750] [adversarial loss: 1.371863, acc: 0.125000]\n",
      "11711: [discriminator loss: 0.619939, acc: 0.648438] [adversarial loss: 0.878283, acc: 0.437500]\n",
      "11712: [discriminator loss: 0.560716, acc: 0.750000] [adversarial loss: 1.495277, acc: 0.093750]\n",
      "11713: [discriminator loss: 0.647036, acc: 0.664062] [adversarial loss: 0.835487, acc: 0.421875]\n",
      "11714: [discriminator loss: 0.550940, acc: 0.742188] [adversarial loss: 1.305722, acc: 0.125000]\n",
      "11715: [discriminator loss: 0.573853, acc: 0.687500] [adversarial loss: 0.924706, acc: 0.343750]\n",
      "11716: [discriminator loss: 0.541858, acc: 0.765625] [adversarial loss: 1.111395, acc: 0.281250]\n",
      "11717: [discriminator loss: 0.580407, acc: 0.687500] [adversarial loss: 0.950403, acc: 0.390625]\n",
      "11718: [discriminator loss: 0.552418, acc: 0.710938] [adversarial loss: 1.581124, acc: 0.125000]\n",
      "11719: [discriminator loss: 0.635108, acc: 0.609375] [adversarial loss: 0.988750, acc: 0.406250]\n",
      "11720: [discriminator loss: 0.619750, acc: 0.648438] [adversarial loss: 1.141373, acc: 0.250000]\n",
      "11721: [discriminator loss: 0.535208, acc: 0.726562] [adversarial loss: 0.977117, acc: 0.421875]\n",
      "11722: [discriminator loss: 0.627008, acc: 0.679688] [adversarial loss: 1.375209, acc: 0.187500]\n",
      "11723: [discriminator loss: 0.625870, acc: 0.648438] [adversarial loss: 0.919539, acc: 0.375000]\n",
      "11724: [discriminator loss: 0.583005, acc: 0.648438] [adversarial loss: 1.413990, acc: 0.093750]\n",
      "11725: [discriminator loss: 0.577953, acc: 0.679688] [adversarial loss: 1.228883, acc: 0.250000]\n",
      "11726: [discriminator loss: 0.559729, acc: 0.726562] [adversarial loss: 1.107940, acc: 0.218750]\n",
      "11727: [discriminator loss: 0.525757, acc: 0.718750] [adversarial loss: 1.046445, acc: 0.343750]\n",
      "11728: [discriminator loss: 0.493085, acc: 0.750000] [adversarial loss: 1.016208, acc: 0.296875]\n",
      "11729: [discriminator loss: 0.556444, acc: 0.703125] [adversarial loss: 1.361057, acc: 0.296875]\n",
      "11730: [discriminator loss: 0.554954, acc: 0.695312] [adversarial loss: 1.018932, acc: 0.328125]\n",
      "11731: [discriminator loss: 0.546442, acc: 0.718750] [adversarial loss: 1.033869, acc: 0.281250]\n",
      "11732: [discriminator loss: 0.540027, acc: 0.695312] [adversarial loss: 1.381052, acc: 0.125000]\n",
      "11733: [discriminator loss: 0.506473, acc: 0.773438] [adversarial loss: 1.076540, acc: 0.250000]\n",
      "11734: [discriminator loss: 0.601468, acc: 0.632812] [adversarial loss: 1.162173, acc: 0.203125]\n",
      "11735: [discriminator loss: 0.533328, acc: 0.726562] [adversarial loss: 1.188753, acc: 0.250000]\n",
      "11736: [discriminator loss: 0.593164, acc: 0.726562] [adversarial loss: 1.321235, acc: 0.234375]\n",
      "11737: [discriminator loss: 0.539953, acc: 0.695312] [adversarial loss: 1.276331, acc: 0.203125]\n",
      "11738: [discriminator loss: 0.533244, acc: 0.734375] [adversarial loss: 1.121616, acc: 0.281250]\n",
      "11739: [discriminator loss: 0.527533, acc: 0.710938] [adversarial loss: 1.062406, acc: 0.265625]\n",
      "11740: [discriminator loss: 0.502557, acc: 0.718750] [adversarial loss: 1.086345, acc: 0.296875]\n",
      "11741: [discriminator loss: 0.514261, acc: 0.734375] [adversarial loss: 1.184854, acc: 0.156250]\n",
      "11742: [discriminator loss: 0.574306, acc: 0.726562] [adversarial loss: 1.248945, acc: 0.171875]\n",
      "11743: [discriminator loss: 0.483733, acc: 0.765625] [adversarial loss: 1.165534, acc: 0.171875]\n",
      "11744: [discriminator loss: 0.588110, acc: 0.695312] [adversarial loss: 0.990707, acc: 0.281250]\n",
      "11745: [discriminator loss: 0.506318, acc: 0.757812] [adversarial loss: 1.352200, acc: 0.187500]\n",
      "11746: [discriminator loss: 0.577286, acc: 0.656250] [adversarial loss: 0.845041, acc: 0.500000]\n",
      "11747: [discriminator loss: 0.604512, acc: 0.648438] [adversarial loss: 1.644280, acc: 0.093750]\n",
      "11748: [discriminator loss: 0.519737, acc: 0.742188] [adversarial loss: 1.011896, acc: 0.296875]\n",
      "11749: [discriminator loss: 0.509666, acc: 0.734375] [adversarial loss: 1.422184, acc: 0.156250]\n",
      "11750: [discriminator loss: 0.605179, acc: 0.671875] [adversarial loss: 0.860984, acc: 0.421875]\n",
      "11751: [discriminator loss: 0.568335, acc: 0.695312] [adversarial loss: 1.386126, acc: 0.234375]\n",
      "11752: [discriminator loss: 0.609001, acc: 0.648438] [adversarial loss: 0.821240, acc: 0.515625]\n",
      "11753: [discriminator loss: 0.633759, acc: 0.648438] [adversarial loss: 1.612085, acc: 0.109375]\n",
      "11754: [discriminator loss: 0.566554, acc: 0.695312] [adversarial loss: 0.870711, acc: 0.328125]\n",
      "11755: [discriminator loss: 0.596632, acc: 0.648438] [adversarial loss: 1.375722, acc: 0.125000]\n",
      "11756: [discriminator loss: 0.531533, acc: 0.750000] [adversarial loss: 1.004115, acc: 0.375000]\n",
      "11757: [discriminator loss: 0.518344, acc: 0.742188] [adversarial loss: 1.140809, acc: 0.296875]\n",
      "11758: [discriminator loss: 0.562267, acc: 0.718750] [adversarial loss: 1.394486, acc: 0.125000]\n",
      "11759: [discriminator loss: 0.528947, acc: 0.718750] [adversarial loss: 1.092865, acc: 0.328125]\n",
      "11760: [discriminator loss: 0.490987, acc: 0.750000] [adversarial loss: 1.272762, acc: 0.187500]\n",
      "11761: [discriminator loss: 0.507251, acc: 0.726562] [adversarial loss: 1.151325, acc: 0.312500]\n",
      "11762: [discriminator loss: 0.515522, acc: 0.742188] [adversarial loss: 1.095751, acc: 0.296875]\n",
      "11763: [discriminator loss: 0.491652, acc: 0.820312] [adversarial loss: 1.348340, acc: 0.218750]\n",
      "11764: [discriminator loss: 0.642101, acc: 0.617188] [adversarial loss: 1.135983, acc: 0.218750]\n",
      "11765: [discriminator loss: 0.590768, acc: 0.656250] [adversarial loss: 1.190746, acc: 0.171875]\n",
      "11766: [discriminator loss: 0.548377, acc: 0.718750] [adversarial loss: 1.154901, acc: 0.281250]\n",
      "11767: [discriminator loss: 0.591579, acc: 0.632812] [adversarial loss: 1.338246, acc: 0.171875]\n",
      "11768: [discriminator loss: 0.577088, acc: 0.695312] [adversarial loss: 0.867704, acc: 0.390625]\n",
      "11769: [discriminator loss: 0.549827, acc: 0.742188] [adversarial loss: 1.471575, acc: 0.125000]\n",
      "11770: [discriminator loss: 0.571376, acc: 0.687500] [adversarial loss: 0.989645, acc: 0.437500]\n",
      "11771: [discriminator loss: 0.502423, acc: 0.718750] [adversarial loss: 1.214528, acc: 0.218750]\n",
      "11772: [discriminator loss: 0.535338, acc: 0.742188] [adversarial loss: 0.960785, acc: 0.390625]\n",
      "11773: [discriminator loss: 0.530364, acc: 0.734375] [adversarial loss: 1.347204, acc: 0.203125]\n",
      "11774: [discriminator loss: 0.584715, acc: 0.734375] [adversarial loss: 1.015722, acc: 0.390625]\n",
      "11775: [discriminator loss: 0.536358, acc: 0.695312] [adversarial loss: 1.296935, acc: 0.203125]\n",
      "11776: [discriminator loss: 0.576132, acc: 0.632812] [adversarial loss: 0.757227, acc: 0.484375]\n",
      "11777: [discriminator loss: 0.600808, acc: 0.632812] [adversarial loss: 1.388938, acc: 0.109375]\n",
      "11778: [discriminator loss: 0.588447, acc: 0.687500] [adversarial loss: 0.999981, acc: 0.296875]\n",
      "11779: [discriminator loss: 0.506731, acc: 0.718750] [adversarial loss: 1.161772, acc: 0.312500]\n",
      "11780: [discriminator loss: 0.554339, acc: 0.703125] [adversarial loss: 0.968060, acc: 0.250000]\n",
      "11781: [discriminator loss: 0.622293, acc: 0.648438] [adversarial loss: 1.514649, acc: 0.078125]\n",
      "11782: [discriminator loss: 0.549440, acc: 0.726562] [adversarial loss: 0.854569, acc: 0.500000]\n",
      "11783: [discriminator loss: 0.561118, acc: 0.703125] [adversarial loss: 1.321151, acc: 0.218750]\n",
      "11784: [discriminator loss: 0.566053, acc: 0.687500] [adversarial loss: 1.081997, acc: 0.281250]\n",
      "11785: [discriminator loss: 0.518280, acc: 0.726562] [adversarial loss: 1.010395, acc: 0.328125]\n",
      "11786: [discriminator loss: 0.563825, acc: 0.718750] [adversarial loss: 1.123465, acc: 0.218750]\n",
      "11787: [discriminator loss: 0.551736, acc: 0.718750] [adversarial loss: 1.148530, acc: 0.312500]\n",
      "11788: [discriminator loss: 0.515046, acc: 0.750000] [adversarial loss: 1.216543, acc: 0.250000]\n",
      "11789: [discriminator loss: 0.507874, acc: 0.734375] [adversarial loss: 1.200600, acc: 0.234375]\n",
      "11790: [discriminator loss: 0.588902, acc: 0.703125] [adversarial loss: 1.048820, acc: 0.359375]\n",
      "11791: [discriminator loss: 0.555626, acc: 0.710938] [adversarial loss: 1.410650, acc: 0.187500]\n",
      "11792: [discriminator loss: 0.538039, acc: 0.679688] [adversarial loss: 1.086056, acc: 0.250000]\n",
      "11793: [discriminator loss: 0.544156, acc: 0.671875] [adversarial loss: 0.976113, acc: 0.343750]\n",
      "11794: [discriminator loss: 0.513032, acc: 0.789062] [adversarial loss: 1.200273, acc: 0.171875]\n",
      "11795: [discriminator loss: 0.579118, acc: 0.648438] [adversarial loss: 1.017110, acc: 0.296875]\n",
      "11796: [discriminator loss: 0.519593, acc: 0.726562] [adversarial loss: 1.487250, acc: 0.156250]\n",
      "11797: [discriminator loss: 0.605140, acc: 0.679688] [adversarial loss: 0.709115, acc: 0.578125]\n",
      "11798: [discriminator loss: 0.709153, acc: 0.578125] [adversarial loss: 1.608701, acc: 0.093750]\n",
      "11799: [discriminator loss: 0.603688, acc: 0.679688] [adversarial loss: 0.839114, acc: 0.468750]\n",
      "11800: [discriminator loss: 0.550471, acc: 0.718750] [adversarial loss: 1.133887, acc: 0.250000]\n",
      "11801: [discriminator loss: 0.510961, acc: 0.671875] [adversarial loss: 1.052159, acc: 0.296875]\n",
      "11802: [discriminator loss: 0.519078, acc: 0.718750] [adversarial loss: 0.990319, acc: 0.375000]\n",
      "11803: [discriminator loss: 0.563910, acc: 0.679688] [adversarial loss: 1.144826, acc: 0.218750]\n",
      "11804: [discriminator loss: 0.566828, acc: 0.703125] [adversarial loss: 1.123754, acc: 0.281250]\n",
      "11805: [discriminator loss: 0.534092, acc: 0.734375] [adversarial loss: 1.275069, acc: 0.203125]\n",
      "11806: [discriminator loss: 0.606673, acc: 0.609375] [adversarial loss: 1.190683, acc: 0.187500]\n",
      "11807: [discriminator loss: 0.538492, acc: 0.750000] [adversarial loss: 1.119133, acc: 0.218750]\n",
      "11808: [discriminator loss: 0.512145, acc: 0.718750] [adversarial loss: 1.162590, acc: 0.234375]\n",
      "11809: [discriminator loss: 0.615749, acc: 0.664062] [adversarial loss: 1.200521, acc: 0.156250]\n",
      "11810: [discriminator loss: 0.508366, acc: 0.734375] [adversarial loss: 0.851947, acc: 0.453125]\n",
      "11811: [discriminator loss: 0.551136, acc: 0.656250] [adversarial loss: 1.581442, acc: 0.093750]\n",
      "11812: [discriminator loss: 0.630454, acc: 0.656250] [adversarial loss: 0.811957, acc: 0.468750]\n",
      "11813: [discriminator loss: 0.552802, acc: 0.718750] [adversarial loss: 1.325089, acc: 0.187500]\n",
      "11814: [discriminator loss: 0.553723, acc: 0.726562] [adversarial loss: 1.073764, acc: 0.281250]\n",
      "11815: [discriminator loss: 0.578812, acc: 0.632812] [adversarial loss: 1.296990, acc: 0.218750]\n",
      "11816: [discriminator loss: 0.557589, acc: 0.703125] [adversarial loss: 0.983642, acc: 0.359375]\n",
      "11817: [discriminator loss: 0.565332, acc: 0.703125] [adversarial loss: 1.433084, acc: 0.140625]\n",
      "11818: [discriminator loss: 0.544443, acc: 0.695312] [adversarial loss: 1.025710, acc: 0.296875]\n",
      "11819: [discriminator loss: 0.649687, acc: 0.640625] [adversarial loss: 1.240618, acc: 0.187500]\n",
      "11820: [discriminator loss: 0.553654, acc: 0.726562] [adversarial loss: 1.273793, acc: 0.140625]\n",
      "11821: [discriminator loss: 0.527538, acc: 0.742188] [adversarial loss: 0.961779, acc: 0.421875]\n",
      "11822: [discriminator loss: 0.605765, acc: 0.656250] [adversarial loss: 1.260847, acc: 0.203125]\n",
      "11823: [discriminator loss: 0.476718, acc: 0.789062] [adversarial loss: 1.155681, acc: 0.234375]\n",
      "11824: [discriminator loss: 0.475172, acc: 0.750000] [adversarial loss: 0.860983, acc: 0.468750]\n",
      "11825: [discriminator loss: 0.563795, acc: 0.726562] [adversarial loss: 1.367168, acc: 0.203125]\n",
      "11826: [discriminator loss: 0.568063, acc: 0.687500] [adversarial loss: 1.068864, acc: 0.296875]\n",
      "11827: [discriminator loss: 0.583988, acc: 0.656250] [adversarial loss: 1.456369, acc: 0.156250]\n",
      "11828: [discriminator loss: 0.556872, acc: 0.687500] [adversarial loss: 0.811375, acc: 0.500000]\n",
      "11829: [discriminator loss: 0.583524, acc: 0.703125] [adversarial loss: 1.632377, acc: 0.109375]\n",
      "11830: [discriminator loss: 0.588466, acc: 0.687500] [adversarial loss: 0.954950, acc: 0.406250]\n",
      "11831: [discriminator loss: 0.459616, acc: 0.820312] [adversarial loss: 1.322941, acc: 0.187500]\n",
      "11832: [discriminator loss: 0.607946, acc: 0.617188] [adversarial loss: 1.150488, acc: 0.140625]\n",
      "11833: [discriminator loss: 0.493855, acc: 0.804688] [adversarial loss: 1.357607, acc: 0.156250]\n",
      "11834: [discriminator loss: 0.556184, acc: 0.703125] [adversarial loss: 1.276986, acc: 0.093750]\n",
      "11835: [discriminator loss: 0.515716, acc: 0.757812] [adversarial loss: 0.967110, acc: 0.390625]\n",
      "11836: [discriminator loss: 0.651469, acc: 0.617188] [adversarial loss: 1.143359, acc: 0.171875]\n",
      "11837: [discriminator loss: 0.558043, acc: 0.710938] [adversarial loss: 1.125201, acc: 0.171875]\n",
      "11838: [discriminator loss: 0.509097, acc: 0.726562] [adversarial loss: 0.960728, acc: 0.390625]\n",
      "11839: [discriminator loss: 0.578090, acc: 0.703125] [adversarial loss: 1.518887, acc: 0.109375]\n",
      "11840: [discriminator loss: 0.545879, acc: 0.734375] [adversarial loss: 1.223527, acc: 0.250000]\n",
      "11841: [discriminator loss: 0.571093, acc: 0.703125] [adversarial loss: 1.275734, acc: 0.187500]\n",
      "11842: [discriminator loss: 0.574748, acc: 0.710938] [adversarial loss: 1.057995, acc: 0.203125]\n",
      "11843: [discriminator loss: 0.479678, acc: 0.781250] [adversarial loss: 1.482492, acc: 0.140625]\n",
      "11844: [discriminator loss: 0.593438, acc: 0.656250] [adversarial loss: 0.717705, acc: 0.593750]\n",
      "11845: [discriminator loss: 0.577640, acc: 0.664062] [adversarial loss: 1.490509, acc: 0.109375]\n",
      "11846: [discriminator loss: 0.582081, acc: 0.664062] [adversarial loss: 0.707216, acc: 0.640625]\n",
      "11847: [discriminator loss: 0.570145, acc: 0.664062] [adversarial loss: 1.533906, acc: 0.093750]\n",
      "11848: [discriminator loss: 0.512091, acc: 0.726562] [adversarial loss: 1.043421, acc: 0.343750]\n",
      "11849: [discriminator loss: 0.569979, acc: 0.687500] [adversarial loss: 1.087269, acc: 0.281250]\n",
      "11850: [discriminator loss: 0.621101, acc: 0.671875] [adversarial loss: 1.007122, acc: 0.265625]\n",
      "11851: [discriminator loss: 0.577265, acc: 0.664062] [adversarial loss: 1.450086, acc: 0.125000]\n",
      "11852: [discriminator loss: 0.557520, acc: 0.718750] [adversarial loss: 1.121480, acc: 0.265625]\n",
      "11853: [discriminator loss: 0.499872, acc: 0.726562] [adversarial loss: 1.241809, acc: 0.187500]\n",
      "11854: [discriminator loss: 0.546966, acc: 0.695312] [adversarial loss: 0.953942, acc: 0.359375]\n",
      "11855: [discriminator loss: 0.548337, acc: 0.757812] [adversarial loss: 1.027583, acc: 0.250000]\n",
      "11856: [discriminator loss: 0.543423, acc: 0.750000] [adversarial loss: 1.225221, acc: 0.187500]\n",
      "11857: [discriminator loss: 0.570453, acc: 0.710938] [adversarial loss: 1.348300, acc: 0.171875]\n",
      "11858: [discriminator loss: 0.582483, acc: 0.687500] [adversarial loss: 0.923268, acc: 0.406250]\n",
      "11859: [discriminator loss: 0.491579, acc: 0.750000] [adversarial loss: 1.302641, acc: 0.171875]\n",
      "11860: [discriminator loss: 0.558532, acc: 0.710938] [adversarial loss: 1.162553, acc: 0.296875]\n",
      "11861: [discriminator loss: 0.533786, acc: 0.734375] [adversarial loss: 0.990590, acc: 0.390625]\n",
      "11862: [discriminator loss: 0.505904, acc: 0.742188] [adversarial loss: 1.505723, acc: 0.109375]\n",
      "11863: [discriminator loss: 0.547354, acc: 0.726562] [adversarial loss: 1.070574, acc: 0.281250]\n",
      "11864: [discriminator loss: 0.498971, acc: 0.757812] [adversarial loss: 1.060810, acc: 0.296875]\n",
      "11865: [discriminator loss: 0.539136, acc: 0.671875] [adversarial loss: 1.091991, acc: 0.281250]\n",
      "11866: [discriminator loss: 0.555223, acc: 0.695312] [adversarial loss: 1.033365, acc: 0.343750]\n",
      "11867: [discriminator loss: 0.515274, acc: 0.726562] [adversarial loss: 1.108585, acc: 0.265625]\n",
      "11868: [discriminator loss: 0.635032, acc: 0.656250] [adversarial loss: 1.047650, acc: 0.296875]\n",
      "11869: [discriminator loss: 0.511434, acc: 0.820312] [adversarial loss: 0.875010, acc: 0.484375]\n",
      "11870: [discriminator loss: 0.584779, acc: 0.648438] [adversarial loss: 1.412215, acc: 0.171875]\n",
      "11871: [discriminator loss: 0.515124, acc: 0.679688] [adversarial loss: 0.952144, acc: 0.390625]\n",
      "11872: [discriminator loss: 0.564931, acc: 0.718750] [adversarial loss: 1.300932, acc: 0.187500]\n",
      "11873: [discriminator loss: 0.620435, acc: 0.632812] [adversarial loss: 0.807889, acc: 0.406250]\n",
      "11874: [discriminator loss: 0.645914, acc: 0.617188] [adversarial loss: 1.713401, acc: 0.078125]\n",
      "11875: [discriminator loss: 0.656811, acc: 0.601562] [adversarial loss: 0.830943, acc: 0.406250]\n",
      "11876: [discriminator loss: 0.604804, acc: 0.632812] [adversarial loss: 1.365263, acc: 0.171875]\n",
      "11877: [discriminator loss: 0.493866, acc: 0.773438] [adversarial loss: 0.989082, acc: 0.328125]\n",
      "11878: [discriminator loss: 0.596399, acc: 0.664062] [adversarial loss: 1.379165, acc: 0.218750]\n",
      "11879: [discriminator loss: 0.565128, acc: 0.687500] [adversarial loss: 0.992602, acc: 0.359375]\n",
      "11880: [discriminator loss: 0.517251, acc: 0.757812] [adversarial loss: 1.355495, acc: 0.187500]\n",
      "11881: [discriminator loss: 0.567641, acc: 0.648438] [adversarial loss: 0.870311, acc: 0.453125]\n",
      "11882: [discriminator loss: 0.608706, acc: 0.703125] [adversarial loss: 1.073098, acc: 0.328125]\n",
      "11883: [discriminator loss: 0.540181, acc: 0.718750] [adversarial loss: 1.421310, acc: 0.156250]\n",
      "11884: [discriminator loss: 0.524831, acc: 0.742188] [adversarial loss: 1.124365, acc: 0.265625]\n",
      "11885: [discriminator loss: 0.601711, acc: 0.656250] [adversarial loss: 1.234328, acc: 0.203125]\n",
      "11886: [discriminator loss: 0.525028, acc: 0.734375] [adversarial loss: 1.200521, acc: 0.265625]\n",
      "11887: [discriminator loss: 0.651365, acc: 0.664062] [adversarial loss: 1.199501, acc: 0.265625]\n",
      "11888: [discriminator loss: 0.624466, acc: 0.687500] [adversarial loss: 0.872343, acc: 0.406250]\n",
      "11889: [discriminator loss: 0.538170, acc: 0.742188] [adversarial loss: 1.176655, acc: 0.250000]\n",
      "11890: [discriminator loss: 0.600576, acc: 0.671875] [adversarial loss: 1.002701, acc: 0.296875]\n",
      "11891: [discriminator loss: 0.532837, acc: 0.742188] [adversarial loss: 1.396623, acc: 0.171875]\n",
      "11892: [discriminator loss: 0.645230, acc: 0.554688] [adversarial loss: 0.968136, acc: 0.328125]\n",
      "11893: [discriminator loss: 0.529639, acc: 0.750000] [adversarial loss: 1.327665, acc: 0.203125]\n",
      "11894: [discriminator loss: 0.495620, acc: 0.796875] [adversarial loss: 1.097606, acc: 0.218750]\n",
      "11895: [discriminator loss: 0.525649, acc: 0.750000] [adversarial loss: 1.161232, acc: 0.296875]\n",
      "11896: [discriminator loss: 0.573573, acc: 0.671875] [adversarial loss: 1.173295, acc: 0.171875]\n",
      "11897: [discriminator loss: 0.464695, acc: 0.789062] [adversarial loss: 1.221849, acc: 0.187500]\n",
      "11898: [discriminator loss: 0.482323, acc: 0.789062] [adversarial loss: 1.098790, acc: 0.281250]\n",
      "11899: [discriminator loss: 0.566639, acc: 0.703125] [adversarial loss: 0.865646, acc: 0.437500]\n",
      "11900: [discriminator loss: 0.532890, acc: 0.742188] [adversarial loss: 1.597205, acc: 0.109375]\n",
      "11901: [discriminator loss: 0.551127, acc: 0.695312] [adversarial loss: 0.959772, acc: 0.406250]\n",
      "11902: [discriminator loss: 0.582522, acc: 0.695312] [adversarial loss: 1.184129, acc: 0.250000]\n",
      "11903: [discriminator loss: 0.502782, acc: 0.734375] [adversarial loss: 1.187704, acc: 0.250000]\n",
      "11904: [discriminator loss: 0.530390, acc: 0.679688] [adversarial loss: 0.917398, acc: 0.468750]\n",
      "11905: [discriminator loss: 0.573602, acc: 0.671875] [adversarial loss: 1.387939, acc: 0.125000]\n",
      "11906: [discriminator loss: 0.598568, acc: 0.640625] [adversarial loss: 0.889912, acc: 0.406250]\n",
      "11907: [discriminator loss: 0.525741, acc: 0.703125] [adversarial loss: 1.461496, acc: 0.125000]\n",
      "11908: [discriminator loss: 0.556557, acc: 0.710938] [adversarial loss: 0.686914, acc: 0.546875]\n",
      "11909: [discriminator loss: 0.613260, acc: 0.617188] [adversarial loss: 1.375119, acc: 0.140625]\n",
      "11910: [discriminator loss: 0.558248, acc: 0.664062] [adversarial loss: 0.960015, acc: 0.406250]\n",
      "11911: [discriminator loss: 0.647201, acc: 0.640625] [adversarial loss: 1.793880, acc: 0.078125]\n",
      "11912: [discriminator loss: 0.590439, acc: 0.679688] [adversarial loss: 0.982603, acc: 0.312500]\n",
      "11913: [discriminator loss: 0.494259, acc: 0.765625] [adversarial loss: 1.356036, acc: 0.171875]\n",
      "11914: [discriminator loss: 0.471362, acc: 0.773438] [adversarial loss: 0.888160, acc: 0.406250]\n",
      "11915: [discriminator loss: 0.564686, acc: 0.695312] [adversarial loss: 1.428514, acc: 0.140625]\n",
      "11916: [discriminator loss: 0.494486, acc: 0.757812] [adversarial loss: 1.001151, acc: 0.343750]\n",
      "11917: [discriminator loss: 0.621118, acc: 0.656250] [adversarial loss: 1.346992, acc: 0.156250]\n",
      "11918: [discriminator loss: 0.551959, acc: 0.710938] [adversarial loss: 0.972243, acc: 0.359375]\n",
      "11919: [discriminator loss: 0.596743, acc: 0.656250] [adversarial loss: 1.580116, acc: 0.093750]\n",
      "11920: [discriminator loss: 0.593632, acc: 0.648438] [adversarial loss: 0.876294, acc: 0.390625]\n",
      "11921: [discriminator loss: 0.564953, acc: 0.742188] [adversarial loss: 1.405339, acc: 0.203125]\n",
      "11922: [discriminator loss: 0.586163, acc: 0.671875] [adversarial loss: 1.044548, acc: 0.265625]\n",
      "11923: [discriminator loss: 0.624099, acc: 0.640625] [adversarial loss: 1.224411, acc: 0.171875]\n",
      "11924: [discriminator loss: 0.498747, acc: 0.726562] [adversarial loss: 1.186897, acc: 0.234375]\n",
      "11925: [discriminator loss: 0.593480, acc: 0.703125] [adversarial loss: 1.182824, acc: 0.203125]\n",
      "11926: [discriminator loss: 0.579597, acc: 0.687500] [adversarial loss: 1.136157, acc: 0.265625]\n",
      "11927: [discriminator loss: 0.499666, acc: 0.710938] [adversarial loss: 1.231047, acc: 0.250000]\n",
      "11928: [discriminator loss: 0.581314, acc: 0.656250] [adversarial loss: 0.926347, acc: 0.375000]\n",
      "11929: [discriminator loss: 0.532149, acc: 0.757812] [adversarial loss: 1.514266, acc: 0.093750]\n",
      "11930: [discriminator loss: 0.592551, acc: 0.679688] [adversarial loss: 0.725159, acc: 0.593750]\n",
      "11931: [discriminator loss: 0.582989, acc: 0.703125] [adversarial loss: 1.691529, acc: 0.109375]\n",
      "11932: [discriminator loss: 0.610676, acc: 0.687500] [adversarial loss: 0.878352, acc: 0.468750]\n",
      "11933: [discriminator loss: 0.604655, acc: 0.632812] [adversarial loss: 1.193940, acc: 0.250000]\n",
      "11934: [discriminator loss: 0.529118, acc: 0.703125] [adversarial loss: 1.142474, acc: 0.343750]\n",
      "11935: [discriminator loss: 0.608313, acc: 0.640625] [adversarial loss: 1.093632, acc: 0.250000]\n",
      "11936: [discriminator loss: 0.496626, acc: 0.742188] [adversarial loss: 1.132665, acc: 0.281250]\n",
      "11937: [discriminator loss: 0.533892, acc: 0.679688] [adversarial loss: 1.180630, acc: 0.171875]\n",
      "11938: [discriminator loss: 0.557078, acc: 0.695312] [adversarial loss: 0.851096, acc: 0.437500]\n",
      "11939: [discriminator loss: 0.540385, acc: 0.703125] [adversarial loss: 1.199776, acc: 0.234375]\n",
      "11940: [discriminator loss: 0.447146, acc: 0.765625] [adversarial loss: 1.214898, acc: 0.187500]\n",
      "11941: [discriminator loss: 0.583010, acc: 0.687500] [adversarial loss: 1.009574, acc: 0.343750]\n",
      "11942: [discriminator loss: 0.546244, acc: 0.742188] [adversarial loss: 1.295208, acc: 0.234375]\n",
      "11943: [discriminator loss: 0.588905, acc: 0.664062] [adversarial loss: 0.921097, acc: 0.343750]\n",
      "11944: [discriminator loss: 0.557452, acc: 0.679688] [adversarial loss: 1.202762, acc: 0.218750]\n",
      "11945: [discriminator loss: 0.519234, acc: 0.718750] [adversarial loss: 0.914919, acc: 0.406250]\n",
      "11946: [discriminator loss: 0.559330, acc: 0.671875] [adversarial loss: 1.509772, acc: 0.109375]\n",
      "11947: [discriminator loss: 0.601955, acc: 0.640625] [adversarial loss: 0.872397, acc: 0.515625]\n",
      "11948: [discriminator loss: 0.516229, acc: 0.734375] [adversarial loss: 1.504117, acc: 0.171875]\n",
      "11949: [discriminator loss: 0.608006, acc: 0.632812] [adversarial loss: 0.940379, acc: 0.375000]\n",
      "11950: [discriminator loss: 0.574225, acc: 0.671875] [adversarial loss: 1.196344, acc: 0.281250]\n",
      "11951: [discriminator loss: 0.542629, acc: 0.750000] [adversarial loss: 1.104757, acc: 0.265625]\n",
      "11952: [discriminator loss: 0.576559, acc: 0.671875] [adversarial loss: 1.214780, acc: 0.234375]\n",
      "11953: [discriminator loss: 0.504507, acc: 0.734375] [adversarial loss: 1.113539, acc: 0.250000]\n",
      "11954: [discriminator loss: 0.500346, acc: 0.757812] [adversarial loss: 1.142565, acc: 0.296875]\n",
      "11955: [discriminator loss: 0.554539, acc: 0.710938] [adversarial loss: 1.308633, acc: 0.171875]\n",
      "11956: [discriminator loss: 0.529756, acc: 0.703125] [adversarial loss: 1.089382, acc: 0.281250]\n",
      "11957: [discriminator loss: 0.587819, acc: 0.671875] [adversarial loss: 0.918445, acc: 0.375000]\n",
      "11958: [discriminator loss: 0.567981, acc: 0.687500] [adversarial loss: 1.266358, acc: 0.265625]\n",
      "11959: [discriminator loss: 0.519250, acc: 0.765625] [adversarial loss: 0.781684, acc: 0.531250]\n",
      "11960: [discriminator loss: 0.559062, acc: 0.664062] [adversarial loss: 1.631099, acc: 0.078125]\n",
      "11961: [discriminator loss: 0.605001, acc: 0.664062] [adversarial loss: 0.694039, acc: 0.578125]\n",
      "11962: [discriminator loss: 0.595026, acc: 0.679688] [adversarial loss: 1.565631, acc: 0.062500]\n",
      "11963: [discriminator loss: 0.603660, acc: 0.640625] [adversarial loss: 0.974496, acc: 0.343750]\n",
      "11964: [discriminator loss: 0.648912, acc: 0.625000] [adversarial loss: 1.085433, acc: 0.250000]\n",
      "11965: [discriminator loss: 0.514352, acc: 0.718750] [adversarial loss: 1.135489, acc: 0.218750]\n",
      "11966: [discriminator loss: 0.628571, acc: 0.648438] [adversarial loss: 1.018345, acc: 0.359375]\n",
      "11967: [discriminator loss: 0.490191, acc: 0.765625] [adversarial loss: 1.203395, acc: 0.187500]\n",
      "11968: [discriminator loss: 0.564399, acc: 0.703125] [adversarial loss: 0.988376, acc: 0.281250]\n",
      "11969: [discriminator loss: 0.558095, acc: 0.718750] [adversarial loss: 1.291783, acc: 0.171875]\n",
      "11970: [discriminator loss: 0.570293, acc: 0.679688] [adversarial loss: 1.251172, acc: 0.125000]\n",
      "11971: [discriminator loss: 0.522592, acc: 0.718750] [adversarial loss: 0.888184, acc: 0.406250]\n",
      "11972: [discriminator loss: 0.525091, acc: 0.742188] [adversarial loss: 1.174671, acc: 0.187500]\n",
      "11973: [discriminator loss: 0.539077, acc: 0.687500] [adversarial loss: 1.231852, acc: 0.140625]\n",
      "11974: [discriminator loss: 0.542417, acc: 0.710938] [adversarial loss: 0.894607, acc: 0.421875]\n",
      "11975: [discriminator loss: 0.590915, acc: 0.695312] [adversarial loss: 1.392719, acc: 0.171875]\n",
      "11976: [discriminator loss: 0.578399, acc: 0.664062] [adversarial loss: 0.842464, acc: 0.375000]\n",
      "11977: [discriminator loss: 0.558785, acc: 0.687500] [adversarial loss: 1.385752, acc: 0.062500]\n",
      "11978: [discriminator loss: 0.579733, acc: 0.648438] [adversarial loss: 1.024116, acc: 0.343750]\n",
      "11979: [discriminator loss: 0.557951, acc: 0.703125] [adversarial loss: 1.224704, acc: 0.171875]\n",
      "11980: [discriminator loss: 0.560444, acc: 0.679688] [adversarial loss: 1.049734, acc: 0.250000]\n",
      "11981: [discriminator loss: 0.575013, acc: 0.687500] [adversarial loss: 1.180075, acc: 0.234375]\n",
      "11982: [discriminator loss: 0.533118, acc: 0.734375] [adversarial loss: 1.102257, acc: 0.265625]\n",
      "11983: [discriminator loss: 0.567752, acc: 0.695312] [adversarial loss: 1.182123, acc: 0.296875]\n",
      "11984: [discriminator loss: 0.557805, acc: 0.664062] [adversarial loss: 1.111545, acc: 0.312500]\n",
      "11985: [discriminator loss: 0.514298, acc: 0.718750] [adversarial loss: 1.322133, acc: 0.187500]\n",
      "11986: [discriminator loss: 0.620426, acc: 0.640625] [adversarial loss: 0.996969, acc: 0.328125]\n",
      "11987: [discriminator loss: 0.581872, acc: 0.703125] [adversarial loss: 1.299629, acc: 0.156250]\n",
      "11988: [discriminator loss: 0.591579, acc: 0.687500] [adversarial loss: 1.071993, acc: 0.296875]\n",
      "11989: [discriminator loss: 0.573731, acc: 0.726562] [adversarial loss: 1.152368, acc: 0.250000]\n",
      "11990: [discriminator loss: 0.557123, acc: 0.710938] [adversarial loss: 1.004718, acc: 0.328125]\n",
      "11991: [discriminator loss: 0.576593, acc: 0.757812] [adversarial loss: 1.472722, acc: 0.156250]\n",
      "11992: [discriminator loss: 0.575736, acc: 0.695312] [adversarial loss: 0.881178, acc: 0.453125]\n",
      "11993: [discriminator loss: 0.563847, acc: 0.679688] [adversarial loss: 1.325100, acc: 0.140625]\n",
      "11994: [discriminator loss: 0.532882, acc: 0.687500] [adversarial loss: 0.985325, acc: 0.359375]\n",
      "11995: [discriminator loss: 0.470740, acc: 0.796875] [adversarial loss: 1.417982, acc: 0.093750]\n",
      "11996: [discriminator loss: 0.639867, acc: 0.609375] [adversarial loss: 1.003788, acc: 0.265625]\n",
      "11997: [discriminator loss: 0.594560, acc: 0.679688] [adversarial loss: 1.419266, acc: 0.125000]\n",
      "11998: [discriminator loss: 0.566267, acc: 0.656250] [adversarial loss: 0.903942, acc: 0.328125]\n",
      "11999: [discriminator loss: 0.513770, acc: 0.773438] [adversarial loss: 1.254233, acc: 0.093750]\n",
      "12000: [discriminator loss: 0.554730, acc: 0.718750] [adversarial loss: 0.954394, acc: 0.343750]\n",
      "12001: [discriminator loss: 0.513630, acc: 0.765625] [adversarial loss: 1.318892, acc: 0.171875]\n",
      "12002: [discriminator loss: 0.673289, acc: 0.585938] [adversarial loss: 0.893153, acc: 0.421875]\n",
      "12003: [discriminator loss: 0.522801, acc: 0.718750] [adversarial loss: 1.221325, acc: 0.171875]\n",
      "12004: [discriminator loss: 0.570133, acc: 0.679688] [adversarial loss: 1.230137, acc: 0.203125]\n",
      "12005: [discriminator loss: 0.576648, acc: 0.718750] [adversarial loss: 0.996619, acc: 0.343750]\n",
      "12006: [discriminator loss: 0.522018, acc: 0.757812] [adversarial loss: 1.175602, acc: 0.234375]\n",
      "12007: [discriminator loss: 0.527483, acc: 0.718750] [adversarial loss: 0.907019, acc: 0.453125]\n",
      "12008: [discriminator loss: 0.608032, acc: 0.710938] [adversarial loss: 1.189051, acc: 0.171875]\n",
      "12009: [discriminator loss: 0.478733, acc: 0.765625] [adversarial loss: 0.964916, acc: 0.328125]\n",
      "12010: [discriminator loss: 0.543172, acc: 0.695312] [adversarial loss: 1.398384, acc: 0.156250]\n",
      "12011: [discriminator loss: 0.540634, acc: 0.734375] [adversarial loss: 0.754692, acc: 0.562500]\n",
      "12012: [discriminator loss: 0.510910, acc: 0.726562] [adversarial loss: 1.170131, acc: 0.265625]\n",
      "12013: [discriminator loss: 0.602326, acc: 0.695312] [adversarial loss: 1.167260, acc: 0.187500]\n",
      "12014: [discriminator loss: 0.538541, acc: 0.773438] [adversarial loss: 0.894270, acc: 0.406250]\n",
      "12015: [discriminator loss: 0.558010, acc: 0.726562] [adversarial loss: 1.217520, acc: 0.234375]\n",
      "12016: [discriminator loss: 0.536916, acc: 0.765625] [adversarial loss: 1.256449, acc: 0.171875]\n",
      "12017: [discriminator loss: 0.573514, acc: 0.671875] [adversarial loss: 1.255288, acc: 0.218750]\n",
      "12018: [discriminator loss: 0.608525, acc: 0.671875] [adversarial loss: 1.124499, acc: 0.187500]\n",
      "12019: [discriminator loss: 0.572469, acc: 0.687500] [adversarial loss: 1.478185, acc: 0.093750]\n",
      "12020: [discriminator loss: 0.614814, acc: 0.671875] [adversarial loss: 0.765308, acc: 0.515625]\n",
      "12021: [discriminator loss: 0.602686, acc: 0.648438] [adversarial loss: 1.385986, acc: 0.093750]\n",
      "12022: [discriminator loss: 0.559766, acc: 0.648438] [adversarial loss: 0.744242, acc: 0.578125]\n",
      "12023: [discriminator loss: 0.547012, acc: 0.773438] [adversarial loss: 1.215225, acc: 0.125000]\n",
      "12024: [discriminator loss: 0.554131, acc: 0.710938] [adversarial loss: 1.152496, acc: 0.218750]\n",
      "12025: [discriminator loss: 0.542813, acc: 0.726562] [adversarial loss: 1.177766, acc: 0.203125]\n",
      "12026: [discriminator loss: 0.542416, acc: 0.703125] [adversarial loss: 1.019726, acc: 0.281250]\n",
      "12027: [discriminator loss: 0.583965, acc: 0.656250] [adversarial loss: 1.373937, acc: 0.109375]\n",
      "12028: [discriminator loss: 0.610011, acc: 0.609375] [adversarial loss: 0.815899, acc: 0.406250]\n",
      "12029: [discriminator loss: 0.576681, acc: 0.656250] [adversarial loss: 1.465532, acc: 0.125000]\n",
      "12030: [discriminator loss: 0.639052, acc: 0.640625] [adversarial loss: 0.943864, acc: 0.375000]\n",
      "12031: [discriminator loss: 0.571541, acc: 0.679688] [adversarial loss: 1.466624, acc: 0.125000]\n",
      "12032: [discriminator loss: 0.556271, acc: 0.679688] [adversarial loss: 0.901838, acc: 0.359375]\n",
      "12033: [discriminator loss: 0.574034, acc: 0.687500] [adversarial loss: 1.163075, acc: 0.250000]\n",
      "12034: [discriminator loss: 0.572408, acc: 0.687500] [adversarial loss: 1.193337, acc: 0.250000]\n",
      "12035: [discriminator loss: 0.586886, acc: 0.687500] [adversarial loss: 1.219983, acc: 0.296875]\n",
      "12036: [discriminator loss: 0.513360, acc: 0.734375] [adversarial loss: 1.132525, acc: 0.250000]\n",
      "12037: [discriminator loss: 0.563702, acc: 0.703125] [adversarial loss: 1.264842, acc: 0.203125]\n",
      "12038: [discriminator loss: 0.595269, acc: 0.679688] [adversarial loss: 0.906053, acc: 0.437500]\n",
      "12039: [discriminator loss: 0.585597, acc: 0.656250] [adversarial loss: 1.539088, acc: 0.078125]\n",
      "12040: [discriminator loss: 0.606019, acc: 0.625000] [adversarial loss: 0.968989, acc: 0.390625]\n",
      "12041: [discriminator loss: 0.519183, acc: 0.750000] [adversarial loss: 1.060889, acc: 0.296875]\n",
      "12042: [discriminator loss: 0.573245, acc: 0.710938] [adversarial loss: 1.236819, acc: 0.265625]\n",
      "12043: [discriminator loss: 0.493882, acc: 0.734375] [adversarial loss: 1.218086, acc: 0.171875]\n",
      "12044: [discriminator loss: 0.595635, acc: 0.679688] [adversarial loss: 1.113662, acc: 0.250000]\n",
      "12045: [discriminator loss: 0.570813, acc: 0.695312] [adversarial loss: 1.184357, acc: 0.171875]\n",
      "12046: [discriminator loss: 0.547110, acc: 0.726562] [adversarial loss: 1.071142, acc: 0.296875]\n",
      "12047: [discriminator loss: 0.573935, acc: 0.718750] [adversarial loss: 1.205044, acc: 0.296875]\n",
      "12048: [discriminator loss: 0.580098, acc: 0.695312] [adversarial loss: 1.103345, acc: 0.296875]\n",
      "12049: [discriminator loss: 0.564340, acc: 0.695312] [adversarial loss: 1.345156, acc: 0.093750]\n",
      "12050: [discriminator loss: 0.554129, acc: 0.695312] [adversarial loss: 0.849038, acc: 0.500000]\n",
      "12051: [discriminator loss: 0.559426, acc: 0.695312] [adversarial loss: 1.394918, acc: 0.156250]\n",
      "12052: [discriminator loss: 0.548552, acc: 0.703125] [adversarial loss: 0.907793, acc: 0.421875]\n",
      "12053: [discriminator loss: 0.514967, acc: 0.796875] [adversarial loss: 1.341020, acc: 0.171875]\n",
      "12054: [discriminator loss: 0.511482, acc: 0.710938] [adversarial loss: 0.966452, acc: 0.375000]\n",
      "12055: [discriminator loss: 0.573308, acc: 0.695312] [adversarial loss: 1.216631, acc: 0.203125]\n",
      "12056: [discriminator loss: 0.599634, acc: 0.648438] [adversarial loss: 0.828134, acc: 0.468750]\n",
      "12057: [discriminator loss: 0.546251, acc: 0.664062] [adversarial loss: 1.623041, acc: 0.046875]\n",
      "12058: [discriminator loss: 0.522435, acc: 0.710938] [adversarial loss: 0.868479, acc: 0.421875]\n",
      "12059: [discriminator loss: 0.505329, acc: 0.726562] [adversarial loss: 1.581044, acc: 0.078125]\n",
      "12060: [discriminator loss: 0.592389, acc: 0.679688] [adversarial loss: 0.750085, acc: 0.562500]\n",
      "12061: [discriminator loss: 0.589333, acc: 0.671875] [adversarial loss: 1.476970, acc: 0.093750]\n",
      "12062: [discriminator loss: 0.567948, acc: 0.671875] [adversarial loss: 0.915183, acc: 0.453125]\n",
      "12063: [discriminator loss: 0.601876, acc: 0.687500] [adversarial loss: 1.364819, acc: 0.187500]\n",
      "12064: [discriminator loss: 0.554131, acc: 0.695312] [adversarial loss: 1.031483, acc: 0.343750]\n",
      "12065: [discriminator loss: 0.559209, acc: 0.703125] [adversarial loss: 1.164106, acc: 0.265625]\n",
      "12066: [discriminator loss: 0.514014, acc: 0.742188] [adversarial loss: 1.111999, acc: 0.234375]\n",
      "12067: [discriminator loss: 0.526096, acc: 0.710938] [adversarial loss: 1.093883, acc: 0.265625]\n",
      "12068: [discriminator loss: 0.523911, acc: 0.765625] [adversarial loss: 1.306004, acc: 0.250000]\n",
      "12069: [discriminator loss: 0.597579, acc: 0.687500] [adversarial loss: 0.961067, acc: 0.375000]\n",
      "12070: [discriminator loss: 0.533694, acc: 0.734375] [adversarial loss: 1.504493, acc: 0.109375]\n",
      "12071: [discriminator loss: 0.553707, acc: 0.695312] [adversarial loss: 1.209473, acc: 0.203125]\n",
      "12072: [discriminator loss: 0.556507, acc: 0.687500] [adversarial loss: 1.213929, acc: 0.187500]\n",
      "12073: [discriminator loss: 0.484699, acc: 0.734375] [adversarial loss: 1.419527, acc: 0.140625]\n",
      "12074: [discriminator loss: 0.557542, acc: 0.703125] [adversarial loss: 1.024509, acc: 0.312500]\n",
      "12075: [discriminator loss: 0.582831, acc: 0.679688] [adversarial loss: 1.432449, acc: 0.093750]\n",
      "12076: [discriminator loss: 0.555638, acc: 0.687500] [adversarial loss: 0.949572, acc: 0.359375]\n",
      "12077: [discriminator loss: 0.602187, acc: 0.648438] [adversarial loss: 1.249633, acc: 0.187500]\n",
      "12078: [discriminator loss: 0.548752, acc: 0.687500] [adversarial loss: 1.151400, acc: 0.125000]\n",
      "12079: [discriminator loss: 0.570308, acc: 0.726562] [adversarial loss: 1.218760, acc: 0.281250]\n",
      "12080: [discriminator loss: 0.565382, acc: 0.695312] [adversarial loss: 1.086763, acc: 0.281250]\n",
      "12081: [discriminator loss: 0.515650, acc: 0.765625] [adversarial loss: 1.161710, acc: 0.218750]\n",
      "12082: [discriminator loss: 0.508050, acc: 0.804688] [adversarial loss: 1.081398, acc: 0.312500]\n",
      "12083: [discriminator loss: 0.581414, acc: 0.664062] [adversarial loss: 1.055611, acc: 0.328125]\n",
      "12084: [discriminator loss: 0.604483, acc: 0.648438] [adversarial loss: 1.420237, acc: 0.140625]\n",
      "12085: [discriminator loss: 0.551802, acc: 0.734375] [adversarial loss: 0.820690, acc: 0.406250]\n",
      "12086: [discriminator loss: 0.540236, acc: 0.742188] [adversarial loss: 1.387531, acc: 0.156250]\n",
      "12087: [discriminator loss: 0.603128, acc: 0.656250] [adversarial loss: 0.859131, acc: 0.406250]\n",
      "12088: [discriminator loss: 0.604501, acc: 0.656250] [adversarial loss: 1.516254, acc: 0.156250]\n",
      "12089: [discriminator loss: 0.567005, acc: 0.718750] [adversarial loss: 1.003961, acc: 0.359375]\n",
      "12090: [discriminator loss: 0.537695, acc: 0.742188] [adversarial loss: 1.319905, acc: 0.125000]\n",
      "12091: [discriminator loss: 0.581590, acc: 0.695312] [adversarial loss: 1.064292, acc: 0.296875]\n",
      "12092: [discriminator loss: 0.600865, acc: 0.679688] [adversarial loss: 1.137362, acc: 0.218750]\n",
      "12093: [discriminator loss: 0.502025, acc: 0.726562] [adversarial loss: 1.067797, acc: 0.187500]\n",
      "12094: [discriminator loss: 0.517886, acc: 0.734375] [adversarial loss: 0.883300, acc: 0.468750]\n",
      "12095: [discriminator loss: 0.566015, acc: 0.703125] [adversarial loss: 1.364767, acc: 0.140625]\n",
      "12096: [discriminator loss: 0.601608, acc: 0.664062] [adversarial loss: 1.012212, acc: 0.375000]\n",
      "12097: [discriminator loss: 0.530991, acc: 0.765625] [adversarial loss: 1.443269, acc: 0.156250]\n",
      "12098: [discriminator loss: 0.587844, acc: 0.742188] [adversarial loss: 1.376139, acc: 0.296875]\n",
      "12099: [discriminator loss: 0.592759, acc: 0.679688] [adversarial loss: 1.138958, acc: 0.156250]\n",
      "12100: [discriminator loss: 0.647924, acc: 0.632812] [adversarial loss: 1.157971, acc: 0.234375]\n",
      "12101: [discriminator loss: 0.535582, acc: 0.703125] [adversarial loss: 1.075714, acc: 0.312500]\n",
      "12102: [discriminator loss: 0.497257, acc: 0.726562] [adversarial loss: 1.044047, acc: 0.312500]\n",
      "12103: [discriminator loss: 0.548150, acc: 0.695312] [adversarial loss: 1.425316, acc: 0.125000]\n",
      "12104: [discriminator loss: 0.549990, acc: 0.750000] [adversarial loss: 0.867480, acc: 0.375000]\n",
      "12105: [discriminator loss: 0.541012, acc: 0.648438] [adversarial loss: 1.700469, acc: 0.046875]\n",
      "12106: [discriminator loss: 0.545735, acc: 0.710938] [adversarial loss: 0.885082, acc: 0.437500]\n",
      "12107: [discriminator loss: 0.562980, acc: 0.703125] [adversarial loss: 1.453361, acc: 0.156250]\n",
      "12108: [discriminator loss: 0.525082, acc: 0.742188] [adversarial loss: 0.917258, acc: 0.390625]\n",
      "12109: [discriminator loss: 0.527362, acc: 0.742188] [adversarial loss: 1.089726, acc: 0.265625]\n",
      "12110: [discriminator loss: 0.479165, acc: 0.773438] [adversarial loss: 1.302811, acc: 0.156250]\n",
      "12111: [discriminator loss: 0.513551, acc: 0.742188] [adversarial loss: 0.896321, acc: 0.468750]\n",
      "12112: [discriminator loss: 0.502376, acc: 0.773438] [adversarial loss: 1.467089, acc: 0.140625]\n",
      "12113: [discriminator loss: 0.594637, acc: 0.679688] [adversarial loss: 0.999250, acc: 0.250000]\n",
      "12114: [discriminator loss: 0.513822, acc: 0.773438] [adversarial loss: 1.369429, acc: 0.156250]\n",
      "12115: [discriminator loss: 0.593795, acc: 0.679688] [adversarial loss: 1.034864, acc: 0.328125]\n",
      "12116: [discriminator loss: 0.613867, acc: 0.687500] [adversarial loss: 1.145378, acc: 0.250000]\n",
      "12117: [discriminator loss: 0.520488, acc: 0.773438] [adversarial loss: 0.841261, acc: 0.406250]\n",
      "12118: [discriminator loss: 0.558827, acc: 0.695312] [adversarial loss: 1.363653, acc: 0.218750]\n",
      "12119: [discriminator loss: 0.565102, acc: 0.710938] [adversarial loss: 0.983698, acc: 0.375000]\n",
      "12120: [discriminator loss: 0.596771, acc: 0.671875] [adversarial loss: 1.287317, acc: 0.203125]\n",
      "12121: [discriminator loss: 0.566041, acc: 0.695312] [adversarial loss: 1.008359, acc: 0.437500]\n",
      "12122: [discriminator loss: 0.663471, acc: 0.625000] [adversarial loss: 1.475738, acc: 0.171875]\n",
      "12123: [discriminator loss: 0.583722, acc: 0.656250] [adversarial loss: 0.898234, acc: 0.359375]\n",
      "12124: [discriminator loss: 0.556898, acc: 0.773438] [adversarial loss: 1.374222, acc: 0.125000]\n",
      "12125: [discriminator loss: 0.494477, acc: 0.765625] [adversarial loss: 0.997175, acc: 0.390625]\n",
      "12126: [discriminator loss: 0.563458, acc: 0.664062] [adversarial loss: 1.475639, acc: 0.109375]\n",
      "12127: [discriminator loss: 0.591420, acc: 0.656250] [adversarial loss: 0.727089, acc: 0.593750]\n",
      "12128: [discriminator loss: 0.555700, acc: 0.726562] [adversarial loss: 1.229126, acc: 0.140625]\n",
      "12129: [discriminator loss: 0.547356, acc: 0.718750] [adversarial loss: 0.975450, acc: 0.359375]\n",
      "12130: [discriminator loss: 0.595990, acc: 0.617188] [adversarial loss: 1.160710, acc: 0.234375]\n",
      "12131: [discriminator loss: 0.568833, acc: 0.687500] [adversarial loss: 0.992966, acc: 0.343750]\n",
      "12132: [discriminator loss: 0.499063, acc: 0.773438] [adversarial loss: 1.308768, acc: 0.125000]\n",
      "12133: [discriminator loss: 0.551765, acc: 0.687500] [adversarial loss: 0.898064, acc: 0.406250]\n",
      "12134: [discriminator loss: 0.584954, acc: 0.695312] [adversarial loss: 1.305719, acc: 0.156250]\n",
      "12135: [discriminator loss: 0.611738, acc: 0.671875] [adversarial loss: 1.244907, acc: 0.250000]\n",
      "12136: [discriminator loss: 0.607305, acc: 0.671875] [adversarial loss: 0.879090, acc: 0.437500]\n",
      "12137: [discriminator loss: 0.573974, acc: 0.679688] [adversarial loss: 1.428349, acc: 0.093750]\n",
      "12138: [discriminator loss: 0.552087, acc: 0.656250] [adversarial loss: 1.157944, acc: 0.187500]\n",
      "12139: [discriminator loss: 0.513956, acc: 0.765625] [adversarial loss: 1.241856, acc: 0.187500]\n",
      "12140: [discriminator loss: 0.540253, acc: 0.718750] [adversarial loss: 0.866743, acc: 0.437500]\n",
      "12141: [discriminator loss: 0.669230, acc: 0.570312] [adversarial loss: 1.462664, acc: 0.062500]\n",
      "12142: [discriminator loss: 0.565914, acc: 0.687500] [adversarial loss: 0.798805, acc: 0.500000]\n",
      "12143: [discriminator loss: 0.563267, acc: 0.718750] [adversarial loss: 1.185273, acc: 0.296875]\n",
      "12144: [discriminator loss: 0.518615, acc: 0.703125] [adversarial loss: 1.325758, acc: 0.171875]\n",
      "12145: [discriminator loss: 0.504340, acc: 0.710938] [adversarial loss: 1.014445, acc: 0.343750]\n",
      "12146: [discriminator loss: 0.530565, acc: 0.710938] [adversarial loss: 1.198071, acc: 0.218750]\n",
      "12147: [discriminator loss: 0.560583, acc: 0.734375] [adversarial loss: 1.145797, acc: 0.265625]\n",
      "12148: [discriminator loss: 0.505988, acc: 0.734375] [adversarial loss: 1.203894, acc: 0.234375]\n",
      "12149: [discriminator loss: 0.475169, acc: 0.773438] [adversarial loss: 1.138869, acc: 0.281250]\n",
      "12150: [discriminator loss: 0.621665, acc: 0.671875] [adversarial loss: 1.373983, acc: 0.250000]\n",
      "12151: [discriminator loss: 0.475967, acc: 0.750000] [adversarial loss: 0.922698, acc: 0.421875]\n",
      "12152: [discriminator loss: 0.530076, acc: 0.726562] [adversarial loss: 1.196953, acc: 0.250000]\n",
      "12153: [discriminator loss: 0.550528, acc: 0.726562] [adversarial loss: 1.081449, acc: 0.203125]\n",
      "12154: [discriminator loss: 0.556859, acc: 0.742188] [adversarial loss: 1.119373, acc: 0.281250]\n",
      "12155: [discriminator loss: 0.593368, acc: 0.671875] [adversarial loss: 0.957984, acc: 0.406250]\n",
      "12156: [discriminator loss: 0.509230, acc: 0.750000] [adversarial loss: 1.176265, acc: 0.218750]\n",
      "12157: [discriminator loss: 0.623551, acc: 0.648438] [adversarial loss: 1.157755, acc: 0.234375]\n",
      "12158: [discriminator loss: 0.489171, acc: 0.781250] [adversarial loss: 0.956108, acc: 0.359375]\n",
      "12159: [discriminator loss: 0.523349, acc: 0.750000] [adversarial loss: 1.438128, acc: 0.171875]\n",
      "12160: [discriminator loss: 0.542436, acc: 0.726562] [adversarial loss: 0.834432, acc: 0.468750]\n",
      "12161: [discriminator loss: 0.548427, acc: 0.742188] [adversarial loss: 1.421586, acc: 0.156250]\n",
      "12162: [discriminator loss: 0.601630, acc: 0.671875] [adversarial loss: 1.109064, acc: 0.281250]\n",
      "12163: [discriminator loss: 0.543074, acc: 0.671875] [adversarial loss: 1.262203, acc: 0.156250]\n",
      "12164: [discriminator loss: 0.551447, acc: 0.742188] [adversarial loss: 1.164156, acc: 0.250000]\n",
      "12165: [discriminator loss: 0.601688, acc: 0.640625] [adversarial loss: 1.183902, acc: 0.296875]\n",
      "12166: [discriminator loss: 0.550795, acc: 0.679688] [adversarial loss: 1.132234, acc: 0.265625]\n",
      "12167: [discriminator loss: 0.529944, acc: 0.710938] [adversarial loss: 1.142221, acc: 0.265625]\n",
      "12168: [discriminator loss: 0.543642, acc: 0.718750] [adversarial loss: 1.062226, acc: 0.281250]\n",
      "12169: [discriminator loss: 0.624151, acc: 0.593750] [adversarial loss: 1.152861, acc: 0.125000]\n",
      "12170: [discriminator loss: 0.546210, acc: 0.703125] [adversarial loss: 1.038828, acc: 0.328125]\n",
      "12171: [discriminator loss: 0.596015, acc: 0.648438] [adversarial loss: 0.969718, acc: 0.406250]\n",
      "12172: [discriminator loss: 0.599878, acc: 0.671875] [adversarial loss: 0.981571, acc: 0.359375]\n",
      "12173: [discriminator loss: 0.565104, acc: 0.679688] [adversarial loss: 1.347062, acc: 0.171875]\n",
      "12174: [discriminator loss: 0.558524, acc: 0.742188] [adversarial loss: 1.303963, acc: 0.140625]\n",
      "12175: [discriminator loss: 0.598955, acc: 0.687500] [adversarial loss: 0.968082, acc: 0.312500]\n",
      "12176: [discriminator loss: 0.538887, acc: 0.734375] [adversarial loss: 1.361234, acc: 0.125000]\n",
      "12177: [discriminator loss: 0.594504, acc: 0.695312] [adversarial loss: 1.067123, acc: 0.375000]\n",
      "12178: [discriminator loss: 0.525638, acc: 0.710938] [adversarial loss: 1.000326, acc: 0.312500]\n",
      "12179: [discriminator loss: 0.617272, acc: 0.625000] [adversarial loss: 1.017458, acc: 0.312500]\n",
      "12180: [discriminator loss: 0.530961, acc: 0.695312] [adversarial loss: 1.100308, acc: 0.296875]\n",
      "12181: [discriminator loss: 0.512450, acc: 0.734375] [adversarial loss: 1.141069, acc: 0.187500]\n",
      "12182: [discriminator loss: 0.597340, acc: 0.679688] [adversarial loss: 1.154056, acc: 0.218750]\n",
      "12183: [discriminator loss: 0.573479, acc: 0.734375] [adversarial loss: 1.073574, acc: 0.250000]\n",
      "12184: [discriminator loss: 0.544843, acc: 0.734375] [adversarial loss: 1.482719, acc: 0.093750]\n",
      "12185: [discriminator loss: 0.645835, acc: 0.664062] [adversarial loss: 0.661819, acc: 0.593750]\n",
      "12186: [discriminator loss: 0.563905, acc: 0.710938] [adversarial loss: 1.581282, acc: 0.062500]\n",
      "12187: [discriminator loss: 0.584420, acc: 0.703125] [adversarial loss: 1.199371, acc: 0.218750]\n",
      "12188: [discriminator loss: 0.468518, acc: 0.750000] [adversarial loss: 1.020429, acc: 0.375000]\n",
      "12189: [discriminator loss: 0.581795, acc: 0.671875] [adversarial loss: 1.146647, acc: 0.265625]\n",
      "12190: [discriminator loss: 0.522472, acc: 0.687500] [adversarial loss: 1.211022, acc: 0.203125]\n",
      "12191: [discriminator loss: 0.555252, acc: 0.718750] [adversarial loss: 1.302521, acc: 0.156250]\n",
      "12192: [discriminator loss: 0.556487, acc: 0.695312] [adversarial loss: 0.974624, acc: 0.406250]\n",
      "12193: [discriminator loss: 0.572799, acc: 0.687500] [adversarial loss: 1.258252, acc: 0.218750]\n",
      "12194: [discriminator loss: 0.600477, acc: 0.687500] [adversarial loss: 0.874980, acc: 0.484375]\n",
      "12195: [discriminator loss: 0.476530, acc: 0.789062] [adversarial loss: 1.161463, acc: 0.250000]\n",
      "12196: [discriminator loss: 0.556443, acc: 0.742188] [adversarial loss: 1.046734, acc: 0.281250]\n",
      "12197: [discriminator loss: 0.621147, acc: 0.617188] [adversarial loss: 1.208017, acc: 0.281250]\n",
      "12198: [discriminator loss: 0.565928, acc: 0.703125] [adversarial loss: 1.035991, acc: 0.250000]\n",
      "12199: [discriminator loss: 0.596297, acc: 0.687500] [adversarial loss: 0.997722, acc: 0.375000]\n",
      "12200: [discriminator loss: 0.557153, acc: 0.726562] [adversarial loss: 1.062233, acc: 0.250000]\n",
      "12201: [discriminator loss: 0.591782, acc: 0.648438] [adversarial loss: 1.254810, acc: 0.281250]\n",
      "12202: [discriminator loss: 0.606328, acc: 0.617188] [adversarial loss: 1.004954, acc: 0.328125]\n",
      "12203: [discriminator loss: 0.593753, acc: 0.687500] [adversarial loss: 1.346723, acc: 0.125000]\n",
      "12204: [discriminator loss: 0.602617, acc: 0.687500] [adversarial loss: 0.888634, acc: 0.406250]\n",
      "12205: [discriminator loss: 0.574277, acc: 0.671875] [adversarial loss: 1.525248, acc: 0.125000]\n",
      "12206: [discriminator loss: 0.664850, acc: 0.656250] [adversarial loss: 0.767752, acc: 0.578125]\n",
      "12207: [discriminator loss: 0.654853, acc: 0.578125] [adversarial loss: 1.639098, acc: 0.062500]\n",
      "12208: [discriminator loss: 0.580039, acc: 0.687500] [adversarial loss: 0.819662, acc: 0.437500]\n",
      "12209: [discriminator loss: 0.524016, acc: 0.757812] [adversarial loss: 1.422982, acc: 0.093750]\n",
      "12210: [discriminator loss: 0.481757, acc: 0.789062] [adversarial loss: 1.045864, acc: 0.296875]\n",
      "12211: [discriminator loss: 0.571672, acc: 0.687500] [adversarial loss: 1.192338, acc: 0.218750]\n",
      "12212: [discriminator loss: 0.549034, acc: 0.687500] [adversarial loss: 0.714815, acc: 0.578125]\n",
      "12213: [discriminator loss: 0.554411, acc: 0.710938] [adversarial loss: 1.360403, acc: 0.156250]\n",
      "12214: [discriminator loss: 0.644094, acc: 0.617188] [adversarial loss: 0.913500, acc: 0.390625]\n",
      "12215: [discriminator loss: 0.584720, acc: 0.734375] [adversarial loss: 1.116563, acc: 0.265625]\n",
      "12216: [discriminator loss: 0.516483, acc: 0.742188] [adversarial loss: 1.004866, acc: 0.390625]\n",
      "12217: [discriminator loss: 0.531259, acc: 0.757812] [adversarial loss: 0.955141, acc: 0.343750]\n",
      "12218: [discriminator loss: 0.534594, acc: 0.750000] [adversarial loss: 1.273924, acc: 0.093750]\n",
      "12219: [discriminator loss: 0.582599, acc: 0.679688] [adversarial loss: 1.089867, acc: 0.328125]\n",
      "12220: [discriminator loss: 0.576269, acc: 0.726562] [adversarial loss: 1.050017, acc: 0.375000]\n",
      "12221: [discriminator loss: 0.530811, acc: 0.765625] [adversarial loss: 1.312141, acc: 0.140625]\n",
      "12222: [discriminator loss: 0.518529, acc: 0.757812] [adversarial loss: 0.980343, acc: 0.281250]\n",
      "12223: [discriminator loss: 0.445026, acc: 0.781250] [adversarial loss: 1.091164, acc: 0.296875]\n",
      "12224: [discriminator loss: 0.493988, acc: 0.726562] [adversarial loss: 0.850184, acc: 0.453125]\n",
      "12225: [discriminator loss: 0.549061, acc: 0.718750] [adversarial loss: 1.957736, acc: 0.062500]\n",
      "12226: [discriminator loss: 0.629891, acc: 0.671875] [adversarial loss: 0.747326, acc: 0.609375]\n",
      "12227: [discriminator loss: 0.534166, acc: 0.750000] [adversarial loss: 1.138386, acc: 0.265625]\n",
      "12228: [discriminator loss: 0.540708, acc: 0.703125] [adversarial loss: 1.010547, acc: 0.328125]\n",
      "12229: [discriminator loss: 0.545265, acc: 0.734375] [adversarial loss: 1.305264, acc: 0.187500]\n",
      "12230: [discriminator loss: 0.570818, acc: 0.742188] [adversarial loss: 1.039352, acc: 0.375000]\n",
      "12231: [discriminator loss: 0.554958, acc: 0.734375] [adversarial loss: 1.065041, acc: 0.312500]\n",
      "12232: [discriminator loss: 0.492756, acc: 0.742188] [adversarial loss: 1.434431, acc: 0.093750]\n",
      "12233: [discriminator loss: 0.553649, acc: 0.687500] [adversarial loss: 1.069728, acc: 0.312500]\n",
      "12234: [discriminator loss: 0.521462, acc: 0.734375] [adversarial loss: 1.119234, acc: 0.265625]\n",
      "12235: [discriminator loss: 0.592516, acc: 0.640625] [adversarial loss: 1.024356, acc: 0.328125]\n",
      "12236: [discriminator loss: 0.523217, acc: 0.734375] [adversarial loss: 1.593594, acc: 0.125000]\n",
      "12237: [discriminator loss: 0.520488, acc: 0.718750] [adversarial loss: 0.834702, acc: 0.453125]\n",
      "12238: [discriminator loss: 0.589206, acc: 0.695312] [adversarial loss: 1.196081, acc: 0.218750]\n",
      "12239: [discriminator loss: 0.542718, acc: 0.679688] [adversarial loss: 1.174132, acc: 0.187500]\n",
      "12240: [discriminator loss: 0.514850, acc: 0.765625] [adversarial loss: 1.140713, acc: 0.250000]\n",
      "12241: [discriminator loss: 0.565475, acc: 0.671875] [adversarial loss: 1.143050, acc: 0.328125]\n",
      "12242: [discriminator loss: 0.546837, acc: 0.687500] [adversarial loss: 1.306991, acc: 0.218750]\n",
      "12243: [discriminator loss: 0.626346, acc: 0.617188] [adversarial loss: 1.228147, acc: 0.265625]\n",
      "12244: [discriminator loss: 0.564314, acc: 0.710938] [adversarial loss: 0.994959, acc: 0.390625]\n",
      "12245: [discriminator loss: 0.617669, acc: 0.648438] [adversarial loss: 1.236542, acc: 0.250000]\n",
      "12246: [discriminator loss: 0.602448, acc: 0.656250] [adversarial loss: 0.938307, acc: 0.343750]\n",
      "12247: [discriminator loss: 0.514913, acc: 0.757812] [adversarial loss: 1.304165, acc: 0.218750]\n",
      "12248: [discriminator loss: 0.521890, acc: 0.773438] [adversarial loss: 0.889335, acc: 0.359375]\n",
      "12249: [discriminator loss: 0.576630, acc: 0.734375] [adversarial loss: 1.388040, acc: 0.125000]\n",
      "12250: [discriminator loss: 0.556195, acc: 0.718750] [adversarial loss: 0.921402, acc: 0.453125]\n",
      "12251: [discriminator loss: 0.583265, acc: 0.695312] [adversarial loss: 1.394410, acc: 0.109375]\n",
      "12252: [discriminator loss: 0.593440, acc: 0.671875] [adversarial loss: 0.844518, acc: 0.484375]\n",
      "12253: [discriminator loss: 0.583779, acc: 0.687500] [adversarial loss: 1.312186, acc: 0.218750]\n",
      "12254: [discriminator loss: 0.531429, acc: 0.726562] [adversarial loss: 1.074427, acc: 0.281250]\n",
      "12255: [discriminator loss: 0.510291, acc: 0.757812] [adversarial loss: 1.200094, acc: 0.250000]\n",
      "12256: [discriminator loss: 0.563337, acc: 0.671875] [adversarial loss: 1.232815, acc: 0.328125]\n",
      "12257: [discriminator loss: 0.536050, acc: 0.710938] [adversarial loss: 1.311696, acc: 0.156250]\n",
      "12258: [discriminator loss: 0.586266, acc: 0.695312] [adversarial loss: 0.951416, acc: 0.375000]\n",
      "12259: [discriminator loss: 0.543481, acc: 0.718750] [adversarial loss: 1.198344, acc: 0.187500]\n",
      "12260: [discriminator loss: 0.507131, acc: 0.750000] [adversarial loss: 1.206838, acc: 0.234375]\n",
      "12261: [discriminator loss: 0.490246, acc: 0.757812] [adversarial loss: 1.219021, acc: 0.234375]\n",
      "12262: [discriminator loss: 0.549413, acc: 0.703125] [adversarial loss: 1.087108, acc: 0.328125]\n",
      "12263: [discriminator loss: 0.538611, acc: 0.726562] [adversarial loss: 1.273791, acc: 0.218750]\n",
      "12264: [discriminator loss: 0.587397, acc: 0.648438] [adversarial loss: 1.030629, acc: 0.265625]\n",
      "12265: [discriminator loss: 0.583026, acc: 0.671875] [adversarial loss: 1.247320, acc: 0.234375]\n",
      "12266: [discriminator loss: 0.496584, acc: 0.750000] [adversarial loss: 1.006224, acc: 0.406250]\n",
      "12267: [discriminator loss: 0.536099, acc: 0.718750] [adversarial loss: 1.577551, acc: 0.093750]\n",
      "12268: [discriminator loss: 0.557482, acc: 0.718750] [adversarial loss: 0.857696, acc: 0.500000]\n",
      "12269: [discriminator loss: 0.519329, acc: 0.718750] [adversarial loss: 1.606014, acc: 0.125000]\n",
      "12270: [discriminator loss: 0.502646, acc: 0.726562] [adversarial loss: 0.961380, acc: 0.359375]\n",
      "12271: [discriminator loss: 0.575350, acc: 0.703125] [adversarial loss: 1.201258, acc: 0.218750]\n",
      "12272: [discriminator loss: 0.491120, acc: 0.734375] [adversarial loss: 1.215055, acc: 0.203125]\n",
      "12273: [discriminator loss: 0.565372, acc: 0.679688] [adversarial loss: 1.080639, acc: 0.328125]\n",
      "12274: [discriminator loss: 0.502387, acc: 0.757812] [adversarial loss: 1.342303, acc: 0.140625]\n",
      "12275: [discriminator loss: 0.584908, acc: 0.679688] [adversarial loss: 1.010181, acc: 0.406250]\n",
      "12276: [discriminator loss: 0.588275, acc: 0.726562] [adversarial loss: 1.322798, acc: 0.234375]\n",
      "12277: [discriminator loss: 0.637301, acc: 0.656250] [adversarial loss: 0.868661, acc: 0.375000]\n",
      "12278: [discriminator loss: 0.533777, acc: 0.734375] [adversarial loss: 1.192644, acc: 0.234375]\n",
      "12279: [discriminator loss: 0.587673, acc: 0.710938] [adversarial loss: 1.112253, acc: 0.250000]\n",
      "12280: [discriminator loss: 0.565403, acc: 0.648438] [adversarial loss: 1.272937, acc: 0.140625]\n",
      "12281: [discriminator loss: 0.546585, acc: 0.710938] [adversarial loss: 0.766861, acc: 0.468750]\n",
      "12282: [discriminator loss: 0.589246, acc: 0.710938] [adversarial loss: 1.472700, acc: 0.109375]\n",
      "12283: [discriminator loss: 0.576810, acc: 0.679688] [adversarial loss: 1.048403, acc: 0.265625]\n",
      "12284: [discriminator loss: 0.546582, acc: 0.710938] [adversarial loss: 1.226592, acc: 0.250000]\n",
      "12285: [discriminator loss: 0.596613, acc: 0.679688] [adversarial loss: 1.102442, acc: 0.265625]\n",
      "12286: [discriminator loss: 0.499385, acc: 0.734375] [adversarial loss: 1.195395, acc: 0.265625]\n",
      "12287: [discriminator loss: 0.584990, acc: 0.679688] [adversarial loss: 0.791879, acc: 0.500000]\n",
      "12288: [discriminator loss: 0.544058, acc: 0.710938] [adversarial loss: 1.255286, acc: 0.218750]\n",
      "12289: [discriminator loss: 0.495124, acc: 0.757812] [adversarial loss: 1.147897, acc: 0.250000]\n",
      "12290: [discriminator loss: 0.474958, acc: 0.750000] [adversarial loss: 1.337560, acc: 0.125000]\n",
      "12291: [discriminator loss: 0.471553, acc: 0.804688] [adversarial loss: 1.018850, acc: 0.343750]\n",
      "12292: [discriminator loss: 0.546010, acc: 0.742188] [adversarial loss: 1.125406, acc: 0.265625]\n",
      "12293: [discriminator loss: 0.671149, acc: 0.617188] [adversarial loss: 0.980964, acc: 0.296875]\n",
      "12294: [discriminator loss: 0.503119, acc: 0.742188] [adversarial loss: 1.425719, acc: 0.125000]\n",
      "12295: [discriminator loss: 0.619042, acc: 0.640625] [adversarial loss: 0.808792, acc: 0.437500]\n",
      "12296: [discriminator loss: 0.571266, acc: 0.726562] [adversarial loss: 1.213229, acc: 0.203125]\n",
      "12297: [discriminator loss: 0.501195, acc: 0.726562] [adversarial loss: 0.993436, acc: 0.359375]\n",
      "12298: [discriminator loss: 0.532741, acc: 0.750000] [adversarial loss: 1.295715, acc: 0.265625]\n",
      "12299: [discriminator loss: 0.527052, acc: 0.710938] [adversarial loss: 0.983845, acc: 0.375000]\n",
      "12300: [discriminator loss: 0.538272, acc: 0.710938] [adversarial loss: 1.254760, acc: 0.312500]\n",
      "12301: [discriminator loss: 0.585779, acc: 0.671875] [adversarial loss: 0.857146, acc: 0.437500]\n",
      "12302: [discriminator loss: 0.546150, acc: 0.726562] [adversarial loss: 1.258196, acc: 0.218750]\n",
      "12303: [discriminator loss: 0.466624, acc: 0.781250] [adversarial loss: 1.076383, acc: 0.328125]\n",
      "12304: [discriminator loss: 0.543716, acc: 0.703125] [adversarial loss: 1.111143, acc: 0.234375]\n",
      "12305: [discriminator loss: 0.546020, acc: 0.742188] [adversarial loss: 1.196295, acc: 0.296875]\n",
      "12306: [discriminator loss: 0.569320, acc: 0.640625] [adversarial loss: 1.185816, acc: 0.265625]\n",
      "12307: [discriminator loss: 0.639161, acc: 0.625000] [adversarial loss: 1.034144, acc: 0.265625]\n",
      "12308: [discriminator loss: 0.517225, acc: 0.773438] [adversarial loss: 1.255169, acc: 0.250000]\n",
      "12309: [discriminator loss: 0.571626, acc: 0.656250] [adversarial loss: 1.112547, acc: 0.312500]\n",
      "12310: [discriminator loss: 0.484476, acc: 0.781250] [adversarial loss: 1.219274, acc: 0.265625]\n",
      "12311: [discriminator loss: 0.495336, acc: 0.812500] [adversarial loss: 0.930687, acc: 0.390625]\n",
      "12312: [discriminator loss: 0.517297, acc: 0.726562] [adversarial loss: 0.974798, acc: 0.328125]\n",
      "12313: [discriminator loss: 0.514211, acc: 0.734375] [adversarial loss: 1.045770, acc: 0.312500]\n",
      "12314: [discriminator loss: 0.568120, acc: 0.679688] [adversarial loss: 0.927005, acc: 0.437500]\n",
      "12315: [discriminator loss: 0.523849, acc: 0.718750] [adversarial loss: 1.367558, acc: 0.140625]\n",
      "12316: [discriminator loss: 0.586162, acc: 0.679688] [adversarial loss: 1.193989, acc: 0.250000]\n",
      "12317: [discriminator loss: 0.588346, acc: 0.695312] [adversarial loss: 1.190449, acc: 0.281250]\n",
      "12318: [discriminator loss: 0.627708, acc: 0.679688] [adversarial loss: 1.509549, acc: 0.109375]\n",
      "12319: [discriminator loss: 0.570762, acc: 0.656250] [adversarial loss: 0.514429, acc: 0.796875]\n",
      "12320: [discriminator loss: 0.656070, acc: 0.617188] [adversarial loss: 1.798265, acc: 0.046875]\n",
      "12321: [discriminator loss: 0.705295, acc: 0.617188] [adversarial loss: 0.711467, acc: 0.593750]\n",
      "12322: [discriminator loss: 0.632243, acc: 0.656250] [adversarial loss: 1.255247, acc: 0.156250]\n",
      "12323: [discriminator loss: 0.540078, acc: 0.695312] [adversarial loss: 0.872104, acc: 0.468750]\n",
      "12324: [discriminator loss: 0.545764, acc: 0.742188] [adversarial loss: 1.345452, acc: 0.156250]\n",
      "12325: [discriminator loss: 0.598315, acc: 0.695312] [adversarial loss: 0.797080, acc: 0.562500]\n",
      "12326: [discriminator loss: 0.528133, acc: 0.679688] [adversarial loss: 1.381183, acc: 0.062500]\n",
      "12327: [discriminator loss: 0.596863, acc: 0.726562] [adversarial loss: 1.072366, acc: 0.265625]\n",
      "12328: [discriminator loss: 0.571908, acc: 0.718750] [adversarial loss: 1.190791, acc: 0.218750]\n",
      "12329: [discriminator loss: 0.556927, acc: 0.710938] [adversarial loss: 1.073383, acc: 0.312500]\n",
      "12330: [discriminator loss: 0.574707, acc: 0.679688] [adversarial loss: 0.999430, acc: 0.312500]\n",
      "12331: [discriminator loss: 0.563363, acc: 0.718750] [adversarial loss: 1.281691, acc: 0.140625]\n",
      "12332: [discriminator loss: 0.536262, acc: 0.718750] [adversarial loss: 1.086139, acc: 0.203125]\n",
      "12333: [discriminator loss: 0.590900, acc: 0.679688] [adversarial loss: 0.961943, acc: 0.406250]\n",
      "12334: [discriminator loss: 0.522617, acc: 0.757812] [adversarial loss: 0.923071, acc: 0.421875]\n",
      "12335: [discriminator loss: 0.606470, acc: 0.640625] [adversarial loss: 1.215991, acc: 0.218750]\n",
      "12336: [discriminator loss: 0.492348, acc: 0.757812] [adversarial loss: 1.265792, acc: 0.203125]\n",
      "12337: [discriminator loss: 0.560109, acc: 0.687500] [adversarial loss: 1.084965, acc: 0.375000]\n",
      "12338: [discriminator loss: 0.542027, acc: 0.773438] [adversarial loss: 1.499586, acc: 0.109375]\n",
      "12339: [discriminator loss: 0.524423, acc: 0.781250] [adversarial loss: 0.930278, acc: 0.421875]\n",
      "12340: [discriminator loss: 0.526988, acc: 0.718750] [adversarial loss: 1.168153, acc: 0.187500]\n",
      "12341: [discriminator loss: 0.503150, acc: 0.750000] [adversarial loss: 1.256194, acc: 0.265625]\n",
      "12342: [discriminator loss: 0.577465, acc: 0.664062] [adversarial loss: 1.168327, acc: 0.328125]\n",
      "12343: [discriminator loss: 0.470935, acc: 0.734375] [adversarial loss: 1.010179, acc: 0.328125]\n",
      "12344: [discriminator loss: 0.569587, acc: 0.742188] [adversarial loss: 1.161186, acc: 0.156250]\n",
      "12345: [discriminator loss: 0.625251, acc: 0.601562] [adversarial loss: 1.065982, acc: 0.281250]\n",
      "12346: [discriminator loss: 0.532386, acc: 0.781250] [adversarial loss: 0.976076, acc: 0.328125]\n",
      "12347: [discriminator loss: 0.514781, acc: 0.742188] [adversarial loss: 1.252544, acc: 0.187500]\n",
      "12348: [discriminator loss: 0.570341, acc: 0.695312] [adversarial loss: 1.225177, acc: 0.234375]\n",
      "12349: [discriminator loss: 0.534914, acc: 0.734375] [adversarial loss: 1.029728, acc: 0.343750]\n",
      "12350: [discriminator loss: 0.511514, acc: 0.773438] [adversarial loss: 1.309251, acc: 0.171875]\n",
      "12351: [discriminator loss: 0.601075, acc: 0.656250] [adversarial loss: 0.912287, acc: 0.453125]\n",
      "12352: [discriminator loss: 0.624914, acc: 0.640625] [adversarial loss: 1.578708, acc: 0.062500]\n",
      "12353: [discriminator loss: 0.520483, acc: 0.757812] [adversarial loss: 0.868235, acc: 0.468750]\n",
      "12354: [discriminator loss: 0.691267, acc: 0.609375] [adversarial loss: 1.496905, acc: 0.171875]\n",
      "12355: [discriminator loss: 0.597688, acc: 0.664062] [adversarial loss: 1.097337, acc: 0.265625]\n",
      "12356: [discriminator loss: 0.570712, acc: 0.718750] [adversarial loss: 1.169350, acc: 0.250000]\n",
      "12357: [discriminator loss: 0.546536, acc: 0.742188] [adversarial loss: 1.001579, acc: 0.406250]\n",
      "12358: [discriminator loss: 0.528062, acc: 0.773438] [adversarial loss: 1.245942, acc: 0.187500]\n",
      "12359: [discriminator loss: 0.503360, acc: 0.750000] [adversarial loss: 1.155955, acc: 0.250000]\n",
      "12360: [discriminator loss: 0.504182, acc: 0.726562] [adversarial loss: 1.195934, acc: 0.250000]\n",
      "12361: [discriminator loss: 0.521729, acc: 0.742188] [adversarial loss: 1.103671, acc: 0.296875]\n",
      "12362: [discriminator loss: 0.563146, acc: 0.632812] [adversarial loss: 1.113926, acc: 0.312500]\n",
      "12363: [discriminator loss: 0.592903, acc: 0.679688] [adversarial loss: 1.197821, acc: 0.328125]\n",
      "12364: [discriminator loss: 0.555143, acc: 0.726562] [adversarial loss: 1.130413, acc: 0.296875]\n",
      "12365: [discriminator loss: 0.548755, acc: 0.703125] [adversarial loss: 1.462395, acc: 0.093750]\n",
      "12366: [discriminator loss: 0.583649, acc: 0.687500] [adversarial loss: 1.020602, acc: 0.375000]\n",
      "12367: [discriminator loss: 0.572259, acc: 0.703125] [adversarial loss: 1.151520, acc: 0.234375]\n",
      "12368: [discriminator loss: 0.562672, acc: 0.664062] [adversarial loss: 1.013200, acc: 0.281250]\n",
      "12369: [discriminator loss: 0.492133, acc: 0.773438] [adversarial loss: 1.051096, acc: 0.375000]\n",
      "12370: [discriminator loss: 0.611059, acc: 0.648438] [adversarial loss: 1.066784, acc: 0.250000]\n",
      "12371: [discriminator loss: 0.594575, acc: 0.687500] [adversarial loss: 0.700468, acc: 0.625000]\n",
      "12372: [discriminator loss: 0.612645, acc: 0.695312] [adversarial loss: 1.310407, acc: 0.156250]\n",
      "12373: [discriminator loss: 0.523155, acc: 0.718750] [adversarial loss: 0.758243, acc: 0.562500]\n",
      "12374: [discriminator loss: 0.502796, acc: 0.726562] [adversarial loss: 1.412482, acc: 0.109375]\n",
      "12375: [discriminator loss: 0.562760, acc: 0.734375] [adversarial loss: 0.903566, acc: 0.375000]\n",
      "12376: [discriminator loss: 0.577463, acc: 0.703125] [adversarial loss: 1.298231, acc: 0.140625]\n",
      "12377: [discriminator loss: 0.490991, acc: 0.734375] [adversarial loss: 1.094218, acc: 0.234375]\n",
      "12378: [discriminator loss: 0.525527, acc: 0.710938] [adversarial loss: 0.876501, acc: 0.421875]\n",
      "12379: [discriminator loss: 0.547745, acc: 0.671875] [adversarial loss: 1.131485, acc: 0.281250]\n",
      "12380: [discriminator loss: 0.551643, acc: 0.726562] [adversarial loss: 0.851629, acc: 0.468750]\n",
      "12381: [discriminator loss: 0.651598, acc: 0.570312] [adversarial loss: 1.780551, acc: 0.078125]\n",
      "12382: [discriminator loss: 0.633355, acc: 0.625000] [adversarial loss: 0.780504, acc: 0.468750]\n",
      "12383: [discriminator loss: 0.594067, acc: 0.726562] [adversarial loss: 1.324958, acc: 0.156250]\n",
      "12384: [discriminator loss: 0.563174, acc: 0.679688] [adversarial loss: 1.162120, acc: 0.328125]\n",
      "12385: [discriminator loss: 0.489753, acc: 0.796875] [adversarial loss: 1.302110, acc: 0.218750]\n",
      "12386: [discriminator loss: 0.556805, acc: 0.710938] [adversarial loss: 0.989633, acc: 0.359375]\n",
      "12387: [discriminator loss: 0.524261, acc: 0.765625] [adversarial loss: 1.260265, acc: 0.250000]\n",
      "12388: [discriminator loss: 0.495750, acc: 0.742188] [adversarial loss: 1.139951, acc: 0.250000]\n",
      "12389: [discriminator loss: 0.546913, acc: 0.765625] [adversarial loss: 1.404374, acc: 0.125000]\n",
      "12390: [discriminator loss: 0.555406, acc: 0.679688] [adversarial loss: 0.860703, acc: 0.390625]\n",
      "12391: [discriminator loss: 0.564542, acc: 0.726562] [adversarial loss: 1.378636, acc: 0.140625]\n",
      "12392: [discriminator loss: 0.538793, acc: 0.742188] [adversarial loss: 0.942238, acc: 0.359375]\n",
      "12393: [discriminator loss: 0.511315, acc: 0.773438] [adversarial loss: 1.461055, acc: 0.125000]\n",
      "12394: [discriminator loss: 0.503980, acc: 0.734375] [adversarial loss: 1.233333, acc: 0.187500]\n",
      "12395: [discriminator loss: 0.496508, acc: 0.718750] [adversarial loss: 1.193750, acc: 0.234375]\n",
      "12396: [discriminator loss: 0.556577, acc: 0.687500] [adversarial loss: 1.128891, acc: 0.250000]\n",
      "12397: [discriminator loss: 0.580714, acc: 0.703125] [adversarial loss: 1.004858, acc: 0.359375]\n",
      "12398: [discriminator loss: 0.500555, acc: 0.804688] [adversarial loss: 1.174292, acc: 0.234375]\n",
      "12399: [discriminator loss: 0.575296, acc: 0.687500] [adversarial loss: 1.054820, acc: 0.312500]\n",
      "12400: [discriminator loss: 0.599673, acc: 0.710938] [adversarial loss: 0.902589, acc: 0.468750]\n",
      "12401: [discriminator loss: 0.558396, acc: 0.742188] [adversarial loss: 1.251649, acc: 0.234375]\n",
      "12402: [discriminator loss: 0.556879, acc: 0.718750] [adversarial loss: 0.958853, acc: 0.406250]\n",
      "12403: [discriminator loss: 0.538257, acc: 0.703125] [adversarial loss: 1.455082, acc: 0.093750]\n",
      "12404: [discriminator loss: 0.515265, acc: 0.734375] [adversarial loss: 0.895765, acc: 0.468750]\n",
      "12405: [discriminator loss: 0.613208, acc: 0.664062] [adversarial loss: 1.731089, acc: 0.078125]\n",
      "12406: [discriminator loss: 0.583110, acc: 0.671875] [adversarial loss: 0.878031, acc: 0.390625]\n",
      "12407: [discriminator loss: 0.552393, acc: 0.742188] [adversarial loss: 1.220211, acc: 0.203125]\n",
      "12408: [discriminator loss: 0.541942, acc: 0.710938] [adversarial loss: 1.043077, acc: 0.359375]\n",
      "12409: [discriminator loss: 0.551799, acc: 0.671875] [adversarial loss: 1.105070, acc: 0.296875]\n",
      "12410: [discriminator loss: 0.575006, acc: 0.671875] [adversarial loss: 1.028398, acc: 0.328125]\n",
      "12411: [discriminator loss: 0.557827, acc: 0.695312] [adversarial loss: 1.157245, acc: 0.250000]\n",
      "12412: [discriminator loss: 0.568689, acc: 0.718750] [adversarial loss: 0.971676, acc: 0.390625]\n",
      "12413: [discriminator loss: 0.621740, acc: 0.648438] [adversarial loss: 1.327621, acc: 0.140625]\n",
      "12414: [discriminator loss: 0.549351, acc: 0.695312] [adversarial loss: 1.053964, acc: 0.328125]\n",
      "12415: [discriminator loss: 0.479542, acc: 0.796875] [adversarial loss: 1.467734, acc: 0.156250]\n",
      "12416: [discriminator loss: 0.644525, acc: 0.648438] [adversarial loss: 0.641712, acc: 0.656250]\n",
      "12417: [discriminator loss: 0.612854, acc: 0.648438] [adversarial loss: 1.748287, acc: 0.078125]\n",
      "12418: [discriminator loss: 0.564724, acc: 0.703125] [adversarial loss: 0.929917, acc: 0.375000]\n",
      "12419: [discriminator loss: 0.593624, acc: 0.679688] [adversarial loss: 1.387937, acc: 0.125000]\n",
      "12420: [discriminator loss: 0.545811, acc: 0.695312] [adversarial loss: 0.993387, acc: 0.343750]\n",
      "12421: [discriminator loss: 0.543791, acc: 0.718750] [adversarial loss: 1.135657, acc: 0.203125]\n",
      "12422: [discriminator loss: 0.524369, acc: 0.734375] [adversarial loss: 1.252156, acc: 0.203125]\n",
      "12423: [discriminator loss: 0.527818, acc: 0.734375] [adversarial loss: 0.937510, acc: 0.312500]\n",
      "12424: [discriminator loss: 0.485680, acc: 0.781250] [adversarial loss: 1.327989, acc: 0.140625]\n",
      "12425: [discriminator loss: 0.503338, acc: 0.789062] [adversarial loss: 1.035185, acc: 0.343750]\n",
      "12426: [discriminator loss: 0.535110, acc: 0.687500] [adversarial loss: 1.237195, acc: 0.203125]\n",
      "12427: [discriminator loss: 0.561400, acc: 0.679688] [adversarial loss: 1.066478, acc: 0.375000]\n",
      "12428: [discriminator loss: 0.529167, acc: 0.718750] [adversarial loss: 1.275170, acc: 0.218750]\n",
      "12429: [discriminator loss: 0.572984, acc: 0.703125] [adversarial loss: 1.102037, acc: 0.343750]\n",
      "12430: [discriminator loss: 0.558252, acc: 0.687500] [adversarial loss: 1.362295, acc: 0.125000]\n",
      "12431: [discriminator loss: 0.635214, acc: 0.679688] [adversarial loss: 0.906032, acc: 0.375000]\n",
      "12432: [discriminator loss: 0.507445, acc: 0.710938] [adversarial loss: 1.296624, acc: 0.062500]\n",
      "12433: [discriminator loss: 0.545279, acc: 0.695312] [adversarial loss: 1.055102, acc: 0.265625]\n",
      "12434: [discriminator loss: 0.530358, acc: 0.695312] [adversarial loss: 1.299047, acc: 0.109375]\n",
      "12435: [discriminator loss: 0.533768, acc: 0.726562] [adversarial loss: 0.860835, acc: 0.421875]\n",
      "12436: [discriminator loss: 0.512890, acc: 0.757812] [adversarial loss: 1.179199, acc: 0.203125]\n",
      "12437: [discriminator loss: 0.498524, acc: 0.757812] [adversarial loss: 1.127841, acc: 0.250000]\n",
      "12438: [discriminator loss: 0.508005, acc: 0.742188] [adversarial loss: 0.994062, acc: 0.359375]\n",
      "12439: [discriminator loss: 0.600643, acc: 0.648438] [adversarial loss: 0.975879, acc: 0.375000]\n",
      "12440: [discriminator loss: 0.537374, acc: 0.734375] [adversarial loss: 1.150799, acc: 0.281250]\n",
      "12441: [discriminator loss: 0.556312, acc: 0.726562] [adversarial loss: 1.138616, acc: 0.203125]\n",
      "12442: [discriminator loss: 0.601835, acc: 0.671875] [adversarial loss: 1.340526, acc: 0.109375]\n",
      "12443: [discriminator loss: 0.580124, acc: 0.695312] [adversarial loss: 1.028104, acc: 0.281250]\n",
      "12444: [discriminator loss: 0.514459, acc: 0.734375] [adversarial loss: 1.258070, acc: 0.156250]\n",
      "12445: [discriminator loss: 0.509964, acc: 0.742188] [adversarial loss: 1.093333, acc: 0.312500]\n",
      "12446: [discriminator loss: 0.502655, acc: 0.781250] [adversarial loss: 1.049487, acc: 0.281250]\n",
      "12447: [discriminator loss: 0.511791, acc: 0.710938] [adversarial loss: 1.050627, acc: 0.296875]\n",
      "12448: [discriminator loss: 0.556840, acc: 0.726562] [adversarial loss: 1.108713, acc: 0.296875]\n",
      "12449: [discriminator loss: 0.539974, acc: 0.757812] [adversarial loss: 1.111593, acc: 0.281250]\n",
      "12450: [discriminator loss: 0.514133, acc: 0.734375] [adversarial loss: 1.198431, acc: 0.203125]\n",
      "12451: [discriminator loss: 0.525985, acc: 0.710938] [adversarial loss: 1.226751, acc: 0.187500]\n",
      "12452: [discriminator loss: 0.582868, acc: 0.671875] [adversarial loss: 0.919787, acc: 0.406250]\n",
      "12453: [discriminator loss: 0.598039, acc: 0.710938] [adversarial loss: 1.964282, acc: 0.015625]\n",
      "12454: [discriminator loss: 0.689652, acc: 0.625000] [adversarial loss: 0.821215, acc: 0.421875]\n",
      "12455: [discriminator loss: 0.602230, acc: 0.671875] [adversarial loss: 1.496950, acc: 0.078125]\n",
      "12456: [discriminator loss: 0.645807, acc: 0.648438] [adversarial loss: 0.866434, acc: 0.453125]\n",
      "12457: [discriminator loss: 0.611350, acc: 0.656250] [adversarial loss: 1.206062, acc: 0.187500]\n",
      "12458: [discriminator loss: 0.578261, acc: 0.679688] [adversarial loss: 0.944663, acc: 0.343750]\n",
      "12459: [discriminator loss: 0.453880, acc: 0.820312] [adversarial loss: 1.275814, acc: 0.187500]\n",
      "12460: [discriminator loss: 0.564652, acc: 0.695312] [adversarial loss: 1.099433, acc: 0.234375]\n",
      "12461: [discriminator loss: 0.592169, acc: 0.656250] [adversarial loss: 1.028256, acc: 0.375000]\n",
      "12462: [discriminator loss: 0.625626, acc: 0.648438] [adversarial loss: 1.345693, acc: 0.203125]\n",
      "12463: [discriminator loss: 0.551751, acc: 0.656250] [adversarial loss: 0.956780, acc: 0.390625]\n",
      "12464: [discriminator loss: 0.566903, acc: 0.695312] [adversarial loss: 1.065077, acc: 0.296875]\n",
      "12465: [discriminator loss: 0.509537, acc: 0.750000] [adversarial loss: 0.977936, acc: 0.265625]\n",
      "12466: [discriminator loss: 0.628446, acc: 0.640625] [adversarial loss: 1.220315, acc: 0.187500]\n",
      "12467: [discriminator loss: 0.565720, acc: 0.726562] [adversarial loss: 1.109045, acc: 0.343750]\n",
      "12468: [discriminator loss: 0.545257, acc: 0.718750] [adversarial loss: 0.942207, acc: 0.359375]\n",
      "12469: [discriminator loss: 0.563202, acc: 0.671875] [adversarial loss: 1.172920, acc: 0.203125]\n",
      "12470: [discriminator loss: 0.554351, acc: 0.710938] [adversarial loss: 1.143675, acc: 0.250000]\n",
      "12471: [discriminator loss: 0.550015, acc: 0.695312] [adversarial loss: 0.994511, acc: 0.343750]\n",
      "12472: [discriminator loss: 0.472609, acc: 0.804688] [adversarial loss: 1.229058, acc: 0.203125]\n",
      "12473: [discriminator loss: 0.538289, acc: 0.742188] [adversarial loss: 0.890080, acc: 0.468750]\n",
      "12474: [discriminator loss: 0.548598, acc: 0.671875] [adversarial loss: 1.383848, acc: 0.125000]\n",
      "12475: [discriminator loss: 0.500204, acc: 0.742188] [adversarial loss: 0.992865, acc: 0.328125]\n",
      "12476: [discriminator loss: 0.583742, acc: 0.687500] [adversarial loss: 1.701904, acc: 0.109375]\n",
      "12477: [discriminator loss: 0.534246, acc: 0.679688] [adversarial loss: 0.877948, acc: 0.375000]\n",
      "12478: [discriminator loss: 0.602942, acc: 0.679688] [adversarial loss: 1.440747, acc: 0.109375]\n",
      "12479: [discriminator loss: 0.574655, acc: 0.687500] [adversarial loss: 0.949089, acc: 0.312500]\n",
      "12480: [discriminator loss: 0.541178, acc: 0.718750] [adversarial loss: 1.281908, acc: 0.265625]\n",
      "12481: [discriminator loss: 0.478093, acc: 0.773438] [adversarial loss: 1.298266, acc: 0.187500]\n",
      "12482: [discriminator loss: 0.588996, acc: 0.664062] [adversarial loss: 0.845149, acc: 0.500000]\n",
      "12483: [discriminator loss: 0.497723, acc: 0.750000] [adversarial loss: 1.273637, acc: 0.250000]\n",
      "12484: [discriminator loss: 0.542193, acc: 0.648438] [adversarial loss: 1.017753, acc: 0.375000]\n",
      "12485: [discriminator loss: 0.579966, acc: 0.625000] [adversarial loss: 1.421450, acc: 0.156250]\n",
      "12486: [discriminator loss: 0.547124, acc: 0.718750] [adversarial loss: 0.910587, acc: 0.390625]\n",
      "12487: [discriminator loss: 0.585639, acc: 0.695312] [adversarial loss: 1.180819, acc: 0.265625]\n",
      "12488: [discriminator loss: 0.555398, acc: 0.703125] [adversarial loss: 1.025770, acc: 0.296875]\n",
      "12489: [discriminator loss: 0.543451, acc: 0.742188] [adversarial loss: 1.256827, acc: 0.218750]\n",
      "12490: [discriminator loss: 0.582859, acc: 0.648438] [adversarial loss: 1.056786, acc: 0.328125]\n",
      "12491: [discriminator loss: 0.580007, acc: 0.687500] [adversarial loss: 1.582407, acc: 0.140625]\n",
      "12492: [discriminator loss: 0.537393, acc: 0.710938] [adversarial loss: 0.706909, acc: 0.578125]\n",
      "12493: [discriminator loss: 0.566905, acc: 0.695312] [adversarial loss: 1.473303, acc: 0.156250]\n",
      "12494: [discriminator loss: 0.505692, acc: 0.750000] [adversarial loss: 1.025987, acc: 0.296875]\n",
      "12495: [discriminator loss: 0.478662, acc: 0.765625] [adversarial loss: 1.230226, acc: 0.234375]\n",
      "12496: [discriminator loss: 0.541783, acc: 0.718750] [adversarial loss: 1.144020, acc: 0.265625]\n",
      "12497: [discriminator loss: 0.590094, acc: 0.679688] [adversarial loss: 1.315881, acc: 0.203125]\n",
      "12498: [discriminator loss: 0.605380, acc: 0.710938] [adversarial loss: 1.121619, acc: 0.265625]\n",
      "12499: [discriminator loss: 0.548434, acc: 0.695312] [adversarial loss: 1.139129, acc: 0.203125]\n",
      "12500: [discriminator loss: 0.507866, acc: 0.781250] [adversarial loss: 1.266619, acc: 0.187500]\n",
      "12501: [discriminator loss: 0.558927, acc: 0.687500] [adversarial loss: 1.098040, acc: 0.296875]\n",
      "12502: [discriminator loss: 0.607940, acc: 0.679688] [adversarial loss: 1.295240, acc: 0.140625]\n",
      "12503: [discriminator loss: 0.466571, acc: 0.773438] [adversarial loss: 1.026797, acc: 0.250000]\n",
      "12504: [discriminator loss: 0.604799, acc: 0.703125] [adversarial loss: 1.562822, acc: 0.078125]\n",
      "12505: [discriminator loss: 0.509982, acc: 0.687500] [adversarial loss: 0.873080, acc: 0.390625]\n",
      "12506: [discriminator loss: 0.551602, acc: 0.679688] [adversarial loss: 1.290037, acc: 0.234375]\n",
      "12507: [discriminator loss: 0.556091, acc: 0.710938] [adversarial loss: 0.902051, acc: 0.406250]\n",
      "12508: [discriminator loss: 0.506449, acc: 0.734375] [adversarial loss: 1.144804, acc: 0.203125]\n",
      "12509: [discriminator loss: 0.516637, acc: 0.765625] [adversarial loss: 1.118165, acc: 0.250000]\n",
      "12510: [discriminator loss: 0.548172, acc: 0.695312] [adversarial loss: 0.903777, acc: 0.359375]\n",
      "12511: [discriminator loss: 0.532471, acc: 0.742188] [adversarial loss: 1.475548, acc: 0.140625]\n",
      "12512: [discriminator loss: 0.531808, acc: 0.718750] [adversarial loss: 1.090838, acc: 0.296875]\n",
      "12513: [discriminator loss: 0.593046, acc: 0.703125] [adversarial loss: 1.253812, acc: 0.234375]\n",
      "12514: [discriminator loss: 0.515355, acc: 0.789062] [adversarial loss: 1.193046, acc: 0.218750]\n",
      "12515: [discriminator loss: 0.557126, acc: 0.726562] [adversarial loss: 0.931902, acc: 0.375000]\n",
      "12516: [discriminator loss: 0.557675, acc: 0.664062] [adversarial loss: 1.575008, acc: 0.046875]\n",
      "12517: [discriminator loss: 0.638699, acc: 0.664062] [adversarial loss: 0.849142, acc: 0.484375]\n",
      "12518: [discriminator loss: 0.583313, acc: 0.687500] [adversarial loss: 1.476981, acc: 0.156250]\n",
      "12519: [discriminator loss: 0.557437, acc: 0.671875] [adversarial loss: 0.920343, acc: 0.375000]\n",
      "12520: [discriminator loss: 0.563281, acc: 0.664062] [adversarial loss: 1.171386, acc: 0.187500]\n",
      "12521: [discriminator loss: 0.513869, acc: 0.726562] [adversarial loss: 0.948099, acc: 0.375000]\n",
      "12522: [discriminator loss: 0.584823, acc: 0.664062] [adversarial loss: 1.253990, acc: 0.234375]\n",
      "12523: [discriminator loss: 0.513912, acc: 0.742188] [adversarial loss: 1.124799, acc: 0.203125]\n",
      "12524: [discriminator loss: 0.525555, acc: 0.734375] [adversarial loss: 0.924686, acc: 0.406250]\n",
      "12525: [discriminator loss: 0.457934, acc: 0.812500] [adversarial loss: 1.353091, acc: 0.156250]\n",
      "12526: [discriminator loss: 0.599049, acc: 0.671875] [adversarial loss: 0.991873, acc: 0.390625]\n",
      "12527: [discriminator loss: 0.550392, acc: 0.710938] [adversarial loss: 1.280823, acc: 0.234375]\n",
      "12528: [discriminator loss: 0.553069, acc: 0.695312] [adversarial loss: 1.062551, acc: 0.359375]\n",
      "12529: [discriminator loss: 0.587274, acc: 0.710938] [adversarial loss: 1.563748, acc: 0.078125]\n",
      "12530: [discriminator loss: 0.626935, acc: 0.664062] [adversarial loss: 0.907033, acc: 0.437500]\n",
      "12531: [discriminator loss: 0.520443, acc: 0.765625] [adversarial loss: 1.435091, acc: 0.093750]\n",
      "12532: [discriminator loss: 0.557566, acc: 0.718750] [adversarial loss: 0.837448, acc: 0.468750]\n",
      "12533: [discriminator loss: 0.506971, acc: 0.718750] [adversarial loss: 1.572130, acc: 0.062500]\n",
      "12534: [discriminator loss: 0.541648, acc: 0.718750] [adversarial loss: 0.695284, acc: 0.562500]\n",
      "12535: [discriminator loss: 0.588433, acc: 0.679688] [adversarial loss: 1.553740, acc: 0.093750]\n",
      "12536: [discriminator loss: 0.651599, acc: 0.648438] [adversarial loss: 0.944432, acc: 0.343750]\n",
      "12537: [discriminator loss: 0.525138, acc: 0.695312] [adversarial loss: 1.142483, acc: 0.265625]\n",
      "12538: [discriminator loss: 0.534940, acc: 0.773438] [adversarial loss: 1.119244, acc: 0.312500]\n",
      "12539: [discriminator loss: 0.595697, acc: 0.632812] [adversarial loss: 1.093930, acc: 0.281250]\n",
      "12540: [discriminator loss: 0.545424, acc: 0.726562] [adversarial loss: 1.058767, acc: 0.359375]\n",
      "12541: [discriminator loss: 0.523279, acc: 0.703125] [adversarial loss: 1.108829, acc: 0.234375]\n",
      "12542: [discriminator loss: 0.527640, acc: 0.718750] [adversarial loss: 1.229323, acc: 0.203125]\n",
      "12543: [discriminator loss: 0.532166, acc: 0.710938] [adversarial loss: 1.246241, acc: 0.218750]\n",
      "12544: [discriminator loss: 0.467708, acc: 0.765625] [adversarial loss: 1.310015, acc: 0.234375]\n",
      "12545: [discriminator loss: 0.494845, acc: 0.773438] [adversarial loss: 1.047284, acc: 0.328125]\n",
      "12546: [discriminator loss: 0.542104, acc: 0.718750] [adversarial loss: 1.518737, acc: 0.109375]\n",
      "12547: [discriminator loss: 0.596693, acc: 0.679688] [adversarial loss: 1.015250, acc: 0.312500]\n",
      "12548: [discriminator loss: 0.614690, acc: 0.664062] [adversarial loss: 1.517485, acc: 0.093750]\n",
      "12549: [discriminator loss: 0.606296, acc: 0.703125] [adversarial loss: 0.886367, acc: 0.359375]\n",
      "12550: [discriminator loss: 0.551836, acc: 0.718750] [adversarial loss: 1.400362, acc: 0.203125]\n",
      "12551: [discriminator loss: 0.554023, acc: 0.718750] [adversarial loss: 1.113454, acc: 0.281250]\n",
      "12552: [discriminator loss: 0.566973, acc: 0.679688] [adversarial loss: 1.078652, acc: 0.250000]\n",
      "12553: [discriminator loss: 0.572721, acc: 0.734375] [adversarial loss: 1.316431, acc: 0.109375]\n",
      "12554: [discriminator loss: 0.547311, acc: 0.710938] [adversarial loss: 0.887288, acc: 0.406250]\n",
      "12555: [discriminator loss: 0.544855, acc: 0.703125] [adversarial loss: 1.264573, acc: 0.281250]\n",
      "12556: [discriminator loss: 0.571972, acc: 0.656250] [adversarial loss: 1.178904, acc: 0.218750]\n",
      "12557: [discriminator loss: 0.502179, acc: 0.781250] [adversarial loss: 1.260210, acc: 0.156250]\n",
      "12558: [discriminator loss: 0.507975, acc: 0.726562] [adversarial loss: 1.058509, acc: 0.312500]\n",
      "12559: [discriminator loss: 0.531681, acc: 0.718750] [adversarial loss: 1.279299, acc: 0.312500]\n",
      "12560: [discriminator loss: 0.493577, acc: 0.742188] [adversarial loss: 1.036824, acc: 0.234375]\n",
      "12561: [discriminator loss: 0.584044, acc: 0.671875] [adversarial loss: 1.035789, acc: 0.312500]\n",
      "12562: [discriminator loss: 0.499001, acc: 0.765625] [adversarial loss: 0.829971, acc: 0.453125]\n",
      "12563: [discriminator loss: 0.589144, acc: 0.718750] [adversarial loss: 1.745841, acc: 0.078125]\n",
      "12564: [discriminator loss: 0.628538, acc: 0.648438] [adversarial loss: 0.902946, acc: 0.406250]\n",
      "12565: [discriminator loss: 0.534023, acc: 0.757812] [adversarial loss: 1.544816, acc: 0.046875]\n",
      "12566: [discriminator loss: 0.536817, acc: 0.734375] [adversarial loss: 0.795875, acc: 0.515625]\n",
      "12567: [discriminator loss: 0.531684, acc: 0.726562] [adversarial loss: 1.306883, acc: 0.218750]\n",
      "12568: [discriminator loss: 0.594980, acc: 0.679688] [adversarial loss: 0.864503, acc: 0.453125]\n",
      "12569: [discriminator loss: 0.565522, acc: 0.718750] [adversarial loss: 1.017780, acc: 0.375000]\n",
      "12570: [discriminator loss: 0.544308, acc: 0.718750] [adversarial loss: 1.215419, acc: 0.187500]\n",
      "12571: [discriminator loss: 0.517503, acc: 0.742188] [adversarial loss: 0.974040, acc: 0.281250]\n",
      "12572: [discriminator loss: 0.563465, acc: 0.687500] [adversarial loss: 1.092835, acc: 0.218750]\n",
      "12573: [discriminator loss: 0.536120, acc: 0.734375] [adversarial loss: 1.380637, acc: 0.125000]\n",
      "12574: [discriminator loss: 0.554081, acc: 0.695312] [adversarial loss: 1.115539, acc: 0.312500]\n",
      "12575: [discriminator loss: 0.535959, acc: 0.734375] [adversarial loss: 1.323349, acc: 0.234375]\n",
      "12576: [discriminator loss: 0.473839, acc: 0.804688] [adversarial loss: 1.507442, acc: 0.156250]\n",
      "12577: [discriminator loss: 0.555902, acc: 0.703125] [adversarial loss: 0.911002, acc: 0.406250]\n",
      "12578: [discriminator loss: 0.568844, acc: 0.718750] [adversarial loss: 1.467719, acc: 0.156250]\n",
      "12579: [discriminator loss: 0.590295, acc: 0.687500] [adversarial loss: 0.698210, acc: 0.578125]\n",
      "12580: [discriminator loss: 0.627623, acc: 0.656250] [adversarial loss: 1.690449, acc: 0.125000]\n",
      "12581: [discriminator loss: 0.550518, acc: 0.703125] [adversarial loss: 0.911896, acc: 0.437500]\n",
      "12582: [discriminator loss: 0.501375, acc: 0.742188] [adversarial loss: 1.191532, acc: 0.281250]\n",
      "12583: [discriminator loss: 0.575577, acc: 0.710938] [adversarial loss: 1.151878, acc: 0.203125]\n",
      "12584: [discriminator loss: 0.576038, acc: 0.687500] [adversarial loss: 1.250670, acc: 0.250000]\n",
      "12585: [discriminator loss: 0.542644, acc: 0.718750] [adversarial loss: 1.092435, acc: 0.250000]\n",
      "12586: [discriminator loss: 0.569035, acc: 0.671875] [adversarial loss: 1.145167, acc: 0.281250]\n",
      "12587: [discriminator loss: 0.582393, acc: 0.718750] [adversarial loss: 1.078842, acc: 0.328125]\n",
      "12588: [discriminator loss: 0.459494, acc: 0.789062] [adversarial loss: 1.137214, acc: 0.203125]\n",
      "12589: [discriminator loss: 0.544500, acc: 0.687500] [adversarial loss: 1.207287, acc: 0.265625]\n",
      "12590: [discriminator loss: 0.549327, acc: 0.710938] [adversarial loss: 1.062990, acc: 0.203125]\n",
      "12591: [discriminator loss: 0.565659, acc: 0.734375] [adversarial loss: 1.412366, acc: 0.093750]\n",
      "12592: [discriminator loss: 0.514582, acc: 0.726562] [adversarial loss: 0.804648, acc: 0.406250]\n",
      "12593: [discriminator loss: 0.593544, acc: 0.656250] [adversarial loss: 1.572638, acc: 0.062500]\n",
      "12594: [discriminator loss: 0.555390, acc: 0.710938] [adversarial loss: 0.825638, acc: 0.406250]\n",
      "12595: [discriminator loss: 0.504676, acc: 0.734375] [adversarial loss: 1.441893, acc: 0.156250]\n",
      "12596: [discriminator loss: 0.533096, acc: 0.718750] [adversarial loss: 0.798735, acc: 0.437500]\n",
      "12597: [discriminator loss: 0.561945, acc: 0.718750] [adversarial loss: 1.639760, acc: 0.093750]\n",
      "12598: [discriminator loss: 0.565020, acc: 0.726562] [adversarial loss: 0.793373, acc: 0.484375]\n",
      "12599: [discriminator loss: 0.587916, acc: 0.687500] [adversarial loss: 1.456585, acc: 0.187500]\n",
      "12600: [discriminator loss: 0.540734, acc: 0.703125] [adversarial loss: 0.986476, acc: 0.390625]\n",
      "12601: [discriminator loss: 0.562788, acc: 0.734375] [adversarial loss: 1.256068, acc: 0.187500]\n",
      "12602: [discriminator loss: 0.577882, acc: 0.632812] [adversarial loss: 1.134469, acc: 0.281250]\n",
      "12603: [discriminator loss: 0.518375, acc: 0.703125] [adversarial loss: 1.475383, acc: 0.171875]\n",
      "12604: [discriminator loss: 0.559793, acc: 0.695312] [adversarial loss: 1.194964, acc: 0.250000]\n",
      "12605: [discriminator loss: 0.582890, acc: 0.710938] [adversarial loss: 1.111459, acc: 0.265625]\n",
      "12606: [discriminator loss: 0.563891, acc: 0.710938] [adversarial loss: 1.103073, acc: 0.296875]\n",
      "12607: [discriminator loss: 0.541704, acc: 0.765625] [adversarial loss: 1.449506, acc: 0.093750]\n",
      "12608: [discriminator loss: 0.596960, acc: 0.671875] [adversarial loss: 1.019842, acc: 0.281250]\n",
      "12609: [discriminator loss: 0.575920, acc: 0.679688] [adversarial loss: 1.512327, acc: 0.171875]\n",
      "12610: [discriminator loss: 0.542388, acc: 0.726562] [adversarial loss: 0.756207, acc: 0.453125]\n",
      "12611: [discriminator loss: 0.585136, acc: 0.679688] [adversarial loss: 1.542818, acc: 0.125000]\n",
      "12612: [discriminator loss: 0.594318, acc: 0.664062] [adversarial loss: 0.842503, acc: 0.453125]\n",
      "12613: [discriminator loss: 0.504136, acc: 0.734375] [adversarial loss: 1.451050, acc: 0.109375]\n",
      "12614: [discriminator loss: 0.613661, acc: 0.664062] [adversarial loss: 0.954397, acc: 0.390625]\n",
      "12615: [discriminator loss: 0.564630, acc: 0.695312] [adversarial loss: 1.148203, acc: 0.250000]\n",
      "12616: [discriminator loss: 0.591277, acc: 0.695312] [adversarial loss: 1.124632, acc: 0.296875]\n",
      "12617: [discriminator loss: 0.556274, acc: 0.718750] [adversarial loss: 1.036962, acc: 0.343750]\n",
      "12618: [discriminator loss: 0.522825, acc: 0.710938] [adversarial loss: 1.240794, acc: 0.203125]\n",
      "12619: [discriminator loss: 0.578600, acc: 0.695312] [adversarial loss: 1.128882, acc: 0.343750]\n",
      "12620: [discriminator loss: 0.503357, acc: 0.750000] [adversarial loss: 1.095732, acc: 0.203125]\n",
      "12621: [discriminator loss: 0.578701, acc: 0.664062] [adversarial loss: 1.427384, acc: 0.140625]\n",
      "12622: [discriminator loss: 0.579026, acc: 0.703125] [adversarial loss: 0.982133, acc: 0.343750]\n",
      "12623: [discriminator loss: 0.533039, acc: 0.734375] [adversarial loss: 1.207861, acc: 0.234375]\n",
      "12624: [discriminator loss: 0.592471, acc: 0.703125] [adversarial loss: 1.396083, acc: 0.203125]\n",
      "12625: [discriminator loss: 0.579684, acc: 0.671875] [adversarial loss: 0.930831, acc: 0.343750]\n",
      "12626: [discriminator loss: 0.518550, acc: 0.750000] [adversarial loss: 1.234470, acc: 0.218750]\n",
      "12627: [discriminator loss: 0.604722, acc: 0.671875] [adversarial loss: 0.865893, acc: 0.437500]\n",
      "12628: [discriminator loss: 0.577803, acc: 0.679688] [adversarial loss: 1.435968, acc: 0.140625]\n",
      "12629: [discriminator loss: 0.577942, acc: 0.718750] [adversarial loss: 1.220762, acc: 0.296875]\n",
      "12630: [discriminator loss: 0.538765, acc: 0.742188] [adversarial loss: 1.178874, acc: 0.218750]\n",
      "12631: [discriminator loss: 0.466895, acc: 0.773438] [adversarial loss: 0.947808, acc: 0.359375]\n",
      "12632: [discriminator loss: 0.525254, acc: 0.742188] [adversarial loss: 1.292667, acc: 0.140625]\n",
      "12633: [discriminator loss: 0.526712, acc: 0.695312] [adversarial loss: 1.061772, acc: 0.296875]\n",
      "12634: [discriminator loss: 0.510480, acc: 0.726562] [adversarial loss: 1.305853, acc: 0.187500]\n",
      "12635: [discriminator loss: 0.523546, acc: 0.734375] [adversarial loss: 1.187307, acc: 0.218750]\n",
      "12636: [discriminator loss: 0.492466, acc: 0.726562] [adversarial loss: 1.134017, acc: 0.265625]\n",
      "12637: [discriminator loss: 0.542785, acc: 0.750000] [adversarial loss: 1.630012, acc: 0.093750]\n",
      "12638: [discriminator loss: 0.571619, acc: 0.687500] [adversarial loss: 0.741274, acc: 0.593750]\n",
      "12639: [discriminator loss: 0.539944, acc: 0.734375] [adversarial loss: 1.736672, acc: 0.062500]\n",
      "12640: [discriminator loss: 0.661636, acc: 0.632812] [adversarial loss: 0.707966, acc: 0.625000]\n",
      "12641: [discriminator loss: 0.624359, acc: 0.703125] [adversarial loss: 1.720188, acc: 0.093750]\n",
      "12642: [discriminator loss: 0.622819, acc: 0.679688] [adversarial loss: 0.795861, acc: 0.484375]\n",
      "12643: [discriminator loss: 0.629405, acc: 0.625000] [adversarial loss: 1.365956, acc: 0.218750]\n",
      "12644: [discriminator loss: 0.527380, acc: 0.734375] [adversarial loss: 1.315835, acc: 0.203125]\n",
      "12645: [discriminator loss: 0.518262, acc: 0.726562] [adversarial loss: 1.046898, acc: 0.390625]\n",
      "12646: [discriminator loss: 0.522792, acc: 0.679688] [adversarial loss: 1.090807, acc: 0.250000]\n",
      "12647: [discriminator loss: 0.572344, acc: 0.671875] [adversarial loss: 0.988485, acc: 0.359375]\n",
      "12648: [discriminator loss: 0.496845, acc: 0.734375] [adversarial loss: 1.439734, acc: 0.140625]\n",
      "12649: [discriminator loss: 0.566258, acc: 0.679688] [adversarial loss: 1.112192, acc: 0.375000]\n",
      "12650: [discriminator loss: 0.531134, acc: 0.742188] [adversarial loss: 0.952957, acc: 0.343750]\n",
      "12651: [discriminator loss: 0.488839, acc: 0.757812] [adversarial loss: 1.275436, acc: 0.250000]\n",
      "12652: [discriminator loss: 0.562468, acc: 0.679688] [adversarial loss: 0.983777, acc: 0.375000]\n",
      "12653: [discriminator loss: 0.530938, acc: 0.750000] [adversarial loss: 1.157447, acc: 0.218750]\n",
      "12654: [discriminator loss: 0.528231, acc: 0.703125] [adversarial loss: 1.353511, acc: 0.156250]\n",
      "12655: [discriminator loss: 0.603777, acc: 0.617188] [adversarial loss: 1.462984, acc: 0.203125]\n",
      "12656: [discriminator loss: 0.596078, acc: 0.695312] [adversarial loss: 1.040319, acc: 0.250000]\n",
      "12657: [discriminator loss: 0.512123, acc: 0.750000] [adversarial loss: 1.386717, acc: 0.187500]\n",
      "12658: [discriminator loss: 0.597736, acc: 0.703125] [adversarial loss: 1.349429, acc: 0.140625]\n",
      "12659: [discriminator loss: 0.641753, acc: 0.632812] [adversarial loss: 0.972352, acc: 0.359375]\n",
      "12660: [discriminator loss: 0.548035, acc: 0.718750] [adversarial loss: 0.993793, acc: 0.281250]\n",
      "12661: [discriminator loss: 0.502763, acc: 0.796875] [adversarial loss: 1.136240, acc: 0.265625]\n",
      "12662: [discriminator loss: 0.491797, acc: 0.757812] [adversarial loss: 0.983172, acc: 0.343750]\n",
      "12663: [discriminator loss: 0.512108, acc: 0.757812] [adversarial loss: 1.038393, acc: 0.281250]\n",
      "12664: [discriminator loss: 0.541114, acc: 0.687500] [adversarial loss: 1.194757, acc: 0.234375]\n",
      "12665: [discriminator loss: 0.625384, acc: 0.671875] [adversarial loss: 0.817923, acc: 0.421875]\n",
      "12666: [discriminator loss: 0.533961, acc: 0.734375] [adversarial loss: 1.314049, acc: 0.125000]\n",
      "12667: [discriminator loss: 0.551443, acc: 0.703125] [adversarial loss: 1.148071, acc: 0.281250]\n",
      "12668: [discriminator loss: 0.579800, acc: 0.687500] [adversarial loss: 1.219041, acc: 0.296875]\n",
      "12669: [discriminator loss: 0.543289, acc: 0.734375] [adversarial loss: 1.063844, acc: 0.265625]\n",
      "12670: [discriminator loss: 0.529148, acc: 0.710938] [adversarial loss: 1.654942, acc: 0.109375]\n",
      "12671: [discriminator loss: 0.634597, acc: 0.648438] [adversarial loss: 0.708351, acc: 0.578125]\n",
      "12672: [discriminator loss: 0.692501, acc: 0.656250] [adversarial loss: 1.773747, acc: 0.062500]\n",
      "12673: [discriminator loss: 0.617239, acc: 0.695312] [adversarial loss: 0.841375, acc: 0.484375]\n",
      "12674: [discriminator loss: 0.551825, acc: 0.671875] [adversarial loss: 1.402594, acc: 0.078125]\n",
      "12675: [discriminator loss: 0.631676, acc: 0.640625] [adversarial loss: 0.934064, acc: 0.328125]\n",
      "12676: [discriminator loss: 0.545599, acc: 0.718750] [adversarial loss: 1.486136, acc: 0.140625]\n",
      "12677: [discriminator loss: 0.533823, acc: 0.710938] [adversarial loss: 1.093003, acc: 0.250000]\n",
      "12678: [discriminator loss: 0.550062, acc: 0.656250] [adversarial loss: 1.154902, acc: 0.187500]\n",
      "12679: [discriminator loss: 0.494453, acc: 0.750000] [adversarial loss: 1.013546, acc: 0.390625]\n",
      "12680: [discriminator loss: 0.558702, acc: 0.679688] [adversarial loss: 1.217478, acc: 0.203125]\n",
      "12681: [discriminator loss: 0.515001, acc: 0.750000] [adversarial loss: 1.063095, acc: 0.281250]\n",
      "12682: [discriminator loss: 0.494451, acc: 0.773438] [adversarial loss: 1.416857, acc: 0.140625]\n",
      "12683: [discriminator loss: 0.537669, acc: 0.687500] [adversarial loss: 1.314384, acc: 0.140625]\n",
      "12684: [discriminator loss: 0.487157, acc: 0.773438] [adversarial loss: 1.075603, acc: 0.296875]\n",
      "12685: [discriminator loss: 0.525609, acc: 0.742188] [adversarial loss: 1.227050, acc: 0.218750]\n",
      "12686: [discriminator loss: 0.499969, acc: 0.796875] [adversarial loss: 1.173227, acc: 0.234375]\n",
      "12687: [discriminator loss: 0.560187, acc: 0.750000] [adversarial loss: 1.371656, acc: 0.187500]\n",
      "12688: [discriminator loss: 0.533699, acc: 0.726562] [adversarial loss: 1.264876, acc: 0.265625]\n",
      "12689: [discriminator loss: 0.568485, acc: 0.671875] [adversarial loss: 1.145205, acc: 0.328125]\n",
      "12690: [discriminator loss: 0.652956, acc: 0.585938] [adversarial loss: 0.988335, acc: 0.406250]\n",
      "12691: [discriminator loss: 0.530171, acc: 0.742188] [adversarial loss: 1.210683, acc: 0.187500]\n",
      "12692: [discriminator loss: 0.522456, acc: 0.710938] [adversarial loss: 1.045359, acc: 0.312500]\n",
      "12693: [discriminator loss: 0.547746, acc: 0.687500] [adversarial loss: 1.409119, acc: 0.109375]\n",
      "12694: [discriminator loss: 0.573392, acc: 0.656250] [adversarial loss: 0.929857, acc: 0.375000]\n",
      "12695: [discriminator loss: 0.549849, acc: 0.718750] [adversarial loss: 1.569929, acc: 0.140625]\n",
      "12696: [discriminator loss: 0.626259, acc: 0.664062] [adversarial loss: 0.783626, acc: 0.500000]\n",
      "12697: [discriminator loss: 0.596330, acc: 0.695312] [adversarial loss: 1.397032, acc: 0.171875]\n",
      "12698: [discriminator loss: 0.545710, acc: 0.734375] [adversarial loss: 0.804352, acc: 0.500000]\n",
      "12699: [discriminator loss: 0.579366, acc: 0.718750] [adversarial loss: 1.626583, acc: 0.109375]\n"
     ]
    }
   ],
   "source": [
    "def build_and_train_models():\n",
    "    # load MNIST dataset\n",
    "    (x_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # reshape data for CNN as (28, 28, 1) and normalize\n",
    "    image_size = x_train.shape[1]\n",
    "    x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "\n",
    "    model_name = \"dcgan_mnist\"\n",
    "    # network parameters\n",
    "    # the latent or z vector is 100-dim\n",
    "    latent_size = 100\n",
    "    batch_size = 64\n",
    "    train_steps = 40000\n",
    "    lr = 2e-4\n",
    "    decay = 6e-8\n",
    "    input_shape = (image_size, image_size, 1)\n",
    "\n",
    "    # build discriminator model\n",
    "    inputs = Input(shape=input_shape, name='discriminator_input')\n",
    "    discriminator = build_discriminator(inputs)\n",
    "    # [1] or original paper uses Adam, \n",
    "    # but discriminator converges easily with RMSprop\n",
    "    optimizer = RMSprop(learning_rate=lr, decay=decay)\n",
    "    discriminator.compile(loss='binary_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=['accuracy'])\n",
    "    discriminator.summary()\n",
    "\n",
    "    # build generator model\n",
    "    input_shape = (latent_size, )\n",
    "    inputs = Input(shape=input_shape, name='z_input')\n",
    "    generator = build_generator(inputs, image_size)\n",
    "    generator.summary()\n",
    "\n",
    "    # build adversarial model\n",
    "    optimizer = RMSprop(learning_rate=lr * 0.5, decay=decay * 0.5)\n",
    "    # 在对抗训练期间冻结鉴别器的权重\n",
    "    discriminator.trainable = False\n",
    "    # adversarial = generator + discriminator\n",
    "    adversarial = Model(inputs, \n",
    "                        discriminator(generator(inputs)),\n",
    "                        name=model_name)\n",
    "    adversarial.compile(loss='binary_crossentropy',\n",
    "                        optimizer=optimizer,\n",
    "                        metrics=['accuracy'])\n",
    "    adversarial.summary()\n",
    "\n",
    "    # train discriminator and adversarial networks\n",
    "    models = (generator, discriminator, adversarial)\n",
    "    params = (batch_size, latent_size, train_steps, model_name)\n",
    "    train(models, x_train, params)\n",
    "\n",
    "\n",
    "def test_generator(generator):\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "    plot_images(generator,\n",
    "                noise_input=noise_input,\n",
    "                show=True,\n",
    "                model_name=\"test_outputs\")\n",
    "\n",
    "    \n",
    "run_generator =False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#    parser = argparse.ArgumentParser()\n",
    "#    help_ = \"Load generator h5 model with trained weights\"\n",
    "#    parser.add_argument(\"-g\", \"--generator\", help=help_)\n",
    "#    args = parser.parse_args()\n",
    "    if run_generator:\n",
    "        generator = load_model(model_name)\n",
    "        test_generator(generator)\n",
    "    else:\n",
    "        build_and_train_models()\n",
    "        \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# #    parser = argparse.ArgumentParser()\n",
    "# #    help_ = \"Load generator h5 model with trained weights\"\n",
    "# #    parser.add_argument(\"-g\", \"--generator\", help=help_)\n",
    "# #    args = parser.parse_args()\n",
    "#     if args.generator:\n",
    "#         generator = load_model(args.generator)\n",
    "#         test_generator(generator)\n",
    "#     else:\n",
    "#         build_and_train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-heavy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
