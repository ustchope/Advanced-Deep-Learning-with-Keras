{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tribal-twins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 15.9 ms (started: 2021-08-11 09:26:01 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# 自动计算cell的计算时间\n",
    "%load_ext autotime\n",
    "\n",
    "%config InlineBackend.figure_format='svg' #矢量图设置，让绘图更清晰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "catholic-judge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n",
      "time: 58.1 s (started: 2021-08-11 09:24:33 +08:00)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# 增加更新\n",
    "git add *.ipynb\n",
    "\n",
    "git remote -v\n",
    "\n",
    "git commit -m '更新 ch1 #2 change Aug 11, 2021'\n",
    "\n",
    "git push origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eleven-finland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.93 s (started: 2021-08-11 09:26:03 +08:00)\n"
     ]
    }
   ],
   "source": [
    "#设置使用的gpu\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "   \n",
    "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-listing",
   "metadata": {},
   "source": [
    "# 使用 Keras 引入高级深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-senegal",
   "metadata": {},
   "source": [
    "在第一章中，我们将介绍我们将在整本书中使用的三个深度学习人工神经网络。 这些网络是 MLP、CNN 和 RNN（在第 2 节中定义和描述），它们是本书涵盖的选定高级深度学习主题的构建块，例如自回归网络（自动编码器、GAN 和 VAE）、深度强化学习 ，对象检测和分割，以及使用互信息的无监督学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-franklin",
   "metadata": {},
   "source": [
    "在本章中，我们将一起讨论如何使用 Keras 库实现基于 MLP、CNN 和 RNN 的模型。 更具体地说，我们将使用名为 tf.keras 的 TensorFlow Keras 库。 我们将首先了解为什么 tf.keras 是我们作为工具的绝佳选择。 接下来，我们将深入研究三个深度学习网络中的实现细节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-radiation",
   "metadata": {},
   "source": [
    "本章将：\n",
    "* 确定为什么 tf.keras 库是用于高级深度学习的绝佳选择\n",
    "* 介绍 MLP、CNN 和 RNN——高级深度学习模型的核心构建块，我们将在本书中使用它们\n",
    "* 提供如何使用 tf.keras 实现基于 MLP、CNN 和 RNN 的模型的示例\n",
    "* 沿着这条线，开始介绍重要的深度学习概念，包括优化、正则化和损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-municipality",
   "metadata": {},
   "source": [
    "在本章结束时，我们将使用 tf.keras 实现基本的深度学习网络。 在下一章中，我们将进入建立在这些基础之上的高级深度学习主题。 让我们通过讨论 Keras 及其作为深度学习库的功能开始本章。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-circus",
   "metadata": {},
   "source": [
    "## 为什么 Keras 是完美的深度学习库？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-hawaii",
   "metadata": {},
   "source": [
    "Keras 是一个流行的深度学习库，在撰写本文时有超过 370,000 名开发人员在使用它——这个数字每年增加约 35%。 超过 800 名贡献者积极维护它。 我们将在本书中使用的一些示例已贡献给官方 Keras GitHub 存储库。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-norman",
   "metadata": {},
   "source": [
    "谷歌的 TensorFlow 是一个流行的开源深度学习库，它使用 Keras 作为其库的高级 API。 它通常被称为 tf.keras。 在本书中，我们将交替使用 Keras 和 tf.keras 这两个词。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-dancing",
   "metadata": {},
   "source": [
    "tf.keras 是深度学习库的流行选择，因为它高度集成到 TensorFlow 中，TensorFlow 在生产部署中以其可靠性而闻名。 TensorFlow 还提供了各种用于生产部署和维护、调试和可视化以及在嵌入式设备和浏览器上运行模型的工具。 在科技行业，谷歌、Netflix、优步和英伟达都在使用 Keras。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-synthetic",
   "metadata": {},
   "source": [
    "我们选择 tf.keras 作为我们在本书中使用的首选工具，因为它是一个致力于加速深度学习模型实现的库。 这使得 Keras 非常适合我们想要实用和动手的情况，例如当我们探索本书中的高级深度学习概念时。 因为 Keras 旨在加速深度学习模型的开发、训练和验证，所以在有人可以最大限度地使用库之前，必须学习该领域的关键概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-partner",
   "metadata": {},
   "source": [
    "> 本书中的所有示例都可以在 GitHub 上的以下链接中找到：https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-flood",
   "metadata": {},
   "source": [
    "在 tf.keras 库中，层像乐高积木一样相互连接，从而产生一个干净且易于理解的模型。 模型训练很简单，只需要数据、多个训练时期和要监控的指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-aurora",
   "metadata": {},
   "source": [
    "最终结果是，与 PyTorch 等其他深度学习库相比，大多数深度学习模型的实现代码行数要少得多。 通过使用 Keras，我们将通过节省代码实现时间来提高生产力，而这些时间可以用于更关键的任务，例如制定更好的深度学习算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-worker",
   "metadata": {},
   "source": [
    "同样，Keras 是快速实现深度学习模型的理想选择，就像我们将在本书中使用的模型一样。 使用 Sequential 模型 API，只需几行代码即可构建典型模型。 但是，不要被它的简单性所误导。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-grass",
   "metadata": {},
   "source": [
    "Keras 还可以使用其功能 API 以及用于动态图的模型和层类构建更高级和复杂的模型，这些模型可以进行定制以满足独特的需求。 函数式 API 支持构建图样模型、层重用以及创建行为类似于 Python 函数的模型。 同时，Model 和 Layer 类为实现不常见或实验性的深度学习模型和层提供了框架。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-bracelet",
   "metadata": {},
   "source": [
    "### 安装 Keras 和 TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-intro",
   "metadata": {},
   "source": [
    "Keras 不是一个独立的深度学习库。 如图 1.1.1 所示，它建立在另一个深度学习库或后端之上。 这可能是 Google 的 TensorFlow、MILA 的 Theano、微软的 CNTK 或 Apache MXNet。 然而，与本书的前一版不同，我们将使用由 TensorFlow 2.0（tf2 或简称为 tf）提供的 Keras，即 tf.keras，以利用 tf2 提供的有用工具。 tf.keras 也被认为是 TensorFlow 的事实上的前端，它在生产环境中展示了其经过验证的可靠性。 此外，Keras 对 TensorFlow 以外的后端的支持在不久的将来将不再可用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-remainder",
   "metadata": {},
   "source": [
    "从 Keras 迁移到 tf.keras 通常与更改一样简单：\n",
    "```\n",
    "from keras... import ...\n",
    "```\n",
    "到\n",
    "```\n",
    "from tensorflow.keras... import ...\n",
    "```\n",
    "本书中的代码示例均使用 Python 3 编写，以支持 Python 2到 2020 年结束。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-symphony",
   "metadata": {},
   "source": [
    "在硬件上，Keras 在 CPU、GPU 和 Google 的 TPU 上运行。 在本书中，我们将在 CPU 和 NVIDIA GPU（特别是 GTX 1060、GTX 1080Ti、RTX 2080Ti、V100 和 Quadro RTX 8000 型号）上进行测试："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-address",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNly1gtb3uk02tpj31f40j675z.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-elder",
   "metadata": {},
   "source": [
    "在继续本书的其余部分之前，我们需要确保 tf2 已正确安装。 有多种安装方式； 一个例子是使用 pip3 安装 tf2："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-accounting",
   "metadata": {},
   "source": [
    "本书不包括完整的 Keras API。 我们将只涵盖解释本书中选定的高级深度学习主题所需的材料。 更多信息，我们可以查阅官方 Keras 文档，可以在 https://keras.io 或 https://www.tensorflow.org/guide/keras/overview 找到。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-matrix",
   "metadata": {},
   "source": [
    "在接下来的部分中，将讨论 MLP、CNN 和 RNN 的细节。 这些网络将用于使用 tf.keras 构建一个简单的分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-ready",
   "metadata": {},
   "source": [
    "## MLP, CNN和 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-assumption",
   "metadata": {},
   "source": [
    "我们已经提到我们将使用三个深度学习网络，它们是：\n",
    "* MLP：多层感知器\n",
    "* CNN：卷积神经网络\n",
    "* RNN：循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-valuable",
   "metadata": {},
   "source": [
    "这是我们将在本书中使用的三个网络。 稍后，您会发现它们经常组合在一起以利用每个网络的优势。\n",
    "\n",
    "在本章中，我们将更详细地讨论这些构建块。 在以下部分中，MLP 与其他重要主题（例如损失函数、优化器和正则化器）一起讨论。 在此之后，我们将涵盖 CNN 和 RNN。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-allocation",
   "metadata": {},
   "source": [
    "### MLP、CNN、RNN的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-movie",
   "metadata": {},
   "source": [
    "MLP 是一个全连接 (FC) 网络。 在某些文献中，您经常会发现它被称为深度前馈网络或前馈神经网络。 在本书中，我们将使用术语 MLP。 从已知目标应用程序的角度理解这个网络将有助于我们深入了解设计高级深度学习模型的根本原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-forward",
   "metadata": {},
   "source": [
    "MLP 在简单的逻辑和线性回归问题中很常见。 然而，MLP 并不是处理顺序和多维数据模式的最佳选择。 按照设计，MLP 很难记住顺序数据中的模式，并且需要大量参数来处理多维数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-fundamentals",
   "metadata": {},
   "source": [
    "对于顺序数据输入，RNN 很受欢迎，因为其内部设计允许网络发现数据历史中的依赖关系，这对预测很有用。 对于图像和视频等多维数据，CNN 擅长提取用于分类、分割、生成和其他下游任务的特征图。 在某些情况下，一维卷积形式的 CNN 也用于具有顺序输入数据的网络。 然而，在大多数深度学习模型中，MLP 和 CNN 或 RNN 相结合，以充分利用每个网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-tracker",
   "metadata": {},
   "source": [
    "MLP、CNN 和 RNN 并没有完成深度网络的全貌。 需要确定目标或损失函数、优化器和正则化器。 目标是在训练期间减少损失函数值，因为这种减少是模型正在学习的一个很好的指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-bermuda",
   "metadata": {},
   "source": [
    "为了最小化这个值，模型使用了一个优化器。 这是一种确定在每个训练步骤中应如何调整权重和偏差的算法。 经过训练的模型不仅必须适用于训练数据，还必须适用于训练环境之外的数据。 正则化器的作用是确保训练后的模型泛化到新数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-professional",
   "metadata": {},
   "source": [
    "现在，让我们进入三个网络——我们将从讨论 MLP 网络开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-gravity",
   "metadata": {},
   "source": [
    "## 多层感知器 (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-giving",
   "metadata": {},
   "source": [
    "我们将要研究的三个网络中的第一个是 MLP 网络。 假设目标是创建一个神经网络，用于根据手写数字识别数字。 例如，当网络的输入是手写数字 8 的图像时，相应的预测也必须是数字 8。这是分类器网络的经典工作，可以使用逻辑回归进行训练。 为了训练和验证分类器网络，必须有足够大的手写数字数据集。 修改后的美国国家标准与技术研究院数据集，简称 MNIST，通常被认为是 Hello World！ 深度学习数据集。 它是一个适合手写数字分类的数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-stereo",
   "metadata": {},
   "source": [
    "在我们讨论 MLP 分类器模型之前，我们必须了解 MNIST 数据集。 本书中的大量示例使用了 MNIST 数据集。 MNIST 用于解释和验证许多深度学习理论，因为它包含的 70,000 个样本很小，但信息却足够丰富："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-design",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtb44zs672j31ba0kw0uv.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-gazette",
   "metadata": {},
   "source": [
    "在下一节中，我们将简要介绍 MNIST。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-republic",
   "metadata": {},
   "source": [
    "### MNIST 数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-outline",
   "metadata": {},
   "source": [
    "MNIST 是一个范围从 0 到 9 的手写数字的集合。它有一个包含 60,000 张图像的训练集和 10,000 张分类到相应类别或标签的测试图像。 在某些文献中，术语目标或基本事实也用于指代标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-breeding",
   "metadata": {},
   "source": [
    "在上图中，可以看到 MNIST 数字的示例图像，每个图像的大小为 28 x 28 像素，灰度级。 为了在 Keras 中使用 MNIST 数据集，提供了一个 API 来自动下载和提取图像和标签。 代码清单 1.3.1 演示了如何在一行中加载 MNIST 数据集，允许我们计算训练和测试标签，然后绘制 25 个随机数字图像。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-convergence",
   "metadata": {},
   "source": [
    "> 清单 1.3.1：mnist-sampler-1.3.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "municipal-change",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels:  {0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n",
      "Test labels:  {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg height=\"286.2pt\" version=\"1.1\" viewBox=\"0 0 292.158621 286.2\" width=\"292.158621pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2021-08-10T22:05:27.749772</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 286.2 \n",
       "L 292.158621 286.2 \n",
       "L 292.158621 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g clip-path=\"url(#p10f095cd9f)\">\n",
       "    <image height=\"47\" id=\"imageef1ee59348\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"7.2\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-7.062069\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g clip-path=\"url(#pf4bbbbb33d)\">\n",
       "    <image height=\"47\" id=\"image579ff0126c\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"64.924138\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-7.062069\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_3\">\n",
       "   <g clip-path=\"url(#p251b8c6497)\">\n",
       "    <image height=\"47\" id=\"image7a7c84e602\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"122.648276\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAC8AAAAvCAYAAABzJ5OsAAAFYUlEQVR4nO2ZW2sTaxeAn5nptLFNbEqbniOKPcakKm1KvCooKKgXgtdeeKM3onjnz/Av+AM8IXihUjwVESoYmtqDKSYoTdNM7dikSdrJZO0Lsez9fRtr0qZxgw+8d/OueWZmrfXOO6MAwn8UtdoCO+GPfLX4I18tako5uLa2FrfbzYEDB/B4PCQSCUzTJBaLVUhve+RXh8fjkXPnzsnDhw8ll8vJnTt35Nq1a6Ioyi/H2M1R0p3/gaIo1NTUEAwGcTgcTE5OEo/HSSQS5YQrm5Lli8UiIoKiKPT29iIiHDlyhGw2SzKZpFgsVsLzXympYC3LYnV1FdM0SafTADidTgYHBzl8+DDd3d3oul4R0X+jLPlYLMbs7Cz5fJ59+/bR09PD0NAQwWAQp9OJoiiV8v0/Si6UkZERuXz5snz+/Fls2xbbtiWVSsnU1JT4/X7RNG1PCrasPm+aJktLS6yurrK+vg5AQ0MDra2tBINBjh07hqZpFX8CZXWbbDbLysoKhmHgdrtpaGigrq4OTdM4evQotm0zMzPD5uYmhUJht53/Qen9taZG6uvrJRQKyY0bNySXy0mhUBDbtmV5eVnevHkjly5dEr/f//ulTaFQIJfLsbCwwMzMDLOzs6ysrADQ3NxMd3c3Pp+P9vZ2dF2vaPrs6Oo9Ho9cvHhR7t+/v1W86XRawuGw3Lx5U9ra2kTX9d9nhf07mUyGSCTCkydPyGaznD59GpfLRXt7O2NjY9TW1nL37l2WlpbIZDI7Pd0/2LF8Lpdjbm4ORVFYXl5mZGQEt9tNS0sLoVCIgwcPEg6HyefzZLPZXV+Bd+UROhwOaW1tlXv37kk0GhXbtmVjY0PS6bQ8e/ZMbt++LV6vV1wu1++TNj/I5/PYts309DS6rtPc3IzD4aC+vp7e3l4AfD4f0Wh069ViN9jVImppaZGxsTF58eKFxOPxrSL+9u2bTExMyNWrV6vbKn9GJpMhkUjw+vVr5ufnMU0T27apq6ujq6uL/v5+hoeHcTqdOz7Xrsvn83mWl5d5+fIlHz58IJVKYVkWmqbR1tbGwMAAo6OjNDY27kr/3/X+q2mauN1u8fl8cubMGXn69KnE43GxLEsMw5C5uTm5cuWKjI6Oiqqq1S/Yv2PbNqZpsrm5ydevX/n06RONjY10dnbS1NSEy+XC6/WSSCRQVbXs9lkR+R9ks1k2NjYYHx/HNE0CgQC6rqNpGl6vl2QyiaqWn7kVlYfv28ZIJMLGxgb9/f309fXR19eH3+9H0zSeP39OMpkklUqVHLvi8iJCJBJhcXGR/fv3c/78efr6+jh+/Dgej4fHjx+jqmpZ8nv20Wl9fZ2JiQnev39PLBYjn8/j8Xi4desWFy5coK2tjdra2pJi7pm8ZVkkk0ni8Tizs7Osra2hKAoDAwMcOnSIlpYW6urqSqqBPZMvFouk02kePXrE9evXefXqFYuLi6iqSldXFydOnKCzs7OkDXzFc/5/yeVyrKysEIlEtlZdr9fLqVOnMAwDRVGYn59HRLaNtefym5ubWJbF27dvsSyLkydP0tPTQ29vL9FoFMuyWFhY+KXev+fy8L0DvXv3DsMwGBoawufz4ff7CYVCOBwOxsfHsSxr2zhVkQe2UmR6ehq3283g4CAul4umpqZfLtqqyQOsra3x4MEDdF1neHiYSCRCOBxGURQ0TcO27Z/Or6q8bdukUim+fPnCwsICHR0diAjd3d0YhoFhGD+dX9U/I4VCgWQyycePH5mcnCQQCHD27FkCgQDt7e3bzlf4/npZVZqbm+no6KCjowNVVZmamiKdTm+7Xfwt5MvlP/1D7Y98tfgjXy3+ApZadf3PsFWHAAAAAElFTkSuQmCC\" y=\"-7.062069\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_4\">\n",
       "   <g clip-path=\"url(#pbfe0e8ebe1)\">\n",
       "    <image height=\"47\" id=\"imageda856e4549\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"180.372414\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-7.062069\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_5\">\n",
       "   <g clip-path=\"url(#pfe56a93cab)\">\n",
       "    <image height=\"47\" id=\"image95f33b667c\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"238.096552\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-7.062069\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_6\">\n",
       "   <g clip-path=\"url(#pca188346cf)\">\n",
       "    <image height=\"47\" id=\"image9c5452b414\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"7.2\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAC8AAAAvCAYAAABzJ5OsAAAD+ElEQVR4nO2ZzU4ySRSGn6Yb7AbBEMAmiiaGTzBo1JWJceeKBL0Gr8DL0pWJulAvwZ2JcWMkShRIjKCI0oL9Q30LM8wwW6o/ZxLfpDa9qPfpU1Wnz+lSAMH/VIHvBhhFP/DfJU32hLquo2kamqbx+fmJbdt4nifbBpAMr2kaOzs7LCwssLa2xtHREYeHh1SrVT4/P2VaARK3jaZphMNh8vk8i4uL/Pr1i9XVVTY2NkilUui6LstqSELGmJiYENlsVpyfn4u3tzfhuq5otVri9vZWlEolkclkhKIoUrz+GtIiH4/HmZubY3x8nFAoBICqqhiGQbFYpFgsEgjIzQ/S9nwsFiOdThMOh9G0r2k9z8O2bXK5HLZtEwgEpB5eafDhcJhkMjkABzg+PmZvb49KpUK73cZxHFl2gET4QCCAoiiDAfD29ka1WqVWq9Hr9WRZ/e0payLP83AcByH+XKkkDd6yLFqt1tALzM7Osr6+TjQalWUzJGnwvV6Pl5cXHMeh3+8jhMA0TVZWVgiHw7JshiRtzz88PPD6+srT0xPpdNq3aP9T0iJv2zadTgfXden3+wCEQiHGx8cHB1i2fK0qk8kkuVzOt20jFV4IgWVZWJYFwMTEBDMzM2SzWWZnZ6WvgPSSuN1u0+l0EEIQjUbRdZ1sNkur1aJWq0lNpVIj77ouJycnnJ6eDiBVVWVra4vt7W1UVZVpJzfyQggqlQqJRALLsjAMA03TME2TdDqNqqq4rist+lLh+/0+l5eXCCG4ubkhk8lgmiaxWIx4PI5hGIMvsQxJzzaO4/Dx8cHLywvdbhf4qjhN06RQKGCapjQv6fCe59Hr9Wg2m3S7XRRFGcAvLy8zNTUlzcuXPN/r9SiXyzSbzcGzyclJdnd32dzcJBaLSTm8vsDbtk21Wh0q1EKhEHNzc0xOTmIYhpSuyhf4VqvF2dkZl5eXNBoNXNdFURSCwSDRaJRUKjVoFUeRL/Ce5/H+/s7T0xP1eh3HcQZNSiKRIJ/PYxjGyD6+wPf7fSzLotFocH9/j23bA/hkMkmhUCASiYzs42th9vj4yNXVFZZlIYRACEEwGJRWafoK3+12aTabQ92VYRgkEgl0XR854/gK3263qdfr2LY9eJbNZimVSszPz5NIJEZaAV/hX19fqVQq1Ot1Go3GIGVGIhGSySSpVOq/C99sNrm+vqZcLlOtVhFCoKoquq6TyWSYnp4eCV56Pf9veZ7HwcEBd3d3LC0tEQwGpc3tO7wQgtvbW6LRKI7jDP1RG1V/5Gak0+nQbrd5fn7Gsixp7aDvkYevlFmv19nf3ycSiTA2NsbFxcXIbaHCz1Xm9+gH/rv0A/9d+oH/Lv0GPyXQmF71iScAAAAASUVORK5CYII=\" y=\"-63.296552\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_7\">\n",
       "   <g clip-path=\"url(#p227ceb0828)\">\n",
       "    <image height=\"47\" id=\"image19f83e1b66\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"64.924138\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-63.296552\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_8\">\n",
       "   <g clip-path=\"url(#p662857d73f)\">\n",
       "    <image height=\"47\" id=\"imagea0fd200c20\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"122.648276\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-63.296552\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_9\">\n",
       "   <g clip-path=\"url(#p2ca402d8aa)\">\n",
       "    <image height=\"47\" id=\"image554db6864b\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"180.372414\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-63.296552\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_10\">\n",
       "   <g clip-path=\"url(#pc6d73732cb)\">\n",
       "    <image height=\"47\" id=\"imaged9f3017b0b\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"238.096552\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-63.296552\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_11\">\n",
       "   <g clip-path=\"url(#p4383db41a7)\">\n",
       "    <image height=\"47\" id=\"image8cf9723b11\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"7.2\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-119.531034\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_12\">\n",
       "   <g clip-path=\"url(#p8095b0f377)\">\n",
       "    <image height=\"47\" id=\"image34282ed2f8\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"64.924138\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-119.531034\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_13\">\n",
       "   <g clip-path=\"url(#paaac28c57d)\">\n",
       "    <image height=\"47\" id=\"image49803798a3\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"122.648276\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-119.531034\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_14\">\n",
       "   <g clip-path=\"url(#p4f8f2df59d)\">\n",
       "    <image height=\"47\" id=\"image00ef27fbf9\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"180.372414\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAC8AAAAvCAYAAABzJ5OsAAAGKklEQVR4nO1YT0wTXxD+Xrd/3BbqFipQqKHaIpaEggfBaIwhmhgjR28GL8QD8eTFGxeN8eSFizcPmBijkSPR2DTRxJhINK2FpoYWqIXYUGylUstud3d+B8NqhR8W2EhM+JJ3mffmzbez8+bNPAaA8I/CsNsEdoI98rsFo94b2mw2GI1GcBwHxhiICIVCAbIs621KX/JmsxnDw8Po7OxER0cHGGMol8u4dOkSIpGInqYA6EyeMQav14v29nZ4PB4AQLlcRltbG/L5PNLpNIj0S266xrzBYIDP54PX69VkHMfh/PnzuHDhAjiO09Pcv31g/wp5juNgMOhvSvds8zsYYzh69CiKxSIYY7ruras71tLit2/fKuQHDhyA2+2G1+tFXV2dbvZ0Ja+qKqanp5FIJLSswhiDz+dDb28vLl++jGPHjulmT9ewURQFjx49QjKZRCAQqMguayGjZ+j80fOMMXAcB57nYbVaYbVawfM8LBbLOiKKoiAYDCIUCkEURaiqWrGP2WyGyWSCyWTSdE0m045SKG02PB4PnTlzhkKhEEWjUYpGozQ+Pk737t0jl8v1vzojIyM0MTFBayiXy5TNZmlsbIyGhobI5XJRXV0dXb16lU6dOrUph/8bfwwbQRDQ3NwMv9+PpqYmTeZwOBAIBMDzPGZmZip0ZFlGNpvF9+/fNZnRaITT6YTP50M+n0c2m4Uoijh+/DjsdjsYY5idnd3wwG/b8/39/XTnzh3K5XL0K8rlMr1584bu3r1LBoOhQsftdtPw8DC9fPmSNoKqqiTLMsmyTIqikKIoJEkSXb9+nXp6eojjOH08H4/HoSgKrly5AofDock5jkNrays4jsPt27crapba2lp0dXVp9c3vWDtHv8JgMKC/vx9utxvRaBSlUqkax//5CwVBoI8fP5KiKBt6Ui8oikLxeJzsdntVnq8qz4uiiMePH+P58+dVeWO7ePbsGZ48eYJyuVzV+qryvKIomJychNlsRmdnJwRBQE1NzR/1JEmCKIqw2Wwb1jalUgmxWAwcx8FiseD9+/eIRqNbalyq+0UGA7W3t9PNmzfp7du3VYXB3NwcvXr1igqFwobzU1NT5HA4yOPx0Llz58jpdK47/JuNqm9YVVWRzWYRCoXQ3NyMxsZGuFwumEymn14gQiaTwdLSEsLhMGZmZpBKpdDU1ASPx4PBwcGKg6qqKiRJQi6XQyKRwMrKSsXFppvnfx3Xrl2jUChExWJx3YGbmJigBw8eUF9fH7W2tmo63d3dVCwWSVXVCs/b7XZijG3rktpWYfb06VPcuHEDmUxmw/lisYhIJFIxn8vlMDo6infv3mkyQRAwMDCA3t7e7dDYXlWZyWQQi8UwPT2N+fl5Tc4Yg9Vqhc1mA8/zMBp/RqUkSVhYWMDy8rImM5lMaGlpgSAIf4888CN93rp1CyMjI1qcMsbg9/tx+vRpDA4O6lr+boRtk1dVFalUCpOTk3j9+rX2B9a8f+jQoYobWZIkpNNpfP36VZPxPI8TJ04gEAjg4MGD2Ldv398hT0SYn59HLBbDixcvMDc3p81ZLBYcPny4omuSJAnJZBJfvnzRZDU1Nejr60NPTw98Ph94nt8Shx03I5lMBqOjo2CMwW63o62tDTabDV1dXWhsbNTWlUolRKNRhMNhRCIReL1e7aLz+/0YGBjA4uIi8vl81bZ33AaKoohUKoV4PI4PHz5gdXUVHMdh//79FWGgKAqWl5fx+fNnJBIJrK6uanP19fXo6OiAzWbbkm3d2sCxsTEEg0EEAoGKWP8d4XAYoijiyJEjcDqdAICGhgbU19fDbrdvyaZu5GVZxsrKCoLBID59+oSGhgYsLCysW7e8vIzZ2dkKz6+VyFvtb3VtwGVZxsOHD9HS0oLu7m4kEol1a/L5PAqFQtX1+mbQlfza00c6ncbU1BRyudyW9M+ePQur1Yrx8fGqymLdX8wKhQIKhQIWFxc3Xbe0tIRsNgun06mFS21tLQRB2FL4bKso2slgjNHJkydpaGiISqUSEf3oay9evEgWi0X/klhPEBHS6TQA4P79+zCZTCAiJJNJiKJY9T4MP77in8Te+/xuYY/8bmGP/G7hnyb/H5wOEPwIVDhbAAAAAElFTkSuQmCC\" y=\"-119.531034\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_15\">\n",
       "   <g clip-path=\"url(#pc232a3bdd2)\">\n",
       "    <image height=\"47\" id=\"imagea206fbb99f\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"238.096552\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAC8AAAAvCAYAAABzJ5OsAAAHP0lEQVR4nO2ZS09TXxfGf+f0CrSUtvRCSsBQvAGaQBQ1RqMTJ0aNGibEiWM/g/Ej+AWcGKeGxDjAGAEvMZEgJKQKRRQ0FGiFcmk50PZc9n9gOL4VEIQa8iY+yZl07+71dO21n/XsUwkQ/J9C3m8Ce8E/8vsF699YVJZlHA4HNpsNq9WKpmlomsbq6mpJ4/wV8sFgkI6ODs6ePcvJkyfp7+8nFotx//59CoUChmGUJE7JydtsNqqqqmhtbeXIkSPU19eTzWaRJImWlhZmZmaYmZkpWTxRysfr9YpLly6JdDotcrmc0HVd6LouUqmUePDggbh+/XrJYpU081arlVu3btHW1kZZWRkWi8Uc0zSNVCpFNpstXbySrQRYLBbOnDlDW1sbNpsNWf4pZoZhsLa2hqZpJYtXUqnUNI0nT57w9OlTdF1HiJ/Nu6qqio6ODk6cOIHb7S7ald2ipOSFEHz58oXx8XHS6TRra2vmmM1mIxQKUVdXRzQapaysbM/xSkreMAzev3/P69evGRoaIplMmmNWq5Xq6mra29vp7OwkGAzuOd62Ne92u6moqKCyshKAQqFAOp3+7cHL5XKMj48TCARoaGjYsF4kEqGyspKysrKi3SkpeVmW8Xq9BAIBwuEwkiShKAq6rqMoypbNJp/P8/XrV5qamjaMlZeXEwqF8Hg8lJWVkcvlis5GSci7XC5qa2u5ffs2Fy5cwOfzAT+y+ujRI549e8bY2Bj5fH7DdzOZDG/fvqWlpWXDmM/nw2KxUFNTw/T0NIuLi7si/lvysixjtVoJh8M0NjbidruRJAlVVWlubiaVSmG328lkMiwvL6MoCmtra6bK5PN5dF3fsK7NZqO8vByv14vH40GSpNJnXlVVMpkMsizj8XjMz+12O5cvX+bcuXN8/vyZRCLBq1ev+PDhA58+fUJRFKxWKz6fb1NFsVgsOBwOotEoi4uLDA8P79rrbEle0zSy2SwLCwt8//4dv99vavN692xsbCQUChEIBDh9+jSpVIp8Po/D4SASiXDs2LEN667vqMvlMndzt9iS/Pqh/P79O9PT07jdbhwOBxaLBafTidPpNBWopaUFRVFYXV3FMAxkWcbpdOJwOMyyWCcpSRJWqxW3243L5SrqwrvBlsZHkiRRW1srTp06JR4+fCj6+/tNo/Xro6qqyOfz5qOqqtB1XQghhGEYG+bH43HR1dUl6uvrhdvtLr0xE0IwOztLJpNhcnISj8eDrutmFv83a7IsI8uyefiEEAghtqxnn89HOBympqYGVVV3Zdi23TNd18nlcgwPDxOPx1lbWzMfTdMwDKOIsK7rFAoFc856KW1Gvr6+nosXL3L48OE/Jg47dJW6rjM6Oko2m2V5eRn4oTrXrl0jHA4TDAZN4n19fUxPTzMxMYEkSVgsFpqbm/H7/dTV1VFVVYXX6zXXXVpaMg+5qqp/pDw7Jh+Px4nH47x48QL4oTjRaBRZlk2fYhgGPT09DA0N0dvbiyRJ2O12rly5QmNjI+fPn6ehoaGI/OLiIoVCgfLycrLZbOnJ7xaGYZDP5+nt7WV0dBSr1YrFYiEajQIQCoW4d+8e79694/nz5/T19RWZue2wa50SQjA/P8/c3ByKopiXjOrqaoLBIF6vF6fTiRCCdDpNMpkkmUyysrJirmG32zl48CBHjx6lqakJj8fzRz5/15lXVZXHjx8Tj8dNHxQOh7lx4wbHjx/HMAxisRgjIyPm/HXl+hWHDh3C7/fT39/P/Pw8CwsLO7IMu868YRhMTU0xOTlJIpFgZWUFSZLw+Xz4/X7Ky8ux2WxFPzaZTJJKpZibm0NVVXPM6XTi9Xrx+/14vd4dd909lc3U1BQTExN8+/bNzKjH48Hn81FRUYHV+nNjVVVlZmaGRCJBIpEocqN2ux2Px0MgEKC6unrHXXfPNylFUfj48SPz8/PmZ36/n5s3bxb5eU3TmJ+fp7u7m7t37zI4OMjc3JxZHpIk0dnZyZ07d/D7/Tidzm1j71ltCoUCyWSSTCaDpmmmolRWVhYREEJQKBSYnZ1ldXWVmZkZIpEIfr/fLJMDBw4ghCAUCpnN8XfYc+az2SwDAwOMj4+btby0tERPTw8TExOb/thsNsvIyAgjIyNFul5ZWUkkEuHq1au0trZuG3vPmV93n2NjY7x8+ZJgMEg6nWZgYIDp6elN5xcKBfMS86uqrO/aTt4u7Jn8+sukwcFBstkswWCQTCZDX18fiqJsOr9QKLCwsLCpJMqyjMvlwuFw/H3y65iammJhYQG73Y6maSwvL296DYQf9f/mzRumpqaoqamhqanJvO/abDai0ajZkX99efVXyCuKsmmmt8Ls7Cy6rhOLxXC5XDQ0NBQdcEmSttX7v+ptfod1U9bd3c3S0hKSJNHe3o5hGHR1dTE8PFzUyDbDvpGHn2+OY7EYAGNjYwghiMVizM7Obvt9iX9/Ze4P/pHfL/wjv1/4R36/8B+b3Ac/x00DXgAAAABJRU5ErkJggg==\" y=\"-119.531034\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_16\">\n",
       "   <g clip-path=\"url(#p2ab69fe1ee)\">\n",
       "    <image height=\"47\" id=\"image606f6038ea\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"7.2\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-175.765517\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_17\">\n",
       "   <g clip-path=\"url(#p44bacfbea5)\">\n",
       "    <image height=\"47\" id=\"image751bbd6540\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"64.924138\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-175.765517\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_18\">\n",
       "   <g clip-path=\"url(#p0c2fa789bf)\">\n",
       "    <image height=\"47\" id=\"image0770b9cdfe\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"122.648276\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-175.765517\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_19\">\n",
       "   <g clip-path=\"url(#pbd1b22b002)\">\n",
       "    <image height=\"47\" id=\"image5b2fcae52d\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"180.372414\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-175.765517\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_20\">\n",
       "   <g clip-path=\"url(#p69d6f4d3d6)\">\n",
       "    <image height=\"47\" id=\"image1692e4084e\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"238.096552\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-175.765517\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_21\">\n",
       "   <g clip-path=\"url(#pc38d664879)\">\n",
       "    <image height=\"47\" id=\"image92bd4b29b8\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"7.2\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAC8AAAAvCAYAAABzJ5OsAAAD6ElEQVR4nO2ZzUojWxSFv0pMTGyJ0SoixLZtJWIEB06cOLChX0AURCc+hQ/h2McQGhFHjpTWiYIiCKIxRvIjGhOjFXJicpJK1R1c2sv9GXXqXAm4HmDtj137rLM5pQEOHSrPewO0o46G71Jh+vXrV9bW1nh6euLh4YHt7W0KhYLrdZTAG4bBysoK2WyW6+tr9vb2Ogf+l6LRKL29vXz69EmJv1L4p6cn0uk0tVpNib9S+OPjY378+MHj46MSf6VpEwwG6e/vp6tLTY+UwgcCAcLhcGfC9/X1MTw8jN/vV+KvFN7r9eL3+9E0TYn/xw37TwkhOD09pdVqMTIy0lljU6/Xubq6QkpJNBqlu7tbyegoG5tWq8XAwACxWIyhoSF0XXe9hhJ427aRUgJ/ZX1fX5/r3VcC32w2KRQK1Go1urq6GBsbY2xsrDPga7UaiUSCYrGI4ziMj48zMTGBx+NuOSVpU6/XSaVSlEolbNtmdHSUcrncGZ2XUpLJZMhkMmSzWRqNhooyauAdx0FKSbVapVKp0Gq1VJRRuxILId7mXoWUrgemaZJOpzEMgy9fvhAMBl3dMJXCPz8/k0qlMAyDkZERent78fl8rvkrhU+n0xweHpLJZKjVaszOzjI6Ouqav1J4IQT5fJ5cLodpmsRiMQzDcM1f6YGtVqvU63W2traYnJxkYWGBSqXCwcGBK/5tw3u9Xnp6epBS/mee27ZNMplE0zReX19pNpvtlnxTW/CapuHz+RgYGMA0TZrN5r9i0XEcEokEzWYT0zSRUqJpmivx+dvwXq+Xubk54vE4i4uLHB0dcXJywsHBAeVyGcdx/gYopeT29pZIJMLS0hL7+/sUi8X3gdc0jXA4zOfPn5mZmcGyLGzbJp/Pv+00UkqklAgh8Pl8+P1+otEouq6TSCRoNBqUy+X/H95xHO7v7ykWiwSDQb5//863b99YXl6mWq0ihODm5oZkMsnu7i6GYbC8vEwwGMTj8RAIBDg5OWFzc/O3z8Fvw9u2zcPDA7lcjru7O3RdJxQKEYlEKJfLlEol7u/vuby85Pn5Gcuy2NnZYXBwEF3XMU0Ty7Lamv22Op/NZgmFQlxdXRGPxwmFQoTDYSzL4vHxkYuLC37+/EmlUqFQKLCxscHExARTU1Pc3d21NTJtwf9SLpdjfX2d1dVV5ufn0XX9bZ/P5/MIIbBtm0ajQTabpVQqcX5+Tr1ep9FotLVxtg0vhODs7Izp6Wni8TiRSIRCoUAmk+Hl5QXLsoA/v5QQAiGEaw+vGi79UAsEAvj9fjwez1vSWJalbJcHF+HfQx393PcB/176gH8vfcC/lzoa/g+d9OjSJiegNAAAAABJRU5ErkJggg==\" y=\"-232\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_22\">\n",
       "   <g clip-path=\"url(#pd3cc1d0076)\">\n",
       "    <image height=\"47\" id=\"image689a7558a4\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"64.924138\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-232\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_23\">\n",
       "   <g clip-path=\"url(#p56d2f4ee8b)\">\n",
       "    <image height=\"47\" id=\"image227010a63f\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"122.648276\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAC8AAAAvCAYAAABzJ5OsAAAFmklEQVR4nO2YTU8TXRvHf9OZdjp0mLa0tgiCCQltIMQNQcGYsBLc+AHc+B1MTNSP40rXGlfGhUbUxAQxxvDSmNLQF0s7YqdvMHTmPAtumof7xjzaZ0DuxH9ylv1fv3PNdc519UiA4F8qxSujWCxGLBZjfHwc13XJZrNUKhVM0/QqxLESXqxr166Ju3fviq2tLZHNZsWDBw/E1atXPfH+0fJ5lQFd1xkcHMTv9xMOh5mfnyeVSqHrOj6fZ2GOyDNXVVW7oJqmkU6nGR0dJRaLoSieVecReQbfarUwTRPHcVBVleHhYW7cuMGdO3c4f/68V2GOyDP4SqXCxsYG1WqVZrOJoigkk0nS6TSapiFJklehjsiTwyPLsjAMQzx+/Fisra0J13XF7u6uqFarYnZ2Vvj9/rN7YF3XZW9vj5cvX7K0tITruiiKgqZpLCwssLCw4PnB9ewkCSHY39/n48ePqKrK/v4+gUAAVVWZmZnB5/Px/PlzbNv2KuRBXC9XKBQSs7Oz4t27d6JQKAjXdcXOzo549eqVGBkZEbqun72yOVSr1aJarfLp0ye2t7eBgx4QDocxDANVVT2L5Tm8EIJyuczDhw9ZWVkBQJZlNE1jaGiI/v5+z2KdSOvb29tjc3OTXC5HPp/Htm2i0Si3bt1iZmaGQCDgydV5IvC2bVMqlcjn8+RyOWzbxjAMFhcXmZqaoq+vz7Ob58QGp7GxMbG4uChWV1eF67pif39fPHnyRNy+fVskk8mzd2D/W6ZpkslkKBQKmKaJLMskEgkmJyeJRCIEAoH/y/9kJqa/VKvVaLVavHjxAsuyuHnzJqlUimg0yuvXr7Esi69fvyKE6Mn/ROHhoPN++fKFSCSCEAJN04jFYkxMTNBsNtne3sZxnJ68T7Rs4AB+c3OTbDaL67qoqko0GiWdTpNOp5FluWfvE4cXQlAsFtna2sKyLPb29pAkifn5eRYWFgiHwz3X/onDAzQaDXZ2diiVStTrdQDi8ThDQ0Mkk8meG9epwH///p1cLsezZ89YXV0FIBwOMzIywvXr15mYmOjJ91Tg4SD7S0tLZDIZ2u02rusSCAQYGRkhEon05Hmq8G/fviWTydBoNHAcB0VRuHDhAtFotCfPU4N3HAfLsigUCqytrdFqtQiFQszNzTE+Pt7TvHNq8ACdTgfLsiiVSti2jSzLxOPx7qvDmYYHKBaLvH//nnq9jiRJqKra89PID38ly3LXWJIk2u02juP03A0BJEkiGAyi6zqyLHczLUlSTyPyD+EPX74ON1CtVrFtm93d3e4schjwZ2YTSZJQFIVQKHTkIUoI0V2ewPv9fqanp7l//z79/f34/X5yuRz1eh3TNFlfX6dUKjE3N4fruqyvr9NsNtnd3T02SDQaJRKJcOnSJdLpNJOTk5w7d45Op4NpmtRqNVzX/eUNHAsvSRKGYTA1NdXNfjKZpNFo8O3bN/r7+8nn81y5cgXHcdB1nUajQbvdPjbIwMAAAwMDTE9PMzg4yPDwMJ1Oh2azSaFQoFar9ZR5iYPB/uiOFIXLly9z7949EokE4XCYRCKBqqoEg0Ecx0EI0R2q/lfWDmv6sM4lSWJra4t8Ps+jR49YXl7mzZs3vwx/bOZd16VYLPL06VN0XScUCpFKpTAMg3g8Tl9fX3eY8vv93dJSFAXDMI5MikIIKpUKrVYLy7IwTZNKpUI2m6VcLvPhwwcKhcIvg8MPMv+PHf71JZLJJGNjYyQSiW5X1DSN0dFRdF2nr6+PixcvEgwGjyRieXmZcrlMNpvl8+fPrKyssLGxQb1ep9Pp9AT+0/CSJHVrX9M0AoEAfr8fOLhSg8EgPp+v+7z39z/XtVoN27ZptVo0m00sy6LZbHbL70Thz6pOvcN6qT/wv0t/4H+X/sD/Lv2B/136V8P/B599/aaQQnnlAAAAAElFTkSuQmCC\" y=\"-232\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_24\">\n",
       "   <g clip-path=\"url(#p2160673453)\">\n",
       "    <image height=\"47\" id=\"image7f7da264a6\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"180.372414\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAC8AAAAvCAYAAABzJ5OsAAAGB0lEQVR4nO2ZS08TaxjHf3Nh2gpM7YVihSKJ41TxAqluSLxsTNQta5cmfgg/gTF+B5ds2JC4MMaFJEQTwEgwwRCQgCmtLcilNzqd6XsWpnPo4RzPYVol5vhP3mQyb955/vPMcx8JEPyikI+bQCv4Tf648Jv8ceE3+eOC2s6HSZJELBbj5MmTmKaJYRgYhgHA3t4eU1NTrKyssLS01BZ5bSMvSRKKonDq1CkSiQQ3btxgdHSU0dFRJEkim81i2zYA6XSaarVKvV6nXq97l0mbMmwkEqG3t5dHjx6RTCZJJBKcOHGCQCCAJEnUajVyuRxra2ssLy8zPj7OysoKmUwGy7Ko1WpHltmy5hVFIRAIcPbsWYaGhjBNk4GBAUKhULMgVSUajSJJEoFAgPX1dXp6enj79i3b29tsbW15ki9aWZ2dnWJ4eFg8efJELC8vi1KpJGq1mrts2xaO4wjHcZruW5YlVldXxYMHD8S1a9c8yW5Z8/F4nIcPHzIyMkI4HEZVVSzLIpvNMj8/z8LCAqdPn6a3t5dbt26haRqqqiJJEqqq0tPTQ1dXlyfZLZOPxWKMjY3R2dlJIBAAoFwus7GxwdTUFJOTkwwNDZFMJhkZGUHXdTo6OgDo6Oigp6eHYDCIqqo4joMQ/90FWyavqiqhUAhV/fNRxWKR2dlZdnd3CYfDzMzMsLi4SL1e5/bt29y9exeAcDjM/fv3icfjyLLM9PQ0uVzuP8tuOUk1QqQkSQghyGazZDIZCoUCkUiEVCqFEILNzU2WlpbI5/PuWUVRiEQixONxBgcH8fv9R5LdlgzbiNe1Wo3JyUmeP39OtVrlzp07PH36lIGBASqVCh8+fCCTySCEaFrhcJiLFy8e2fbbmmFlWcY0Tfr6+vD7/QwODqKqKqlUClVVyWQy1Go1KpUKPp8PSZKAb1+vcf3TyMuy3CRUkiTOnTsHQCgUch3z8uXLAGxsbGBZFuVyGVVVm/zEk3yvBxVFIZVKceXKlSYNRqNRotEoPp/P9YV4PE4kEmFzc5PZ2VkmJiaabN8rPL+6JEn09/cTj8ebtL+3tweArusu+Wg0SiKRwDAMuru72d3dxXGc4yOvKApXr17l0qVL7j3HcXj58iVCCG7evEkwGKSrq4tUKkUymWR4eJjNzU22trZaNpmWyMM3m29ot4FqtUqhUODdu3ecP38e0zRd5xwYGCASiVAqldB13T2Xz+d5//49hULh55FvRImDNm9ZFl+/fiWXy6HrOslkEk3T0DSNzs7OQxnUcRyy2Sxzc3Ouyf0U8gdjNeDW5nt7e8zNzRGLxTBNk1gshqZph2r3fD7Ps2fPePPmDQsLC5TL5Z9DXpZlN1Qe1KYsy9i2TSaT4dOnT3z8+BG/34+u6wgh3C9Vr9cplUosLy+TTqePbDKeyauqiqZp+P1+fD5f014gEEBRFNLpNBMTE0xPT/P48WNM00SSJDo6OtA0jVKpRD6fp1wue2pEPJMPBoP09vZiGAaJRMLVvizLDA0Nsb+/T1dXF9VqlXQ6zfj4OLFYDJ/PR3d3N7quk06n+fLlC4uLiy3F/CM3AYZhiHv37omVlRW32bBt2200ZmZmhGEYIhgMumcURRHBYFAYhiGuX7/etOd1edJ8Ixzu7OwQDofRdb1pX9M0+vr6qFar7O7uAt+iSrFYxLIs8vk8xWLRi+gmeCoPLMuiWCyys7PjOtrBsCnLMt3d3Wia1nTOcRwqlcrxZthcLkehUGB2dhbHcejv72/a1zSNUCjkdlY/Cp40L4TAcRxs2/7b1q3Rw7bDNL6HlpJUown5a7La399ndXWV7e3ttpD8J3guiW3b5sWLF7x+/fqQ5sPhMGNjY1y4cKFlgt+DZ/L1ep21tTU+f/58aC8QCGCaJtFotCVy/wbP5IUQpNNpstnsIc37fD7OnDlDMBhsmeD30FID/r1BqaIoyPKPnaD/0vP5lslvb2/z6tUr1tfX3UrzR2u8gZZ7sWKxyPz8vDt8gm/1fKNi/JFoeT7fGHEfHGUIIbBtm/39farVajt4/i3a9nPhOPD/dtjjxC9N/g/MF/dtUnlWSQAAAABJRU5ErkJggg==\" y=\"-232\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_25\">\n",
       "   <g clip-path=\"url(#pd9825b9aee)\">\n",
       "    <image height=\"47\" id=\"image0f8ab08b40\" transform=\"scale(1 -1)translate(0 -47)\" width=\"47\" x=\"238.096552\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-232\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p10f095cd9f\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"7.2\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pf4bbbbb33d\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"64.924138\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p251b8c6497\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"122.648276\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pbfe0e8ebe1\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"180.372414\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pfe56a93cab\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"238.096552\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pca188346cf\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"7.2\" y=\"63.434483\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p227ceb0828\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"64.924138\" y=\"63.434483\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p662857d73f\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"122.648276\" y=\"63.434483\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p2ca402d8aa\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"180.372414\" y=\"63.434483\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pc6d73732cb\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"238.096552\" y=\"63.434483\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p4383db41a7\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"7.2\" y=\"119.668966\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p8095b0f377\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"64.924138\" y=\"119.668966\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"paaac28c57d\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"122.648276\" y=\"119.668966\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p4f8f2df59d\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"180.372414\" y=\"119.668966\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pc232a3bdd2\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"238.096552\" y=\"119.668966\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p2ab69fe1ee\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"7.2\" y=\"175.903448\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p44bacfbea5\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"64.924138\" y=\"175.903448\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p0c2fa789bf\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"122.648276\" y=\"175.903448\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pbd1b22b002\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"180.372414\" y=\"175.903448\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p69d6f4d3d6\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"238.096552\" y=\"175.903448\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pc38d664879\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"7.2\" y=\"232.137931\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pd3cc1d0076\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"64.924138\" y=\"232.137931\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p56d2f4ee8b\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"122.648276\" y=\"232.137931\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p2160673453\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"180.372414\" y=\"232.137931\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pd9825b9aee\">\n",
       "   <rect height=\"46.862069\" width=\"46.862069\" x=\"238.096552\" y=\"232.137931\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 360x360 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.3 s (started: 2021-08-10 22:05:26 +08:00)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Demonstrates how to sample and plot MNIST digits\n",
    "using Keras API\n",
    "https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# count the number of unique train labels\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Train labels: \", dict(zip(unique, counts)))\n",
    "\n",
    "# count the number of unique test labels\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"Test labels: \", dict(zip(unique, counts)))\n",
    "\n",
    "# sample 25 mnist digits from train dataset\n",
    "indexes = np.random.randint(0, x_train.shape[0], size=25)\n",
    "images = x_train[indexes]\n",
    "labels = y_train[indexes]\n",
    "\n",
    "# plot the 25 mnist digits\n",
    "plt.figure(figsize=(5,5))\n",
    "for i in range(len(indexes)):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    image = images[i]\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.savefig(\"mnist-samples.png\")\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "governing-flashing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57644, 57266, 46377, 28391, 25006, 23649,  5421, 41707, 25137,\n",
       "       18805, 57306, 24848,   442, 35879, 59599, 15041, 34437, 47110,\n",
       "       38454, 13515, 27083, 38081,  1463,  2998, 35876])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-fraud",
   "metadata": {},
   "source": [
    "mnist.load_data() 方法很方便，因为不需要单独加载所有 70,000 个图像和标签并将它们存储在数组中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-third",
   "metadata": {},
   "source": [
    "在讨论 MLP 分类器模型之前，必须记住，虽然 MNIST 数据由二维张量组成，但应根据输入层的类型对其进行重塑。 下图 1.3.2 显示了如何为 MLP、CNN 和 RNN 输入层重塑 3 × 3 灰度图像："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-narrow",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtb4fa9g4vj31b00nugnt.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-blond",
   "metadata": {},
   "source": [
    "在以下部分中，将介绍 MNIST 的 MLP 分类器模型。 我们将演示如何使用 tf.keras 高效地构建、训练和验证模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-ordinary",
   "metadata": {},
   "source": [
    "### MNIST 数字分类器模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-brass",
   "metadata": {},
   "source": [
    "图 1.3.3 所示的拟议 MLP 模型可用于 MNIST 数字分类。 当暴露单元或感知器时，MLP 模型是一个全连接网络，如图 1.3.4 所示。 我们还将展示感知器的输出如何根据输入计算为第 n 个单元的权重 wi 和偏差 bn 的函数。 相应的 tf.keras 实现如清单 1.3.2 所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-publisher",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtb4h31x9cj31c00gaabp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-sunday",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtb4hl26w8j310k0tctbg.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-grocery",
   "metadata": {},
   "source": [
    "> 清单 1.3.2：mlp-mnist-1.3.2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "democratic-bouquet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "time: 464 ms (started: 2021-08-10 22:16:07 +08:00)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A MLP network for MNIST digits classification\n",
    "\n",
    "98.3% test accuracy in 20epochs\n",
    "\n",
    "https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# load mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# compute the number of labels\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "# convert to one-hot vector\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# image dimensions (assumed square)\n",
    "image_size = x_train.shape[1]\n",
    "input_size = image_size * image_size\n",
    "\n",
    "# resize and normalize\n",
    "x_train = np.reshape(x_train, [-1, input_size])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = np.reshape(x_test, [-1, input_size])\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "batch_size = 128\n",
    "hidden_units = 256\n",
    "dropout = 0.45\n",
    "\n",
    "# model is a 3-layer MLP with ReLU and dropout after each layer\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_units, input_dim=input_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(hidden_units))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_labels))\n",
    "# this is the output for one-hot vector\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "binary-vacuum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 8s 13ms/step - loss: 0.4286 - accuracy: 0.8686\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.1970 - accuracy: 0.9416\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.1524 - accuracy: 0.9542\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.1276 - accuracy: 0.9613\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.1152 - accuracy: 0.9645\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.1038 - accuracy: 0.9676\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0952 - accuracy: 0.9707\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0900 - accuracy: 0.9720\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0807 - accuracy: 0.9746\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0768 - accuracy: 0.9765\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0737 - accuracy: 0.9765\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0695 - accuracy: 0.9778\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0685 - accuracy: 0.9779\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0640 - accuracy: 0.9795\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0625 - accuracy: 0.9797\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0605 - accuracy: 0.9806\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0561 - accuracy: 0.9817\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0565 - accuracy: 0.9815\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0547 - accuracy: 0.9827\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0534 - accuracy: 0.9826\n",
      "\n",
      "Test accuracy: 98.2%\n",
      "time: 2min 8s (started: 2021-08-10 03:04:01 +08:00)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# enable this if pydot can be installed\n",
    "# pip install pydot\n",
    "#plot_model(model, to_file='mlp-mnist.png', show_shapes=True)\n",
    "\n",
    "# loss function for one-hot vector\n",
    "# use of adam optimizer\n",
    "# accuracy is good metric for classification tasks\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# train the network\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=batch_size)\n",
    "\n",
    "# validate the model on test dataset to determine generalization\n",
    "_, acc = model.evaluate(x_test,\n",
    "                        y_test,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "采用混合精度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "average-square",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.4338 - accuracy: 0.8666\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.1940 - accuracy: 0.9416\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.1534 - accuracy: 0.9536\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.1293 - accuracy: 0.9610\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.1128 - accuracy: 0.9656\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.1021 - accuracy: 0.9684\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0952 - accuracy: 0.9697\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0858 - accuracy: 0.9733\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0848 - accuracy: 0.9729\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0784 - accuracy: 0.9751\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0744 - accuracy: 0.9759\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0705 - accuracy: 0.9777\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0698 - accuracy: 0.9781\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0665 - accuracy: 0.9787\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0643 - accuracy: 0.9794\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0587 - accuracy: 0.9811\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0616 - accuracy: 0.9803\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0576 - accuracy: 0.9808\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0558 - accuracy: 0.9818\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0544 - accuracy: 0.9826\n",
      "\n",
      "Test accuracy: 98.3%\n",
      "time: 2min 51s (started: 2021-08-10 03:07:11 +08:00)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A MLP network for MNIST digits classification\n",
    "\n",
    "98.3% test accuracy in 20epochs\n",
    "\n",
    "https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow import keras\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# load mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# compute the number of labels\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "# convert to one-hot vector\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# image dimensions (assumed square)\n",
    "image_size = x_train.shape[1]\n",
    "input_size = image_size * image_size\n",
    "\n",
    "# resize and normalize\n",
    "x_train = np.reshape(x_train, [-1, input_size])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = np.reshape(x_test, [-1, input_size])\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "batch_size = 128\n",
    "hidden_units = 256\n",
    "dropout = 0.45\n",
    "\n",
    "# model is a 3-layer MLP with ReLU and dropout after each layer\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_units, input_dim=input_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(hidden_units))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_labels))\n",
    "# this is the output for one-hot vector\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "#\n",
    "# enable this if pydot can be installed\n",
    "# pip install pydot\n",
    "#plot_model(model, to_file='mlp-mnist.png', show_shapes=True)\n",
    "\n",
    "# loss function for one-hot vector\n",
    "# use of adam optimizer\n",
    "# accuracy is good metric for classification tasks\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# train the network\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=batch_size)\n",
    "\n",
    "# validate the model on test dataset to determine generalization\n",
    "_, acc = model.evaluate(x_test,\n",
    "                        y_test,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-health",
   "metadata": {},
   "source": [
    "> 似乎用了混合精度后，反而变慢了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "直接用1,2等作为label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dressed-script",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "469/469 [==============================] - 7s 13ms/step - loss: 8.2153 - accuracy: 0.5494\n",
      "Epoch 2/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 1.2062 - accuracy: 0.6690\n",
      "Epoch 3/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 1.0111 - accuracy: 0.7293\n",
      "Epoch 4/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.8725 - accuracy: 0.7677\n",
      "Epoch 5/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.7659 - accuracy: 0.8021\n",
      "Epoch 6/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.6684 - accuracy: 0.8284\n",
      "Epoch 7/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.5939 - accuracy: 0.8466\n",
      "Epoch 8/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.5182 - accuracy: 0.8651\n",
      "Epoch 9/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.4646 - accuracy: 0.8778\n",
      "Epoch 10/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.4353 - accuracy: 0.8885\n",
      "Epoch 11/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.4077 - accuracy: 0.8928\n",
      "Epoch 12/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.3832 - accuracy: 0.8999\n",
      "Epoch 13/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.3675 - accuracy: 0.9044\n",
      "Epoch 14/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.3496 - accuracy: 0.9081\n",
      "Epoch 15/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.3440 - accuracy: 0.9102\n",
      "Epoch 16/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.3291 - accuracy: 0.9146\n",
      "Epoch 17/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.3220 - accuracy: 0.9168\n",
      "Epoch 18/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.3129 - accuracy: 0.9184\n",
      "Epoch 19/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.3101 - accuracy: 0.9211\n",
      "Epoch 20/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.3053 - accuracy: 0.9212\n",
      "Epoch 21/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.2995 - accuracy: 0.9241\n",
      "Epoch 22/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.2946 - accuracy: 0.9253\n",
      "Epoch 23/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.2832 - accuracy: 0.9271\n",
      "Epoch 24/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.2846 - accuracy: 0.9266\n",
      "Epoch 25/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.2761 - accuracy: 0.9305\n",
      "Epoch 26/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.2711 - accuracy: 0.9308\n",
      "Epoch 27/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.2714 - accuracy: 0.9298\n",
      "Epoch 28/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.2656 - accuracy: 0.9321\n",
      "Epoch 29/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.2704 - accuracy: 0.9300\n",
      "Epoch 30/30\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.2621 - accuracy: 0.9329\n",
      "\n",
      "Test accuracy: 95.7%\n",
      "time: 3min 2s (started: 2021-08-10 03:23:04 +08:00)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A MLP network for MNIST digits classification\n",
    "\n",
    "98.3% test accuracy in 20epochs\n",
    "\n",
    "https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# load mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# compute the number of labels\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "# convert to one-hot vector\n",
    "#y_train = to_categorical(y_train)\n",
    "#y_test = to_categorical(y_test)\n",
    "\n",
    "# image dimensions (assumed square)\n",
    "image_size = x_train.shape[1]\n",
    "input_size = image_size * image_size\n",
    "\n",
    "# resize and normalize\n",
    "x_train = np.reshape(x_train, [-1, input_size])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = np.reshape(x_test, [-1, input_size])\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "batch_size = 128\n",
    "hidden_units = 256\n",
    "dropout = 0.45\n",
    "\n",
    "# model is a 3-layer MLP with ReLU and dropout after each layer\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_units, input_dim=input_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(hidden_units))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_labels))\n",
    "# this is the output for one-hot vector\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "#\n",
    "# enable this if pydot can be installed\n",
    "# pip install pydot\n",
    "#plot_model(model, to_file='mlp-mnist.png', show_shapes=True)\n",
    "\n",
    "# loss function for one-hot vector\n",
    "# use of adam optimizer\n",
    "# accuracy is good metric for classification tasks\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# train the network\n",
    "model.fit(x_train, y_train, epochs=30, batch_size=batch_size)\n",
    "\n",
    "# validate the model on test dataset to determine generalization\n",
    "_, acc = model.evaluate(x_test,\n",
    "                        y_test,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-knife",
   "metadata": {},
   "source": [
    "在讨论模型实现之前，数据必须采用正确的形状和格式。 加载 MNIST 数据集后，标签数量计算如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of labels\n",
    "num_labels = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-advocacy",
   "metadata": {},
   "source": [
    "硬编码 num_labels = 10 也是一种选择。 但是，让计算机完成它的工作总是一个好习惯。 代码假设 y_train 的标签为 0 到 9。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-cutting",
   "metadata": {},
   "source": [
    "此时，标签是数字格式，即从 0 到 9。标签的这种稀疏标量表示不适用于输出每个类的概率的神经网络预测层。 更合适的格式称为 one-hot 向量，一个 10 维向量，除数字类的索引外，所有元素均为 0。 例如，如果标签为 2，则等效的 one-hot 向量为 [0,0,1,0,0,0,0,0,0,0]。 第一个标签的索引为 0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-selling",
   "metadata": {},
   "source": [
    "以下几行将每个标签转换为一个独热向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to one-hot vector\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-glossary",
   "metadata": {},
   "source": [
    "在深度学习中，数据存储在张量中。 术语张量适用于标量（0D 张量）、向量（1D 张量）、矩阵（二维张量）和多维张量。\n",
    "\n",
    "从这一点来看，除非标量、向量或矩阵使解释更清晰，否则使用术语张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-overhead",
   "metadata": {},
   "source": [
    "如下所示的其余代码计算图像尺寸、第一个密集层的 input_size 值，并将每个像素值从 0 到 255 缩放到从 0.0 到 1.0 的范围。 虽然可以直接使用原始像素值，但最好对输入数据进行归一化，以避免可能导致训练困难的大梯度值。 网络的输出也被归一化。 训练后，可以选择通过将输出张量乘以 255 将所有内容恢复为整数像素值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-memorabilia",
   "metadata": {},
   "source": [
    "所提出的模型基于 MLP 层。 因此，输入预计为一维张量。 因此，x_train 和 x_test 分别被重塑为 [60,000, 28 * 28] 和 [10,000, 28 * 28]。 在 NumPy 中，大小为 -1 意味着让库计算正确的维度。 对于 x_train，这是 60,000。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image dimensions (assumed square) 400\n",
    "image_size = x_train.shape[1]\n",
    "input_size = image_size * image_size\n",
    "# resize and normalize\n",
    "x_train = np.reshape(x_train, [-1, input_size])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = np.reshape(x_test, [-1, input_size])\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-support",
   "metadata": {},
   "source": [
    "准备好数据集后，下面重点介绍使用 Keras 的 Sequential API 构建 MLP 分类器模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-gather",
   "metadata": {},
   "source": [
    "### 使用 MLP 和 Keras 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-emerald",
   "metadata": {},
   "source": [
    "由三个 MLP 层组成。 在 Keras 中，MLP 层被称为密集层，它代表密集连接层。 第一层和第二层 MLP 层本质上是相同的，每个层都有 256 个单元，然后是整流线性单元 (ReLU) 激活和删除。 选择了 256 个单元，因为 128、512 和 1,024 个单元的性能指标较低。 在 128 个单元时，网络收敛速度很快，但测试精度较低。 512 或 1,024 的额外单元数不会显着提高测试精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-suicide",
   "metadata": {},
   "source": [
    "单位数是一个超参数。 它控制网络的容量。 容量是网络可以近似的函数复杂度的度量。 例如，对于多项式，度数就是超参数。 随着度数的增加，函数的容量也会增加。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-release",
   "metadata": {},
   "source": [
    "Keras 的顺序 API。 如果模型需要由一系列层处理的一个输入和一个输出，这就足够了。 为简单起见，我们暂时使用它； 然而，在第 2 章“深度神经网络”中，将引入 Keras 的功能 API 来实现需要更复杂结构（例如多输入和多输出）的高级深度学习模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dedicated-december",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_units' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-89ac83ef611a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# model is a 3-layer MLP with ReLU and dropout after each layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hidden_units' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.33 s (started: 2021-08-10 22:15:47 +08:00)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "# model is a 3-layer MLP with ReLU and dropout after each layer\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_units, input_dim=input_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(hidden_units))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_labels))\n",
    "# this is the output for one-hot vector model.\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daily-elephant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "time: 5.26 ms (started: 2021-08-10 13:33:31 +08:00)\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-looking",
   "metadata": {},
   "source": [
    "由于 Dense 层是线性运算，因此 Dense 层序列只能近似线性函数。 问题在于 MNIST 数字分类本质上是一个非线性过程。 在 Dense 层之间插入 relu 激活将使 MLP 网络能够对非线性映射进行建模。 relu 或 ReLU 是一个简单的非线性函数。 它非常像一个过滤器，它允许正输入不变地通过，同时将其他一切都钳位为零。 在数学上，relu 用以下等式表示，并绘制在图 1.3.5 中："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-korea",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtbok4c0goj312y0osmyq.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-substitute",
   "metadata": {},
   "source": [
    "还有其他非线性函数可以使用，例如 elu、selu、softplus、sigmoid 和 tanh。 然而，relu 是最常用的函数，并且由于其简单性而具有计算效率。 sigmoid 和 tanh 函数用作输出层中的激活函数，将在后面介绍。 表 1.3.1 显示了每个激活函数的方程："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-placement",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtboxl2sn1j318y0rsadh.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-disorder",
   "metadata": {},
   "source": [
    "尽管我们已经完成了 MLP 分类器模型的关键层，但我们还没有解决泛化或模型在训练数据集之外执行的能力的问题。 为了解决这个问题，我们将在下一节介绍正则化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-laptop",
   "metadata": {},
   "source": [
    "### 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-college",
   "metadata": {},
   "source": [
    "神经网络倾向于记住其训练数据，尤其是当它包含足够多的容量时。 在这种情况下，网络在接受测试数据时会发生灾难性的故障。 这是网络无法泛化的经典案例。 为了避免这种趋势，模型使用了正则化层或函数。 常见的正则化层是 Dropout。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-species",
   "metadata": {},
   "source": [
    "Dropout的想法很简单。 给定一个 dropout 率（这里，它被设置为 dropout = 0.45），Dropout 层会随机删除参与下一层的那部分单元。 例如，如果第一层有 256 个单位，应用 dropout = 0.45 后，只有 (1 - 0.45) * 256 个单位 = 来自第 1 层的 140 个单位参与第 2 层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-alcohol",
   "metadata": {},
   "source": [
    "Dropout 层使神经网络对不可预见的输入数据具有鲁棒性，因为网络经过训练可以正确预测，即使缺少某些单元也是如此。 值得注意的是，输出层中没有使用 dropout，它仅在训练期间处于活动状态。 此外，在预测过程中不存在 dropout。\n",
    "\n",
    "除了诸如 l1 或 l2 之类的 dropout 之外，还可以使用正则化器。 在 Keras 中，可以对每层的偏差、权重和激活输出进行正则化。 l1 和 l2 通过添加惩罚函数来支持较小的参数值。 l1 和 l2 都使用参数值的绝对值 (l1) 或平方 (l2) 的总和的一部分来强制执行惩罚。 换句话说，惩罚函数迫使优化器找到较小的参数值。 具有小参数值的神经网络对输入数据中噪声的存在更不敏感。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-skiing",
   "metadata": {},
   "source": [
    "例如，分数=0.001 的 l2 权重正则化器可以实现为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model.add(Dense(hidden_units,\n",
    "    kernel_regularizer=l2(0.001),\n",
    "    input_dim=input_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-schema",
   "metadata": {},
   "source": [
    "如果使用 l1 或 l2 正则化，则不会添加额外的层。 正则化是在 Dense 层内部进行的。 对于所提出的模型，dropout 仍然具有比 l2 更好的性能。\n",
    "\n",
    "我们几乎完成了我们的模型。 下一节重点介绍输出层和损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-still",
   "metadata": {},
   "source": [
    "### 输出激活和损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-proposition",
   "metadata": {},
   "source": [
    "输出层有 10 个单元，后跟一个 softmax 激活层。 10 个单位对应 10 个可能的标签、类别或类别。 softmax 激活可以用数学表示，如下面的等式所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-methodology",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtbs76ve6aj318k04edfx.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-anxiety",
   "metadata": {},
   "source": [
    "该方程适用于所有 N = 10 个输出，xi 表示 i = 0，1 … 9 表示最终预测。 softmax 的想法非常简单。 它通过规范化预测将输出压缩为概率。 这里，每个预测输出是索引是给定输入图像的正确标签的概率。 所有输出的所有概率之和为 1.0。 例如，当 softmax 层生成预测时，它将是一个 10 维的一维张量，可能类似于以下输出："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-protein",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtbs9o88zbj316406oq3r.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-chemistry",
   "metadata": {},
   "source": [
    "预测输出张量表明输入图像将是 7，因为它的索引具有最高的概率。 numpy.argmax() 方法可用于确定具有最高值的元素的索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-concrete",
   "metadata": {},
   "source": [
    "输出激活层还有其他选择，例如线性、sigmoid 或 tanh。 线性激活是一个恒等函数。 它将其输入复制到其输出。 sigmoid 函数更具体地称为逻辑 sigmoid。 如果预测张量的元素将在 0.0 和 1.0 之间独立映射，则将使用此选项。 与 softmax 不同，预测张量的所有元素的总和不限于 1.0。 例如，sigmoid 用作情感预测（从 0.0 到 1.0，0.0 为坏，1.0 为好）或图像生成（0.0 映射到像素级别 0，1.0 映射到像素 255）的最后一层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-gardening",
   "metadata": {},
   "source": [
    "tanh 函数将其输入映射到 -1.0 到 1.0 的范围内。 如果输出可以在正值和负值之间摆动，这一点很重要。 tanh 函数更广泛地用于循环神经网络的内部层，但也被用作输出层激活。 如果在输出激活中使用 tanh 替换 sigmoid，则必须适当缩放所使用的数据。 例如，不是使用$x = \\frac{x}{255}$在 [0.0 1.0] 范围内缩放每个灰度像素，而是使用在[-1.0 1.0] 范围内使用$x = \\frac{x-17.5}{127.5}$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-snake",
   "metadata": {},
   "source": [
    "图 1.3.6 中的下图显示了 sigmoid 和 tanh 函数。 在数学上，sigmoid 可以表示为以下等式："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-harvard",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtbsfn66erj318c03g3yl.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-summer",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtbsg20h6ej30ym0mm0u7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-council",
   "metadata": {},
   "source": [
    "预测的张量与 one-hot ground truth 向量的距离称为损失。 一种类型的损失函数是 mean_squared_error (MSE)，或目标或标签与预测之间差异的平方的平均值。 在当前示例中，我们使用的是 categorical_crossentropy。 它是目标或标签的乘积与每个类别的预测的对数之和的负数。 Keras 中还有其他可用的损失函数，例如 mean_ absolute_error 和 binary_crossentropy。 表 1.3.2 总结了常见的损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-north",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtbsh8chypj30tw0ri40b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-punishment",
   "metadata": {},
   "source": [
    "模型正在学习。 对于按类别分类，在 softmax 激活层之后，categorical_crossentropy 或 mean_squared_error 都是不错的选择。 binary_crossentropy 损失函数通常在 sigmoid 激活层之后使用，而 mean_squared_error 是 tanh 输出的一个选项。 在下一节中，我们将讨论优化算法以最小化我们在这里讨论的损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-mentor",
   "metadata": {},
   "source": [
    "### 优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-willow",
   "metadata": {},
   "source": [
    "通过优化，目标是最小化损失函数。 这个想法是，如果损失减少到可接受的水平，模型已经间接学习了将输入映射到输出的函数。 性能指标用于确定模型是否已了解基础数据分布。 Keras 中的默认指标是损失。 在训练、验证和测试期间，还可以包括其他指标，例如准确性。 准确度是基于真实情况的正确预测的百分比或分数。 在深度学习中，还有许多其他性能指标。 但是，这取决于模型的目标应用程序。 在文献中，报告了在测试数据集上训练模型的性能指标，以与其他深度学习模型进行比较。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-chuck",
   "metadata": {},
   "source": [
    "在 Keras 中，优化器有多种选择。 最常用的优化器是随机梯度下降 (SGD)、自适应矩 (Adam) 和均方根传播 (RMSprop)。 每个优化器都具有可调参数，如学习率、动量和衰减。 Adam 和 RMSprop 是具有自适应学习率的 SGD 变体。 在提议的分类器网络中，使用了 Adam，因为它具有最高的测试准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-vermont",
   "metadata": {},
   "source": [
    "SGD 被认为是最基本的优化器。 它是微积分中梯度下降的更简单版本。 在梯度下降 (GD) 中，跟踪函数曲线下坡会找到最小值，就像在山谷中下坡直到到达底部一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-investment",
   "metadata": {},
   "source": [
    "学习率的选择至关重要。 𝜖 的大值可能找不到最小值，因为搜索只会围绕最小值来回摆动。 一方面，在找到最小值之前，𝜖 的大值可能需要大量的迭代。 在多个最小值的情况下，搜索可能会陷入局部最小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-robert",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtc105ws48j30zq0rcjtl.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-royal",
   "metadata": {},
   "source": [
    "多个最小值的示例如图 1.3.8 所示。 如果由于某种原因搜索从图的左侧开始并且学习率非常小，则 GD 很有可能找到 x = -1.51 作为 y 的最小值。 GD 不会在 x = 1.66 处找到全局最小值。 一个足够有价值的学习率将使 GD 能够克服 x = 0.0 处的小山。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-speaker",
   "metadata": {},
   "source": [
    "在深度学习实践中，通常建议从较大的学习率（例如，0.1 到 0.001）开始，并随着损失接近最小值而逐渐降低该值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-acoustic",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtc11kvlmkj311k0pwdhs.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-series",
   "metadata": {},
   "source": [
    "GD 通常不用于深度神经网络，因为通常会遇到数百万个参数进行训练。 执行完整的 GD 在计算上是低效的。 相反，使用 SGD。 在 SGD 中，选择一小批样本来计算下降的近似值。 参数（例如，权重和偏差）通过以下等式进行调整："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-oriental",
   "metadata": {},
   "source": [
    "公式 1.3.8 计算最后一层参数更新。 那么，我们如何调整前面各层的参数呢？ 在这种情况下，应用微分链式法则将导数传播到较低层并相应地计算梯度。 这种算法在深度学习中被称为反向传播。 反向传播的细节超出了本书的范围。 但是，可以在 http://neuralnetworksanddeeplearning.com 上找到一个很好的在线参考资料。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-apparel",
   "metadata": {},
   "source": [
    "由于优化基于微分，因此损失函数的一个重要标准是它必须是平滑的或可微的。 在引入新的损失函数时，这是一个需要牢记的重要约束。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-arabic",
   "metadata": {},
   "source": [
    "给定训练数据集、损失函数的选择、优化器和正则化器，现在可以通过调用 fit() 函数来训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function for one-hot vector \n",
    "# use of adam optimizer\n",
    "# accuracy is a good metric for classification tasks model.\n",
    "compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# train the network\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-klein",
   "metadata": {},
   "source": [
    "这是 Keras 的另一个有用功能。 通过只提供 x 和 y 数据、要训练的时期数和批量大小，fit() 会完成剩下的工作。 在其他深度学习框架中，这会转化为多项任务，例如以正确的格式准备输入和输出数据、加载、监控等。 虽然所有这些都必须在 for 循环中完成，但在 Keras 中，一切都在一行中完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-utility",
   "metadata": {},
   "source": [
    "在 fit() 函数中，一个 epoch 是对整个训练数据的完整采样。 batch_size 参数是每个训练步骤要处理的输入数量的样本大小。 为了完成一个 epoch，fit() 将处理等于训练数据集大小除以批量大小加 1 以补偿任何小数部分的步数。\n",
    "\n",
    "训练模型后，我们现在可以评估其性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-commerce",
   "metadata": {},
   "source": [
    "### 绩效评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-stopping",
   "metadata": {},
   "source": [
    "至此，MNIST 数字分类器的模型现已完成。 性能评估将是确定所提出的训练模型是否提出令人满意的解决方案的下一个关键步骤。 将模型训练 20 个 epoch 就足以获得可比较的性能指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-investment",
   "metadata": {},
   "source": [
    "下表（表 1.3.3）显示了不同的网络配置和相应的性能指标。 在 Layers 下，显示了第 1 层到第 3 层的单元数。对于每个优化器，使用 tf.keras 中的默认参数。 可以观察到改变正则化器、优化器和每层单元数的影响。 表 1.3.3 中的另一个重要观察结果是，更大的网络不一定会转化为更好的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-arlington",
   "metadata": {},
   "source": [
    "增加该网络的深度在训练和测试数据集的准确性方面没有显示出额外的好处。 另一方面，较少数量的单元，如 128，也会降低测试和训练的准确性。 去除正则化器后获得了 99.93% 的最佳训练准确率，每层使用 256 个单位。 然而，由于网络过度拟合，测试准确度要低得多，为 98.0%。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-westminster",
   "metadata": {},
   "source": [
    "最高的测试准确度是使用 Adam 优化器和 Dropout(0.45) 达到 98.5%。 从技术上讲，鉴于其训练准确率为 99.39%，仍然存在一定程度的过度拟合。 对于 256-512-256、Dropout(0.45) 和 SGD，训练和测试准确度都为 98.2%。 删除正则化层和 ReLU 层会导致其性能最差。 通常，我们会发现 Dropout 层的性能比 l2 更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-patent",
   "metadata": {},
   "source": [
    "下表展示了调优期间典型的深度神经网络性能："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-diabetes",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtc19tmv4hj31da0kywis.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-share",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtc1a4f87pj31do0mkn29.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-presence",
   "metadata": {},
   "source": [
    "该示例表明需要改进网络架构。 在下一节讨论了 MLP 分类器模型总结之后，我们将介绍另一个 MNIST 分类器。 下一个模型基于 CNN，并展示了测试准确性的显着提高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-david",
   "metadata": {},
   "source": [
    "### 模型总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-absorption",
   "metadata": {},
   "source": [
    "使用 Keras 库为我们提供了一种快速机制，可以通过调用以下方法来双重检查模型描述："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bigger-dialogue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "time: 4.16 ms (started: 2021-08-10 22:16:12 +08:00)\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-nutrition",
   "metadata": {},
   "source": [
    "下面的清单 1.3.3 显示了所提议网络的模型摘要。 它总共需要 269,322 个参数。 考虑到我们有一个对 MNIST 数字进行分类的简单任务，这很重要。 MLP 的参数效率不高。 通过关注感知器的输出是如何计算的，可以从图 1.3.4 计算参数的数量。 从输入到 Dense 层：784 × 256 + 256 = 200,960。 从第一个 Dense 层到第二个 Dense 层：256 × 256 + 256 = 65,792。 从第二个 Dense 层到输出层：10 × 256 + 10 = 2,570。 总数为 269,322。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "另一种验证网络的方法是调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "contrary-africa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "time: 50.6 ms (started: 2021-08-10 22:16:15 +08:00)\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-israel",
   "metadata": {},
   "source": [
    "图 1.3.9 显示了该图。 你会发现这和summary()的结果很相似但以图形方式显示了每一层的互连和 I/O。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-smart",
   "metadata": {},
   "source": [
    "总结了我们的模型后，我们对 MLP 的讨论到此结束。 在下一节中，我们将建立一个基于 CNN 的 MNIST 数字分类器模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-reunion",
   "metadata": {},
   "source": [
    "## 卷积神经网络 (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-geometry",
   "metadata": {},
   "source": [
    "我们现在将进入第二个人工神经网络 CNN。 在本节中，我们将解决相同的 MNIST 数字分类问题，但这次使用的是 CNN。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-governor",
   "metadata": {},
   "source": [
    "图 1.4.1 显示了我们将用于 MNIST 数字分类的 CNN 模型，而其实现如清单 1.4.1 所示。 实现 CNN 模型需要对之前的模型进行一些更改。 对于灰度 MNIST 图像，输入张量现在不再具有输入向量，而是具有新的维度（高度、宽度、通道）或 (image_size, image_size, 1) = (28, 28, 1)。 需要调整训练和测试图像的大小以符合此输入形状要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-storm",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtc2eygzjvj31e20dwmyn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-acquisition",
   "metadata": {},
   "source": [
    "实现上图："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-spray",
   "metadata": {},
   "source": [
    "> 清单 1.4.1：cnn-mnist-1.4.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "finished-breed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                5770      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 80,266\n",
      "Trainable params: 80,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "time: 2.51 s (started: 2021-08-11 09:26:15 +08:00)\n"
     ]
    }
   ],
   "source": [
    "''' CNN MNIST digits classification\n",
    "\n",
    "3-layer CNN for MNIST digits classification \n",
    "First 2 layers - Conv2D-ReLU-MaxPool\n",
    "3rd layer - Conv2D-ReLU-Dropout\n",
    "4th layer - Dense(10)\n",
    "Output Activation - softmax\n",
    "Optimizer - Adam\n",
    "\n",
    "99.4% test accuracy in 10epochs\n",
    "\n",
    "https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# load mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# compute the number of labels\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "# convert to one-hot vector\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# input image dimensions\n",
    "image_size = x_train.shape[1]\n",
    "# resize and normalize\n",
    "x_train = np.reshape(x_train,[-1, image_size, image_size, 1])\n",
    "x_test = np.reshape(x_test,[-1, image_size, image_size, 1])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "# image is processed as is (square grayscale)\n",
    "input_shape = (image_size, image_size, 1)\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "filters = 64\n",
    "dropout = 0.2\n",
    "\n",
    "# model is a stack of CNN-ReLU-MaxPooling\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=filters,\n",
    "                 kernel_size=kernel_size,\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size))\n",
    "model.add(Conv2D(filters=filters,\n",
    "                 kernel_size=kernel_size,\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size))\n",
    "model.add(Conv2D(filters=filters,\n",
    "                 kernel_size=kernel_size,\n",
    "                 activation='relu'))\n",
    "model.add(Flatten())\n",
    "# dropout added as regularizer\n",
    "model.add(Dropout(dropout))\n",
    "# output layer is 10-dim one-hot vector\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aging-general",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 18s 16ms/step - loss: 0.2590 - accuracy: 0.9208\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0661 - accuracy: 0.9796\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0465 - accuracy: 0.9853\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0382 - accuracy: 0.9883\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0326 - accuracy: 0.9900\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0285 - accuracy: 0.9911\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0242 - accuracy: 0.9925\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0212 - accuracy: 0.9931\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0183 - accuracy: 0.9939\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0154 - accuracy: 0.9949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b0b2ff218e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 15s (started: 2021-08-11 09:26:21 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# enable this if pydot can be installed\n",
    "# pip install pydot\n",
    "#plot_model(model, to_file='cnn-mnist.png', show_shapes=True)\n",
    "\n",
    "# loss function for one-hot vector\n",
    "# use of adam optimizer\n",
    "# accuracy is good metric for classification tasks\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train the network\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "correct-material",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 99.3%\n",
      "time: 1.58 s (started: 2021-08-11 09:27:47 +08:00)\n"
     ]
    }
   ],
   "source": [
    "_, acc = model.evaluate(x_test,\n",
    "                        y_test,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-month",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
