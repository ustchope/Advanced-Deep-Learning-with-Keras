{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªåŠ¨è®¡ç®—cellçš„è®¡ç®—æ—¶é—´\n",
    "%load_ext autotime\n",
    "\n",
    "%config InlineBackend.figure_format='svg' #çŸ¢é‡å›¾è®¾ç½®ï¼Œè®©ç»˜å›¾æ›´æ¸…æ™°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# å¢žåŠ æ›´æ–°\n",
    "git add *.ipynb\n",
    "\n",
    "git remote -v\n",
    "\n",
    "git commit -m 'æ›´æ–° ch4 #2 change Aug 12, 2021'\n",
    "\n",
    "git push origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "positive-banner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.68 s (started: 2021-08-12 16:45:11 +08:00)\n"
     ]
    }
   ],
   "source": [
    "#è®¾ç½®ä½¿ç”¨çš„gpu\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "   \n",
    "    gpu0 = gpus[0] #å¦‚æžœæœ‰å¤šä¸ªGPUï¼Œä»…ä½¿ç”¨ç¬¬0ä¸ªGPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #è®¾ç½®GPUæ˜¾å­˜ç”¨é‡æŒ‰éœ€ä½¿ç”¨\n",
    "    # æˆ–è€…ä¹Ÿå¯ä»¥è®¾ç½®GPUæ˜¾å­˜ä¸ºå›ºå®šä½¿ç”¨é‡(ä¾‹å¦‚ï¼š4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-louis",
   "metadata": {},
   "source": [
    "# ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-cross",
   "metadata": {},
   "source": [
    "åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN)ã€‚ GAN å±žäºŽç”Ÿæˆæ¨¡åž‹å®¶æ—ã€‚ ç„¶è€Œï¼Œä¸Žè‡ªåŠ¨ç¼–ç å™¨ä¸åŒï¼Œç”Ÿæˆæ¨¡åž‹èƒ½å¤Ÿåœ¨ç»™å®šä»»æ„ç¼–ç çš„æƒ…å†µä¸‹åˆ›å»ºæ–°çš„ã€æœ‰æ„ä¹‰çš„è¾“å‡ºã€‚\n",
    "\n",
    "åœ¨æœ¬ç« ä¸­ï¼Œå°†è®¨è®º GAN çš„å·¥ä½œåŽŸç†ã€‚ æˆ‘ä»¬è¿˜å°†å›žé¡¾ä½¿ç”¨ tf.keras çš„å‡ ä¸ªæ—©æœŸ GAN çš„å®žçŽ°ï¼Œè€Œåœ¨æœ¬ç« çš„åŽé¢ï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºå®žçŽ°ç¨³å®šè®­ç»ƒæ‰€éœ€çš„æŠ€æœ¯ã€‚ æœ¬ç« çš„èŒƒå›´æ¶µç›–äº† GAN å®žçŽ°çš„ä¸¤ä¸ªæµè¡Œç¤ºä¾‹ï¼Œæ·±åº¦å·ç§¯ GAN (DCGAN) å’Œæ¡ä»¶ GAN (CGAN)ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-working",
   "metadata": {},
   "source": [
    "æ€»ä¹‹ï¼Œæœ¬ç« çš„ç›®æ ‡æ˜¯ï¼š\n",
    "* ä»‹ç» GAN çš„åŽŸç†\n",
    "* å±•ç¤º GAN çš„æ—©æœŸå·¥ä½œå®žçŽ°ä¹‹ä¸€ï¼Œç§°ä¸ºDCGAN\n",
    "* ä¸€ç§ç§°ä¸º CGAN çš„æ”¹è¿› DCGANï¼Œå®ƒä½¿ç”¨æ¡ä»¶\n",
    "* åœ¨ tf.keras ä¸­å®žçŽ° DCGAN å’Œ CGAN\n",
    "\n",
    "è®©æˆ‘ä»¬ä»Ž GAN çš„æ¦‚è¿°å¼€å§‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-killing",
   "metadata": {},
   "source": [
    "## GAN æ¦‚è¿°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-emergency",
   "metadata": {},
   "source": [
    "åœ¨æˆ‘ä»¬è¿›å…¥ GAN çš„æ›´é«˜çº§æ¦‚å¿µä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆå›žé¡¾ä¸€ä¸‹ GAN å¹¶ä»‹ç»å®ƒä»¬èƒŒåŽçš„åŸºæœ¬æ¦‚å¿µã€‚ GAN éžå¸¸å¼ºå¤§ï¼› è¿™ä¸ªç®€å•çš„é™ˆè¿°å¾—åˆ°äº†ä»¥ä¸‹äº‹å®žçš„è¯æ˜Žï¼šå®ƒä»¬å¯ä»¥é€šè¿‡æ‰§è¡Œæ½œåœ¨ç©ºé—´æ’å€¼æ¥ç”Ÿæˆä¸æ˜¯çœŸäººçš„æ–°äººè„¸ã€‚\n",
    "\n",
    "åœ¨è¿™äº› YouTube è§†é¢‘ä¸­å¯ä»¥çœ‹åˆ° GAN çš„é«˜çº§åŠŸèƒ½ï¼š\n",
    "* æ¸è¿›å¼ GAN [4]ï¼šhttps://youtu.be/G06dEcZ-QTg\n",
    "* StyleGAN v1 [5]ï¼šhttps://youtu.be/kSLJriaOumA\n",
    "* StyleGAN v2 [6]ï¼šhttps://youtu.be/c-NJtV9Jvp0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-cosmetic",
   "metadata": {},
   "source": [
    "å±•ç¤ºå¦‚ä½•åˆ©ç”¨ GAN ç”Ÿæˆé€¼çœŸäººè„¸çš„è§†é¢‘å±•ç¤ºäº†å®ƒä»¬çš„å¼ºå¤§åŠŸèƒ½ã€‚è¿™ä¸ªä¸»é¢˜æ¯”æˆ‘ä»¬ä¹‹å‰åœ¨æœ¬ä¹¦ä¸­çœ‹åˆ°çš„ä»»ä½•å†…å®¹éƒ½è¦å…ˆè¿›å¾—å¤šã€‚ä¾‹å¦‚ï¼Œä¸Šé¢çš„è§†é¢‘æ¼”ç¤ºäº†è‡ªåŠ¨ç¼–ç å™¨æ— æ³•è½»æ¾å®Œæˆçš„äº‹æƒ…ï¼Œæˆ‘ä»¬åœ¨ç¬¬ 3 ç« ï¼Œè‡ªåŠ¨ç¼–ç å™¨ä¸­ä»‹ç»äº†è¿™äº›å†…å®¹ã€‚\n",
    "\n",
    "GAN èƒ½å¤Ÿé€šè¿‡è®­ç»ƒä¸¤ä¸ªè¢«ç§°ä¸ºç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨ï¼ˆæœ‰æ—¶ç§°ä¸ºè¯„è®ºå®¶ï¼‰çš„ç«žäº‰ï¼ˆå’Œåˆä½œï¼‰ç½‘ç»œæ¥å­¦ä¹ å¦‚ä½•å¯¹è¾“å…¥åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚ç”Ÿæˆå™¨çš„ä½œç”¨æ˜¯ä¸æ–­å¼„æ¸…æ¥šå¦‚ä½•ç”Ÿæˆå¯ä»¥æ¬ºéª—é‰´åˆ«å™¨çš„è™šå‡æ•°æ®æˆ–ä¿¡å·ï¼ˆåŒ…æ‹¬éŸ³é¢‘å’Œå›¾åƒï¼‰ã€‚åŒæ—¶ï¼Œé‰´åˆ«å™¨è¢«è®­ç»ƒæ¥åŒºåˆ†å‡ä¿¡å·å’ŒçœŸå®žä¿¡å·ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œåˆ¤åˆ«å™¨å°†ä¸å†èƒ½å¤Ÿçœ‹åˆ°åˆæˆç”Ÿæˆçš„æ•°æ®å’ŒçœŸå®žæ•°æ®ä¹‹é—´çš„å·®å¼‚ã€‚ä»Žé‚£é‡Œï¼Œå¯ä»¥ä¸¢å¼ƒé‰´åˆ«å™¨ï¼Œç„¶åŽå¯ä»¥ä½¿ç”¨ç”Ÿæˆå™¨æ¥åˆ›å»ºä»¥å‰ä»Žæœªè§‚å¯Ÿåˆ°çš„æ–°çš„çœŸå®žæ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-greenhouse",
   "metadata": {},
   "source": [
    "GAN çš„åŸºæœ¬æ¦‚å¿µå¾ˆç®€å•ã€‚ ç„¶è€Œï¼Œæˆ‘ä»¬ä¼šå‘çŽ°çš„ä¸€ä»¶äº‹æ˜¯ï¼Œæœ€å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜æ˜¯æˆ‘ä»¬å¦‚ä½•å®žçŽ°ç”Ÿæˆå™¨-é‰´åˆ«å™¨ç½‘ç»œçš„ç¨³å®šè®­ç»ƒï¼Ÿ ç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨ä¹‹é—´å¿…é¡»å­˜åœ¨è‰¯æ€§ç«žäº‰ï¼Œä»¥ä¾¿ä¸¤ä¸ªç½‘ç»œèƒ½å¤ŸåŒæ—¶å­¦ä¹ ã€‚ ç”±äºŽæŸå¤±å‡½æ•°æ˜¯æ ¹æ®é‰´åˆ«å™¨çš„è¾“å‡ºè®¡ç®—çš„ï¼Œå› æ­¤å…¶å‚æ•°æ›´æ–°å¾ˆå¿«ã€‚ å½“åˆ¤åˆ«å™¨æ”¶æ•›å¾—æ›´å¿«æ—¶ï¼Œç”Ÿæˆå™¨ä¸å†ä¸ºå…¶å‚æ•°æŽ¥æ”¶è¶³å¤Ÿçš„æ¢¯åº¦æ›´æ–°å¹¶ä¸”æ— æ³•æ”¶æ•›ã€‚ é™¤äº†éš¾ä»¥è®­ç»ƒä¹‹å¤–ï¼ŒGAN è¿˜å¯èƒ½é­å—éƒ¨åˆ†æˆ–å…¨éƒ¨æ¨¡æ€å´©æºƒï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”Ÿæˆå™¨ä¸ºä¸åŒçš„æ½œåœ¨ç¼–ç äº§ç”Ÿå‡ ä¹Žç›¸ä¼¼çš„è¾“å‡ºã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-supply",
   "metadata": {},
   "source": [
    "### GANsåŽŸç†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-premiere",
   "metadata": {},
   "source": [
    "å¦‚å›¾ 4.1.1 æ‰€ç¤ºï¼ŒGAN ç±»ä¼¼äºŽé€ å‡è€…ï¼ˆç”Ÿæˆå™¨ï¼‰-è­¦å¯Ÿï¼ˆé‰´åˆ«å™¨ï¼‰çš„åœºæ™¯ã€‚ åœ¨å­¦é™¢é‡Œï¼Œè­¦å¯Ÿè¢«æ•™å¯¼å¦‚ä½•ç¡®å®šä¸€ç¾Žå…ƒé’žç¥¨æ˜¯çœŸè¿˜æ˜¯å‡ã€‚ æ¥è‡ªé“¶è¡Œçš„çœŸç¾Žå…ƒé’žç¥¨æ ·æœ¬å’Œæ¥è‡ªä¼ªé€ è€…çš„å‡å¸æ ·æœ¬è¢«ç”¨æ¥è®­ç»ƒè­¦å¯Ÿã€‚ ç„¶è€Œï¼Œæœ‰æ—¶ï¼Œé€ å‡è€…ä¼šè¯•å›¾å‡è£…ä»–å°çš„æ˜¯çœŸé’žç¥¨ã€‚ æœ€åˆï¼Œè­¦å¯Ÿä¸ä¼šä¸Šå½“ï¼Œä¼šå‘Šè¯‰é€ å‡è€…ä¸ºä»€ä¹ˆé’±æ˜¯å‡çš„ã€‚ è€ƒè™‘åˆ°è¿™ç§åé¦ˆï¼Œé€ å‡è€…å†æ¬¡ç£¨ç»ƒè‡ªå·±çš„æŠ€èƒ½ï¼Œå¹¶è¯•å›¾åˆ¶é€ æ–°çš„å‡ç¾Žå…ƒé’žç¥¨ã€‚ æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œè­¦æ–¹å°†èƒ½å¤Ÿå‘çŽ°è¿™äº›é’±æ˜¯å‡çš„ï¼Œå¹¶è¯æ˜Žç¾Žå…ƒé’žç¥¨æ˜¯å‡çš„ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-broadcast",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte4futnx5j618q0u079802.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-inquiry",
   "metadata": {},
   "source": [
    "è¿™ä¸ªè¿‡ç¨‹ä¼šæ— é™æœŸåœ°æŒç»­ä¸‹åŽ»ï¼Œä½†æœ€ç»ˆé€ å‡è€…å·²ç»æŽŒæ¡äº†åˆ¶é€ å‡å¸çš„èƒ½åŠ›ï¼Œä»¥è‡³äºŽå‡å¸ä¸ŽçœŸé’±æ— æ³•åŒºåˆ†â€”â€”å³ä½¿æ˜¯æœ€ä¸“ä¸šçš„è­¦å¯Ÿä¹Ÿæ˜¯å¦‚æ­¤ã€‚ ç„¶åŽï¼Œé€ å‡è€…å¯ä»¥æ— é™å°åˆ·ç¾Žå…ƒé’žç¥¨è€Œä¸ä¼šè¢«è­¦å¯ŸæŠ“ä½ï¼Œå› ä¸ºå®ƒä»¬ä¸å†è¢«è¯†åˆ«ä¸ºä¼ªé€ å“ã€‚\n",
    "\n",
    "å¦‚å›¾ 4.1.2 æ‰€ç¤ºï¼Œä¸€ä¸ª GAN ç”±ä¸¤ä¸ªç½‘ç»œç»„æˆï¼Œä¸€ä¸ªç”Ÿæˆå™¨å’Œä¸€ä¸ªåˆ¤åˆ«å™¨ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-perspective",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte4j3gtpdj61ag0ooju302.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-bermuda",
   "metadata": {},
   "source": [
    "å‘ç”Ÿå™¨çš„è¾“å…¥æ˜¯å™ªå£°ï¼Œè¾“å‡ºæ˜¯åˆæˆæ•°æ®ã€‚åŒæ—¶ï¼Œé‰´åˆ«å™¨çš„è¾“å…¥å°†æ˜¯çœŸå®žæ•°æ®æˆ–åˆæˆæ•°æ®ã€‚çœŸå®žæ•°æ®æ¥è‡ªçœŸå®žé‡‡æ ·æ•°æ®ï¼Œè€Œå‡æ•°æ®æ¥è‡ªç”Ÿæˆå™¨ã€‚æ‰€æœ‰æœ‰æ•ˆæ•°æ®éƒ½æ ‡è®°ä¸º 1.0ï¼ˆå³ 100% çš„çœŸå®žæ¦‚çŽ‡ï¼‰ï¼Œè€Œæ‰€æœ‰åˆæˆæ•°æ®éƒ½æ ‡è®°ä¸º 0.0ï¼ˆå³çœŸå®žæ¦‚çŽ‡ä¸º 0%ï¼‰ã€‚ç”±äºŽæ ‡è®°è¿‡ç¨‹æ˜¯è‡ªåŠ¨åŒ–çš„ï¼ŒGAN ä»ç„¶è¢«è®¤ä¸ºæ˜¯æ·±åº¦å­¦ä¹ ä¸­æ— ç›‘ç£å­¦ä¹ æ–¹æ³•çš„ä¸€éƒ¨åˆ†ã€‚\n",
    "\n",
    "é‰´åˆ«å™¨çš„ç›®æ ‡æ˜¯ä»Žè¿™ä¸ªæä¾›çš„æ•°æ®é›†ä¸­å­¦ä¹ å¦‚ä½•åŒºåˆ†çœŸå®žæ•°æ®å’Œè™šå‡æ•°æ®ã€‚åœ¨è¿™éƒ¨åˆ† GAN è®­ç»ƒæœŸé—´ï¼Œåªä¼šæ›´æ–°é‰´åˆ«å™¨å‚æ•°ã€‚ä¸Žå…¸åž‹çš„äºŒå…ƒåˆ†ç±»å™¨ä¸€æ ·ï¼Œé‰´åˆ«å™¨è¢«è®­ç»ƒä»¥åœ¨ 0.0 åˆ° 1.0 çš„èŒƒå›´å†…é¢„æµ‹ç»™å®šè¾“å…¥æ•°æ®ä¸ŽçœŸå®žæ•°æ®çš„æŽ¥è¿‘ç¨‹åº¦ã€‚ç„¶è€Œï¼Œè¿™åªæ˜¯æ•…äº‹çš„ä¸€åŠã€‚\n",
    "\n",
    "æ¯éš”ä¸€æ®µæ—¶é—´ï¼Œç”Ÿæˆå™¨ä¼šå‡è£…å…¶è¾“å‡ºæ˜¯çœŸå®žæ•°æ®ï¼Œå¹¶è¦æ±‚ GAN å°†å…¶æ ‡è®°ä¸º 1.0ã€‚å½“å‡æ•°æ®è¢«å‘ˆçŽ°ç»™é‰´åˆ«å™¨æ—¶ï¼Œè‡ªç„¶ä¼šè¢«å½’ç±»ä¸ºæ ‡ç­¾æŽ¥è¿‘ 0.0 çš„å‡æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-stations",
   "metadata": {},
   "source": [
    "ä¼˜åŒ–å™¨æ ¹æ®æ˜¾ç¤ºçš„æ ‡ç­¾ï¼ˆå³ 1.0ï¼‰è®¡ç®—ç”Ÿæˆå™¨å‚æ•°æ›´æ–°ã€‚åœ¨å¯¹è¿™äº›æ–°æ•°æ®è¿›è¡Œè®­ç»ƒæ—¶ï¼Œå®ƒè¿˜è€ƒè™‘äº†è‡ªå·±çš„é¢„æµ‹ã€‚æ¢å¥è¯è¯´ï¼Œåˆ¤åˆ«å™¨å¯¹å…¶é¢„æµ‹æœ‰ä¸€äº›æ€€ç–‘ï¼Œå› æ­¤ï¼ŒGAN ä¼šè€ƒè™‘åˆ°è¿™ä¸€ç‚¹ã€‚è¿™ä¸€æ¬¡ï¼ŒGAN ä¼šè®©æ¢¯åº¦ä»Žé‰´åˆ«å™¨çš„æœ€åŽä¸€å±‚å‘ä¸‹ä¼ æ’­åˆ°ç”Ÿæˆå™¨çš„ç¬¬ä¸€å±‚ã€‚ç„¶è€Œï¼Œåœ¨å¤§å¤šæ•°å®žè·µä¸­ï¼Œåœ¨è¿™ä¸ªè®­ç»ƒé˜¶æ®µï¼Œé‰´åˆ«å™¨å‚æ•°ä¼šè¢«æš‚æ—¶å†»ç»“ã€‚ç”Ÿæˆå™¨å°†ä½¿ç”¨æ¢¯åº¦æ›´æ–°å…¶å‚æ•°å¹¶æé«˜å…¶åˆæˆå‡æ•°æ®çš„èƒ½åŠ›ã€‚\n",
    "\n",
    "æ€»çš„æ¥è¯´ï¼Œæ•´ä¸ªè¿‡ç¨‹ç±»ä¼¼äºŽä¸¤ä¸ªç½‘ç»œç›¸äº’ç«žäº‰ï¼ŒåŒæ—¶ä»ç„¶åˆä½œã€‚å½“ GAN è®­ç»ƒæ”¶æ•›æ—¶ï¼Œæœ€ç»ˆç»“æžœæ˜¯ä¸€ä¸ªå¯ä»¥åˆæˆçœ‹èµ·æ¥çœŸå®žçš„æ•°æ®çš„ç”Ÿæˆå™¨ã€‚é‰´åˆ«å™¨è®¤ä¸ºè¿™ä¸ªåˆæˆæ•°æ®æ˜¯çœŸå®žçš„æˆ–è€…æ ‡ç­¾æŽ¥è¿‘ 1.0ï¼Œè¿™æ„å‘³ç€é‰´åˆ«å™¨å¯ä»¥è¢«ä¸¢å¼ƒã€‚å‘ç”Ÿå™¨éƒ¨åˆ†å°†æœ‰åŠ©äºŽä»Žä»»æ„å™ªå£°è¾“å…¥äº§ç”Ÿæœ‰æ„ä¹‰çš„è¾“å‡ºã€‚\n",
    "\n",
    "ä¸‹é¢çš„å›¾ 4.1.3 æ¦‚è¿°äº†è¯¥è¿‡ç¨‹ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-equilibrium",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte4nf7lqvj61as0m0gon02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-future",
   "metadata": {},
   "source": [
    "å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¯ä»¥é€šè¿‡æœ€å°åŒ–ä»¥ä¸‹ç­‰å¼ä¸­çš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒåˆ¤åˆ«å™¨ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-kernel",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte4rsbhgzj618004cwet02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-watershed",
   "metadata": {},
   "source": [
    "è¯¥æ–¹ç¨‹åªæ˜¯æ ‡å‡†çš„äºŒå…ƒäº¤å‰ç†µæˆæœ¬å‡½æ•°ã€‚ æŸå¤±æ˜¯æ­£ç¡®è¯†åˆ«çœŸå®žæ•°æ®çš„æœŸæœ›å€¼ ð’Ÿ(ð’™) å’Œ 1.0 å‡åŽ»æ­£ç¡®è¯†åˆ«åˆæˆæ•°æ®çš„æœŸæœ›å€¼ 1 âˆ’ ð’Ÿ(ð’¢(ð’›)) çš„è´Ÿå’Œã€‚ æ—¥å¿—ä¸ä¼šæ›´æ”¹å±€éƒ¨æœ€å°å€¼çš„ä½ç½®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-orbit",
   "metadata": {},
   "source": [
    "è®­ç»ƒæœŸé—´å‘åˆ¤åˆ«å™¨æä¾›ä¸¤ä¸ªå°æ‰¹é‡æ•°æ®ï¼š\n",
    "1. xï¼Œæ¥è‡ªé‡‡æ ·æ•°æ®çš„çœŸå®žæ•°æ®ï¼ˆæ¢å¥è¯è¯´ï¼Œ$ð’™ \\sim p_{data}$ï¼‰ï¼Œæ ‡ç­¾ä¸º 1.0\n",
    "2. ð’™â€² = ð’¢(ð’›)ï¼Œæ¥è‡ªç”Ÿæˆå™¨çš„å‡æ•°æ®ï¼Œæ ‡ç­¾ä¸º 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-musical",
   "metadata": {},
   "source": [
    "ä¸ºäº†æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Œé‰´åˆ«å™¨å‚æ•° ðœ½(ð·) å°†é€šè¿‡åå‘ä¼ æ’­é€šè¿‡æ­£ç¡®è¯†åˆ«çœŸå®žæ•°æ® ð’Ÿ(ð’™) å’Œåˆæˆæ•°æ® 1 âˆ’ ð’Ÿ(ð’¢(ð’›)) è¿›è¡Œæ›´æ–°ã€‚ æ­£ç¡®è¯†åˆ«çœŸå®žæ•°æ®ç›¸å½“äºŽ ð’Ÿ(ð’™) â†’ 1.0ï¼Œè€Œæ­£ç¡®è¯†åˆ«è™šå‡æ•°æ®ç›¸å½“äºŽ ð’Ÿ(ð’¢(ð’›)) â†’ 0.0 æˆ– (1 âˆ’ ð’Ÿ(ð’¢(ð’›))) â†’ 1.0ã€‚ åœ¨è¿™ä¸ªç­‰å¼ä¸­ï¼Œz æ˜¯ç”Ÿæˆå™¨ç”¨æ¥åˆæˆæ–°ä¿¡å·çš„ä»»æ„ç¼–ç æˆ–å™ªå£°å‘é‡ã€‚ ä¸¤è€…éƒ½æœ‰åŠ©äºŽæœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-adult",
   "metadata": {},
   "source": [
    "ä¸ºäº†è®­ç»ƒç”Ÿæˆå™¨ï¼ŒGAN å°†é‰´åˆ«å™¨å’Œç”Ÿæˆå™¨æŸå¤±çš„æ€»å’Œè§†ä¸ºé›¶å’Œæ¸¸æˆã€‚ ç”Ÿæˆå™¨æŸå¤±å‡½æ•°åªæ˜¯åˆ¤åˆ«å™¨æŸå¤±å‡½æ•°çš„è´Ÿå€¼ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-caution",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte5dpon0rj617u0420sv02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-stomach",
   "metadata": {},
   "source": [
    "ç„¶åŽå¯ä»¥æ›´æ°å½“åœ°å°†å…¶é‡å†™ä¸ºå€¼å‡½æ•°ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-thinking",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte5h7dz59j617o03gt8u02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-progressive",
   "metadata": {},
   "source": [
    "ä»Žç”Ÿæˆå™¨çš„è§’åº¦æ¥çœ‹ï¼Œå…¬å¼ 4.1.3 åº”è¯¥æœ€å°åŒ–ã€‚ ä»Žåˆ¤åˆ«å™¨çš„è§’åº¦æ¥çœ‹ï¼Œåº”è¯¥æœ€å¤§åŒ–ä»·å€¼å‡½æ•°ã€‚ å› æ­¤ï¼Œç”Ÿæˆå™¨è®­ç»ƒå‡†åˆ™å¯ä»¥å†™æˆä¸€ä¸ªæžå¤§æžå°é—®é¢˜ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-gasoline",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte5i3hssoj617i04074h02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-carbon",
   "metadata": {},
   "source": [
    "æœ‰æ—¶ï¼Œæˆ‘ä»¬ä¼šå‡è£…åˆæˆæ•°æ®æ˜¯çœŸå®žçš„ï¼Œæ ‡ç­¾ä¸º 1.0ï¼Œä»¥æ­¤æ¥æ¬ºéª—é‰´åˆ«å™¨ã€‚ é€šè¿‡æœ€å¤§åŒ– ðœ½(ð·)ï¼Œä¼˜åŒ–å™¨å‘é‰´åˆ«å™¨å‚æ•°å‘é€æ¢¯åº¦æ›´æ–°ï¼Œä»¥å°†è¿™äº›åˆæˆæ•°æ®è§†ä¸ºçœŸå®žæ•°æ®ã€‚ åŒæ—¶ï¼Œé€šè¿‡å…³äºŽ ðœ½(ðº) çš„æœ€å°åŒ–ï¼Œä¼˜åŒ–å™¨å°†è®­ç»ƒç”Ÿæˆå™¨çš„å‚æ•°å¦‚ä½•æ¬ºéª—é‰´åˆ«å™¨ã€‚ ç„¶è€Œï¼Œåœ¨å®žè·µä¸­ï¼Œé‰´åˆ«å™¨å¯¹å…¶å°†åˆæˆæ•°æ®åˆ†ç±»ä¸ºå‡æ•°æ®çš„é¢„æµ‹å……æ»¡ä¿¡å¿ƒï¼Œå¹¶ä¸”ä¸ä¼šæ›´æ–° GAN å‚æ•°ã€‚ æ­¤å¤–ï¼Œæ¢¯åº¦æ›´æ–°å¾ˆå°ï¼Œå¹¶ä¸”éšç€å®ƒä»¬ä¼ æ’­åˆ°ç”Ÿæˆå™¨å±‚è€Œæ˜¾ç€å‡å°‘ã€‚ ç»“æžœï¼Œç”Ÿæˆå™¨æ— æ³•æ”¶æ•›ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-ordering",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte5kt2i6kj61c60kkgnw02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-matrix",
   "metadata": {},
   "source": [
    "è§£å†³æ–¹æ¡ˆæ˜¯å°†ç”Ÿæˆå™¨çš„æŸå¤±å‡½æ•°é‡æ–°è¡¨è¿°ä¸ºä»¥ä¸‹å½¢å¼ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-silver",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte5lllabvj619203u3yn02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-namibia",
   "metadata": {},
   "source": [
    "æŸå¤±å‡½æ•°åªæ˜¯é€šè¿‡è®­ç»ƒç”Ÿæˆå™¨æ¥æœ€å¤§åŒ–é‰´åˆ«å™¨ç›¸ä¿¡åˆæˆæ•°æ®æ˜¯çœŸå®žçš„æœºä¼šã€‚æ–°å…¬å¼ä¸å†æ˜¯é›¶å’Œï¼Œè€Œæ˜¯çº¯ç²¹çš„å¯å‘å¼é©±åŠ¨ã€‚å›¾ 4.1.4 æ˜¾ç¤ºäº†è®­ç»ƒæœŸé—´çš„ç”Ÿæˆå™¨ã€‚åœ¨è¿™ä¸ªå›¾ä¸­ï¼Œç”Ÿæˆå™¨å‚æ•°åªæœ‰åœ¨æ•´ä¸ªå¯¹æŠ—ç½‘ç»œéƒ½è¢«è®­ç»ƒæ—¶æ‰ä¼šæ›´æ–°ã€‚è¿™æ˜¯å› ä¸ºæ¢¯åº¦ä»Žé‰´åˆ«å™¨ä¼ é€’åˆ°ç”Ÿæˆå™¨ã€‚ç„¶è€Œï¼Œåœ¨å®žè·µä¸­ï¼Œé‰´åˆ«å™¨æƒé‡åªæ˜¯åœ¨å¯¹æŠ—è®­ç»ƒæœŸé—´æš‚æ—¶å†»ç»“ã€‚\n",
    "\n",
    "åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨éƒ½å¯ä»¥ä½¿ç”¨åˆé€‚çš„ç¥žç»ç½‘ç»œæž¶æž„æ¥å®žçŽ°ã€‚å¦‚æžœæ•°æ®æˆ–ä¿¡å·æ˜¯å›¾åƒï¼Œç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨ç½‘ç»œéƒ½å°†ä½¿ç”¨ CNNã€‚å¯¹äºŽéŸ³é¢‘ç­‰ä¸€ç»´åºåˆ—ï¼Œä¸¤ä¸ªç½‘ç»œé€šå¸¸éƒ½æ˜¯å¾ªçŽ¯çš„ï¼ˆRNNã€LSTM æˆ– GRUï¼‰ã€‚\n",
    "\n",
    "åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬äº†è§£åˆ° GAN èƒŒåŽçš„åŽŸç†å¾ˆç®€å•ã€‚æˆ‘ä»¬è¿˜å­¦ä¹ äº†å¦‚ä½•é€šè¿‡ç†Ÿæ‚‰çš„ç½‘ç»œå±‚å®žçŽ° GANã€‚ GAN ä¸Žå…¶ä»–ç½‘ç»œçš„ä¸åŒä¹‹å¤„åœ¨äºŽå®ƒä»¬å¾ˆéš¾è®­ç»ƒã€‚åƒå±‚ä¸­çš„å¾®å°å˜åŒ–è¿™æ ·ç®€å•çš„äº‹æƒ…å°±ä¼šå¯¼è‡´ç½‘ç»œè®­ç»ƒä¸ç¨³å®šã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶ä½¿ç”¨æ·±åº¦ CNN çš„ GAN æ—©æœŸæˆåŠŸå®žçŽ°ä¹‹ä¸€ã€‚å®ƒè¢«ç§°ä¸º DCGAN [3]ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-throw",
   "metadata": {},
   "source": [
    "## åœ¨ Keras ä¸­å®žçŽ° DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-austria",
   "metadata": {},
   "source": [
    "å›¾ 4.2.1 æ˜¾ç¤ºäº†ç”¨äºŽç”Ÿæˆå‡ MNIST å›¾åƒçš„ DCGANï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-tribute",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gte5po6v6yj61840u0q5y02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-inflation",
   "metadata": {},
   "source": [
    "DCGAN å®žçŽ°äº†ä»¥ä¸‹è®¾è®¡åŽŸåˆ™ï¼š\n",
    "* ä½¿ç”¨strides > 1 å’Œå·ç§¯è€Œä¸æ˜¯MaxPooling2D æˆ–UpSampling2Dã€‚ éšç€æ­¥å¹… > 1ï¼ŒCNN å­¦ä¹ å¦‚ä½•è°ƒæ•´ç‰¹å¾å›¾çš„å¤§å°ã€‚\n",
    "* é¿å…ä½¿ç”¨å¯†é›†å±‚ã€‚ åœ¨æ‰€æœ‰å±‚ä¸­ä½¿ç”¨ CNNã€‚ å¯†é›†å±‚ä»…ç”¨ä½œç”Ÿæˆå™¨çš„ç¬¬ä¸€å±‚ä»¥æŽ¥å— z å‘é‡ã€‚ Dense å±‚çš„è¾“å‡ºè¢«è°ƒæ•´å¤§å°å¹¶æˆä¸ºåŽç»­ CNN å±‚çš„è¾“å…¥ã€‚\n",
    "* ä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ– (BN) é€šè¿‡å°†æ¯ä¸€å±‚çš„è¾“å…¥å½’ä¸€åŒ–ä¸ºé›¶å‡å€¼å’Œå•ä½æ–¹å·®æ¥ç¨³å®šå­¦ä¹ ã€‚ ç”Ÿæˆå™¨è¾“å‡ºå±‚å’Œé‰´åˆ«å™¨è¾“å…¥å±‚æ²¡æœ‰BNã€‚ åœ¨æ­¤å¤„ä»‹ç»çš„å®žçŽ°ç¤ºä¾‹ä¸­ï¼Œé‰´åˆ«å™¨ä¸­æ²¡æœ‰ä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–ã€‚\n",
    "* æ•´æµçº¿æ€§å•å…ƒ (ReLU) ç”¨äºŽç”Ÿæˆå™¨çš„æ‰€æœ‰å±‚ï¼Œè¾“å‡ºå±‚é™¤å¤–ï¼Œå…¶ä¸­ä½¿ç”¨äº† tanh æ¿€æ´»ã€‚ åœ¨æ­¤å¤„ä»‹ç»çš„å®žçŽ°ç¤ºä¾‹ä¸­ï¼Œåœ¨ç”Ÿæˆå™¨çš„è¾“å‡ºä¸­ä½¿ç”¨ sigmoid ä»£æ›¿ tanhï¼Œå› ä¸ºå®ƒé€šå¸¸ä¼šå¯¼è‡´å¯¹ MNIST æ•°å­—çš„æ›´ç¨³å®šçš„è®­ç»ƒã€‚\n",
    "* åœ¨é‰´åˆ«å™¨çš„æ‰€æœ‰å±‚ä¸­ä½¿ç”¨Leaky ReLUã€‚ ä¸Ž ReLU ä¸åŒçš„æ˜¯ï¼ŒLeaky ReLU ä¸ä¼šåœ¨è¾“å…¥å°äºŽé›¶æ—¶å°†æ‰€æœ‰è¾“å‡ºå½’é›¶ï¼Œè€Œæ˜¯ç”Ÿæˆä¸€ä¸ªä¸Ž alpha x è¾“å…¥ç›¸ç­‰çš„å°æ¢¯åº¦ã€‚ åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œalpha = 0.2ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-feeding",
   "metadata": {},
   "source": [
    "ç”Ÿæˆå™¨å­¦ä¹ ä»Ž 100 ç»´è¾“å…¥å‘é‡ï¼ˆ[-1.0, 1.0] èŒƒå›´å†… 100 ç»´å‡åŒ€åˆ†å¸ƒçš„éšæœºå™ªå£°ï¼‰ç”Ÿæˆå‡å›¾åƒã€‚é‰´åˆ«å™¨å°†çœŸå®žå›¾åƒä¸Žè™šå‡å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œä½†åœ¨è®­ç»ƒå¯¹æŠ—ç½‘ç»œæ—¶æ— æ„ä¸­æŒ‡å¯¼ç”Ÿæˆå™¨å¦‚ä½•ç”ŸæˆçœŸå®žå›¾åƒã€‚æˆ‘ä»¬çš„ DCGAN å®žçŽ°ä¸­ä½¿ç”¨çš„å†…æ ¸å¤§å°ä¸º 5ã€‚è¿™æ˜¯ä¸ºäº†å¢žåŠ å·ç§¯çš„æ„Ÿå—é‡Žå¤§å°å’Œè¡¨è¾¾èƒ½åŠ›ã€‚\n",
    "\n",
    "ç”Ÿæˆå™¨æŽ¥å—ç”±èŒƒå›´ä¸º -1.0 åˆ° 1.0 çš„å‡åŒ€åˆ†å¸ƒç”Ÿæˆçš„ 100 ç»´ z å‘é‡ã€‚ç”Ÿæˆå™¨çš„ç¬¬ä¸€å±‚æ˜¯ä¸€ä¸ª 7 x 7 x 128 = 6,272 å•å…ƒçš„ Dense å±‚ã€‚å•ä½æ•°æ˜¯æ ¹æ®è¾“å‡ºå›¾åƒçš„é¢„æœŸæœ€ç»ˆå°ºå¯¸ï¼ˆ28 x 28 x 1ï¼Œ28 æ˜¯ 7 çš„å€æ•°ï¼‰å’Œç¬¬ä¸€ä¸ª Conv2DTranspose çš„æ»¤æ³¢å™¨æ•°é‡è®¡ç®—çš„ï¼Œè¯¥æ•°é‡ç­‰äºŽ 128ã€‚\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥å°†è½¬ç½® CNNï¼ˆConv2DTransposeï¼‰æƒ³è±¡æˆ CNN çš„é€†è¿‡ç¨‹ã€‚åœ¨ä¸€ä¸ªç®€å•çš„ä¾‹å­ä¸­ï¼Œå¦‚æžœ CNN å°†å›¾åƒè½¬æ¢ä¸ºç‰¹å¾å›¾ï¼Œåˆ™è½¬ç½®çš„ CNN å°†ç”Ÿæˆç»™å®šç‰¹å¾å›¾çš„å›¾åƒã€‚å› æ­¤ï¼Œå‰ä¸€ç« çš„è§£ç å™¨å’Œæœ¬ç« çš„ç”Ÿæˆå™¨éƒ½ä½¿ç”¨äº†è½¬ç½® CNNã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-cologne",
   "metadata": {},
   "source": [
    "> æ¸…å• 4.2.1ï¼šdcgan-mnist-4.2.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hairy-potato",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 397 ms (started: 2021-08-12 18:49:39 +08:00)\n"
     ]
    }
   ],
   "source": [
    "'''Trains DCGAN on MNIST using Keras\n",
    "\n",
    "DCGAN æ˜¯ä½¿ç”¨ CNN çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN)ã€‚ ç”Ÿæˆå™¨è¯•å›¾é€šè¿‡ç”Ÿæˆå‡å›¾åƒæ¥æ¬ºéª—é‰´åˆ«å™¨ã€‚ \n",
    "é‰´åˆ«å™¨å­¦ä¹ åŒºåˆ†çœŸå‡å›¾åƒã€‚ ç”Ÿæˆå™¨+é‰´åˆ«å™¨å½¢æˆå¯¹æŠ—ç½‘ç»œã€‚ DCGAN äº¤æ›¿è®­ç»ƒé‰´åˆ«å™¨å’Œå¯¹æŠ—ç½‘ç»œã€‚ \n",
    "åœ¨è®­ç»ƒæœŸé—´ï¼Œé‰´åˆ«å™¨ä¸ä»…å­¦ä¹ åŒºåˆ†çœŸå‡å›¾åƒï¼Œè¿˜æŒ‡å¯¼å¯¹æŠ—çš„ç”Ÿæˆå™¨éƒ¨åˆ†å¦‚ä½•æé«˜å…¶ç”Ÿæˆå‡å›¾åƒçš„èƒ½åŠ›ã€‚\n",
    "\n",
    "[1] Radford, Alec, Luke Metz, and Soumith Chintala.\n",
    "\"Unsupervised representation learning with deep convolutional\n",
    "generative adversarial networks.\" arXiv preprint arXiv:1511.06434 (2015).\n",
    "'''\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Activation, Dense, Input\n",
    "from tensorflow.keras.layers import Conv2D, Flatten\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "def build_generator(inputs, image_size):\n",
    "    \"\"\"Build a Generator Model\n",
    "\n",
    "    BN-ReLU-Conv2DTranposeå †æ ˆç”Ÿæˆå‡å›¾åƒ\n",
    "    è¾“å‡ºæ¿€æ´»æ˜¯ sigmoid è€Œä¸æ˜¯ [1] ä¸­çš„ tanhã€‚\n",
    "    Sigmoid å¾ˆå®¹æ˜“æ”¶æ•›ã€‚\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "         è¾“å…¥ï¼ˆå±‚ï¼‰ï¼šç”Ÿæˆå™¨çš„è¾“å…¥å±‚ï¼Œz å‘é‡ï¼‰\n",
    "         image_sizeï¼ˆå¼ é‡ï¼‰ï¼šä¸€ä¾§çš„ç›®æ ‡å¤§å°ï¼Œï¼ˆå‡è®¾æ–¹å½¢å›¾åƒï¼‰\n",
    "\n",
    "    Returns:\n",
    "        generator (Model): Generator Model\n",
    "    \"\"\"\n",
    "\n",
    "    image_resize = image_size // 4\n",
    "    # network parameters \n",
    "    kernel_size = 5\n",
    "    layer_filters = [128, 64, 32, 1]\n",
    "\n",
    "    x = Dense(image_resize * image_resize * layer_filters[0])(inputs)\n",
    "    x = Reshape((image_resize, image_resize, layer_filters[0]))(x)\n",
    "\n",
    "    for filters in layer_filters:\n",
    "        # å‰ä¸¤ä¸ªå·ç§¯å±‚ä½¿ç”¨ strides = 2\n",
    "        # æœ€åŽä¸¤ä¸ªä½¿ç”¨strides = 1\n",
    "        if filters > layer_filters[-2]:\n",
    "            strides = 2\n",
    "        else:\n",
    "            strides = 1\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2DTranspose(filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            strides=strides,\n",
    "                            padding='same')(x)\n",
    "\n",
    "    x = Activation('sigmoid')(x)\n",
    "    generator = Model(inputs, x, name='generator')\n",
    "#     generator.summary()\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "synthetic-rachel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DT (None, 14, 14, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DT (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DT (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_19 (Conv2DT (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,301,505\n",
      "Trainable params: 1,300,801\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n",
      "time: 152 ms (started: 2021-08-12 19:14:36 +08:00)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "build_generator(layers.Input(shape=(100,)), 28).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "buried-somewhere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 530 ms (started: 2021-08-12 19:15:08 +08:00)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(build_generator(layers.Input(shape=(100,)), 28), show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "brown-clearance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.62 ms (started: 2021-08-12 19:11:05 +08:00)\n"
     ]
    }
   ],
   "source": [
    "def build_discriminator(inputs):\n",
    "    \"\"\"Build a Discriminator Model\n",
    "\n",
    "    ç”¨äºŽåŒºåˆ†çœŸå‡çš„ LeakyReLU-Conv2D å †æ ˆã€‚\n",
    "     ç½‘ç»œä¸ä¸Ž BN æ”¶æ•›æ‰€ä»¥è¿™é‡Œä¸ä½¿ç”¨\n",
    "     ä¸Ž [1] æˆ–åŽŸå§‹è®ºæ–‡ä¸åŒã€‚\n",
    "\n",
    "     å‚æ•°ï¼š\n",
    "         è¾“å…¥ï¼ˆå±‚ï¼‰ï¼šé‰´åˆ«å™¨çš„è¾“å…¥å±‚ï¼ˆå›¾åƒï¼‰\n",
    "\n",
    "    Returns:\n",
    "        discriminator (Model): Discriminator Model\n",
    "    \"\"\"\n",
    "    kernel_size = 5\n",
    "    layer_filters = [32, 64, 128, 256]\n",
    "\n",
    "    x = inputs\n",
    "    for filters in layer_filters:\n",
    "        # first 3 convolution layers use strides = 2\n",
    "        # last one uses strides = 1\n",
    "        if filters == layer_filters[-1]:\n",
    "            strides = 1\n",
    "        else:\n",
    "            strides = 2\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Conv2D(filters=filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   strides=strides,\n",
    "                   padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    discriminator = Model(inputs, x, name='discriminator')\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "steady-medium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 32)        832       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 64)          51264     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 4097      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,080,577\n",
      "Trainable params: 1,080,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "time: 103 ms (started: 2021-08-12 19:13:30 +08:00)\n"
     ]
    }
   ],
   "source": [
    "build_discriminator(layers.Input(shape=(28,28,1))).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "standard-factory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 352 ms (started: 2021-08-12 19:15:59 +08:00)\n"
     ]
    }
   ],
   "source": [
    "plot_model(build_discriminator(layers.Input(shape=(28,28,1))), show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dynamic-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.84 ms (started: 2021-08-12 21:41:27 +08:00)\n"
     ]
    }
   ],
   "source": [
    "def train(models, x_train, params):\n",
    "    \"\"\"Train the Discriminator and Adversarial Networks\n",
    "\n",
    "    äº¤æ›¿è®­ç»ƒåˆ¤åˆ«å™¨å’Œå¯¹æŠ—ç½‘ç»œã€‚\n",
    "    é¦–å…ˆç”¨æ­£ç¡®çš„çœŸå‡å›¾åƒè®­ç»ƒé‰´åˆ«å™¨ã€‚\n",
    "    æŽ¥ä¸‹æ¥ç”¨å‡è£…æ˜¯çœŸå®žçš„å‡å›¾åƒè®­ç»ƒå¯¹æŠ—\n",
    "    æ¯ä¸ª save_interval ç”Ÿæˆç¤ºä¾‹å›¾åƒã€‚\n",
    "\n",
    "    Arguments:\n",
    "        models (list): Generator, Discriminator, Adversarial models\n",
    "        x_train (tensor): Train images\n",
    "        params (list) : Networks parameters\n",
    "\n",
    "    \"\"\"\n",
    "    # the GAN component models\n",
    "    generator, discriminator, adversarial = models\n",
    "    # network parameters\n",
    "    batch_size, latent_size, train_steps, model_name = params\n",
    "    # the generator image is saved every 500 steps\n",
    "    save_interval = 500\n",
    "    # noise vector to see how the generator output evolves during training\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
    "    # number of elements in train dataset\n",
    "    train_size = x_train.shape[0]\n",
    "    for i in range(train_steps):\n",
    "        \n",
    "        # è®­ç»ƒ 1 ä¸ªæ‰¹æ¬¡çš„é‰´åˆ«å™¨\n",
    "        # 1 æ‰¹çœŸå®žï¼ˆæ ‡ç­¾=1.0ï¼‰å’Œå‡å›¾åƒï¼ˆæ ‡ç­¾=0.0ï¼‰\n",
    "        # ä»Žæ•°æ®é›†ä¸­éšæœºé€‰æ‹©çœŸå®žå›¾åƒ\n",
    "        rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
    "        real_images = x_train[rand_indexes]\n",
    "        # generate fake images from noise using generator \n",
    "        # generate noise using uniform distribution\n",
    "        noise = np.random.uniform(-1.0,\n",
    "                                  1.0,\n",
    "                                  size=[batch_size, latent_size])\n",
    "        # generate fake images\n",
    "        fake_images = generator.predict(noise)\n",
    "        # real + fake images = 1 batch of train data\n",
    "        x = np.concatenate((real_images, fake_images))\n",
    "        # label real and fake images\n",
    "        # real images label is 1.0\n",
    "        y = np.ones([2 * batch_size, 1])\n",
    "        # fake images label is 0.0\n",
    "        y[batch_size:, :] = 0.0\n",
    "        # è®­ç»ƒé‰´åˆ«å™¨ç½‘ç»œï¼Œè®°å½•æŸå¤±å’Œå‡†ç¡®æ€§\n",
    "        loss, acc = discriminator.train_on_batch(x, y)\n",
    "        log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss, acc)\n",
    "\n",
    "        # è®­ç»ƒ 1 ä¸ªæ‰¹æ¬¡çš„å¯¹æŠ—ç½‘ç»œ\n",
    "        # 1 æ‰¹å¸¦æœ‰ label=1.0 çš„å‡å›¾åƒ\n",
    "        # å› ä¸ºåˆ¤åˆ«å™¨æƒé‡è¢«å†»ç»“åœ¨å¯¹æŠ—ç½‘ç»œä¸­\n",
    "        # åªä½¿ç”¨å‡åŒ€åˆ†å¸ƒç”Ÿæˆå™ªå£°è®­ç»ƒç”Ÿæˆå™¨\n",
    "        noise = np.random.uniform(-1.0,\n",
    "                                  1.0, \n",
    "                                  size=[batch_size, latent_size])\n",
    "        # label fake images as real or 1.0\n",
    "        y = np.ones([batch_size, 1])\n",
    "        # è®­ç»ƒå¯¹æŠ—ç½‘ç»œ\n",
    "        # è¯·æ³¨æ„ï¼Œä¸Žåˆ¤åˆ«å™¨è®­ç»ƒä¸åŒï¼Œ\n",
    "        # æˆ‘ä»¬ä¸å°†å‡å›¾åƒä¿å­˜åœ¨å˜é‡ä¸­\n",
    "        # å‡å›¾åƒè¿›å…¥å¯¹æŠ—çš„é‰´åˆ«å™¨è¾“å…¥\n",
    "        # ç”¨äºŽåˆ†ç±»\n",
    "        # è®°å½•æŸå¤±å’Œå‡†ç¡®çŽ‡\n",
    "        loss, acc = adversarial.train_on_batch(noise, y)\n",
    "        log = \"%s [adversarial loss: %f, acc: %f]\" % (log, loss, acc)\n",
    "        print(log)\n",
    "        if (i + 1) % save_interval == 0:\n",
    "            # plot generator images on a periodic basis\n",
    "            plot_images(generator,\n",
    "                        noise_input=noise_input,\n",
    "                        show=False,\n",
    "                        step=(i + 1),\n",
    "                        model_name=model_name)\n",
    "   \n",
    "    # è®­ç»ƒç”Ÿæˆå™¨åŽä¿å­˜æ¨¡åž‹\n",
    "    # è®­ç»ƒå¥½çš„ç”Ÿæˆå™¨å¯ä»¥é‡æ–°åŠ è½½\n",
    "    # æœªæ¥çš„ MNIST æ•°å­—ç”Ÿæˆ\n",
    "    generator.save(model_name + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "apart-algorithm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.52 ms (started: 2021-08-12 21:44:01 +08:00)\n"
     ]
    }
   ],
   "source": [
    "def plot_images(generator,\n",
    "                noise_input,\n",
    "                show=False,\n",
    "                step=0,\n",
    "                model_name=\"gan\"):\n",
    "    \"\"\"Generate fake images and plot them\n",
    "        å‡ºäºŽå¯è§†åŒ–ç›®çš„ï¼Œç”Ÿæˆå‡å›¾åƒ\n",
    "        ç„¶åŽå°†å®ƒä»¬ç»˜åˆ¶åœ¨æ–¹å½¢ç½‘æ ¼ä¸­\n",
    "\n",
    "        å‚æ•°ï¼š\n",
    "            generatorï¼ˆModelï¼‰ï¼šç”¨äºŽç”Ÿæˆå‡å›¾åƒçš„ç”Ÿæˆå™¨æ¨¡åž‹\n",
    "            noise_input (ndarray)ï¼šz å‘é‡æ•°ç»„\n",
    "            show (bool): æ˜¯å¦æ˜¾ç¤ºæƒ…èŠ‚\n",
    "            stepï¼ˆintï¼‰ï¼šé™„åŠ åˆ°ä¿å­˜å›¾åƒçš„æ–‡ä»¶å\n",
    "            model_nameï¼ˆstringï¼‰ï¼šæ¨¡åž‹åç§°\n",
    "\n",
    "    \"\"\"\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "    filename = os.path.join(model_name, \"%05d.png\" % step)\n",
    "    images = generator.predict(noise_input)\n",
    "    plt.figure(figsize=(2.2, 2.2))\n",
    "    num_images = images.shape[0]\n",
    "    image_size = images.shape[1]\n",
    "    rows = int(math.sqrt(noise_input.shape[0]))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(rows, rows, i + 1)\n",
    "        image = np.reshape(images[i], [image_size, image_size])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-offense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 14, 14, 32)        832       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 7, 7, 64)          51264     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 4097      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,080,577\n",
      "Trainable params: 1,080,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_input (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_28 (Conv2DT (None, 14, 14, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_29 (Conv2DT (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_30 (Conv2DT (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_31 (Conv2DT (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,301,505\n",
      "Trainable params: 1,300,801\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n",
      "Model: \"dcgan_mnist\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_input (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "generator (Functional)       (None, 28, 28, 1)         1301505   \n",
      "_________________________________________________________________\n",
      "discriminator (Functional)   (None, 1)                 1080577   \n",
      "=================================================================\n",
      "Total params: 2,382,082\n",
      "Trainable params: 1,300,801\n",
      "Non-trainable params: 1,081,281\n",
      "_________________________________________________________________\n",
      "0: [discriminator loss: 0.692629, acc: 0.429688] [adversarial loss: 0.811663, acc: 0.000000]\n",
      "1: [discriminator loss: 0.631186, acc: 0.992188] [adversarial loss: 0.890095, acc: 0.000000]\n",
      "2: [discriminator loss: 0.526727, acc: 1.000000] [adversarial loss: 0.918299, acc: 0.000000]\n",
      "3: [discriminator loss: 0.370951, acc: 1.000000] [adversarial loss: 0.606873, acc: 1.000000]\n",
      "4: [discriminator loss: 0.221915, acc: 1.000000] [adversarial loss: 0.589779, acc: 1.000000]\n",
      "5: [discriminator loss: 0.118282, acc: 1.000000] [adversarial loss: 0.045596, acc: 1.000000]\n",
      "6: [discriminator loss: 0.174475, acc: 1.000000] [adversarial loss: 1.369035, acc: 0.000000]\n",
      "7: [discriminator loss: 0.158651, acc: 0.984375] [adversarial loss: 0.181273, acc: 1.000000]\n",
      "8: [discriminator loss: 0.039685, acc: 1.000000] [adversarial loss: 0.084142, acc: 1.000000]\n",
      "9: [discriminator loss: 0.027333, acc: 1.000000] [adversarial loss: 0.056574, acc: 1.000000]\n",
      "10: [discriminator loss: 0.019674, acc: 1.000000] [adversarial loss: 0.044777, acc: 1.000000]\n",
      "11: [discriminator loss: 0.017203, acc: 1.000000] [adversarial loss: 0.035343, acc: 1.000000]\n",
      "12: [discriminator loss: 0.013786, acc: 1.000000] [adversarial loss: 0.029360, acc: 1.000000]\n",
      "13: [discriminator loss: 0.012462, acc: 1.000000] [adversarial loss: 0.022836, acc: 1.000000]\n",
      "14: [discriminator loss: 0.010414, acc: 1.000000] [adversarial loss: 0.019308, acc: 1.000000]\n",
      "15: [discriminator loss: 0.008905, acc: 1.000000] [adversarial loss: 0.016090, acc: 1.000000]\n",
      "16: [discriminator loss: 0.007579, acc: 1.000000] [adversarial loss: 0.014160, acc: 1.000000]\n",
      "17: [discriminator loss: 0.007057, acc: 1.000000] [adversarial loss: 0.011820, acc: 1.000000]\n",
      "18: [discriminator loss: 0.006857, acc: 1.000000] [adversarial loss: 0.009087, acc: 1.000000]\n",
      "19: [discriminator loss: 0.005223, acc: 1.000000] [adversarial loss: 0.008352, acc: 1.000000]\n",
      "20: [discriminator loss: 0.004537, acc: 1.000000] [adversarial loss: 0.007111, acc: 1.000000]\n",
      "21: [discriminator loss: 0.003758, acc: 1.000000] [adversarial loss: 0.006627, acc: 1.000000]\n",
      "22: [discriminator loss: 0.003598, acc: 1.000000] [adversarial loss: 0.005834, acc: 1.000000]\n",
      "23: [discriminator loss: 0.002822, acc: 1.000000] [adversarial loss: 0.005594, acc: 1.000000]\n",
      "24: [discriminator loss: 0.003253, acc: 1.000000] [adversarial loss: 0.004519, acc: 1.000000]\n",
      "25: [discriminator loss: 0.002325, acc: 1.000000] [adversarial loss: 0.004332, acc: 1.000000]\n",
      "26: [discriminator loss: 0.002210, acc: 1.000000] [adversarial loss: 0.003922, acc: 1.000000]\n",
      "27: [discriminator loss: 0.002279, acc: 1.000000] [adversarial loss: 0.003420, acc: 1.000000]\n",
      "28: [discriminator loss: 0.002033, acc: 1.000000] [adversarial loss: 0.002997, acc: 1.000000]\n",
      "29: [discriminator loss: 0.001712, acc: 1.000000] [adversarial loss: 0.002778, acc: 1.000000]\n",
      "30: [discriminator loss: 0.001634, acc: 1.000000] [adversarial loss: 0.002426, acc: 1.000000]\n",
      "31: [discriminator loss: 0.001320, acc: 1.000000] [adversarial loss: 0.002362, acc: 1.000000]\n",
      "32: [discriminator loss: 0.001265, acc: 1.000000] [adversarial loss: 0.002187, acc: 1.000000]\n",
      "33: [discriminator loss: 0.001521, acc: 1.000000] [adversarial loss: 0.001782, acc: 1.000000]\n",
      "34: [discriminator loss: 0.000981, acc: 1.000000] [adversarial loss: 0.001702, acc: 1.000000]\n",
      "35: [discriminator loss: 0.000877, acc: 1.000000] [adversarial loss: 0.001722, acc: 1.000000]\n",
      "36: [discriminator loss: 0.000789, acc: 1.000000] [adversarial loss: 0.001676, acc: 1.000000]\n",
      "37: [discriminator loss: 0.000769, acc: 1.000000] [adversarial loss: 0.001575, acc: 1.000000]\n",
      "38: [discriminator loss: 0.000787, acc: 1.000000] [adversarial loss: 0.001411, acc: 1.000000]\n",
      "39: [discriminator loss: 0.000777, acc: 1.000000] [adversarial loss: 0.001279, acc: 1.000000]\n",
      "40: [discriminator loss: 0.000596, acc: 1.000000] [adversarial loss: 0.001210, acc: 1.000000]\n",
      "41: [discriminator loss: 0.000732, acc: 1.000000] [adversarial loss: 0.001022, acc: 1.000000]\n",
      "42: [discriminator loss: 0.000593, acc: 1.000000] [adversarial loss: 0.000946, acc: 1.000000]\n",
      "43: [discriminator loss: 0.000486, acc: 1.000000] [adversarial loss: 0.000948, acc: 1.000000]\n",
      "44: [discriminator loss: 0.000559, acc: 1.000000] [adversarial loss: 0.000834, acc: 1.000000]\n",
      "45: [discriminator loss: 0.000527, acc: 1.000000] [adversarial loss: 0.000773, acc: 1.000000]\n",
      "46: [discriminator loss: 0.000482, acc: 1.000000] [adversarial loss: 0.000701, acc: 1.000000]\n",
      "47: [discriminator loss: 0.000476, acc: 1.000000] [adversarial loss: 0.000614, acc: 1.000000]\n",
      "48: [discriminator loss: 0.000362, acc: 1.000000] [adversarial loss: 0.000592, acc: 1.000000]\n",
      "49: [discriminator loss: 0.000330, acc: 1.000000] [adversarial loss: 0.000576, acc: 1.000000]\n",
      "50: [discriminator loss: 0.000315, acc: 1.000000] [adversarial loss: 0.000542, acc: 1.000000]\n",
      "51: [discriminator loss: 0.000250, acc: 1.000000] [adversarial loss: 0.000557, acc: 1.000000]\n",
      "52: [discriminator loss: 0.000300, acc: 1.000000] [adversarial loss: 0.000485, acc: 1.000000]\n",
      "53: [discriminator loss: 0.000240, acc: 1.000000] [adversarial loss: 0.000482, acc: 1.000000]\n",
      "54: [discriminator loss: 0.000253, acc: 1.000000] [adversarial loss: 0.000442, acc: 1.000000]\n",
      "55: [discriminator loss: 0.000290, acc: 1.000000] [adversarial loss: 0.000370, acc: 1.000000]\n",
      "56: [discriminator loss: 0.000195, acc: 1.000000] [adversarial loss: 0.000376, acc: 1.000000]\n",
      "57: [discriminator loss: 0.000169, acc: 1.000000] [adversarial loss: 0.000386, acc: 1.000000]\n",
      "58: [discriminator loss: 0.000181, acc: 1.000000] [adversarial loss: 0.000366, acc: 1.000000]\n",
      "59: [discriminator loss: 0.000180, acc: 1.000000] [adversarial loss: 0.000335, acc: 1.000000]\n",
      "60: [discriminator loss: 0.000146, acc: 1.000000] [adversarial loss: 0.000333, acc: 1.000000]\n",
      "61: [discriminator loss: 0.000148, acc: 1.000000] [adversarial loss: 0.000317, acc: 1.000000]\n",
      "62: [discriminator loss: 0.000163, acc: 1.000000] [adversarial loss: 0.000278, acc: 1.000000]\n",
      "63: [discriminator loss: 0.000135, acc: 1.000000] [adversarial loss: 0.000253, acc: 1.000000]\n",
      "64: [discriminator loss: 0.000102, acc: 1.000000] [adversarial loss: 0.000270, acc: 1.000000]\n",
      "65: [discriminator loss: 0.000125, acc: 1.000000] [adversarial loss: 0.000244, acc: 1.000000]\n",
      "66: [discriminator loss: 0.000099, acc: 1.000000] [adversarial loss: 0.000239, acc: 1.000000]\n",
      "67: [discriminator loss: 0.000111, acc: 1.000000] [adversarial loss: 0.000219, acc: 1.000000]\n",
      "68: [discriminator loss: 0.000091, acc: 1.000000] [adversarial loss: 0.000205, acc: 1.000000]\n",
      "69: [discriminator loss: 0.000089, acc: 1.000000] [adversarial loss: 0.000206, acc: 1.000000]\n",
      "70: [discriminator loss: 0.000076, acc: 1.000000] [adversarial loss: 0.000196, acc: 1.000000]\n",
      "71: [discriminator loss: 0.000081, acc: 1.000000] [adversarial loss: 0.000184, acc: 1.000000]\n",
      "72: [discriminator loss: 0.000064, acc: 1.000000] [adversarial loss: 0.000183, acc: 1.000000]\n",
      "73: [discriminator loss: 0.000070, acc: 1.000000] [adversarial loss: 0.000168, acc: 1.000000]\n",
      "74: [discriminator loss: 0.000068, acc: 1.000000] [adversarial loss: 0.000159, acc: 1.000000]\n",
      "75: [discriminator loss: 0.000057, acc: 1.000000] [adversarial loss: 0.000151, acc: 1.000000]\n",
      "76: [discriminator loss: 0.000061, acc: 1.000000] [adversarial loss: 0.000139, acc: 1.000000]\n",
      "77: [discriminator loss: 0.000050, acc: 1.000000] [adversarial loss: 0.000138, acc: 1.000000]\n",
      "78: [discriminator loss: 0.000048, acc: 1.000000] [adversarial loss: 0.000130, acc: 1.000000]\n",
      "79: [discriminator loss: 0.000052, acc: 1.000000] [adversarial loss: 0.000114, acc: 1.000000]\n",
      "80: [discriminator loss: 0.000066, acc: 1.000000] [adversarial loss: 0.000075, acc: 1.000000]\n",
      "81: [discriminator loss: 0.000039, acc: 1.000000] [adversarial loss: 0.000084, acc: 1.000000]\n",
      "82: [discriminator loss: 0.000042, acc: 1.000000] [adversarial loss: 0.000078, acc: 1.000000]\n",
      "83: [discriminator loss: 0.000041, acc: 1.000000] [adversarial loss: 0.000074, acc: 1.000000]\n",
      "84: [discriminator loss: 0.000032, acc: 1.000000] [adversarial loss: 0.000076, acc: 1.000000]\n",
      "85: [discriminator loss: 0.000037, acc: 1.000000] [adversarial loss: 0.000064, acc: 1.000000]\n",
      "86: [discriminator loss: 0.000036, acc: 1.000000] [adversarial loss: 0.000058, acc: 1.000000]\n",
      "87: [discriminator loss: 0.000038, acc: 1.000000] [adversarial loss: 0.000045, acc: 1.000000]\n",
      "88: [discriminator loss: 0.000024, acc: 1.000000] [adversarial loss: 0.000050, acc: 1.000000]\n",
      "89: [discriminator loss: 0.000027, acc: 1.000000] [adversarial loss: 0.000050, acc: 1.000000]\n",
      "90: [discriminator loss: 0.000025, acc: 1.000000] [adversarial loss: 0.000049, acc: 1.000000]\n",
      "91: [discriminator loss: 0.000022, acc: 1.000000] [adversarial loss: 0.000048, acc: 1.000000]\n",
      "92: [discriminator loss: 0.000020, acc: 1.000000] [adversarial loss: 0.000046, acc: 1.000000]\n",
      "93: [discriminator loss: 0.000022, acc: 1.000000] [adversarial loss: 0.000039, acc: 1.000000]\n",
      "94: [discriminator loss: 0.000017, acc: 1.000000] [adversarial loss: 0.000041, acc: 1.000000]\n",
      "95: [discriminator loss: 0.000021, acc: 1.000000] [adversarial loss: 0.000029, acc: 1.000000]\n",
      "96: [discriminator loss: 0.000022, acc: 1.000000] [adversarial loss: 0.000023, acc: 1.000000]\n",
      "97: [discriminator loss: 0.000019, acc: 1.000000] [adversarial loss: 0.000026, acc: 1.000000]\n",
      "98: [discriminator loss: 0.000016, acc: 1.000000] [adversarial loss: 0.000028, acc: 1.000000]\n",
      "99: [discriminator loss: 0.000012, acc: 1.000000] [adversarial loss: 0.000031, acc: 1.000000]\n",
      "100: [discriminator loss: 0.000023, acc: 1.000000] [adversarial loss: 0.000014, acc: 1.000000]\n",
      "101: [discriminator loss: 0.000012, acc: 1.000000] [adversarial loss: 0.000022, acc: 1.000000]\n",
      "102: [discriminator loss: 0.000015, acc: 1.000000] [adversarial loss: 0.000017, acc: 1.000000]\n",
      "103: [discriminator loss: 0.000012, acc: 1.000000] [adversarial loss: 0.000021, acc: 1.000000]\n",
      "104: [discriminator loss: 0.000011, acc: 1.000000] [adversarial loss: 0.000024, acc: 1.000000]\n",
      "105: [discriminator loss: 0.000010, acc: 1.000000] [adversarial loss: 0.000022, acc: 1.000000]\n",
      "106: [discriminator loss: 0.000009, acc: 1.000000] [adversarial loss: 0.000024, acc: 1.000000]\n",
      "107: [discriminator loss: 0.000009, acc: 1.000000] [adversarial loss: 0.000021, acc: 1.000000]\n",
      "108: [discriminator loss: 0.000013, acc: 1.000000] [adversarial loss: 0.000011, acc: 1.000000]\n",
      "109: [discriminator loss: 0.000008, acc: 1.000000] [adversarial loss: 0.000017, acc: 1.000000]\n",
      "110: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000023, acc: 1.000000]\n",
      "111: [discriminator loss: 0.000005, acc: 1.000000] [adversarial loss: 0.000027, acc: 1.000000]\n",
      "112: [discriminator loss: 0.000008, acc: 1.000000] [adversarial loss: 0.000019, acc: 1.000000]\n",
      "113: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000022, acc: 1.000000]\n",
      "114: [discriminator loss: 0.000005, acc: 1.000000] [adversarial loss: 0.000023, acc: 1.000000]\n",
      "115: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000026, acc: 1.000000]\n",
      "116: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000019, acc: 1.000000]\n",
      "117: [discriminator loss: 0.000010, acc: 1.000000] [adversarial loss: 0.000009, acc: 1.000000]\n",
      "118: [discriminator loss: 0.000005, acc: 1.000000] [adversarial loss: 0.000019, acc: 1.000000]\n",
      "119: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000016, acc: 1.000000]\n",
      "120: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000021, acc: 1.000000]\n",
      "121: [discriminator loss: 0.000005, acc: 1.000000] [adversarial loss: 0.000017, acc: 1.000000]\n",
      "122: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000017, acc: 1.000000]\n",
      "123: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000024, acc: 1.000000]\n",
      "124: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000028, acc: 1.000000]\n",
      "125: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000025, acc: 1.000000]\n",
      "126: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000012, acc: 1.000000]\n",
      "127: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000024, acc: 1.000000]\n",
      "128: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000019, acc: 1.000000]\n",
      "129: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000030, acc: 1.000000]\n",
      "130: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000032, acc: 1.000000]\n",
      "131: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000038, acc: 1.000000]\n",
      "132: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000043, acc: 1.000000]\n",
      "133: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000036, acc: 1.000000]\n",
      "134: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000028, acc: 1.000000]\n",
      "135: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000019, acc: 1.000000]\n",
      "136: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000031, acc: 1.000000]\n",
      "137: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000041, acc: 1.000000]\n",
      "138: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000047, acc: 1.000000]\n",
      "139: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000016, acc: 1.000000]\n",
      "140: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000041, acc: 1.000000]\n",
      "141: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000061, acc: 1.000000]\n",
      "142: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000059, acc: 1.000000]\n",
      "143: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000074, acc: 1.000000]\n",
      "144: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000001, acc: 1.000000]\n",
      "145: [discriminator loss: 0.000012, acc: 1.000000] [adversarial loss: 0.393000, acc: 1.000000]\n",
      "146: [discriminator loss: 0.000012, acc: 1.000000] [adversarial loss: 0.000009, acc: 1.000000]\n",
      "147: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000057, acc: 1.000000]\n",
      "148: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000093, acc: 1.000000]\n",
      "149: [discriminator loss: 0.000000, acc: 1.000000] [adversarial loss: 0.000143, acc: 1.000000]\n",
      "150: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000191, acc: 1.000000]\n",
      "151: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000242, acc: 1.000000]\n",
      "152: [discriminator loss: 0.000000, acc: 1.000000] [adversarial loss: 0.000330, acc: 1.000000]\n",
      "153: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000317, acc: 1.000000]\n",
      "154: [discriminator loss: 0.000000, acc: 1.000000] [adversarial loss: 0.000477, acc: 1.000000]\n",
      "155: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000230, acc: 1.000000]\n",
      "156: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000525, acc: 1.000000]\n",
      "157: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000858, acc: 1.000000]\n",
      "158: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000579, acc: 1.000000]\n",
      "159: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.001293, acc: 1.000000]\n",
      "160: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.001647, acc: 1.000000]\n",
      "161: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.001473, acc: 1.000000]\n",
      "162: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000169, acc: 1.000000]\n",
      "163: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.003201, acc: 1.000000]\n",
      "164: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000503, acc: 1.000000]\n",
      "165: [discriminator loss: 0.000000, acc: 1.000000] [adversarial loss: 0.000727, acc: 1.000000]\n",
      "166: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000225, acc: 1.000000]\n",
      "167: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.001447, acc: 1.000000]\n",
      "168: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.001049, acc: 1.000000]\n",
      "169: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.000929, acc: 1.000000]\n",
      "170: [discriminator loss: 0.000000, acc: 1.000000] [adversarial loss: 0.002695, acc: 1.000000]\n",
      "171: [discriminator loss: 0.000000, acc: 1.000000] [adversarial loss: 0.003668, acc: 1.000000]\n",
      "172: [discriminator loss: 0.000000, acc: 1.000000] [adversarial loss: 0.005107, acc: 1.000000]\n",
      "173: [discriminator loss: 0.000001, acc: 1.000000] [adversarial loss: 0.003954, acc: 1.000000]\n",
      "174: [discriminator loss: 0.000013, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "175: [discriminator loss: 3.074631, acc: 0.500000] [adversarial loss: 4.469131, acc: 0.000000]\n",
      "176: [discriminator loss: 0.000089, acc: 1.000000] [adversarial loss: 2.066056, acc: 0.000000]\n",
      "177: [discriminator loss: 0.000034, acc: 1.000000] [adversarial loss: 0.632038, acc: 0.718750]\n",
      "178: [discriminator loss: 0.000017, acc: 1.000000] [adversarial loss: 0.381725, acc: 1.000000]\n",
      "179: [discriminator loss: 0.000016, acc: 1.000000] [adversarial loss: 0.306640, acc: 1.000000]\n",
      "180: [discriminator loss: 0.000025, acc: 1.000000] [adversarial loss: 0.261930, acc: 1.000000]\n",
      "181: [discriminator loss: 0.000152, acc: 1.000000] [adversarial loss: 0.227509, acc: 1.000000]\n",
      "182: [discriminator loss: 0.000026, acc: 1.000000] [adversarial loss: 0.202063, acc: 1.000000]\n",
      "183: [discriminator loss: 0.000009, acc: 1.000000] [adversarial loss: 0.181091, acc: 1.000000]\n",
      "184: [discriminator loss: 0.000266, acc: 1.000000] [adversarial loss: 0.163089, acc: 1.000000]\n",
      "185: [discriminator loss: 0.000036, acc: 1.000000] [adversarial loss: 0.147243, acc: 1.000000]\n",
      "186: [discriminator loss: 0.000133, acc: 1.000000] [adversarial loss: 0.134891, acc: 1.000000]\n",
      "187: [discriminator loss: 0.000828, acc: 1.000000] [adversarial loss: 0.118054, acc: 1.000000]\n",
      "188: [discriminator loss: 0.000636, acc: 1.000000] [adversarial loss: 0.105946, acc: 1.000000]\n",
      "189: [discriminator loss: 0.000005, acc: 1.000000] [adversarial loss: 0.099219, acc: 1.000000]\n",
      "190: [discriminator loss: 0.000126, acc: 1.000000] [adversarial loss: 0.090394, acc: 1.000000]\n",
      "191: [discriminator loss: 0.000012, acc: 1.000000] [adversarial loss: 0.085255, acc: 1.000000]\n",
      "192: [discriminator loss: 0.000066, acc: 1.000000] [adversarial loss: 0.080108, acc: 1.000000]\n",
      "193: [discriminator loss: 0.000130, acc: 1.000000] [adversarial loss: 0.074605, acc: 1.000000]\n",
      "194: [discriminator loss: 0.000022, acc: 1.000000] [adversarial loss: 0.070802, acc: 1.000000]\n",
      "195: [discriminator loss: 0.000053, acc: 1.000000] [adversarial loss: 0.066564, acc: 1.000000]\n",
      "196: [discriminator loss: 0.000042, acc: 1.000000] [adversarial loss: 0.062368, acc: 1.000000]\n",
      "197: [discriminator loss: 0.000093, acc: 1.000000] [adversarial loss: 0.058161, acc: 1.000000]\n",
      "198: [discriminator loss: 0.000018, acc: 1.000000] [adversarial loss: 0.054978, acc: 1.000000]\n",
      "199: [discriminator loss: 0.000013, acc: 1.000000] [adversarial loss: 0.051900, acc: 1.000000]\n",
      "200: [discriminator loss: 0.005818, acc: 0.992188] [adversarial loss: 0.022905, acc: 1.000000]\n",
      "201: [discriminator loss: 0.000096, acc: 1.000000] [adversarial loss: 0.022164, acc: 1.000000]\n",
      "202: [discriminator loss: 0.000065, acc: 1.000000] [adversarial loss: 0.021260, acc: 1.000000]\n",
      "203: [discriminator loss: 0.000025, acc: 1.000000] [adversarial loss: 0.020720, acc: 1.000000]\n",
      "204: [discriminator loss: 0.000119, acc: 1.000000] [adversarial loss: 0.020006, acc: 1.000000]\n",
      "205: [discriminator loss: 0.000040, acc: 1.000000] [adversarial loss: 0.019543, acc: 1.000000]\n",
      "206: [discriminator loss: 0.000037, acc: 1.000000] [adversarial loss: 0.019217, acc: 1.000000]\n",
      "207: [discriminator loss: 0.000055, acc: 1.000000] [adversarial loss: 0.018677, acc: 1.000000]\n",
      "208: [discriminator loss: 0.000061, acc: 1.000000] [adversarial loss: 0.018593, acc: 1.000000]\n",
      "209: [discriminator loss: 0.000129, acc: 1.000000] [adversarial loss: 0.017584, acc: 1.000000]\n",
      "210: [discriminator loss: 0.000069, acc: 1.000000] [adversarial loss: 0.017185, acc: 1.000000]\n",
      "211: [discriminator loss: 0.000084, acc: 1.000000] [adversarial loss: 0.016733, acc: 1.000000]\n",
      "212: [discriminator loss: 0.000086, acc: 1.000000] [adversarial loss: 0.016817, acc: 1.000000]\n",
      "213: [discriminator loss: 0.000108, acc: 1.000000] [adversarial loss: 0.016609, acc: 1.000000]\n",
      "214: [discriminator loss: 0.000110, acc: 1.000000] [adversarial loss: 0.016511, acc: 1.000000]\n",
      "215: [discriminator loss: 0.000137, acc: 1.000000] [adversarial loss: 0.015916, acc: 1.000000]\n",
      "216: [discriminator loss: 0.000319, acc: 1.000000] [adversarial loss: 0.014903, acc: 1.000000]\n",
      "217: [discriminator loss: 0.003257, acc: 1.000000] [adversarial loss: 0.005091, acc: 1.000000]\n",
      "218: [discriminator loss: 0.000542, acc: 1.000000] [adversarial loss: 0.006446, acc: 1.000000]\n",
      "219: [discriminator loss: 0.000488, acc: 1.000000] [adversarial loss: 0.007944, acc: 1.000000]\n",
      "220: [discriminator loss: 0.000447, acc: 1.000000] [adversarial loss: 0.009904, acc: 1.000000]\n",
      "221: [discriminator loss: 0.000416, acc: 1.000000] [adversarial loss: 0.010968, acc: 1.000000]\n",
      "222: [discriminator loss: 0.000507, acc: 1.000000] [adversarial loss: 0.010871, acc: 1.000000]\n",
      "223: [discriminator loss: 0.000398, acc: 1.000000] [adversarial loss: 0.010461, acc: 1.000000]\n",
      "224: [discriminator loss: 0.000410, acc: 1.000000] [adversarial loss: 0.009787, acc: 1.000000]\n",
      "225: [discriminator loss: 0.000453, acc: 1.000000] [adversarial loss: 0.009764, acc: 1.000000]\n",
      "226: [discriminator loss: 0.000532, acc: 1.000000] [adversarial loss: 0.010422, acc: 1.000000]\n",
      "227: [discriminator loss: 0.001475, acc: 1.000000] [adversarial loss: 0.006993, acc: 1.000000]\n",
      "228: [discriminator loss: 0.000989, acc: 1.000000] [adversarial loss: 0.016782, acc: 1.000000]\n",
      "229: [discriminator loss: 0.000879, acc: 1.000000] [adversarial loss: 0.033202, acc: 1.000000]\n",
      "230: [discriminator loss: 0.001501, acc: 1.000000] [adversarial loss: 0.044340, acc: 1.000000]\n",
      "231: [discriminator loss: 0.004072, acc: 1.000000] [adversarial loss: 0.041254, acc: 1.000000]\n",
      "232: [discriminator loss: 0.006392, acc: 1.000000] [adversarial loss: 3.278895, acc: 0.000000]\n",
      "233: [discriminator loss: 0.000699, acc: 1.000000] [adversarial loss: 1.221486, acc: 0.000000]\n",
      "234: [discriminator loss: 0.001699, acc: 1.000000] [adversarial loss: 0.016269, acc: 1.000000]\n",
      "235: [discriminator loss: 0.003160, acc: 1.000000] [adversarial loss: 0.001468, acc: 1.000000]\n",
      "236: [discriminator loss: 0.003864, acc: 1.000000] [adversarial loss: 0.484180, acc: 0.984375]\n",
      "237: [discriminator loss: 0.001097, acc: 1.000000] [adversarial loss: 0.008840, acc: 1.000000]\n",
      "238: [discriminator loss: 0.000772, acc: 1.000000] [adversarial loss: 0.011124, acc: 1.000000]\n",
      "239: [discriminator loss: 0.000508, acc: 1.000000] [adversarial loss: 0.018507, acc: 1.000000]\n",
      "240: [discriminator loss: 0.004028, acc: 1.000000] [adversarial loss: 0.000010, acc: 1.000000]\n",
      "241: [discriminator loss: 0.040712, acc: 1.000000] [adversarial loss: 18.892662, acc: 0.000000]\n",
      "242: [discriminator loss: 0.621940, acc: 0.843750] [adversarial loss: 0.000001, acc: 1.000000]\n",
      "243: [discriminator loss: 0.161439, acc: 0.984375] [adversarial loss: 4.696508, acc: 0.000000]\n",
      "244: [discriminator loss: 0.027216, acc: 0.992188] [adversarial loss: 0.136349, acc: 1.000000]\n",
      "245: [discriminator loss: 0.000182, acc: 1.000000] [adversarial loss: 0.075143, acc: 1.000000]\n",
      "246: [discriminator loss: 0.012189, acc: 0.992188] [adversarial loss: 0.010782, acc: 1.000000]\n",
      "247: [discriminator loss: 0.000608, acc: 1.000000] [adversarial loss: 0.009782, acc: 1.000000]\n",
      "248: [discriminator loss: 0.000622, acc: 1.000000] [adversarial loss: 0.009255, acc: 1.000000]\n",
      "249: [discriminator loss: 0.000632, acc: 1.000000] [adversarial loss: 0.009846, acc: 1.000000]\n",
      "250: [discriminator loss: 0.000688, acc: 1.000000] [adversarial loss: 0.008143, acc: 1.000000]\n",
      "251: [discriminator loss: 0.000664, acc: 1.000000] [adversarial loss: 0.007791, acc: 1.000000]\n",
      "252: [discriminator loss: 0.000745, acc: 1.000000] [adversarial loss: 0.007432, acc: 1.000000]\n",
      "253: [discriminator loss: 0.000725, acc: 1.000000] [adversarial loss: 0.006502, acc: 1.000000]\n",
      "254: [discriminator loss: 0.000790, acc: 1.000000] [adversarial loss: 0.006305, acc: 1.000000]\n",
      "255: [discriminator loss: 0.000892, acc: 1.000000] [adversarial loss: 0.006284, acc: 1.000000]\n",
      "256: [discriminator loss: 0.000737, acc: 1.000000] [adversarial loss: 0.006284, acc: 1.000000]\n",
      "257: [discriminator loss: 0.000805, acc: 1.000000] [adversarial loss: 0.005561, acc: 1.000000]\n",
      "258: [discriminator loss: 0.000864, acc: 1.000000] [adversarial loss: 0.005529, acc: 1.000000]\n",
      "259: [discriminator loss: 0.000914, acc: 1.000000] [adversarial loss: 0.005045, acc: 1.000000]\n",
      "260: [discriminator loss: 0.000743, acc: 1.000000] [adversarial loss: 0.005277, acc: 1.000000]\n",
      "261: [discriminator loss: 0.000901, acc: 1.000000] [adversarial loss: 0.005270, acc: 1.000000]\n",
      "262: [discriminator loss: 0.000825, acc: 1.000000] [adversarial loss: 0.005131, acc: 1.000000]\n",
      "263: [discriminator loss: 0.000779, acc: 1.000000] [adversarial loss: 0.005779, acc: 1.000000]\n",
      "264: [discriminator loss: 0.007284, acc: 0.992188] [adversarial loss: 0.000488, acc: 1.000000]\n",
      "265: [discriminator loss: 0.002164, acc: 1.000000] [adversarial loss: 0.001438, acc: 1.000000]\n",
      "266: [discriminator loss: 0.001560, acc: 1.000000] [adversarial loss: 0.002484, acc: 1.000000]\n",
      "267: [discriminator loss: 0.001080, acc: 1.000000] [adversarial loss: 0.003504, acc: 1.000000]\n",
      "268: [discriminator loss: 0.001043, acc: 1.000000] [adversarial loss: 0.004467, acc: 1.000000]\n",
      "269: [discriminator loss: 0.000965, acc: 1.000000] [adversarial loss: 0.005472, acc: 1.000000]\n",
      "270: [discriminator loss: 0.001112, acc: 1.000000] [adversarial loss: 0.005207, acc: 1.000000]\n",
      "271: [discriminator loss: 0.001231, acc: 1.000000] [adversarial loss: 0.005397, acc: 1.000000]\n",
      "272: [discriminator loss: 0.003754, acc: 1.000000] [adversarial loss: 0.001163, acc: 1.000000]\n",
      "273: [discriminator loss: 0.002145, acc: 1.000000] [adversarial loss: 0.005785, acc: 1.000000]\n",
      "274: [discriminator loss: 0.001131, acc: 1.000000] [adversarial loss: 0.010113, acc: 1.000000]\n",
      "275: [discriminator loss: 0.001501, acc: 1.000000] [adversarial loss: 0.019239, acc: 1.000000]\n",
      "276: [discriminator loss: 0.007628, acc: 0.992188] [adversarial loss: 0.000275, acc: 1.000000]\n",
      "277: [discriminator loss: 0.011370, acc: 1.000000] [adversarial loss: 2.729088, acc: 0.000000]\n",
      "278: [discriminator loss: 0.235505, acc: 0.929688] [adversarial loss: 20.770042, acc: 0.000000]\n",
      "279: [discriminator loss: 3.783760, acc: 0.609375] [adversarial loss: 0.000089, acc: 1.000000]\n",
      "280: [discriminator loss: 0.288025, acc: 0.875000] [adversarial loss: 3.096408, acc: 0.000000]\n",
      "281: [discriminator loss: 0.196604, acc: 0.960938] [adversarial loss: 0.131280, acc: 1.000000]\n",
      "282: [discriminator loss: 0.074799, acc: 0.976562] [adversarial loss: 0.023171, acc: 1.000000]\n",
      "283: [discriminator loss: 0.083570, acc: 0.976562] [adversarial loss: 0.005685, acc: 1.000000]\n",
      "284: [discriminator loss: 0.013722, acc: 1.000000] [adversarial loss: 0.009932, acc: 1.000000]\n",
      "285: [discriminator loss: 0.090662, acc: 0.992188] [adversarial loss: 0.007801, acc: 1.000000]\n",
      "286: [discriminator loss: 0.026725, acc: 0.992188] [adversarial loss: 0.008639, acc: 1.000000]\n",
      "287: [discriminator loss: 0.013772, acc: 1.000000] [adversarial loss: 0.015355, acc: 1.000000]\n",
      "288: [discriminator loss: 0.008979, acc: 1.000000] [adversarial loss: 0.027882, acc: 1.000000]\n",
      "289: [discriminator loss: 0.020343, acc: 0.992188] [adversarial loss: 0.020636, acc: 1.000000]\n",
      "290: [discriminator loss: 0.041154, acc: 0.992188] [adversarial loss: 0.013732, acc: 1.000000]\n",
      "291: [discriminator loss: 0.045994, acc: 0.992188] [adversarial loss: 0.009620, acc: 1.000000]\n",
      "292: [discriminator loss: 0.020056, acc: 1.000000] [adversarial loss: 0.021732, acc: 1.000000]\n",
      "293: [discriminator loss: 0.011006, acc: 1.000000] [adversarial loss: 0.034923, acc: 1.000000]\n",
      "294: [discriminator loss: 0.046853, acc: 0.984375] [adversarial loss: 0.005502, acc: 1.000000]\n",
      "295: [discriminator loss: 0.019860, acc: 1.000000] [adversarial loss: 0.044791, acc: 1.000000]\n",
      "296: [discriminator loss: 0.005979, acc: 1.000000] [adversarial loss: 0.054498, acc: 1.000000]\n",
      "297: [discriminator loss: 0.004267, acc: 1.000000] [adversarial loss: 0.061151, acc: 1.000000]\n",
      "298: [discriminator loss: 0.003795, acc: 1.000000] [adversarial loss: 0.051682, acc: 1.000000]\n",
      "299: [discriminator loss: 0.003188, acc: 1.000000] [adversarial loss: 0.042174, acc: 1.000000]\n",
      "300: [discriminator loss: 0.044003, acc: 0.992188] [adversarial loss: 0.006411, acc: 1.000000]\n",
      "301: [discriminator loss: 0.006404, acc: 1.000000] [adversarial loss: 0.015376, acc: 1.000000]\n",
      "302: [discriminator loss: 0.003783, acc: 1.000000] [adversarial loss: 0.023835, acc: 1.000000]\n",
      "303: [discriminator loss: 0.003263, acc: 1.000000] [adversarial loss: 0.030540, acc: 1.000000]\n",
      "304: [discriminator loss: 0.010216, acc: 1.000000] [adversarial loss: 0.005417, acc: 1.000000]\n",
      "305: [discriminator loss: 0.006025, acc: 1.000000] [adversarial loss: 0.014934, acc: 1.000000]\n",
      "306: [discriminator loss: 0.003705, acc: 1.000000] [adversarial loss: 0.024400, acc: 1.000000]\n",
      "307: [discriminator loss: 0.054153, acc: 0.984375] [adversarial loss: 0.000144, acc: 1.000000]\n",
      "308: [discriminator loss: 0.043063, acc: 1.000000] [adversarial loss: 0.730242, acc: 0.453125]\n",
      "309: [discriminator loss: 0.001214, acc: 1.000000] [adversarial loss: 0.014022, acc: 1.000000]\n",
      "310: [discriminator loss: 0.003712, acc: 1.000000] [adversarial loss: 0.005351, acc: 1.000000]\n",
      "311: [discriminator loss: 0.001529, acc: 1.000000] [adversarial loss: 0.006991, acc: 1.000000]\n",
      "312: [discriminator loss: 0.004736, acc: 1.000000] [adversarial loss: 0.002332, acc: 1.000000]\n",
      "313: [discriminator loss: 0.002814, acc: 1.000000] [adversarial loss: 0.004702, acc: 1.000000]\n",
      "314: [discriminator loss: 0.003416, acc: 1.000000] [adversarial loss: 0.002997, acc: 1.000000]\n",
      "315: [discriminator loss: 0.002656, acc: 1.000000] [adversarial loss: 0.004422, acc: 1.000000]\n",
      "316: [discriminator loss: 0.005638, acc: 1.000000] [adversarial loss: 0.001342, acc: 1.000000]\n",
      "317: [discriminator loss: 0.004027, acc: 1.000000] [adversarial loss: 0.005021, acc: 1.000000]\n",
      "318: [discriminator loss: 0.002892, acc: 1.000000] [adversarial loss: 0.004070, acc: 1.000000]\n",
      "319: [discriminator loss: 0.004644, acc: 1.000000] [adversarial loss: 0.002063, acc: 1.000000]\n",
      "320: [discriminator loss: 0.003017, acc: 1.000000] [adversarial loss: 0.005719, acc: 1.000000]\n",
      "321: [discriminator loss: 0.002068, acc: 1.000000] [adversarial loss: 0.008707, acc: 1.000000]\n",
      "322: [discriminator loss: 0.002205, acc: 1.000000] [adversarial loss: 0.007025, acc: 1.000000]\n",
      "323: [discriminator loss: 0.001944, acc: 1.000000] [adversarial loss: 0.006568, acc: 1.000000]\n",
      "324: [discriminator loss: 0.027530, acc: 0.992188] [adversarial loss: 0.000006, acc: 1.000000]\n",
      "325: [discriminator loss: 0.062060, acc: 1.000000] [adversarial loss: 7.444027, acc: 0.000000]\n",
      "326: [discriminator loss: 0.195373, acc: 0.937500] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "327: [discriminator loss: 0.819653, acc: 0.507812] [adversarial loss: 6.404305, acc: 0.000000]\n",
      "328: [discriminator loss: 0.187879, acc: 0.968750] [adversarial loss: 0.000153, acc: 1.000000]\n",
      "329: [discriminator loss: 0.054350, acc: 0.992188] [adversarial loss: 0.000323, acc: 1.000000]\n",
      "330: [discriminator loss: 0.023204, acc: 1.000000] [adversarial loss: 0.000520, acc: 1.000000]\n",
      "331: [discriminator loss: 0.026304, acc: 0.992188] [adversarial loss: 0.000466, acc: 1.000000]\n",
      "332: [discriminator loss: 0.022007, acc: 1.000000] [adversarial loss: 0.001043, acc: 1.000000]\n",
      "333: [discriminator loss: 0.024873, acc: 0.992188] [adversarial loss: 0.000677, acc: 1.000000]\n",
      "334: [discriminator loss: 0.019023, acc: 1.000000] [adversarial loss: 0.001452, acc: 1.000000]\n",
      "335: [discriminator loss: 0.012977, acc: 1.000000] [adversarial loss: 0.002282, acc: 1.000000]\n",
      "336: [discriminator loss: 0.044246, acc: 0.992188] [adversarial loss: 0.000503, acc: 1.000000]\n",
      "337: [discriminator loss: 0.017893, acc: 1.000000] [adversarial loss: 0.002377, acc: 1.000000]\n",
      "338: [discriminator loss: 0.028065, acc: 0.992188] [adversarial loss: 0.001005, acc: 1.000000]\n",
      "339: [discriminator loss: 0.013656, acc: 1.000000] [adversarial loss: 0.002624, acc: 1.000000]\n",
      "340: [discriminator loss: 0.008930, acc: 1.000000] [adversarial loss: 0.004160, acc: 1.000000]\n",
      "341: [discriminator loss: 0.007695, acc: 1.000000] [adversarial loss: 0.007397, acc: 1.000000]\n",
      "342: [discriminator loss: 0.005849, acc: 1.000000] [adversarial loss: 0.007244, acc: 1.000000]\n",
      "343: [discriminator loss: 0.094022, acc: 0.984375] [adversarial loss: 0.000060, acc: 1.000000]\n",
      "344: [discriminator loss: 0.056279, acc: 1.000000] [adversarial loss: 0.037783, acc: 1.000000]\n",
      "345: [discriminator loss: 0.004813, acc: 1.000000] [adversarial loss: 0.013578, acc: 1.000000]\n",
      "346: [discriminator loss: 0.049302, acc: 0.984375] [adversarial loss: 0.000065, acc: 1.000000]\n",
      "347: [discriminator loss: 0.042609, acc: 1.000000] [adversarial loss: 0.038119, acc: 1.000000]\n",
      "348: [discriminator loss: 0.003397, acc: 1.000000] [adversarial loss: 0.023486, acc: 1.000000]\n",
      "349: [discriminator loss: 0.011407, acc: 0.992188] [adversarial loss: 0.002014, acc: 1.000000]\n",
      "350: [discriminator loss: 0.010885, acc: 1.000000] [adversarial loss: 0.021308, acc: 1.000000]\n",
      "351: [discriminator loss: 0.009907, acc: 0.992188] [adversarial loss: 0.003036, acc: 1.000000]\n",
      "352: [discriminator loss: 0.009781, acc: 1.000000] [adversarial loss: 0.038284, acc: 1.000000]\n",
      "353: [discriminator loss: 0.004582, acc: 1.000000] [adversarial loss: 0.051579, acc: 1.000000]\n",
      "354: [discriminator loss: 0.005821, acc: 1.000000] [adversarial loss: 0.077984, acc: 1.000000]\n",
      "355: [discriminator loss: 0.006856, acc: 1.000000] [adversarial loss: 0.121063, acc: 1.000000]\n",
      "356: [discriminator loss: 0.051797, acc: 0.992188] [adversarial loss: 0.002088, acc: 1.000000]\n",
      "357: [discriminator loss: 0.050562, acc: 1.000000] [adversarial loss: 9.303889, acc: 0.000000]\n",
      "358: [discriminator loss: 0.114171, acc: 0.968750] [adversarial loss: 0.000947, acc: 1.000000]\n",
      "359: [discriminator loss: 0.057435, acc: 1.000000] [adversarial loss: 0.814167, acc: 0.218750]\n",
      "360: [discriminator loss: 0.009786, acc: 1.000000] [adversarial loss: 1.239618, acc: 0.000000]\n",
      "361: [discriminator loss: 0.021487, acc: 0.992188] [adversarial loss: 0.098430, acc: 1.000000]\n",
      "362: [discriminator loss: 0.082956, acc: 1.000000] [adversarial loss: 10.940429, acc: 0.000000]\n",
      "363: [discriminator loss: 0.474956, acc: 0.875000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "364: [discriminator loss: 2.975321, acc: 0.500000] [adversarial loss: 1.737488, acc: 0.000000]\n",
      "365: [discriminator loss: 1.562104, acc: 0.460938] [adversarial loss: 4.298094, acc: 0.000000]\n",
      "366: [discriminator loss: 1.072625, acc: 0.742188] [adversarial loss: 0.001905, acc: 1.000000]\n",
      "367: [discriminator loss: 0.951105, acc: 0.500000] [adversarial loss: 0.854441, acc: 0.140625]\n",
      "368: [discriminator loss: 0.094710, acc: 0.976562] [adversarial loss: 0.362557, acc: 1.000000]\n",
      "369: [discriminator loss: 0.067801, acc: 0.984375] [adversarial loss: 0.315370, acc: 1.000000]\n",
      "370: [discriminator loss: 0.073858, acc: 0.992188] [adversarial loss: 0.349328, acc: 1.000000]\n",
      "371: [discriminator loss: 0.129005, acc: 0.976562] [adversarial loss: 0.147485, acc: 1.000000]\n",
      "372: [discriminator loss: 0.061705, acc: 1.000000] [adversarial loss: 0.422297, acc: 1.000000]\n",
      "373: [discriminator loss: 0.077444, acc: 0.992188] [adversarial loss: 0.221904, acc: 1.000000]\n",
      "374: [discriminator loss: 0.037463, acc: 1.000000] [adversarial loss: 0.413774, acc: 1.000000]\n",
      "375: [discriminator loss: 0.028262, acc: 1.000000] [adversarial loss: 0.370822, acc: 1.000000]\n",
      "376: [discriminator loss: 0.019065, acc: 1.000000] [adversarial loss: 0.397247, acc: 1.000000]\n",
      "377: [discriminator loss: 0.071746, acc: 0.976562] [adversarial loss: 0.040265, acc: 1.000000]\n",
      "378: [discriminator loss: 0.046206, acc: 1.000000] [adversarial loss: 0.187539, acc: 1.000000]\n",
      "379: [discriminator loss: 0.018176, acc: 1.000000] [adversarial loss: 0.154998, acc: 1.000000]\n",
      "380: [discriminator loss: 0.021575, acc: 1.000000] [adversarial loss: 0.074275, acc: 1.000000]\n",
      "381: [discriminator loss: 0.066894, acc: 0.984375] [adversarial loss: 0.015464, acc: 1.000000]\n",
      "382: [discriminator loss: 0.039432, acc: 1.000000] [adversarial loss: 0.164460, acc: 1.000000]\n",
      "383: [discriminator loss: 0.052791, acc: 0.992188] [adversarial loss: 0.024741, acc: 1.000000]\n",
      "384: [discriminator loss: 0.032195, acc: 1.000000] [adversarial loss: 0.122601, acc: 1.000000]\n",
      "385: [discriminator loss: 0.025298, acc: 1.000000] [adversarial loss: 0.175721, acc: 1.000000]\n",
      "386: [discriminator loss: 0.044952, acc: 1.000000] [adversarial loss: 0.212748, acc: 1.000000]\n",
      "387: [discriminator loss: 0.103574, acc: 0.984375] [adversarial loss: 1.469934, acc: 0.000000]\n",
      "388: [discriminator loss: 0.134410, acc: 0.968750] [adversarial loss: 0.016719, acc: 1.000000]\n",
      "389: [discriminator loss: 0.278037, acc: 0.828125] [adversarial loss: 9.454012, acc: 0.000000]\n",
      "390: [discriminator loss: 1.983835, acc: 0.546875] [adversarial loss: 0.003800, acc: 1.000000]\n",
      "391: [discriminator loss: 0.369232, acc: 0.773438] [adversarial loss: 0.492568, acc: 0.890625]\n",
      "392: [discriminator loss: 0.051624, acc: 1.000000] [adversarial loss: 0.403376, acc: 1.000000]\n",
      "393: [discriminator loss: 0.029255, acc: 1.000000] [adversarial loss: 0.436864, acc: 0.984375]\n",
      "394: [discriminator loss: 0.032259, acc: 0.992188] [adversarial loss: 0.218803, acc: 1.000000]\n",
      "395: [discriminator loss: 0.033756, acc: 0.992188] [adversarial loss: 0.176370, acc: 1.000000]\n",
      "396: [discriminator loss: 0.024654, acc: 1.000000] [adversarial loss: 0.199825, acc: 1.000000]\n",
      "397: [discriminator loss: 0.039595, acc: 0.992188] [adversarial loss: 0.115088, acc: 1.000000]\n",
      "398: [discriminator loss: 0.039777, acc: 0.992188] [adversarial loss: 0.082239, acc: 1.000000]\n",
      "399: [discriminator loss: 0.044352, acc: 0.992188] [adversarial loss: 0.169900, acc: 1.000000]\n",
      "400: [discriminator loss: 0.049870, acc: 0.992188] [adversarial loss: 0.051359, acc: 1.000000]\n",
      "401: [discriminator loss: 0.112513, acc: 0.984375] [adversarial loss: 0.036512, acc: 1.000000]\n",
      "402: [discriminator loss: 0.118134, acc: 0.984375] [adversarial loss: 0.035467, acc: 1.000000]\n",
      "403: [discriminator loss: 0.042179, acc: 1.000000] [adversarial loss: 0.194238, acc: 1.000000]\n",
      "404: [discriminator loss: 0.058655, acc: 0.992188] [adversarial loss: 0.031181, acc: 1.000000]\n",
      "405: [discriminator loss: 0.031304, acc: 1.000000] [adversarial loss: 0.359538, acc: 1.000000]\n",
      "406: [discriminator loss: 0.026109, acc: 1.000000] [adversarial loss: 0.109554, acc: 1.000000]\n",
      "407: [discriminator loss: 0.062966, acc: 0.984375] [adversarial loss: 0.026193, acc: 1.000000]\n",
      "408: [discriminator loss: 0.078395, acc: 0.984375] [adversarial loss: 0.550678, acc: 0.875000]\n",
      "409: [discriminator loss: 0.050486, acc: 0.984375] [adversarial loss: 0.025285, acc: 1.000000]\n",
      "410: [discriminator loss: 0.060182, acc: 1.000000] [adversarial loss: 3.661657, acc: 0.000000]\n",
      "411: [discriminator loss: 0.135225, acc: 0.953125] [adversarial loss: 0.002241, acc: 1.000000]\n",
      "412: [discriminator loss: 0.417988, acc: 0.695312] [adversarial loss: 9.742757, acc: 0.000000]\n",
      "413: [discriminator loss: 1.598039, acc: 0.562500] [adversarial loss: 0.024596, acc: 1.000000]\n",
      "414: [discriminator loss: 0.232434, acc: 0.898438] [adversarial loss: 0.614227, acc: 0.750000]\n",
      "415: [discriminator loss: 0.063578, acc: 1.000000] [adversarial loss: 0.942721, acc: 0.000000]\n",
      "416: [discriminator loss: 0.048382, acc: 1.000000] [adversarial loss: 1.423146, acc: 0.000000]\n",
      "417: [discriminator loss: 0.214359, acc: 0.992188] [adversarial loss: 4.891989, acc: 0.000000]\n",
      "418: [discriminator loss: 0.210070, acc: 0.945312] [adversarial loss: 0.629723, acc: 0.781250]\n",
      "419: [discriminator loss: 0.392839, acc: 0.734375] [adversarial loss: 5.933758, acc: 0.000000]\n",
      "420: [discriminator loss: 0.464666, acc: 0.828125] [adversarial loss: 0.024726, acc: 1.000000]\n",
      "421: [discriminator loss: 0.656098, acc: 0.539062] [adversarial loss: 4.551244, acc: 0.000000]\n",
      "422: [discriminator loss: 0.355209, acc: 0.906250] [adversarial loss: 0.418911, acc: 0.953125]\n",
      "423: [discriminator loss: 0.574904, acc: 0.484375] [adversarial loss: 4.900224, acc: 0.000000]\n",
      "424: [discriminator loss: 0.649304, acc: 0.781250] [adversarial loss: 0.224575, acc: 1.000000]\n",
      "425: [discriminator loss: 0.662036, acc: 0.500000] [adversarial loss: 3.826555, acc: 0.000000]\n",
      "426: [discriminator loss: 0.147645, acc: 0.960938] [adversarial loss: 1.864281, acc: 0.000000]\n",
      "427: [discriminator loss: 0.225644, acc: 0.984375] [adversarial loss: 1.662691, acc: 0.000000]\n",
      "428: [discriminator loss: 0.263842, acc: 0.960938] [adversarial loss: 1.274570, acc: 0.000000]\n",
      "429: [discriminator loss: 0.197137, acc: 0.976562] [adversarial loss: 1.665349, acc: 0.000000]\n",
      "430: [discriminator loss: 0.208680, acc: 0.960938] [adversarial loss: 1.386126, acc: 0.000000]\n",
      "431: [discriminator loss: 0.260422, acc: 0.921875] [adversarial loss: 0.643849, acc: 0.812500]\n",
      "432: [discriminator loss: 0.276395, acc: 0.937500] [adversarial loss: 3.678306, acc: 0.000000]\n",
      "433: [discriminator loss: 0.508041, acc: 0.781250] [adversarial loss: 0.029852, acc: 1.000000]\n",
      "434: [discriminator loss: 0.859526, acc: 0.515625] [adversarial loss: 4.570858, acc: 0.000000]\n",
      "435: [discriminator loss: 0.371746, acc: 0.867188] [adversarial loss: 0.572891, acc: 0.906250]\n",
      "436: [discriminator loss: 0.198449, acc: 0.992188] [adversarial loss: 2.007240, acc: 0.000000]\n",
      "437: [discriminator loss: 0.125979, acc: 0.984375] [adversarial loss: 1.259125, acc: 0.000000]\n",
      "438: [discriminator loss: 0.207180, acc: 0.976562] [adversarial loss: 1.848841, acc: 0.000000]\n",
      "439: [discriminator loss: 0.203214, acc: 0.929688] [adversarial loss: 0.688241, acc: 0.656250]\n",
      "440: [discriminator loss: 0.200825, acc: 0.992188] [adversarial loss: 2.461768, acc: 0.000000]\n",
      "441: [discriminator loss: 0.156233, acc: 0.968750] [adversarial loss: 0.572083, acc: 0.921875]\n",
      "442: [discriminator loss: 0.169986, acc: 1.000000] [adversarial loss: 3.106322, acc: 0.000000]\n",
      "443: [discriminator loss: 0.221280, acc: 0.921875] [adversarial loss: 0.135253, acc: 1.000000]\n",
      "444: [discriminator loss: 0.422887, acc: 0.625000] [adversarial loss: 5.940551, acc: 0.000000]\n",
      "445: [discriminator loss: 0.792870, acc: 0.617188] [adversarial loss: 0.060979, acc: 1.000000]\n",
      "446: [discriminator loss: 0.676593, acc: 0.507812] [adversarial loss: 3.539319, acc: 0.000000]\n",
      "447: [discriminator loss: 0.108753, acc: 0.976562] [adversarial loss: 1.094451, acc: 0.000000]\n",
      "448: [discriminator loss: 0.291471, acc: 0.882812] [adversarial loss: 3.239732, acc: 0.000000]\n",
      "449: [discriminator loss: 0.442393, acc: 0.781250] [adversarial loss: 0.048554, acc: 1.000000]\n",
      "450: [discriminator loss: 0.753209, acc: 0.507812] [adversarial loss: 5.563288, acc: 0.000000]\n",
      "451: [discriminator loss: 0.645132, acc: 0.640625] [adversarial loss: 0.367266, acc: 1.000000]\n",
      "452: [discriminator loss: 0.299419, acc: 0.906250] [adversarial loss: 2.899138, acc: 0.000000]\n",
      "453: [discriminator loss: 0.105952, acc: 0.968750] [adversarial loss: 1.867128, acc: 0.000000]\n",
      "454: [discriminator loss: 0.134189, acc: 0.984375] [adversarial loss: 1.601196, acc: 0.000000]\n",
      "455: [discriminator loss: 0.152945, acc: 0.984375] [adversarial loss: 1.497471, acc: 0.000000]\n",
      "456: [discriminator loss: 0.163234, acc: 0.976562] [adversarial loss: 1.285751, acc: 0.000000]\n",
      "457: [discriminator loss: 0.279826, acc: 0.976562] [adversarial loss: 2.395972, acc: 0.000000]\n",
      "458: [discriminator loss: 0.408671, acc: 0.851562] [adversarial loss: 0.091381, acc: 1.000000]\n",
      "459: [discriminator loss: 0.633315, acc: 0.500000] [adversarial loss: 6.593889, acc: 0.000000]\n",
      "460: [discriminator loss: 1.331397, acc: 0.515625] [adversarial loss: 0.139389, acc: 1.000000]\n",
      "461: [discriminator loss: 0.627692, acc: 0.515625] [adversarial loss: 4.539612, acc: 0.000000]\n",
      "462: [discriminator loss: 0.257419, acc: 0.859375] [adversarial loss: 1.758141, acc: 0.000000]\n",
      "463: [discriminator loss: 0.092704, acc: 1.000000] [adversarial loss: 2.179989, acc: 0.000000]\n",
      "464: [discriminator loss: 0.114908, acc: 1.000000] [adversarial loss: 2.184076, acc: 0.000000]\n",
      "465: [discriminator loss: 0.141419, acc: 0.984375] [adversarial loss: 2.098587, acc: 0.000000]\n",
      "466: [discriminator loss: 0.112049, acc: 0.992188] [adversarial loss: 2.541091, acc: 0.000000]\n",
      "467: [discriminator loss: 0.111491, acc: 0.984375] [adversarial loss: 1.897644, acc: 0.000000]\n",
      "468: [discriminator loss: 0.121442, acc: 1.000000] [adversarial loss: 3.536767, acc: 0.000000]\n",
      "469: [discriminator loss: 0.144886, acc: 0.937500] [adversarial loss: 1.571054, acc: 0.000000]\n",
      "470: [discriminator loss: 0.175133, acc: 1.000000] [adversarial loss: 6.274005, acc: 0.000000]\n",
      "471: [discriminator loss: 0.178974, acc: 0.929688] [adversarial loss: 2.458483, acc: 0.000000]\n",
      "472: [discriminator loss: 0.088551, acc: 1.000000] [adversarial loss: 3.922923, acc: 0.000000]\n",
      "473: [discriminator loss: 0.104161, acc: 0.968750] [adversarial loss: 1.993696, acc: 0.000000]\n",
      "474: [discriminator loss: 0.144448, acc: 0.992188] [adversarial loss: 4.523427, acc: 0.000000]\n",
      "475: [discriminator loss: 0.081686, acc: 0.976562] [adversarial loss: 1.996454, acc: 0.000000]\n",
      "476: [discriminator loss: 0.207242, acc: 1.000000] [adversarial loss: 6.212444, acc: 0.000000]\n",
      "477: [discriminator loss: 0.653736, acc: 0.718750] [adversarial loss: 0.063800, acc: 1.000000]\n",
      "478: [discriminator loss: 1.281138, acc: 0.500000] [adversarial loss: 6.373859, acc: 0.000000]\n",
      "479: [discriminator loss: 0.597060, acc: 0.742188] [adversarial loss: 0.381181, acc: 0.953125]\n",
      "480: [discriminator loss: 0.842213, acc: 0.523438] [adversarial loss: 5.409376, acc: 0.000000]\n",
      "481: [discriminator loss: 0.275850, acc: 0.859375] [adversarial loss: 1.368458, acc: 0.000000]\n",
      "482: [discriminator loss: 0.639225, acc: 0.507812] [adversarial loss: 5.022152, acc: 0.000000]\n",
      "483: [discriminator loss: 0.520691, acc: 0.703125] [adversarial loss: 0.800130, acc: 0.437500]\n",
      "484: [discriminator loss: 0.494865, acc: 0.570312] [adversarial loss: 4.600883, acc: 0.000000]\n",
      "485: [discriminator loss: 0.273619, acc: 0.890625] [adversarial loss: 1.608028, acc: 0.000000]\n",
      "486: [discriminator loss: 0.275105, acc: 0.976562] [adversarial loss: 3.483012, acc: 0.000000]\n",
      "487: [discriminator loss: 0.157070, acc: 0.960938] [adversarial loss: 1.559644, acc: 0.000000]\n",
      "488: [discriminator loss: 0.331941, acc: 0.937500] [adversarial loss: 4.260785, acc: 0.000000]\n",
      "489: [discriminator loss: 0.381996, acc: 0.828125] [adversarial loss: 0.664862, acc: 0.671875]\n",
      "490: [discriminator loss: 0.431881, acc: 0.617188] [adversarial loss: 4.877232, acc: 0.000000]\n",
      "491: [discriminator loss: 0.356559, acc: 0.820312] [adversarial loss: 1.800425, acc: 0.000000]\n",
      "492: [discriminator loss: 0.176202, acc: 1.000000] [adversarial loss: 3.256076, acc: 0.000000]\n",
      "493: [discriminator loss: 0.097876, acc: 0.992188] [adversarial loss: 2.563628, acc: 0.000000]\n",
      "494: [discriminator loss: 0.203597, acc: 0.976562] [adversarial loss: 2.757192, acc: 0.000000]\n",
      "495: [discriminator loss: 0.191412, acc: 0.984375] [adversarial loss: 2.589987, acc: 0.000000]\n",
      "496: [discriminator loss: 0.193816, acc: 0.953125] [adversarial loss: 1.554632, acc: 0.000000]\n",
      "497: [discriminator loss: 0.235298, acc: 0.992188] [adversarial loss: 4.677064, acc: 0.000000]\n",
      "498: [discriminator loss: 0.348542, acc: 0.812500] [adversarial loss: 0.327738, acc: 1.000000]\n",
      "499: [discriminator loss: 0.858552, acc: 0.500000] [adversarial loss: 6.440858, acc: 0.000000]\n",
      "500: [discriminator loss: 0.815740, acc: 0.601562] [adversarial loss: 0.330620, acc: 0.953125]\n",
      "501: [discriminator loss: 0.817501, acc: 0.500000] [adversarial loss: 3.968276, acc: 0.000000]\n",
      "502: [discriminator loss: 0.233278, acc: 0.882812] [adversarial loss: 1.859623, acc: 0.000000]\n",
      "503: [discriminator loss: 0.154474, acc: 0.992188] [adversarial loss: 2.376101, acc: 0.000000]\n",
      "504: [discriminator loss: 0.178215, acc: 0.953125] [adversarial loss: 1.831046, acc: 0.000000]\n",
      "505: [discriminator loss: 0.182798, acc: 0.968750] [adversarial loss: 2.433367, acc: 0.000000]\n",
      "506: [discriminator loss: 0.116898, acc: 0.984375] [adversarial loss: 2.292285, acc: 0.000000]\n",
      "507: [discriminator loss: 0.154596, acc: 0.945312] [adversarial loss: 2.207249, acc: 0.000000]\n",
      "508: [discriminator loss: 0.101285, acc: 0.984375] [adversarial loss: 2.501711, acc: 0.000000]\n",
      "509: [discriminator loss: 0.095949, acc: 0.992188] [adversarial loss: 2.496600, acc: 0.000000]\n",
      "510: [discriminator loss: 0.132096, acc: 0.976562] [adversarial loss: 2.385074, acc: 0.000000]\n",
      "511: [discriminator loss: 0.131962, acc: 0.968750] [adversarial loss: 2.485626, acc: 0.000000]\n",
      "512: [discriminator loss: 0.156122, acc: 1.000000] [adversarial loss: 4.120680, acc: 0.000000]\n",
      "513: [discriminator loss: 0.166722, acc: 0.953125] [adversarial loss: 1.140857, acc: 0.000000]\n",
      "514: [discriminator loss: 0.246857, acc: 0.992188] [adversarial loss: 7.508399, acc: 0.000000]\n",
      "515: [discriminator loss: 0.424630, acc: 0.812500] [adversarial loss: 1.997049, acc: 0.000000]\n",
      "516: [discriminator loss: 0.057206, acc: 1.000000] [adversarial loss: 3.260293, acc: 0.000000]\n",
      "517: [discriminator loss: 0.022311, acc: 1.000000] [adversarial loss: 3.307182, acc: 0.000000]\n",
      "518: [discriminator loss: 0.024052, acc: 0.992188] [adversarial loss: 3.314438, acc: 0.000000]\n",
      "519: [discriminator loss: 0.018390, acc: 1.000000] [adversarial loss: 3.293231, acc: 0.000000]\n",
      "520: [discriminator loss: 0.020006, acc: 1.000000] [adversarial loss: 3.274003, acc: 0.000000]\n",
      "521: [discriminator loss: 0.026519, acc: 0.992188] [adversarial loss: 3.128353, acc: 0.000000]\n",
      "522: [discriminator loss: 0.033309, acc: 0.992188] [adversarial loss: 2.696363, acc: 0.000000]\n",
      "523: [discriminator loss: 0.053483, acc: 0.992188] [adversarial loss: 2.739949, acc: 0.000000]\n",
      "524: [discriminator loss: 0.121033, acc: 1.000000] [adversarial loss: 5.319775, acc: 0.000000]\n",
      "525: [discriminator loss: 0.160574, acc: 0.937500] [adversarial loss: 1.316409, acc: 0.000000]\n",
      "526: [discriminator loss: 0.195348, acc: 0.984375] [adversarial loss: 6.069361, acc: 0.000000]\n",
      "527: [discriminator loss: 0.204545, acc: 0.906250] [adversarial loss: 2.578614, acc: 0.000000]\n",
      "528: [discriminator loss: 0.034768, acc: 1.000000] [adversarial loss: 3.467626, acc: 0.000000]\n",
      "529: [discriminator loss: 0.014154, acc: 1.000000] [adversarial loss: 3.670934, acc: 0.000000]\n",
      "530: [discriminator loss: 0.018957, acc: 1.000000] [adversarial loss: 3.618222, acc: 0.000000]\n",
      "531: [discriminator loss: 0.012516, acc: 1.000000] [adversarial loss: 3.723858, acc: 0.000000]\n",
      "532: [discriminator loss: 0.021972, acc: 1.000000] [adversarial loss: 3.632298, acc: 0.000000]\n",
      "533: [discriminator loss: 0.020886, acc: 1.000000] [adversarial loss: 3.876225, acc: 0.000000]\n",
      "534: [discriminator loss: 0.027342, acc: 0.992188] [adversarial loss: 3.723305, acc: 0.000000]\n",
      "535: [discriminator loss: 0.039579, acc: 0.992188] [adversarial loss: 3.784998, acc: 0.000000]\n",
      "536: [discriminator loss: 0.025534, acc: 1.000000] [adversarial loss: 4.609008, acc: 0.000000]\n",
      "537: [discriminator loss: 0.024354, acc: 0.992188] [adversarial loss: 3.744696, acc: 0.000000]\n",
      "538: [discriminator loss: 0.044578, acc: 0.984375] [adversarial loss: 2.499501, acc: 0.000000]\n",
      "539: [discriminator loss: 0.222196, acc: 0.976562] [adversarial loss: 10.551624, acc: 0.000000]\n",
      "540: [discriminator loss: 1.452564, acc: 0.539062] [adversarial loss: 0.150146, acc: 1.000000]\n",
      "541: [discriminator loss: 0.381385, acc: 0.734375] [adversarial loss: 3.663448, acc: 0.000000]\n",
      "542: [discriminator loss: 0.023355, acc: 1.000000] [adversarial loss: 2.771887, acc: 0.000000]\n",
      "543: [discriminator loss: 0.135753, acc: 0.984375] [adversarial loss: 2.977518, acc: 0.000000]\n",
      "544: [discriminator loss: 0.149477, acc: 0.976562] [adversarial loss: 3.288496, acc: 0.000000]\n",
      "545: [discriminator loss: 0.105929, acc: 0.976562] [adversarial loss: 2.242178, acc: 0.000000]\n",
      "546: [discriminator loss: 0.177449, acc: 0.968750] [adversarial loss: 3.460279, acc: 0.000000]\n",
      "547: [discriminator loss: 0.152285, acc: 0.960938] [adversarial loss: 1.564632, acc: 0.031250]\n",
      "548: [discriminator loss: 0.262502, acc: 0.875000] [adversarial loss: 5.565670, acc: 0.000000]\n",
      "549: [discriminator loss: 0.174536, acc: 0.929688] [adversarial loss: 1.354392, acc: 0.171875]\n",
      "550: [discriminator loss: 0.107264, acc: 0.953125] [adversarial loss: 1.298026, acc: 0.296875]\n",
      "551: [discriminator loss: 0.047989, acc: 0.992188] [adversarial loss: 0.631315, acc: 0.656250]\n",
      "552: [discriminator loss: 0.064678, acc: 0.992188] [adversarial loss: 0.440139, acc: 0.734375]\n",
      "553: [discriminator loss: 0.065332, acc: 0.976562] [adversarial loss: 0.341461, acc: 0.859375]\n",
      "554: [discriminator loss: 0.045939, acc: 0.984375] [adversarial loss: 0.140631, acc: 0.953125]\n",
      "555: [discriminator loss: 0.062904, acc: 0.984375] [adversarial loss: 0.265734, acc: 0.921875]\n",
      "556: [discriminator loss: 0.030869, acc: 1.000000] [adversarial loss: 0.113474, acc: 1.000000]\n",
      "557: [discriminator loss: 0.035187, acc: 0.992188] [adversarial loss: 0.077900, acc: 1.000000]\n",
      "558: [discriminator loss: 0.056232, acc: 0.992188] [adversarial loss: 0.133881, acc: 1.000000]\n",
      "559: [discriminator loss: 0.065327, acc: 0.984375] [adversarial loss: 0.094800, acc: 1.000000]\n",
      "560: [discriminator loss: 0.069371, acc: 0.984375] [adversarial loss: 0.212321, acc: 0.953125]\n",
      "561: [discriminator loss: 0.066599, acc: 0.992188] [adversarial loss: 0.370005, acc: 0.906250]\n",
      "562: [discriminator loss: 0.098373, acc: 0.968750] [adversarial loss: 0.069524, acc: 1.000000]\n",
      "563: [discriminator loss: 0.053818, acc: 0.992188] [adversarial loss: 0.284541, acc: 0.953125]\n",
      "564: [discriminator loss: 0.043876, acc: 0.992188] [adversarial loss: 0.342296, acc: 0.937500]\n",
      "565: [discriminator loss: 0.043130, acc: 1.000000] [adversarial loss: 1.016773, acc: 0.171875]\n",
      "566: [discriminator loss: 0.050672, acc: 0.992188] [adversarial loss: 0.793209, acc: 0.484375]\n",
      "567: [discriminator loss: 0.076171, acc: 0.968750] [adversarial loss: 1.328794, acc: 0.000000]\n",
      "568: [discriminator loss: 0.068363, acc: 0.976562] [adversarial loss: 1.787180, acc: 0.000000]\n",
      "569: [discriminator loss: 0.122221, acc: 0.953125] [adversarial loss: 0.067734, acc: 1.000000]\n",
      "570: [discriminator loss: 0.289390, acc: 0.835938] [adversarial loss: 9.598520, acc: 0.000000]\n",
      "571: [discriminator loss: 1.105969, acc: 0.632812] [adversarial loss: 0.045839, acc: 1.000000]\n",
      "572: [discriminator loss: 0.400274, acc: 0.804688] [adversarial loss: 3.244041, acc: 0.000000]\n",
      "573: [discriminator loss: 0.127921, acc: 0.960938] [adversarial loss: 1.674759, acc: 0.093750]\n",
      "574: [discriminator loss: 0.219696, acc: 0.945312] [adversarial loss: 5.367281, acc: 0.000000]\n",
      "575: [discriminator loss: 0.386951, acc: 0.859375] [adversarial loss: 0.799772, acc: 0.515625]\n",
      "576: [discriminator loss: 0.637703, acc: 0.617188] [adversarial loss: 8.920336, acc: 0.000000]\n",
      "577: [discriminator loss: 0.964975, acc: 0.687500] [adversarial loss: 1.064116, acc: 0.234375]\n",
      "578: [discriminator loss: 0.448855, acc: 0.671875] [adversarial loss: 6.555633, acc: 0.000000]\n",
      "579: [discriminator loss: 0.329258, acc: 0.828125] [adversarial loss: 2.510720, acc: 0.000000]\n",
      "580: [discriminator loss: 0.107680, acc: 0.984375] [adversarial loss: 3.212485, acc: 0.000000]\n",
      "581: [discriminator loss: 0.080714, acc: 0.984375] [adversarial loss: 3.024553, acc: 0.000000]\n",
      "582: [discriminator loss: 0.054647, acc: 0.992188] [adversarial loss: 3.107005, acc: 0.000000]\n",
      "583: [discriminator loss: 0.094154, acc: 0.960938] [adversarial loss: 2.260256, acc: 0.000000]\n",
      "584: [discriminator loss: 0.120062, acc: 0.976562] [adversarial loss: 3.242858, acc: 0.000000]\n",
      "585: [discriminator loss: 0.039803, acc: 1.000000] [adversarial loss: 2.828028, acc: 0.000000]\n",
      "586: [discriminator loss: 0.054228, acc: 0.992188] [adversarial loss: 3.292327, acc: 0.000000]\n",
      "587: [discriminator loss: 0.059974, acc: 0.984375] [adversarial loss: 3.285880, acc: 0.000000]\n",
      "588: [discriminator loss: 0.051224, acc: 0.992188] [adversarial loss: 3.554932, acc: 0.000000]\n",
      "589: [discriminator loss: 0.048387, acc: 0.992188] [adversarial loss: 3.557211, acc: 0.000000]\n",
      "590: [discriminator loss: 0.036110, acc: 1.000000] [adversarial loss: 4.605100, acc: 0.000000]\n",
      "591: [discriminator loss: 0.069617, acc: 0.968750] [adversarial loss: 2.217418, acc: 0.000000]\n",
      "592: [discriminator loss: 0.067786, acc: 1.000000] [adversarial loss: 9.160743, acc: 0.000000]\n",
      "593: [discriminator loss: 0.092446, acc: 0.968750] [adversarial loss: 3.881941, acc: 0.000000]\n",
      "594: [discriminator loss: 0.022014, acc: 1.000000] [adversarial loss: 4.909363, acc: 0.000000]\n",
      "595: [discriminator loss: 0.023199, acc: 0.992188] [adversarial loss: 4.584148, acc: 0.000000]\n",
      "596: [discriminator loss: 0.014779, acc: 1.000000] [adversarial loss: 4.505072, acc: 0.000000]\n",
      "597: [discriminator loss: 0.017737, acc: 1.000000] [adversarial loss: 4.959888, acc: 0.000000]\n",
      "598: [discriminator loss: 0.032476, acc: 1.000000] [adversarial loss: 6.907685, acc: 0.000000]\n",
      "599: [discriminator loss: 0.063821, acc: 0.968750] [adversarial loss: 2.456898, acc: 0.000000]\n",
      "600: [discriminator loss: 0.065218, acc: 1.000000] [adversarial loss: 8.528324, acc: 0.000000]\n",
      "601: [discriminator loss: 0.081165, acc: 0.976562] [adversarial loss: 3.344819, acc: 0.000000]\n",
      "602: [discriminator loss: 0.166436, acc: 0.929688] [adversarial loss: 10.360740, acc: 0.000000]\n",
      "603: [discriminator loss: 0.220958, acc: 0.906250] [adversarial loss: 1.123005, acc: 0.187500]\n",
      "604: [discriminator loss: 0.037095, acc: 0.992188] [adversarial loss: 1.003778, acc: 0.234375]\n",
      "605: [discriminator loss: 0.029767, acc: 0.992188] [adversarial loss: 0.418990, acc: 0.859375]\n",
      "606: [discriminator loss: 0.024799, acc: 1.000000] [adversarial loss: 0.689295, acc: 0.531250]\n",
      "607: [discriminator loss: 0.046955, acc: 0.976562] [adversarial loss: 0.096353, acc: 1.000000]\n",
      "608: [discriminator loss: 0.021459, acc: 0.992188] [adversarial loss: 0.146595, acc: 1.000000]\n",
      "609: [discriminator loss: 0.016385, acc: 1.000000] [adversarial loss: 0.113863, acc: 1.000000]\n",
      "610: [discriminator loss: 0.009465, acc: 1.000000] [adversarial loss: 0.079505, acc: 1.000000]\n",
      "611: [discriminator loss: 0.011383, acc: 1.000000] [adversarial loss: 0.105430, acc: 1.000000]\n",
      "612: [discriminator loss: 0.009564, acc: 1.000000] [adversarial loss: 0.072748, acc: 1.000000]\n",
      "613: [discriminator loss: 0.022790, acc: 0.992188] [adversarial loss: 0.056288, acc: 1.000000]\n",
      "614: [discriminator loss: 0.016242, acc: 1.000000] [adversarial loss: 0.042804, acc: 1.000000]\n",
      "615: [discriminator loss: 0.022054, acc: 0.992188] [adversarial loss: 0.412173, acc: 0.875000]\n",
      "616: [discriminator loss: 0.033924, acc: 0.976562] [adversarial loss: 0.009135, acc: 1.000000]\n",
      "617: [discriminator loss: 0.020284, acc: 1.000000] [adversarial loss: 0.064877, acc: 1.000000]\n",
      "618: [discriminator loss: 0.016153, acc: 1.000000] [adversarial loss: 0.048771, acc: 1.000000]\n",
      "619: [discriminator loss: 0.028900, acc: 0.984375] [adversarial loss: 0.002516, acc: 1.000000]\n",
      "620: [discriminator loss: 0.043950, acc: 0.992188] [adversarial loss: 0.603606, acc: 0.671875]\n",
      "621: [discriminator loss: 0.053406, acc: 0.984375] [adversarial loss: 0.816574, acc: 0.500000]\n",
      "622: [discriminator loss: 0.021900, acc: 1.000000] [adversarial loss: 2.678936, acc: 0.000000]\n",
      "623: [discriminator loss: 0.120170, acc: 0.960938] [adversarial loss: 0.166688, acc: 1.000000]\n",
      "624: [discriminator loss: 0.195680, acc: 0.929688] [adversarial loss: 14.978979, acc: 0.000000]\n",
      "625: [discriminator loss: 1.146311, acc: 0.593750] [adversarial loss: 0.426896, acc: 0.765625]\n",
      "626: [discriminator loss: 0.214236, acc: 0.914062] [adversarial loss: 5.905338, acc: 0.000000]\n",
      "627: [discriminator loss: 0.047330, acc: 0.984375] [adversarial loss: 4.514596, acc: 0.000000]\n",
      "628: [discriminator loss: 0.032595, acc: 0.992188] [adversarial loss: 3.574907, acc: 0.000000]\n",
      "629: [discriminator loss: 0.154338, acc: 0.945312] [adversarial loss: 7.854375, acc: 0.000000]\n",
      "630: [discriminator loss: 0.077494, acc: 0.976562] [adversarial loss: 4.581448, acc: 0.000000]\n",
      "631: [discriminator loss: 0.065599, acc: 0.992188] [adversarial loss: 3.646532, acc: 0.000000]\n",
      "632: [discriminator loss: 0.055349, acc: 0.992188] [adversarial loss: 3.312963, acc: 0.000000]\n",
      "633: [discriminator loss: 0.041926, acc: 0.992188] [adversarial loss: 3.766643, acc: 0.000000]\n",
      "634: [discriminator loss: 0.059565, acc: 0.984375] [adversarial loss: 1.938609, acc: 0.000000]\n",
      "635: [discriminator loss: 0.102939, acc: 0.976562] [adversarial loss: 3.756079, acc: 0.000000]\n",
      "636: [discriminator loss: 0.081032, acc: 0.968750] [adversarial loss: 0.289544, acc: 0.921875]\n",
      "637: [discriminator loss: 0.043093, acc: 0.992188] [adversarial loss: 0.194414, acc: 0.984375]\n",
      "638: [discriminator loss: 0.019488, acc: 0.992188] [adversarial loss: 0.174495, acc: 0.984375]\n",
      "639: [discriminator loss: 0.011553, acc: 1.000000] [adversarial loss: 0.154584, acc: 0.984375]\n",
      "640: [discriminator loss: 0.038061, acc: 0.984375] [adversarial loss: 0.053251, acc: 1.000000]\n",
      "641: [discriminator loss: 0.012966, acc: 1.000000] [adversarial loss: 0.097291, acc: 1.000000]\n",
      "642: [discriminator loss: 0.045093, acc: 0.992188] [adversarial loss: 0.041986, acc: 1.000000]\n",
      "643: [discriminator loss: 0.011568, acc: 1.000000] [adversarial loss: 0.045232, acc: 1.000000]\n",
      "644: [discriminator loss: 0.007296, acc: 1.000000] [adversarial loss: 0.052038, acc: 1.000000]\n",
      "645: [discriminator loss: 0.005665, acc: 1.000000] [adversarial loss: 0.049152, acc: 1.000000]\n",
      "646: [discriminator loss: 0.046061, acc: 0.984375] [adversarial loss: 0.016243, acc: 1.000000]\n",
      "647: [discriminator loss: 0.021301, acc: 0.992188] [adversarial loss: 0.047854, acc: 1.000000]\n",
      "648: [discriminator loss: 0.019348, acc: 1.000000] [adversarial loss: 0.039343, acc: 1.000000]\n",
      "649: [discriminator loss: 0.012037, acc: 1.000000] [adversarial loss: 0.036694, acc: 1.000000]\n",
      "650: [discriminator loss: 0.005266, acc: 1.000000] [adversarial loss: 0.062390, acc: 1.000000]\n",
      "651: [discriminator loss: 0.006834, acc: 1.000000] [adversarial loss: 0.041435, acc: 1.000000]\n",
      "652: [discriminator loss: 0.007614, acc: 1.000000] [adversarial loss: 0.070747, acc: 1.000000]\n",
      "653: [discriminator loss: 0.002614, acc: 1.000000] [adversarial loss: 0.074492, acc: 1.000000]\n",
      "654: [discriminator loss: 0.007375, acc: 0.992188] [adversarial loss: 0.033698, acc: 1.000000]\n",
      "655: [discriminator loss: 0.002694, acc: 1.000000] [adversarial loss: 0.020862, acc: 1.000000]\n",
      "656: [discriminator loss: 0.003779, acc: 1.000000] [adversarial loss: 0.009964, acc: 1.000000]\n",
      "657: [discriminator loss: 0.011183, acc: 0.992188] [adversarial loss: 0.006519, acc: 1.000000]\n",
      "658: [discriminator loss: 0.002542, acc: 1.000000] [adversarial loss: 0.015572, acc: 1.000000]\n",
      "659: [discriminator loss: 0.003395, acc: 1.000000] [adversarial loss: 0.033647, acc: 1.000000]\n",
      "660: [discriminator loss: 0.007654, acc: 1.000000] [adversarial loss: 0.007751, acc: 1.000000]\n",
      "661: [discriminator loss: 0.005431, acc: 1.000000] [adversarial loss: 0.014606, acc: 1.000000]\n",
      "662: [discriminator loss: 0.001428, acc: 1.000000] [adversarial loss: 0.020241, acc: 1.000000]\n",
      "663: [discriminator loss: 0.002338, acc: 1.000000] [adversarial loss: 0.022944, acc: 1.000000]\n",
      "664: [discriminator loss: 0.002073, acc: 1.000000] [adversarial loss: 0.009911, acc: 1.000000]\n",
      "665: [discriminator loss: 0.001985, acc: 1.000000] [adversarial loss: 0.009018, acc: 1.000000]\n",
      "666: [discriminator loss: 0.000657, acc: 1.000000] [adversarial loss: 0.008010, acc: 1.000000]\n",
      "667: [discriminator loss: 0.001604, acc: 1.000000] [adversarial loss: 0.011498, acc: 1.000000]\n",
      "668: [discriminator loss: 0.003193, acc: 1.000000] [adversarial loss: 0.015836, acc: 1.000000]\n",
      "669: [discriminator loss: 0.001384, acc: 1.000000] [adversarial loss: 0.008638, acc: 1.000000]\n",
      "670: [discriminator loss: 0.000480, acc: 1.000000] [adversarial loss: 0.005256, acc: 1.000000]\n",
      "671: [discriminator loss: 0.023886, acc: 0.992188] [adversarial loss: 0.000387, acc: 1.000000]\n",
      "672: [discriminator loss: 0.002310, acc: 1.000000] [adversarial loss: 0.003223, acc: 1.000000]\n",
      "673: [discriminator loss: 0.002925, acc: 1.000000] [adversarial loss: 0.002542, acc: 1.000000]\n",
      "674: [discriminator loss: 0.001280, acc: 1.000000] [adversarial loss: 0.007589, acc: 1.000000]\n",
      "675: [discriminator loss: 0.001907, acc: 1.000000] [adversarial loss: 0.004558, acc: 1.000000]\n",
      "676: [discriminator loss: 0.027743, acc: 0.992188] [adversarial loss: 0.000377, acc: 1.000000]\n",
      "677: [discriminator loss: 0.002971, acc: 1.000000] [adversarial loss: 0.003239, acc: 1.000000]\n",
      "678: [discriminator loss: 0.005275, acc: 1.000000] [adversarial loss: 0.000842, acc: 1.000000]\n",
      "679: [discriminator loss: 0.001229, acc: 1.000000] [adversarial loss: 0.002450, acc: 1.000000]\n",
      "680: [discriminator loss: 0.001168, acc: 1.000000] [adversarial loss: 0.004507, acc: 1.000000]\n",
      "681: [discriminator loss: 0.003556, acc: 1.000000] [adversarial loss: 0.212754, acc: 0.984375]\n",
      "682: [discriminator loss: 0.000446, acc: 1.000000] [adversarial loss: 0.002063, acc: 1.000000]\n",
      "683: [discriminator loss: 0.000088, acc: 1.000000] [adversarial loss: 0.001042, acc: 1.000000]\n",
      "684: [discriminator loss: 0.000141, acc: 1.000000] [adversarial loss: 0.001102, acc: 1.000000]\n",
      "685: [discriminator loss: 0.000101, acc: 1.000000] [adversarial loss: 0.000617, acc: 1.000000]\n",
      "686: [discriminator loss: 0.001728, acc: 1.000000] [adversarial loss: 0.000172, acc: 1.000000]\n",
      "687: [discriminator loss: 0.000677, acc: 1.000000] [adversarial loss: 0.000075, acc: 1.000000]\n",
      "688: [discriminator loss: 0.001064, acc: 1.000000] [adversarial loss: 0.000025, acc: 1.000000]\n",
      "689: [discriminator loss: 0.001153, acc: 1.000000] [adversarial loss: 0.000008, acc: 1.000000]\n",
      "690: [discriminator loss: 0.000326, acc: 1.000000] [adversarial loss: 0.000008, acc: 1.000000]\n",
      "691: [discriminator loss: 0.000221, acc: 1.000000] [adversarial loss: 0.000010, acc: 1.000000]\n",
      "692: [discriminator loss: 0.000302, acc: 1.000000] [adversarial loss: 0.000014, acc: 1.000000]\n",
      "693: [discriminator loss: 0.000198, acc: 1.000000] [adversarial loss: 0.000017, acc: 1.000000]\n",
      "694: [discriminator loss: 0.000154, acc: 1.000000] [adversarial loss: 0.000014, acc: 1.000000]\n",
      "695: [discriminator loss: 0.000235, acc: 1.000000] [adversarial loss: 0.000025, acc: 1.000000]\n",
      "696: [discriminator loss: 0.000235, acc: 1.000000] [adversarial loss: 0.000025, acc: 1.000000]\n",
      "697: [discriminator loss: 0.000496, acc: 1.000000] [adversarial loss: 0.000089, acc: 1.000000]\n",
      "698: [discriminator loss: 0.000170, acc: 1.000000] [adversarial loss: 0.000073, acc: 1.000000]\n",
      "699: [discriminator loss: 0.013831, acc: 0.992188] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "700: [discriminator loss: 0.005103, acc: 1.000000] [adversarial loss: 0.000074, acc: 1.000000]\n",
      "701: [discriminator loss: 0.000218, acc: 1.000000] [adversarial loss: 0.000089, acc: 1.000000]\n",
      "702: [discriminator loss: 0.000194, acc: 1.000000] [adversarial loss: 0.000128, acc: 1.000000]\n",
      "703: [discriminator loss: 0.000273, acc: 1.000000] [adversarial loss: 0.000185, acc: 1.000000]\n",
      "704: [discriminator loss: 0.002296, acc: 1.000000] [adversarial loss: 0.000004, acc: 1.000000]\n",
      "705: [discriminator loss: 0.000573, acc: 1.000000] [adversarial loss: 0.000010, acc: 1.000000]\n",
      "706: [discriminator loss: 0.000582, acc: 1.000000] [adversarial loss: 0.000052, acc: 1.000000]\n",
      "707: [discriminator loss: 0.000569, acc: 1.000000] [adversarial loss: 0.000336, acc: 1.000000]\n",
      "708: [discriminator loss: 0.000268, acc: 1.000000] [adversarial loss: 0.000512, acc: 1.000000]\n",
      "709: [discriminator loss: 0.000577, acc: 1.000000] [adversarial loss: 0.003339, acc: 1.000000]\n",
      "710: [discriminator loss: 0.000733, acc: 1.000000] [adversarial loss: 0.016915, acc: 1.000000]\n",
      "711: [discriminator loss: 0.007308, acc: 1.000000] [adversarial loss: 11.669692, acc: 0.000000]\n",
      "712: [discriminator loss: 0.256091, acc: 0.929688] [adversarial loss: 0.000055, acc: 1.000000]\n",
      "713: [discriminator loss: 0.000025, acc: 1.000000] [adversarial loss: 0.000035, acc: 1.000000]\n",
      "714: [discriminator loss: 0.000026, acc: 1.000000] [adversarial loss: 0.000045, acc: 1.000000]\n",
      "715: [discriminator loss: 0.000027, acc: 1.000000] [adversarial loss: 0.000038, acc: 1.000000]\n",
      "716: [discriminator loss: 0.000027, acc: 1.000000] [adversarial loss: 0.000048, acc: 1.000000]\n",
      "717: [discriminator loss: 0.000030, acc: 1.000000] [adversarial loss: 0.000045, acc: 1.000000]\n",
      "718: [discriminator loss: 0.000028, acc: 1.000000] [adversarial loss: 0.000055, acc: 1.000000]\n",
      "719: [discriminator loss: 0.000031, acc: 1.000000] [adversarial loss: 0.000050, acc: 1.000000]\n",
      "720: [discriminator loss: 0.000032, acc: 1.000000] [adversarial loss: 0.000033, acc: 1.000000]\n",
      "721: [discriminator loss: 0.000031, acc: 1.000000] [adversarial loss: 0.000048, acc: 1.000000]\n",
      "722: [discriminator loss: 0.000083, acc: 1.000000] [adversarial loss: 0.000025, acc: 1.000000]\n",
      "723: [discriminator loss: 0.000034, acc: 1.000000] [adversarial loss: 0.000055, acc: 1.000000]\n",
      "724: [discriminator loss: 0.000036, acc: 1.000000] [adversarial loss: 0.000045, acc: 1.000000]\n",
      "725: [discriminator loss: 0.000038, acc: 1.000000] [adversarial loss: 0.000038, acc: 1.000000]\n",
      "726: [discriminator loss: 0.000039, acc: 1.000000] [adversarial loss: 0.000041, acc: 1.000000]\n",
      "727: [discriminator loss: 0.000045, acc: 1.000000] [adversarial loss: 0.000050, acc: 1.000000]\n",
      "728: [discriminator loss: 0.000042, acc: 1.000000] [adversarial loss: 0.000035, acc: 1.000000]\n",
      "729: [discriminator loss: 0.000052, acc: 1.000000] [adversarial loss: 0.000025, acc: 1.000000]\n",
      "730: [discriminator loss: 0.000047, acc: 1.000000] [adversarial loss: 0.000037, acc: 1.000000]\n",
      "731: [discriminator loss: 0.000054, acc: 1.000000] [adversarial loss: 0.000029, acc: 1.000000]\n",
      "732: [discriminator loss: 0.000055, acc: 1.000000] [adversarial loss: 0.000040, acc: 1.000000]\n",
      "733: [discriminator loss: 0.000062, acc: 1.000000] [adversarial loss: 0.000058, acc: 1.000000]\n",
      "734: [discriminator loss: 0.000068, acc: 1.000000] [adversarial loss: 0.000043, acc: 1.000000]\n",
      "735: [discriminator loss: 0.000075, acc: 1.000000] [adversarial loss: 0.000043, acc: 1.000000]\n",
      "736: [discriminator loss: 0.000072, acc: 1.000000] [adversarial loss: 0.000046, acc: 1.000000]\n",
      "737: [discriminator loss: 0.000079, acc: 1.000000] [adversarial loss: 0.000037, acc: 1.000000]\n",
      "738: [discriminator loss: 0.000093, acc: 1.000000] [adversarial loss: 0.000045, acc: 1.000000]\n",
      "739: [discriminator loss: 0.000095, acc: 1.000000] [adversarial loss: 0.000050, acc: 1.000000]\n",
      "740: [discriminator loss: 0.000109, acc: 1.000000] [adversarial loss: 0.000049, acc: 1.000000]\n",
      "741: [discriminator loss: 0.000108, acc: 1.000000] [adversarial loss: 0.000041, acc: 1.000000]\n",
      "742: [discriminator loss: 0.000120, acc: 1.000000] [adversarial loss: 0.000034, acc: 1.000000]\n",
      "743: [discriminator loss: 0.000141, acc: 1.000000] [adversarial loss: 0.000056, acc: 1.000000]\n",
      "744: [discriminator loss: 0.000157, acc: 1.000000] [adversarial loss: 0.000041, acc: 1.000000]\n",
      "745: [discriminator loss: 0.000175, acc: 1.000000] [adversarial loss: 0.000040, acc: 1.000000]\n",
      "746: [discriminator loss: 0.000200, acc: 1.000000] [adversarial loss: 0.000071, acc: 1.000000]\n",
      "747: [discriminator loss: 0.000196, acc: 1.000000] [adversarial loss: 0.000063, acc: 1.000000]\n",
      "748: [discriminator loss: 0.000198, acc: 1.000000] [adversarial loss: 0.000072, acc: 1.000000]\n",
      "749: [discriminator loss: 0.000225, acc: 1.000000] [adversarial loss: 0.000060, acc: 1.000000]\n",
      "750: [discriminator loss: 0.000217, acc: 1.000000] [adversarial loss: 0.000141, acc: 1.000000]\n",
      "751: [discriminator loss: 0.000292, acc: 1.000000] [adversarial loss: 0.000085, acc: 1.000000]\n",
      "752: [discriminator loss: 0.000312, acc: 1.000000] [adversarial loss: 0.000102, acc: 1.000000]\n",
      "753: [discriminator loss: 0.000293, acc: 1.000000] [adversarial loss: 0.000141, acc: 1.000000]\n",
      "754: [discriminator loss: 0.000345, acc: 1.000000] [adversarial loss: 0.000152, acc: 1.000000]\n",
      "755: [discriminator loss: 0.000311, acc: 1.000000] [adversarial loss: 0.000181, acc: 1.000000]\n",
      "756: [discriminator loss: 0.000370, acc: 1.000000] [adversarial loss: 0.000326, acc: 1.000000]\n",
      "757: [discriminator loss: 0.000443, acc: 1.000000] [adversarial loss: 0.000396, acc: 1.000000]\n",
      "758: [discriminator loss: 0.000626, acc: 1.000000] [adversarial loss: 0.000817, acc: 1.000000]\n",
      "759: [discriminator loss: 0.000704, acc: 1.000000] [adversarial loss: 0.002451, acc: 1.000000]\n",
      "760: [discriminator loss: 0.004861, acc: 1.000000] [adversarial loss: 1.062725, acc: 0.375000]\n",
      "761: [discriminator loss: 2.147281, acc: 0.500000] [adversarial loss: 22.012512, acc: 0.000000]\n",
      "762: [discriminator loss: 1.871903, acc: 0.664062] [adversarial loss: 4.179253, acc: 0.000000]\n",
      "763: [discriminator loss: 0.008961, acc: 1.000000] [adversarial loss: 0.537178, acc: 0.750000]\n",
      "764: [discriminator loss: 0.033516, acc: 1.000000] [adversarial loss: 0.252380, acc: 0.921875]\n",
      "765: [discriminator loss: 0.045539, acc: 0.992188] [adversarial loss: 0.246567, acc: 0.937500]\n",
      "766: [discriminator loss: 0.041842, acc: 0.992188] [adversarial loss: 0.262078, acc: 0.937500]\n",
      "767: [discriminator loss: 0.081032, acc: 0.960938] [adversarial loss: 0.909388, acc: 0.484375]\n",
      "768: [discriminator loss: 0.079814, acc: 0.976562] [adversarial loss: 1.146257, acc: 0.281250]\n",
      "769: [discriminator loss: 0.132562, acc: 0.960938] [adversarial loss: 0.149941, acc: 0.953125]\n",
      "770: [discriminator loss: 0.143936, acc: 0.945312] [adversarial loss: 1.484864, acc: 0.218750]\n",
      "771: [discriminator loss: 0.082249, acc: 0.968750] [adversarial loss: 0.318502, acc: 0.843750]\n",
      "772: [discriminator loss: 0.162873, acc: 0.945312] [adversarial loss: 1.519213, acc: 0.218750]\n",
      "773: [discriminator loss: 0.106130, acc: 0.937500] [adversarial loss: 0.234434, acc: 0.921875]\n",
      "774: [discriminator loss: 0.085440, acc: 0.953125] [adversarial loss: 0.816504, acc: 0.593750]\n",
      "775: [discriminator loss: 0.074977, acc: 0.976562] [adversarial loss: 0.370156, acc: 0.796875]\n",
      "776: [discriminator loss: 0.031508, acc: 0.992188] [adversarial loss: 0.248541, acc: 0.906250]\n",
      "777: [discriminator loss: 0.063330, acc: 0.976562] [adversarial loss: 0.299735, acc: 0.890625]\n",
      "778: [discriminator loss: 0.056095, acc: 0.976562] [adversarial loss: 0.576120, acc: 0.671875]\n",
      "779: [discriminator loss: 0.036633, acc: 0.976562] [adversarial loss: 0.060456, acc: 1.000000]\n",
      "780: [discriminator loss: 0.042512, acc: 0.984375] [adversarial loss: 0.253906, acc: 0.843750]\n",
      "781: [discriminator loss: 0.096439, acc: 0.968750] [adversarial loss: 0.259494, acc: 0.875000]\n",
      "782: [discriminator loss: 0.192275, acc: 0.953125] [adversarial loss: 0.004735, acc: 1.000000]\n",
      "783: [discriminator loss: 0.109477, acc: 0.937500] [adversarial loss: 0.845441, acc: 0.593750]\n",
      "784: [discriminator loss: 0.114347, acc: 0.976562] [adversarial loss: 0.015293, acc: 1.000000]\n",
      "785: [discriminator loss: 0.126417, acc: 0.937500] [adversarial loss: 0.601173, acc: 0.718750]\n",
      "786: [discriminator loss: 0.234068, acc: 0.898438] [adversarial loss: 3.224964, acc: 0.078125]\n",
      "787: [discriminator loss: 0.234731, acc: 0.890625] [adversarial loss: 0.000322, acc: 1.000000]\n",
      "788: [discriminator loss: 0.000479, acc: 1.000000] [adversarial loss: 0.005909, acc: 1.000000]\n",
      "789: [discriminator loss: 0.029166, acc: 0.984375] [adversarial loss: 0.004206, acc: 1.000000]\n",
      "790: [discriminator loss: 0.010386, acc: 1.000000] [adversarial loss: 0.000680, acc: 1.000000]\n",
      "791: [discriminator loss: 0.006288, acc: 1.000000] [adversarial loss: 0.000683, acc: 1.000000]\n",
      "792: [discriminator loss: 0.026285, acc: 0.992188] [adversarial loss: 0.003716, acc: 1.000000]\n",
      "793: [discriminator loss: 0.035983, acc: 0.992188] [adversarial loss: 0.004347, acc: 1.000000]\n",
      "794: [discriminator loss: 0.036302, acc: 0.984375] [adversarial loss: 0.018495, acc: 1.000000]\n",
      "795: [discriminator loss: 0.100327, acc: 0.976562] [adversarial loss: 0.010524, acc: 1.000000]\n",
      "796: [discriminator loss: 0.064262, acc: 0.976562] [adversarial loss: 0.024918, acc: 1.000000]\n",
      "797: [discriminator loss: 0.076997, acc: 0.968750] [adversarial loss: 0.069376, acc: 0.953125]\n",
      "798: [discriminator loss: 0.208559, acc: 0.929688] [adversarial loss: 2.788737, acc: 0.125000]\n",
      "799: [discriminator loss: 0.213765, acc: 0.914062] [adversarial loss: 0.000106, acc: 1.000000]\n",
      "800: [discriminator loss: 0.369876, acc: 0.890625] [adversarial loss: 0.940944, acc: 0.531250]\n",
      "801: [discriminator loss: 0.423273, acc: 0.835938] [adversarial loss: 4.148181, acc: 0.000000]\n",
      "802: [discriminator loss: 0.464887, acc: 0.843750] [adversarial loss: 0.001170, acc: 1.000000]\n",
      "803: [discriminator loss: 0.022751, acc: 0.984375] [adversarial loss: 0.001518, acc: 1.000000]\n",
      "804: [discriminator loss: 0.035420, acc: 0.984375] [adversarial loss: 0.010086, acc: 1.000000]\n",
      "805: [discriminator loss: 0.018642, acc: 0.992188] [adversarial loss: 0.032591, acc: 0.984375]\n",
      "806: [discriminator loss: 0.085312, acc: 0.953125] [adversarial loss: 0.037538, acc: 1.000000]\n",
      "807: [discriminator loss: 0.055387, acc: 0.968750] [adversarial loss: 0.105834, acc: 0.984375]\n",
      "808: [discriminator loss: 0.210907, acc: 0.929688] [adversarial loss: 1.006287, acc: 0.343750]\n",
      "809: [discriminator loss: 0.425949, acc: 0.828125] [adversarial loss: 6.328991, acc: 0.000000]\n",
      "810: [discriminator loss: 0.590531, acc: 0.820312] [adversarial loss: 0.014616, acc: 1.000000]\n",
      "811: [discriminator loss: 0.511152, acc: 0.820312] [adversarial loss: 4.766585, acc: 0.000000]\n",
      "812: [discriminator loss: 0.376041, acc: 0.843750] [adversarial loss: 0.085920, acc: 1.000000]\n",
      "813: [discriminator loss: 0.328362, acc: 0.867188] [adversarial loss: 3.323578, acc: 0.078125]\n",
      "814: [discriminator loss: 0.419885, acc: 0.820312] [adversarial loss: 0.058014, acc: 1.000000]\n",
      "815: [discriminator loss: 0.590220, acc: 0.726562] [adversarial loss: 5.245323, acc: 0.000000]\n",
      "816: [discriminator loss: 0.585817, acc: 0.765625] [adversarial loss: 0.242555, acc: 0.921875]\n",
      "817: [discriminator loss: 0.319573, acc: 0.843750] [adversarial loss: 1.798435, acc: 0.296875]\n",
      "818: [discriminator loss: 0.390574, acc: 0.843750] [adversarial loss: 0.921934, acc: 0.515625]\n",
      "819: [discriminator loss: 0.352327, acc: 0.890625] [adversarial loss: 2.533272, acc: 0.078125]\n",
      "820: [discriminator loss: 0.336556, acc: 0.859375] [adversarial loss: 0.342600, acc: 0.875000]\n",
      "821: [discriminator loss: 0.302305, acc: 0.890625] [adversarial loss: 1.871087, acc: 0.093750]\n",
      "822: [discriminator loss: 0.196821, acc: 0.898438] [adversarial loss: 0.596069, acc: 0.640625]\n",
      "823: [discriminator loss: 0.254110, acc: 0.906250] [adversarial loss: 3.384018, acc: 0.015625]\n",
      "824: [discriminator loss: 0.333927, acc: 0.859375] [adversarial loss: 0.339071, acc: 0.859375]\n",
      "825: [discriminator loss: 0.485475, acc: 0.789062] [adversarial loss: 5.321266, acc: 0.000000]\n",
      "826: [discriminator loss: 0.411242, acc: 0.828125] [adversarial loss: 0.673092, acc: 0.640625]\n",
      "827: [discriminator loss: 0.358183, acc: 0.828125] [adversarial loss: 4.612427, acc: 0.000000]\n",
      "828: [discriminator loss: 0.474476, acc: 0.851562] [adversarial loss: 1.089481, acc: 0.265625]\n",
      "829: [discriminator loss: 0.295809, acc: 0.898438] [adversarial loss: 3.605023, acc: 0.000000]\n",
      "830: [discriminator loss: 0.180031, acc: 0.929688] [adversarial loss: 1.235592, acc: 0.375000]\n",
      "831: [discriminator loss: 0.178949, acc: 0.937500] [adversarial loss: 0.649476, acc: 0.609375]\n",
      "832: [discriminator loss: 0.196671, acc: 0.898438] [adversarial loss: 0.392603, acc: 0.781250]\n",
      "833: [discriminator loss: 0.400375, acc: 0.867188] [adversarial loss: 0.834160, acc: 0.562500]\n",
      "834: [discriminator loss: 0.385256, acc: 0.867188] [adversarial loss: 0.138294, acc: 0.953125]\n",
      "835: [discriminator loss: 0.349065, acc: 0.859375] [adversarial loss: 0.309808, acc: 0.859375]\n",
      "836: [discriminator loss: 0.396449, acc: 0.843750] [adversarial loss: 0.197931, acc: 0.968750]\n",
      "837: [discriminator loss: 0.201278, acc: 0.914062] [adversarial loss: 0.078362, acc: 1.000000]\n",
      "838: [discriminator loss: 0.229813, acc: 0.914062] [adversarial loss: 0.041926, acc: 1.000000]\n",
      "839: [discriminator loss: 0.232990, acc: 0.898438] [adversarial loss: 0.260556, acc: 0.937500]\n",
      "840: [discriminator loss: 0.260420, acc: 0.921875] [adversarial loss: 0.063589, acc: 1.000000]\n",
      "841: [discriminator loss: 0.251861, acc: 0.914062] [adversarial loss: 0.066250, acc: 1.000000]\n",
      "842: [discriminator loss: 0.116714, acc: 0.960938] [adversarial loss: 0.097822, acc: 0.984375]\n",
      "843: [discriminator loss: 0.132937, acc: 0.945312] [adversarial loss: 0.049082, acc: 1.000000]\n",
      "844: [discriminator loss: 0.402222, acc: 0.851562] [adversarial loss: 1.055219, acc: 0.359375]\n",
      "845: [discriminator loss: 0.367140, acc: 0.835938] [adversarial loss: 0.004609, acc: 1.000000]\n",
      "846: [discriminator loss: 0.478420, acc: 0.796875] [adversarial loss: 1.021284, acc: 0.359375]\n",
      "847: [discriminator loss: 0.364807, acc: 0.804688] [adversarial loss: 0.024320, acc: 1.000000]\n",
      "848: [discriminator loss: 0.684747, acc: 0.726562] [adversarial loss: 1.833515, acc: 0.046875]\n",
      "849: [discriminator loss: 0.519761, acc: 0.773438] [adversarial loss: 0.055946, acc: 1.000000]\n",
      "850: [discriminator loss: 0.647859, acc: 0.710938] [adversarial loss: 0.994406, acc: 0.343750]\n",
      "851: [discriminator loss: 0.481033, acc: 0.812500] [adversarial loss: 0.771501, acc: 0.468750]\n",
      "852: [discriminator loss: 0.543189, acc: 0.765625] [adversarial loss: 2.365899, acc: 0.062500]\n",
      "853: [discriminator loss: 0.498809, acc: 0.750000] [adversarial loss: 0.169159, acc: 0.953125]\n",
      "854: [discriminator loss: 0.428389, acc: 0.804688] [adversarial loss: 1.706628, acc: 0.125000]\n",
      "855: [discriminator loss: 0.337915, acc: 0.843750] [adversarial loss: 0.790112, acc: 0.453125]\n",
      "856: [discriminator loss: 0.358937, acc: 0.859375] [adversarial loss: 1.447295, acc: 0.156250]\n",
      "857: [discriminator loss: 0.370217, acc: 0.843750] [adversarial loss: 0.567983, acc: 0.671875]\n",
      "858: [discriminator loss: 0.340086, acc: 0.867188] [adversarial loss: 1.204941, acc: 0.265625]\n",
      "859: [discriminator loss: 0.360985, acc: 0.820312] [adversarial loss: 0.193717, acc: 0.953125]\n",
      "860: [discriminator loss: 0.413434, acc: 0.804688] [adversarial loss: 1.689097, acc: 0.156250]\n",
      "861: [discriminator loss: 0.419846, acc: 0.828125] [adversarial loss: 0.419139, acc: 0.843750]\n",
      "862: [discriminator loss: 0.380024, acc: 0.828125] [adversarial loss: 0.980346, acc: 0.390625]\n",
      "863: [discriminator loss: 0.324689, acc: 0.867188] [adversarial loss: 0.233757, acc: 0.953125]\n",
      "864: [discriminator loss: 0.293823, acc: 0.851562] [adversarial loss: 0.669656, acc: 0.578125]\n",
      "865: [discriminator loss: 0.198456, acc: 0.921875] [adversarial loss: 0.326353, acc: 0.859375]\n",
      "866: [discriminator loss: 0.187053, acc: 0.937500] [adversarial loss: 0.178991, acc: 1.000000]\n",
      "867: [discriminator loss: 0.269451, acc: 0.875000] [adversarial loss: 0.316549, acc: 0.921875]\n",
      "868: [discriminator loss: 0.251906, acc: 0.882812] [adversarial loss: 0.133747, acc: 0.984375]\n",
      "869: [discriminator loss: 0.204167, acc: 0.929688] [adversarial loss: 0.294669, acc: 0.906250]\n",
      "870: [discriminator loss: 0.239902, acc: 0.906250] [adversarial loss: 0.176331, acc: 0.968750]\n",
      "871: [discriminator loss: 0.232678, acc: 0.921875] [adversarial loss: 0.158306, acc: 0.968750]\n",
      "872: [discriminator loss: 0.281592, acc: 0.898438] [adversarial loss: 0.933141, acc: 0.390625]\n",
      "873: [discriminator loss: 0.330890, acc: 0.820312] [adversarial loss: 0.019689, acc: 1.000000]\n",
      "874: [discriminator loss: 0.471114, acc: 0.835938] [adversarial loss: 0.734784, acc: 0.531250]\n",
      "875: [discriminator loss: 0.425029, acc: 0.757812] [adversarial loss: 0.008872, acc: 1.000000]\n",
      "876: [discriminator loss: 0.273057, acc: 0.875000] [adversarial loss: 0.084980, acc: 1.000000]\n",
      "877: [discriminator loss: 0.216940, acc: 0.921875] [adversarial loss: 0.131963, acc: 0.984375]\n",
      "878: [discriminator loss: 0.353190, acc: 0.835938] [adversarial loss: 0.598078, acc: 0.671875]\n",
      "879: [discriminator loss: 0.304177, acc: 0.867188] [adversarial loss: 0.067689, acc: 1.000000]\n",
      "880: [discriminator loss: 0.409067, acc: 0.820312] [adversarial loss: 0.860698, acc: 0.437500]\n",
      "881: [discriminator loss: 0.467995, acc: 0.812500] [adversarial loss: 0.040267, acc: 1.000000]\n",
      "882: [discriminator loss: 0.582653, acc: 0.750000] [adversarial loss: 1.248843, acc: 0.187500]\n",
      "883: [discriminator loss: 0.582874, acc: 0.734375] [adversarial loss: 0.100396, acc: 1.000000]\n",
      "884: [discriminator loss: 0.577708, acc: 0.710938] [adversarial loss: 1.479086, acc: 0.125000]\n",
      "885: [discriminator loss: 0.486774, acc: 0.734375] [adversarial loss: 0.127918, acc: 1.000000]\n",
      "886: [discriminator loss: 0.249783, acc: 0.921875] [adversarial loss: 0.192142, acc: 0.968750]\n",
      "887: [discriminator loss: 0.208682, acc: 0.945312] [adversarial loss: 0.181529, acc: 0.953125]\n",
      "888: [discriminator loss: 0.261369, acc: 0.898438] [adversarial loss: 0.218090, acc: 0.968750]\n",
      "889: [discriminator loss: 0.331276, acc: 0.835938] [adversarial loss: 0.279196, acc: 0.859375]\n",
      "890: [discriminator loss: 0.217164, acc: 0.945312] [adversarial loss: 0.199924, acc: 0.953125]\n",
      "891: [discriminator loss: 0.275637, acc: 0.898438] [adversarial loss: 0.330990, acc: 0.906250]\n",
      "892: [discriminator loss: 0.214515, acc: 0.906250] [adversarial loss: 0.284639, acc: 0.937500]\n",
      "893: [discriminator loss: 0.303471, acc: 0.867188] [adversarial loss: 0.363024, acc: 0.843750]\n",
      "894: [discriminator loss: 0.267766, acc: 0.898438] [adversarial loss: 0.101394, acc: 1.000000]\n",
      "895: [discriminator loss: 0.425627, acc: 0.828125] [adversarial loss: 0.722933, acc: 0.609375]\n",
      "896: [discriminator loss: 0.297628, acc: 0.843750] [adversarial loss: 0.028100, acc: 1.000000]\n",
      "897: [discriminator loss: 0.156935, acc: 0.929688] [adversarial loss: 0.081515, acc: 1.000000]\n",
      "898: [discriminator loss: 0.204607, acc: 0.921875] [adversarial loss: 0.187277, acc: 1.000000]\n",
      "899: [discriminator loss: 0.262844, acc: 0.937500] [adversarial loss: 0.156924, acc: 1.000000]\n",
      "900: [discriminator loss: 0.221409, acc: 0.945312] [adversarial loss: 0.158967, acc: 0.968750]\n",
      "901: [discriminator loss: 0.238137, acc: 0.921875] [adversarial loss: 0.415744, acc: 0.781250]\n",
      "902: [discriminator loss: 0.304934, acc: 0.875000] [adversarial loss: 0.190983, acc: 0.984375]\n",
      "903: [discriminator loss: 0.305774, acc: 0.898438] [adversarial loss: 0.578084, acc: 0.734375]\n",
      "904: [discriminator loss: 0.452642, acc: 0.796875] [adversarial loss: 0.806427, acc: 0.500000]\n",
      "905: [discriminator loss: 0.318104, acc: 0.843750] [adversarial loss: 0.021868, acc: 1.000000]\n",
      "906: [discriminator loss: 0.681897, acc: 0.640625] [adversarial loss: 2.196673, acc: 0.015625]\n",
      "907: [discriminator loss: 0.641413, acc: 0.687500] [adversarial loss: 0.087588, acc: 1.000000]\n",
      "908: [discriminator loss: 0.732773, acc: 0.601562] [adversarial loss: 1.476422, acc: 0.062500]\n",
      "909: [discriminator loss: 0.486770, acc: 0.734375] [adversarial loss: 0.259729, acc: 0.968750]\n",
      "910: [discriminator loss: 0.414666, acc: 0.765625] [adversarial loss: 0.933993, acc: 0.312500]\n",
      "911: [discriminator loss: 0.338029, acc: 0.835938] [adversarial loss: 0.313637, acc: 0.890625]\n",
      "912: [discriminator loss: 0.590402, acc: 0.632812] [adversarial loss: 1.729992, acc: 0.000000]\n",
      "913: [discriminator loss: 0.465122, acc: 0.742188] [adversarial loss: 0.262869, acc: 0.968750]\n",
      "914: [discriminator loss: 0.409167, acc: 0.804688] [adversarial loss: 0.934529, acc: 0.359375]\n",
      "915: [discriminator loss: 0.310065, acc: 0.890625] [adversarial loss: 0.457856, acc: 0.875000]\n",
      "916: [discriminator loss: 0.339694, acc: 0.867188] [adversarial loss: 1.363652, acc: 0.031250]\n",
      "917: [discriminator loss: 0.413517, acc: 0.820312] [adversarial loss: 0.468270, acc: 0.796875]\n",
      "918: [discriminator loss: 0.412641, acc: 0.789062] [adversarial loss: 1.447781, acc: 0.062500]\n",
      "919: [discriminator loss: 0.364872, acc: 0.835938] [adversarial loss: 0.298356, acc: 0.953125]\n",
      "920: [discriminator loss: 0.432641, acc: 0.804688] [adversarial loss: 1.468319, acc: 0.031250]\n",
      "921: [discriminator loss: 0.388734, acc: 0.796875] [adversarial loss: 0.570917, acc: 0.656250]\n",
      "922: [discriminator loss: 0.432723, acc: 0.773438] [adversarial loss: 1.692190, acc: 0.046875]\n",
      "923: [discriminator loss: 0.446626, acc: 0.773438] [adversarial loss: 0.345185, acc: 0.859375]\n",
      "924: [discriminator loss: 0.477679, acc: 0.750000] [adversarial loss: 1.652229, acc: 0.031250]\n",
      "925: [discriminator loss: 0.440278, acc: 0.773438] [adversarial loss: 0.575334, acc: 0.671875]\n",
      "926: [discriminator loss: 0.462863, acc: 0.765625] [adversarial loss: 2.073088, acc: 0.000000]\n",
      "927: [discriminator loss: 0.420823, acc: 0.789062] [adversarial loss: 0.426156, acc: 0.828125]\n",
      "928: [discriminator loss: 0.470785, acc: 0.742188] [adversarial loss: 1.676107, acc: 0.046875]\n",
      "929: [discriminator loss: 0.371072, acc: 0.882812] [adversarial loss: 0.958134, acc: 0.296875]\n",
      "930: [discriminator loss: 0.458302, acc: 0.796875] [adversarial loss: 2.327074, acc: 0.000000]\n",
      "931: [discriminator loss: 0.469781, acc: 0.765625] [adversarial loss: 0.858094, acc: 0.453125]\n",
      "932: [discriminator loss: 0.432732, acc: 0.781250] [adversarial loss: 1.815719, acc: 0.031250]\n",
      "933: [discriminator loss: 0.349779, acc: 0.843750] [adversarial loss: 0.670771, acc: 0.625000]\n",
      "934: [discriminator loss: 0.330548, acc: 0.882812] [adversarial loss: 1.097463, acc: 0.218750]\n",
      "935: [discriminator loss: 0.409161, acc: 0.851562] [adversarial loss: 0.747890, acc: 0.453125]\n",
      "936: [discriminator loss: 0.559477, acc: 0.726562] [adversarial loss: 2.220203, acc: 0.000000]\n",
      "937: [discriminator loss: 0.532497, acc: 0.757812] [adversarial loss: 0.593882, acc: 0.687500]\n",
      "938: [discriminator loss: 0.503643, acc: 0.695312] [adversarial loss: 2.049815, acc: 0.000000]\n",
      "939: [discriminator loss: 0.545677, acc: 0.742188] [adversarial loss: 0.885515, acc: 0.250000]\n",
      "940: [discriminator loss: 0.475876, acc: 0.781250] [adversarial loss: 2.065851, acc: 0.046875]\n",
      "941: [discriminator loss: 0.403886, acc: 0.812500] [adversarial loss: 1.080455, acc: 0.218750]\n",
      "942: [discriminator loss: 0.378690, acc: 0.851562] [adversarial loss: 1.659027, acc: 0.093750]\n",
      "943: [discriminator loss: 0.345564, acc: 0.859375] [adversarial loss: 1.216109, acc: 0.234375]\n",
      "944: [discriminator loss: 0.392865, acc: 0.820312] [adversarial loss: 1.429468, acc: 0.125000]\n",
      "945: [discriminator loss: 0.435265, acc: 0.804688] [adversarial loss: 0.951344, acc: 0.390625]\n",
      "946: [discriminator loss: 0.420390, acc: 0.796875] [adversarial loss: 2.106426, acc: 0.015625]\n",
      "947: [discriminator loss: 0.434722, acc: 0.812500] [adversarial loss: 0.487706, acc: 0.718750]\n",
      "948: [discriminator loss: 0.581889, acc: 0.656250] [adversarial loss: 2.036662, acc: 0.000000]\n",
      "949: [discriminator loss: 0.434930, acc: 0.828125] [adversarial loss: 0.715210, acc: 0.515625]\n",
      "950: [discriminator loss: 0.493276, acc: 0.718750] [adversarial loss: 1.985587, acc: 0.000000]\n",
      "951: [discriminator loss: 0.369094, acc: 0.875000] [adversarial loss: 0.938025, acc: 0.343750]\n",
      "952: [discriminator loss: 0.442344, acc: 0.812500] [adversarial loss: 1.863592, acc: 0.000000]\n",
      "953: [discriminator loss: 0.448018, acc: 0.828125] [adversarial loss: 0.763473, acc: 0.484375]\n",
      "954: [discriminator loss: 0.422366, acc: 0.804688] [adversarial loss: 1.323210, acc: 0.093750]\n",
      "955: [discriminator loss: 0.563861, acc: 0.687500] [adversarial loss: 1.167023, acc: 0.218750]\n",
      "956: [discriminator loss: 0.542967, acc: 0.757812] [adversarial loss: 1.146077, acc: 0.218750]\n",
      "957: [discriminator loss: 0.337603, acc: 0.890625] [adversarial loss: 0.588689, acc: 0.656250]\n",
      "958: [discriminator loss: 0.416551, acc: 0.804688] [adversarial loss: 1.101925, acc: 0.281250]\n",
      "959: [discriminator loss: 0.371413, acc: 0.867188] [adversarial loss: 0.472590, acc: 0.750000]\n",
      "960: [discriminator loss: 0.535622, acc: 0.710938] [adversarial loss: 1.572834, acc: 0.046875]\n",
      "961: [discriminator loss: 0.553938, acc: 0.742188] [adversarial loss: 0.405107, acc: 0.859375]\n",
      "962: [discriminator loss: 0.662405, acc: 0.554688] [adversarial loss: 2.677749, acc: 0.000000]\n",
      "963: [discriminator loss: 0.623589, acc: 0.609375] [adversarial loss: 0.576193, acc: 0.687500]\n",
      "964: [discriminator loss: 0.522420, acc: 0.640625] [adversarial loss: 2.810937, acc: 0.000000]\n",
      "965: [discriminator loss: 0.449990, acc: 0.765625] [adversarial loss: 0.843672, acc: 0.390625]\n",
      "966: [discriminator loss: 0.257031, acc: 0.945312] [adversarial loss: 0.584958, acc: 0.687500]\n",
      "967: [discriminator loss: 0.211120, acc: 0.960938] [adversarial loss: 0.553474, acc: 0.687500]\n",
      "968: [discriminator loss: 0.144485, acc: 0.992188] [adversarial loss: 0.376932, acc: 0.875000]\n",
      "969: [discriminator loss: 0.269123, acc: 0.914062] [adversarial loss: 0.496222, acc: 0.765625]\n",
      "970: [discriminator loss: 0.273470, acc: 0.906250] [adversarial loss: 0.621677, acc: 0.671875]\n",
      "971: [discriminator loss: 0.391664, acc: 0.851562] [adversarial loss: 1.242463, acc: 0.156250]\n",
      "972: [discriminator loss: 0.375262, acc: 0.804688] [adversarial loss: 0.223834, acc: 0.984375]\n",
      "973: [discriminator loss: 0.735494, acc: 0.601562] [adversarial loss: 2.132461, acc: 0.015625]\n",
      "974: [discriminator loss: 0.702181, acc: 0.570312] [adversarial loss: 0.356134, acc: 0.890625]\n",
      "975: [discriminator loss: 0.760798, acc: 0.539062] [adversarial loss: 2.023458, acc: 0.000000]\n",
      "976: [discriminator loss: 0.694654, acc: 0.625000] [adversarial loss: 0.689596, acc: 0.625000]\n",
      "977: [discriminator loss: 0.554584, acc: 0.671875] [adversarial loss: 1.425541, acc: 0.031250]\n",
      "978: [discriminator loss: 0.369543, acc: 0.804688] [adversarial loss: 0.548417, acc: 0.718750]\n",
      "979: [discriminator loss: 0.384534, acc: 0.828125] [adversarial loss: 0.573470, acc: 0.687500]\n",
      "980: [discriminator loss: 0.341900, acc: 0.890625] [adversarial loss: 0.675064, acc: 0.578125]\n",
      "981: [discriminator loss: 0.312830, acc: 0.945312] [adversarial loss: 0.462570, acc: 0.812500]\n",
      "982: [discriminator loss: 0.525405, acc: 0.750000] [adversarial loss: 0.996426, acc: 0.281250]\n",
      "983: [discriminator loss: 0.432361, acc: 0.804688] [adversarial loss: 0.612742, acc: 0.703125]\n",
      "984: [discriminator loss: 0.505387, acc: 0.695312] [adversarial loss: 1.568546, acc: 0.031250]\n",
      "985: [discriminator loss: 0.518517, acc: 0.742188] [adversarial loss: 0.421176, acc: 0.875000]\n",
      "986: [discriminator loss: 0.676403, acc: 0.585938] [adversarial loss: 1.964717, acc: 0.000000]\n",
      "987: [discriminator loss: 0.547519, acc: 0.679688] [adversarial loss: 0.788839, acc: 0.468750]\n",
      "988: [discriminator loss: 0.587004, acc: 0.648438] [adversarial loss: 1.909211, acc: 0.000000]\n",
      "989: [discriminator loss: 0.540639, acc: 0.703125] [adversarial loss: 0.845723, acc: 0.375000]\n",
      "990: [discriminator loss: 0.425099, acc: 0.812500] [adversarial loss: 0.962698, acc: 0.312500]\n",
      "991: [discriminator loss: 0.426298, acc: 0.820312] [adversarial loss: 0.841595, acc: 0.359375]\n",
      "992: [discriminator loss: 0.474740, acc: 0.789062] [adversarial loss: 0.982177, acc: 0.281250]\n",
      "993: [discriminator loss: 0.580305, acc: 0.695312] [adversarial loss: 1.081821, acc: 0.218750]\n",
      "994: [discriminator loss: 0.604214, acc: 0.679688] [adversarial loss: 1.814491, acc: 0.015625]\n",
      "995: [discriminator loss: 0.416923, acc: 0.843750] [adversarial loss: 0.619517, acc: 0.640625]\n",
      "996: [discriminator loss: 0.525190, acc: 0.710938] [adversarial loss: 1.929163, acc: 0.000000]\n",
      "997: [discriminator loss: 0.530358, acc: 0.742188] [adversarial loss: 0.699451, acc: 0.515625]\n",
      "998: [discriminator loss: 0.547555, acc: 0.656250] [adversarial loss: 2.007710, acc: 0.000000]\n",
      "999: [discriminator loss: 0.480087, acc: 0.726562] [adversarial loss: 0.745304, acc: 0.531250]\n",
      "1000: [discriminator loss: 0.504681, acc: 0.742188] [adversarial loss: 1.447005, acc: 0.015625]\n",
      "1001: [discriminator loss: 0.508136, acc: 0.757812] [adversarial loss: 0.523143, acc: 0.781250]\n",
      "1002: [discriminator loss: 0.306041, acc: 0.882812] [adversarial loss: 0.444899, acc: 0.875000]\n",
      "1003: [discriminator loss: 0.431545, acc: 0.804688] [adversarial loss: 0.870414, acc: 0.421875]\n",
      "1004: [discriminator loss: 0.367225, acc: 0.882812] [adversarial loss: 1.100678, acc: 0.203125]\n",
      "1005: [discriminator loss: 0.526873, acc: 0.726562] [adversarial loss: 0.431949, acc: 0.859375]\n",
      "1006: [discriminator loss: 0.596312, acc: 0.609375] [adversarial loss: 1.661216, acc: 0.000000]\n",
      "1007: [discriminator loss: 0.570844, acc: 0.695312] [adversarial loss: 0.706590, acc: 0.625000]\n",
      "1008: [discriminator loss: 0.558326, acc: 0.671875] [adversarial loss: 1.792913, acc: 0.015625]\n",
      "1009: [discriminator loss: 0.502842, acc: 0.750000] [adversarial loss: 1.113303, acc: 0.156250]\n",
      "1010: [discriminator loss: 0.431938, acc: 0.773438] [adversarial loss: 0.865507, acc: 0.468750]\n",
      "1011: [discriminator loss: 0.377400, acc: 0.851562] [adversarial loss: 0.962422, acc: 0.359375]\n",
      "1012: [discriminator loss: 0.417404, acc: 0.789062] [adversarial loss: 0.770503, acc: 0.500000]\n",
      "1013: [discriminator loss: 0.501486, acc: 0.734375] [adversarial loss: 1.034963, acc: 0.250000]\n",
      "1014: [discriminator loss: 0.516240, acc: 0.781250] [adversarial loss: 0.624768, acc: 0.718750]\n",
      "1015: [discriminator loss: 0.638965, acc: 0.609375] [adversarial loss: 2.561543, acc: 0.000000]\n",
      "1016: [discriminator loss: 0.736875, acc: 0.609375] [adversarial loss: 0.509540, acc: 0.718750]\n",
      "1017: [discriminator loss: 0.571369, acc: 0.695312] [adversarial loss: 0.987831, acc: 0.250000]\n",
      "1018: [discriminator loss: 0.568838, acc: 0.726562] [adversarial loss: 0.930521, acc: 0.312500]\n",
      "1019: [discriminator loss: 0.499820, acc: 0.789062] [adversarial loss: 0.630572, acc: 0.625000]\n",
      "1020: [discriminator loss: 0.575409, acc: 0.679688] [adversarial loss: 1.476829, acc: 0.031250]\n",
      "1021: [discriminator loss: 0.535993, acc: 0.742188] [adversarial loss: 0.540796, acc: 0.859375]\n",
      "1022: [discriminator loss: 0.637275, acc: 0.664062] [adversarial loss: 1.806448, acc: 0.000000]\n",
      "1023: [discriminator loss: 0.588526, acc: 0.664062] [adversarial loss: 0.648141, acc: 0.625000]\n",
      "1024: [discriminator loss: 0.417954, acc: 0.835938] [adversarial loss: 0.468121, acc: 0.812500]\n",
      "1025: [discriminator loss: 0.595531, acc: 0.679688] [adversarial loss: 1.220796, acc: 0.062500]\n",
      "1026: [discriminator loss: 0.527647, acc: 0.765625] [adversarial loss: 0.630915, acc: 0.609375]\n",
      "1027: [discriminator loss: 0.482884, acc: 0.812500] [adversarial loss: 0.616729, acc: 0.640625]\n",
      "1028: [discriminator loss: 0.462640, acc: 0.796875] [adversarial loss: 0.778862, acc: 0.515625]\n",
      "1029: [discriminator loss: 0.625786, acc: 0.625000] [adversarial loss: 1.350441, acc: 0.015625]\n",
      "1030: [discriminator loss: 0.465791, acc: 0.773438] [adversarial loss: 0.466207, acc: 0.828125]\n",
      "1031: [discriminator loss: 0.689979, acc: 0.617188] [adversarial loss: 1.283202, acc: 0.109375]\n",
      "1032: [discriminator loss: 0.551211, acc: 0.703125] [adversarial loss: 0.498602, acc: 0.781250]\n",
      "1033: [discriminator loss: 0.696614, acc: 0.546875] [adversarial loss: 1.467946, acc: 0.000000]\n",
      "1034: [discriminator loss: 0.635284, acc: 0.617188] [adversarial loss: 0.487553, acc: 0.828125]\n",
      "1035: [discriminator loss: 0.720491, acc: 0.585938] [adversarial loss: 1.416498, acc: 0.015625]\n",
      "1036: [discriminator loss: 0.658634, acc: 0.687500] [adversarial loss: 0.788965, acc: 0.390625]\n",
      "1037: [discriminator loss: 0.550135, acc: 0.710938] [adversarial loss: 1.246241, acc: 0.093750]\n",
      "1038: [discriminator loss: 0.538609, acc: 0.734375] [adversarial loss: 0.735072, acc: 0.515625]\n",
      "1039: [discriminator loss: 0.559644, acc: 0.734375] [adversarial loss: 1.362804, acc: 0.046875]\n",
      "1040: [discriminator loss: 0.520752, acc: 0.742188] [adversarial loss: 0.595124, acc: 0.656250]\n",
      "1041: [discriminator loss: 0.646291, acc: 0.585938] [adversarial loss: 1.063809, acc: 0.203125]\n",
      "1042: [discriminator loss: 0.557562, acc: 0.742188] [adversarial loss: 0.505855, acc: 0.765625]\n",
      "1043: [discriminator loss: 0.625724, acc: 0.632812] [adversarial loss: 0.930230, acc: 0.187500]\n",
      "1044: [discriminator loss: 0.443172, acc: 0.843750] [adversarial loss: 0.557201, acc: 0.828125]\n",
      "1045: [discriminator loss: 0.475162, acc: 0.757812] [adversarial loss: 0.832468, acc: 0.453125]\n",
      "1046: [discriminator loss: 0.526908, acc: 0.796875] [adversarial loss: 1.073041, acc: 0.140625]\n",
      "1047: [discriminator loss: 0.449740, acc: 0.835938] [adversarial loss: 0.936728, acc: 0.296875]\n",
      "1048: [discriminator loss: 0.495264, acc: 0.734375] [adversarial loss: 1.131880, acc: 0.078125]\n",
      "1049: [discriminator loss: 0.579140, acc: 0.703125] [adversarial loss: 0.589303, acc: 0.640625]\n",
      "1050: [discriminator loss: 0.751850, acc: 0.539062] [adversarial loss: 1.579237, acc: 0.000000]\n",
      "1051: [discriminator loss: 0.735903, acc: 0.593750] [adversarial loss: 0.665928, acc: 0.640625]\n",
      "1052: [discriminator loss: 0.702993, acc: 0.578125] [adversarial loss: 1.194379, acc: 0.078125]\n",
      "1053: [discriminator loss: 0.700767, acc: 0.617188] [adversarial loss: 0.755027, acc: 0.421875]\n",
      "1054: [discriminator loss: 0.683970, acc: 0.539062] [adversarial loss: 1.950139, acc: 0.000000]\n",
      "1055: [discriminator loss: 0.597907, acc: 0.671875] [adversarial loss: 0.561841, acc: 0.765625]\n",
      "1056: [discriminator loss: 0.517994, acc: 0.703125] [adversarial loss: 1.266965, acc: 0.109375]\n",
      "1057: [discriminator loss: 0.542602, acc: 0.781250] [adversarial loss: 0.804019, acc: 0.406250]\n",
      "1058: [discriminator loss: 0.589243, acc: 0.664062] [adversarial loss: 1.539161, acc: 0.046875]\n",
      "1059: [discriminator loss: 0.603145, acc: 0.687500] [adversarial loss: 1.039240, acc: 0.171875]\n",
      "1060: [discriminator loss: 0.466185, acc: 0.789062] [adversarial loss: 0.683027, acc: 0.562500]\n",
      "1061: [discriminator loss: 0.496635, acc: 0.789062] [adversarial loss: 0.787144, acc: 0.437500]\n",
      "1062: [discriminator loss: 0.494505, acc: 0.765625] [adversarial loss: 0.797472, acc: 0.390625]\n",
      "1063: [discriminator loss: 0.468341, acc: 0.796875] [adversarial loss: 0.880785, acc: 0.312500]\n",
      "1064: [discriminator loss: 0.435399, acc: 0.851562] [adversarial loss: 0.867234, acc: 0.343750]\n",
      "1065: [discriminator loss: 0.465683, acc: 0.796875] [adversarial loss: 1.007063, acc: 0.218750]\n",
      "1066: [discriminator loss: 0.528372, acc: 0.789062] [adversarial loss: 1.067067, acc: 0.234375]\n",
      "1067: [discriminator loss: 0.420218, acc: 0.867188] [adversarial loss: 0.682667, acc: 0.593750]\n",
      "1068: [discriminator loss: 0.622279, acc: 0.671875] [adversarial loss: 2.057741, acc: 0.000000]\n",
      "1069: [discriminator loss: 0.732938, acc: 0.554688] [adversarial loss: 0.483326, acc: 0.843750]\n",
      "1070: [discriminator loss: 0.672449, acc: 0.585938] [adversarial loss: 1.846150, acc: 0.000000]\n",
      "1071: [discriminator loss: 0.671010, acc: 0.593750] [adversarial loss: 0.764669, acc: 0.390625]\n",
      "1072: [discriminator loss: 0.528894, acc: 0.781250] [adversarial loss: 0.908402, acc: 0.296875]\n",
      "1073: [discriminator loss: 0.619496, acc: 0.640625] [adversarial loss: 1.178645, acc: 0.093750]\n",
      "1074: [discriminator loss: 0.629162, acc: 0.640625] [adversarial loss: 0.765855, acc: 0.531250]\n",
      "1075: [discriminator loss: 0.556307, acc: 0.703125] [adversarial loss: 1.427145, acc: 0.000000]\n",
      "1076: [discriminator loss: 0.549634, acc: 0.726562] [adversarial loss: 0.814075, acc: 0.406250]\n",
      "1077: [discriminator loss: 0.543065, acc: 0.726562] [adversarial loss: 1.390741, acc: 0.046875]\n",
      "1078: [discriminator loss: 0.546680, acc: 0.695312] [adversarial loss: 0.848762, acc: 0.437500]\n",
      "1079: [discriminator loss: 0.493612, acc: 0.789062] [adversarial loss: 1.011664, acc: 0.171875]\n",
      "1080: [discriminator loss: 0.558627, acc: 0.710938] [adversarial loss: 1.299319, acc: 0.015625]\n",
      "1081: [discriminator loss: 0.487053, acc: 0.781250] [adversarial loss: 0.799654, acc: 0.406250]\n",
      "1082: [discriminator loss: 0.570436, acc: 0.671875] [adversarial loss: 1.013218, acc: 0.203125]\n",
      "1083: [discriminator loss: 0.554535, acc: 0.726562] [adversarial loss: 1.020977, acc: 0.281250]\n",
      "1084: [discriminator loss: 0.551236, acc: 0.757812] [adversarial loss: 0.883839, acc: 0.281250]\n",
      "1085: [discriminator loss: 0.476702, acc: 0.812500] [adversarial loss: 1.067924, acc: 0.156250]\n",
      "1086: [discriminator loss: 0.534658, acc: 0.757812] [adversarial loss: 0.750881, acc: 0.453125]\n",
      "1087: [discriminator loss: 0.535515, acc: 0.710938] [adversarial loss: 1.743795, acc: 0.015625]\n",
      "1088: [discriminator loss: 0.652040, acc: 0.632812] [adversarial loss: 0.538537, acc: 0.734375]\n",
      "1089: [discriminator loss: 0.541341, acc: 0.710938] [adversarial loss: 1.521480, acc: 0.000000]\n",
      "1090: [discriminator loss: 0.509055, acc: 0.757812] [adversarial loss: 0.955404, acc: 0.250000]\n",
      "1091: [discriminator loss: 0.513942, acc: 0.765625] [adversarial loss: 0.693140, acc: 0.593750]\n",
      "1092: [discriminator loss: 0.602527, acc: 0.671875] [adversarial loss: 1.543970, acc: 0.046875]\n",
      "1093: [discriminator loss: 0.588050, acc: 0.671875] [adversarial loss: 0.514931, acc: 0.796875]\n",
      "1094: [discriminator loss: 0.565312, acc: 0.726562] [adversarial loss: 1.055113, acc: 0.187500]\n",
      "1095: [discriminator loss: 0.539590, acc: 0.765625] [adversarial loss: 1.025277, acc: 0.187500]\n",
      "1096: [discriminator loss: 0.571319, acc: 0.664062] [adversarial loss: 1.153144, acc: 0.125000]\n",
      "1097: [discriminator loss: 0.460747, acc: 0.789062] [adversarial loss: 0.672103, acc: 0.562500]\n",
      "1098: [discriminator loss: 0.627137, acc: 0.609375] [adversarial loss: 1.503500, acc: 0.015625]\n",
      "1099: [discriminator loss: 0.585009, acc: 0.648438] [adversarial loss: 0.618332, acc: 0.625000]\n",
      "1100: [discriminator loss: 0.555895, acc: 0.742188] [adversarial loss: 1.419325, acc: 0.031250]\n",
      "1101: [discriminator loss: 0.497127, acc: 0.812500] [adversarial loss: 1.004075, acc: 0.140625]\n",
      "1102: [discriminator loss: 0.521963, acc: 0.773438] [adversarial loss: 1.122219, acc: 0.156250]\n",
      "1103: [discriminator loss: 0.422368, acc: 0.835938] [adversarial loss: 0.552647, acc: 0.718750]\n",
      "1104: [discriminator loss: 0.507761, acc: 0.726562] [adversarial loss: 0.951527, acc: 0.343750]\n",
      "1105: [discriminator loss: 0.538283, acc: 0.765625] [adversarial loss: 0.555695, acc: 0.718750]\n",
      "1106: [discriminator loss: 0.664355, acc: 0.609375] [adversarial loss: 1.395700, acc: 0.015625]\n",
      "1107: [discriminator loss: 0.566515, acc: 0.664062] [adversarial loss: 0.561046, acc: 0.656250]\n",
      "1108: [discriminator loss: 0.676799, acc: 0.617188] [adversarial loss: 1.689267, acc: 0.000000]\n",
      "1109: [discriminator loss: 0.660957, acc: 0.593750] [adversarial loss: 0.722415, acc: 0.453125]\n",
      "1110: [discriminator loss: 0.565721, acc: 0.710938] [adversarial loss: 0.918868, acc: 0.281250]\n",
      "1111: [discriminator loss: 0.522918, acc: 0.781250] [adversarial loss: 1.095489, acc: 0.125000]\n",
      "1112: [discriminator loss: 0.463428, acc: 0.835938] [adversarial loss: 1.169653, acc: 0.078125]\n",
      "1113: [discriminator loss: 0.473065, acc: 0.796875] [adversarial loss: 1.009635, acc: 0.234375]\n",
      "1114: [discriminator loss: 0.455661, acc: 0.843750] [adversarial loss: 0.740499, acc: 0.484375]\n",
      "1115: [discriminator loss: 0.535774, acc: 0.734375] [adversarial loss: 1.045639, acc: 0.187500]\n",
      "1116: [discriminator loss: 0.560865, acc: 0.687500] [adversarial loss: 1.078442, acc: 0.062500]\n",
      "1117: [discriminator loss: 0.572983, acc: 0.679688] [adversarial loss: 1.190702, acc: 0.031250]\n",
      "1118: [discriminator loss: 0.574280, acc: 0.695312] [adversarial loss: 0.794270, acc: 0.453125]\n",
      "1119: [discriminator loss: 0.553145, acc: 0.695312] [adversarial loss: 1.547582, acc: 0.015625]\n",
      "1120: [discriminator loss: 0.608622, acc: 0.617188] [adversarial loss: 0.531825, acc: 0.781250]\n",
      "1121: [discriminator loss: 0.635691, acc: 0.617188] [adversarial loss: 1.640292, acc: 0.015625]\n",
      "1122: [discriminator loss: 0.634884, acc: 0.625000] [adversarial loss: 0.752529, acc: 0.468750]\n",
      "1123: [discriminator loss: 0.511275, acc: 0.687500] [adversarial loss: 1.803684, acc: 0.000000]\n",
      "1124: [discriminator loss: 0.545336, acc: 0.710938] [adversarial loss: 0.678245, acc: 0.500000]\n",
      "1125: [discriminator loss: 0.505012, acc: 0.742188] [adversarial loss: 0.925902, acc: 0.281250]\n",
      "1126: [discriminator loss: 0.551424, acc: 0.687500] [adversarial loss: 1.114231, acc: 0.125000]\n",
      "1127: [discriminator loss: 0.547250, acc: 0.718750] [adversarial loss: 0.563807, acc: 0.625000]\n",
      "1128: [discriminator loss: 0.600301, acc: 0.640625] [adversarial loss: 1.708779, acc: 0.015625]\n",
      "1129: [discriminator loss: 0.573205, acc: 0.671875] [adversarial loss: 0.639899, acc: 0.578125]\n",
      "1130: [discriminator loss: 0.559752, acc: 0.710938] [adversarial loss: 1.280138, acc: 0.031250]\n",
      "1131: [discriminator loss: 0.527351, acc: 0.726562] [adversarial loss: 0.926600, acc: 0.234375]\n",
      "1132: [discriminator loss: 0.518710, acc: 0.718750] [adversarial loss: 0.823360, acc: 0.359375]\n",
      "1133: [discriminator loss: 0.532224, acc: 0.750000] [adversarial loss: 0.675008, acc: 0.546875]\n",
      "1134: [discriminator loss: 0.479725, acc: 0.789062] [adversarial loss: 0.947382, acc: 0.203125]\n",
      "1135: [discriminator loss: 0.562382, acc: 0.671875] [adversarial loss: 0.957104, acc: 0.281250]\n",
      "1136: [discriminator loss: 0.532202, acc: 0.757812] [adversarial loss: 1.480078, acc: 0.015625]\n",
      "1137: [discriminator loss: 0.509713, acc: 0.742188] [adversarial loss: 0.607767, acc: 0.640625]\n",
      "1138: [discriminator loss: 0.525091, acc: 0.726562] [adversarial loss: 1.594212, acc: 0.046875]\n",
      "1139: [discriminator loss: 0.557114, acc: 0.679688] [adversarial loss: 0.647676, acc: 0.562500]\n",
      "1140: [discriminator loss: 0.601535, acc: 0.625000] [adversarial loss: 1.671642, acc: 0.031250]\n",
      "1141: [discriminator loss: 0.466048, acc: 0.765625] [adversarial loss: 0.599664, acc: 0.609375]\n",
      "1142: [discriminator loss: 0.487938, acc: 0.781250] [adversarial loss: 0.818547, acc: 0.437500]\n",
      "1143: [discriminator loss: 0.486266, acc: 0.773438] [adversarial loss: 0.665211, acc: 0.640625]\n",
      "1144: [discriminator loss: 0.503013, acc: 0.789062] [adversarial loss: 0.926137, acc: 0.140625]\n",
      "1145: [discriminator loss: 0.521999, acc: 0.765625] [adversarial loss: 0.904517, acc: 0.265625]\n",
      "1146: [discriminator loss: 0.583511, acc: 0.664062] [adversarial loss: 1.910509, acc: 0.000000]\n",
      "1147: [discriminator loss: 0.485243, acc: 0.742188] [adversarial loss: 0.369221, acc: 0.921875]\n",
      "1148: [discriminator loss: 0.342760, acc: 0.875000] [adversarial loss: 0.510145, acc: 0.734375]\n",
      "1149: [discriminator loss: 0.367793, acc: 0.851562] [adversarial loss: 0.684875, acc: 0.562500]\n",
      "1150: [discriminator loss: 0.375206, acc: 0.890625] [adversarial loss: 0.511932, acc: 0.734375]\n",
      "1151: [discriminator loss: 0.427852, acc: 0.796875] [adversarial loss: 1.038015, acc: 0.234375]\n",
      "1152: [discriminator loss: 0.513110, acc: 0.773438] [adversarial loss: 0.479384, acc: 0.843750]\n",
      "1153: [discriminator loss: 0.494592, acc: 0.734375] [adversarial loss: 1.228323, acc: 0.062500]\n",
      "1154: [discriminator loss: 0.594574, acc: 0.664062] [adversarial loss: 0.321579, acc: 0.953125]\n",
      "1155: [discriminator loss: 0.616406, acc: 0.664062] [adversarial loss: 1.724731, acc: 0.000000]\n",
      "1156: [discriminator loss: 0.667588, acc: 0.625000] [adversarial loss: 0.409033, acc: 0.890625]\n",
      "1157: [discriminator loss: 0.644073, acc: 0.585938] [adversarial loss: 1.400335, acc: 0.046875]\n",
      "1158: [discriminator loss: 0.542180, acc: 0.718750] [adversarial loss: 0.858667, acc: 0.265625]\n",
      "1159: [discriminator loss: 0.544281, acc: 0.710938] [adversarial loss: 1.462620, acc: 0.031250]\n",
      "1160: [discriminator loss: 0.433970, acc: 0.781250] [adversarial loss: 0.657240, acc: 0.656250]\n",
      "1161: [discriminator loss: 0.528080, acc: 0.703125] [adversarial loss: 1.376950, acc: 0.140625]\n",
      "1162: [discriminator loss: 0.567056, acc: 0.710938] [adversarial loss: 0.789184, acc: 0.468750]\n",
      "1163: [discriminator loss: 0.522193, acc: 0.742188] [adversarial loss: 0.960537, acc: 0.203125]\n",
      "1164: [discriminator loss: 0.499315, acc: 0.789062] [adversarial loss: 0.707828, acc: 0.546875]\n",
      "1165: [discriminator loss: 0.555879, acc: 0.679688] [adversarial loss: 1.867600, acc: 0.000000]\n",
      "1166: [discriminator loss: 0.601980, acc: 0.648438] [adversarial loss: 0.650836, acc: 0.656250]\n",
      "1167: [discriminator loss: 0.580606, acc: 0.656250] [adversarial loss: 1.682495, acc: 0.000000]\n",
      "1168: [discriminator loss: 0.537066, acc: 0.726562] [adversarial loss: 0.700465, acc: 0.562500]\n",
      "1169: [discriminator loss: 0.531264, acc: 0.742188] [adversarial loss: 1.362965, acc: 0.062500]\n",
      "1170: [discriminator loss: 0.532972, acc: 0.734375] [adversarial loss: 0.906281, acc: 0.328125]\n",
      "1171: [discriminator loss: 0.518922, acc: 0.726562] [adversarial loss: 1.617748, acc: 0.015625]\n",
      "1172: [discriminator loss: 0.534958, acc: 0.687500] [adversarial loss: 0.887499, acc: 0.375000]\n",
      "1173: [discriminator loss: 0.539849, acc: 0.687500] [adversarial loss: 1.572551, acc: 0.015625]\n",
      "1174: [discriminator loss: 0.627801, acc: 0.679688] [adversarial loss: 1.020298, acc: 0.156250]\n",
      "1175: [discriminator loss: 0.509291, acc: 0.750000] [adversarial loss: 1.641962, acc: 0.000000]\n",
      "1176: [discriminator loss: 0.465693, acc: 0.789062] [adversarial loss: 1.057013, acc: 0.125000]\n",
      "1177: [discriminator loss: 0.460622, acc: 0.820312] [adversarial loss: 1.193745, acc: 0.093750]\n",
      "1178: [discriminator loss: 0.400992, acc: 0.875000] [adversarial loss: 1.142826, acc: 0.109375]\n",
      "1179: [discriminator loss: 0.486256, acc: 0.773438] [adversarial loss: 0.968725, acc: 0.265625]\n",
      "1180: [discriminator loss: 0.511796, acc: 0.742188] [adversarial loss: 1.413513, acc: 0.078125]\n",
      "1181: [discriminator loss: 0.503076, acc: 0.789062] [adversarial loss: 1.475551, acc: 0.093750]\n",
      "1182: [discriminator loss: 0.488426, acc: 0.796875] [adversarial loss: 1.024578, acc: 0.218750]\n",
      "1183: [discriminator loss: 0.499848, acc: 0.742188] [adversarial loss: 1.487092, acc: 0.000000]\n",
      "1184: [discriminator loss: 0.469308, acc: 0.796875] [adversarial loss: 0.946580, acc: 0.265625]\n",
      "1185: [discriminator loss: 0.535156, acc: 0.765625] [adversarial loss: 2.198472, acc: 0.000000]\n",
      "1186: [discriminator loss: 0.628341, acc: 0.656250] [adversarial loss: 0.601027, acc: 0.671875]\n",
      "1187: [discriminator loss: 0.566755, acc: 0.679688] [adversarial loss: 1.801162, acc: 0.000000]\n",
      "1188: [discriminator loss: 0.536312, acc: 0.726562] [adversarial loss: 0.819793, acc: 0.421875]\n",
      "1189: [discriminator loss: 0.555314, acc: 0.703125] [adversarial loss: 1.785412, acc: 0.000000]\n",
      "1190: [discriminator loss: 0.624596, acc: 0.648438] [adversarial loss: 0.866461, acc: 0.359375]\n",
      "1191: [discriminator loss: 0.648279, acc: 0.617188] [adversarial loss: 1.468297, acc: 0.031250]\n",
      "1192: [discriminator loss: 0.536139, acc: 0.718750] [adversarial loss: 1.036813, acc: 0.187500]\n",
      "1193: [discriminator loss: 0.533634, acc: 0.710938] [adversarial loss: 1.509623, acc: 0.031250]\n",
      "1194: [discriminator loss: 0.531872, acc: 0.726562] [adversarial loss: 0.958822, acc: 0.296875]\n",
      "1195: [discriminator loss: 0.489207, acc: 0.789062] [adversarial loss: 1.499392, acc: 0.031250]\n",
      "1196: [discriminator loss: 0.505941, acc: 0.695312] [adversarial loss: 0.802045, acc: 0.437500]\n",
      "1197: [discriminator loss: 0.482684, acc: 0.781250] [adversarial loss: 1.065241, acc: 0.203125]\n",
      "1198: [discriminator loss: 0.456680, acc: 0.781250] [adversarial loss: 1.341083, acc: 0.046875]\n",
      "1199: [discriminator loss: 0.478482, acc: 0.796875] [adversarial loss: 1.124266, acc: 0.093750]\n",
      "1200: [discriminator loss: 0.494453, acc: 0.742188] [adversarial loss: 1.481396, acc: 0.031250]\n",
      "1201: [discriminator loss: 0.528109, acc: 0.734375] [adversarial loss: 0.753980, acc: 0.515625]\n",
      "1202: [discriminator loss: 0.522368, acc: 0.750000] [adversarial loss: 1.440758, acc: 0.093750]\n",
      "1203: [discriminator loss: 0.480020, acc: 0.789062] [adversarial loss: 0.549033, acc: 0.734375]\n",
      "1204: [discriminator loss: 0.509178, acc: 0.765625] [adversarial loss: 1.297480, acc: 0.078125]\n",
      "1205: [discriminator loss: 0.494023, acc: 0.734375] [adversarial loss: 0.450041, acc: 0.828125]\n",
      "1206: [discriminator loss: 0.714829, acc: 0.554688] [adversarial loss: 1.934400, acc: 0.000000]\n",
      "1207: [discriminator loss: 0.732809, acc: 0.593750] [adversarial loss: 0.632157, acc: 0.656250]\n",
      "1208: [discriminator loss: 0.572786, acc: 0.632812] [adversarial loss: 1.569458, acc: 0.000000]\n",
      "1209: [discriminator loss: 0.533762, acc: 0.703125] [adversarial loss: 0.949757, acc: 0.234375]\n",
      "1210: [discriminator loss: 0.506068, acc: 0.781250] [adversarial loss: 1.366690, acc: 0.031250]\n",
      "1211: [discriminator loss: 0.443542, acc: 0.835938] [adversarial loss: 1.282379, acc: 0.125000]\n",
      "1212: [discriminator loss: 0.503677, acc: 0.781250] [adversarial loss: 1.241373, acc: 0.125000]\n",
      "1213: [discriminator loss: 0.433411, acc: 0.820312] [adversarial loss: 1.204255, acc: 0.171875]\n",
      "1214: [discriminator loss: 0.456180, acc: 0.820312] [adversarial loss: 1.250633, acc: 0.125000]\n",
      "1215: [discriminator loss: 0.454671, acc: 0.789062] [adversarial loss: 1.086982, acc: 0.203125]\n",
      "1216: [discriminator loss: 0.554109, acc: 0.742188] [adversarial loss: 1.589124, acc: 0.031250]\n",
      "1217: [discriminator loss: 0.488657, acc: 0.804688] [adversarial loss: 0.962658, acc: 0.234375]\n",
      "1218: [discriminator loss: 0.553423, acc: 0.750000] [adversarial loss: 1.673828, acc: 0.046875]\n",
      "1219: [discriminator loss: 0.495374, acc: 0.750000] [adversarial loss: 0.928800, acc: 0.437500]\n",
      "1220: [discriminator loss: 0.479793, acc: 0.781250] [adversarial loss: 1.799128, acc: 0.093750]\n",
      "1221: [discriminator loss: 0.520839, acc: 0.742188] [adversarial loss: 0.716652, acc: 0.531250]\n",
      "1222: [discriminator loss: 0.540599, acc: 0.671875] [adversarial loss: 1.628395, acc: 0.093750]\n",
      "1223: [discriminator loss: 0.477360, acc: 0.812500] [adversarial loss: 1.104269, acc: 0.250000]\n",
      "1224: [discriminator loss: 0.528931, acc: 0.742188] [adversarial loss: 1.640357, acc: 0.031250]\n",
      "1225: [discriminator loss: 0.552968, acc: 0.710938] [adversarial loss: 0.799991, acc: 0.500000]\n",
      "1226: [discriminator loss: 0.509880, acc: 0.718750] [adversarial loss: 2.068850, acc: 0.015625]\n",
      "1227: [discriminator loss: 0.653174, acc: 0.640625] [adversarial loss: 0.663508, acc: 0.593750]\n",
      "1228: [discriminator loss: 0.561141, acc: 0.703125] [adversarial loss: 1.955674, acc: 0.015625]\n",
      "1229: [discriminator loss: 0.551876, acc: 0.695312] [adversarial loss: 0.915337, acc: 0.281250]\n",
      "1230: [discriminator loss: 0.440797, acc: 0.820312] [adversarial loss: 1.227635, acc: 0.125000]\n",
      "1231: [discriminator loss: 0.433020, acc: 0.843750] [adversarial loss: 0.881296, acc: 0.343750]\n",
      "1232: [discriminator loss: 0.480961, acc: 0.804688] [adversarial loss: 1.334152, acc: 0.078125]\n",
      "1233: [discriminator loss: 0.450674, acc: 0.789062] [adversarial loss: 0.874626, acc: 0.343750]\n",
      "1234: [discriminator loss: 0.535722, acc: 0.734375] [adversarial loss: 1.806466, acc: 0.015625]\n",
      "1235: [discriminator loss: 0.517848, acc: 0.765625] [adversarial loss: 0.886916, acc: 0.343750]\n",
      "1236: [discriminator loss: 0.485861, acc: 0.804688] [adversarial loss: 1.449482, acc: 0.078125]\n",
      "1237: [discriminator loss: 0.456826, acc: 0.804688] [adversarial loss: 1.034981, acc: 0.281250]\n",
      "1238: [discriminator loss: 0.496050, acc: 0.726562] [adversarial loss: 1.556095, acc: 0.015625]\n",
      "1239: [discriminator loss: 0.425887, acc: 0.804688] [adversarial loss: 1.223536, acc: 0.109375]\n",
      "1240: [discriminator loss: 0.477096, acc: 0.789062] [adversarial loss: 1.513884, acc: 0.031250]\n",
      "1241: [discriminator loss: 0.547257, acc: 0.742188] [adversarial loss: 0.819018, acc: 0.390625]\n",
      "1242: [discriminator loss: 0.469804, acc: 0.789062] [adversarial loss: 1.996382, acc: 0.015625]\n",
      "1243: [discriminator loss: 0.533859, acc: 0.718750] [adversarial loss: 0.516512, acc: 0.750000]\n",
      "1244: [discriminator loss: 0.569863, acc: 0.687500] [adversarial loss: 2.066908, acc: 0.000000]\n",
      "1245: [discriminator loss: 0.561546, acc: 0.695312] [adversarial loss: 0.822804, acc: 0.453125]\n",
      "1246: [discriminator loss: 0.513499, acc: 0.726562] [adversarial loss: 1.412014, acc: 0.125000]\n",
      "1247: [discriminator loss: 0.489942, acc: 0.742188] [adversarial loss: 0.880604, acc: 0.359375]\n",
      "1248: [discriminator loss: 0.546579, acc: 0.695312] [adversarial loss: 1.684731, acc: 0.015625]\n",
      "1249: [discriminator loss: 0.498086, acc: 0.718750] [adversarial loss: 1.042312, acc: 0.281250]\n",
      "1250: [discriminator loss: 0.514078, acc: 0.742188] [adversarial loss: 0.866603, acc: 0.343750]\n",
      "1251: [discriminator loss: 0.488496, acc: 0.757812] [adversarial loss: 1.660978, acc: 0.046875]\n",
      "1252: [discriminator loss: 0.511985, acc: 0.703125] [adversarial loss: 0.803411, acc: 0.468750]\n",
      "1253: [discriminator loss: 0.517097, acc: 0.710938] [adversarial loss: 1.796773, acc: 0.015625]\n",
      "1254: [discriminator loss: 0.531232, acc: 0.703125] [adversarial loss: 0.687067, acc: 0.609375]\n",
      "1255: [discriminator loss: 0.569949, acc: 0.679688] [adversarial loss: 1.492981, acc: 0.093750]\n",
      "1256: [discriminator loss: 0.581063, acc: 0.742188] [adversarial loss: 0.995831, acc: 0.265625]\n",
      "1257: [discriminator loss: 0.469634, acc: 0.796875] [adversarial loss: 1.215019, acc: 0.093750]\n",
      "1258: [discriminator loss: 0.459854, acc: 0.789062] [adversarial loss: 1.132756, acc: 0.171875]\n",
      "1259: [discriminator loss: 0.492094, acc: 0.742188] [adversarial loss: 1.176380, acc: 0.140625]\n",
      "1260: [discriminator loss: 0.446716, acc: 0.804688] [adversarial loss: 1.231675, acc: 0.156250]\n",
      "1261: [discriminator loss: 0.412387, acc: 0.843750] [adversarial loss: 0.942534, acc: 0.312500]\n",
      "1262: [discriminator loss: 0.452319, acc: 0.796875] [adversarial loss: 1.326805, acc: 0.171875]\n",
      "1263: [discriminator loss: 0.473311, acc: 0.828125] [adversarial loss: 1.356128, acc: 0.109375]\n",
      "1264: [discriminator loss: 0.505130, acc: 0.750000] [adversarial loss: 1.128226, acc: 0.250000]\n",
      "1265: [discriminator loss: 0.481211, acc: 0.773438] [adversarial loss: 1.811136, acc: 0.015625]\n",
      "1266: [discriminator loss: 0.445912, acc: 0.789062] [adversarial loss: 0.560011, acc: 0.703125]\n",
      "1267: [discriminator loss: 0.624952, acc: 0.656250] [adversarial loss: 2.309898, acc: 0.015625]\n",
      "1268: [discriminator loss: 0.708570, acc: 0.609375] [adversarial loss: 0.548138, acc: 0.687500]\n",
      "1269: [discriminator loss: 0.639221, acc: 0.617188] [adversarial loss: 1.606083, acc: 0.000000]\n",
      "1270: [discriminator loss: 0.494796, acc: 0.765625] [adversarial loss: 1.019364, acc: 0.218750]\n",
      "1271: [discriminator loss: 0.439763, acc: 0.835938] [adversarial loss: 1.345410, acc: 0.109375]\n",
      "1272: [discriminator loss: 0.427848, acc: 0.820312] [adversarial loss: 1.076060, acc: 0.156250]\n",
      "1273: [discriminator loss: 0.437522, acc: 0.820312] [adversarial loss: 1.224128, acc: 0.171875]\n",
      "1274: [discriminator loss: 0.431272, acc: 0.820312] [adversarial loss: 1.427709, acc: 0.078125]\n",
      "1275: [discriminator loss: 0.418247, acc: 0.820312] [adversarial loss: 1.292438, acc: 0.125000]\n",
      "1276: [discriminator loss: 0.492087, acc: 0.765625] [adversarial loss: 1.661557, acc: 0.046875]\n",
      "1277: [discriminator loss: 0.491437, acc: 0.750000] [adversarial loss: 0.629224, acc: 0.671875]\n",
      "1278: [discriminator loss: 0.589003, acc: 0.585938] [adversarial loss: 2.225395, acc: 0.015625]\n",
      "1279: [discriminator loss: 0.644397, acc: 0.609375] [adversarial loss: 0.850441, acc: 0.359375]\n",
      "1280: [discriminator loss: 0.458679, acc: 0.789062] [adversarial loss: 1.259087, acc: 0.109375]\n",
      "1281: [discriminator loss: 0.403480, acc: 0.859375] [adversarial loss: 1.069537, acc: 0.265625]\n",
      "1282: [discriminator loss: 0.521281, acc: 0.781250] [adversarial loss: 1.538008, acc: 0.078125]\n",
      "1283: [discriminator loss: 0.484798, acc: 0.781250] [adversarial loss: 1.067596, acc: 0.234375]\n",
      "1284: [discriminator loss: 0.452485, acc: 0.781250] [adversarial loss: 1.640694, acc: 0.046875]\n",
      "1285: [discriminator loss: 0.453145, acc: 0.789062] [adversarial loss: 1.051427, acc: 0.281250]\n",
      "1286: [discriminator loss: 0.420879, acc: 0.835938] [adversarial loss: 1.186277, acc: 0.171875]\n",
      "1287: [discriminator loss: 0.420144, acc: 0.812500] [adversarial loss: 1.208594, acc: 0.234375]\n",
      "1288: [discriminator loss: 0.444294, acc: 0.820312] [adversarial loss: 1.334833, acc: 0.140625]\n",
      "1289: [discriminator loss: 0.449381, acc: 0.812500] [adversarial loss: 1.184216, acc: 0.171875]\n",
      "1290: [discriminator loss: 0.371611, acc: 0.898438] [adversarial loss: 1.015548, acc: 0.250000]\n",
      "1291: [discriminator loss: 0.489383, acc: 0.734375] [adversarial loss: 2.055637, acc: 0.015625]\n",
      "1292: [discriminator loss: 0.474339, acc: 0.734375] [adversarial loss: 0.488063, acc: 0.734375]\n",
      "1293: [discriminator loss: 0.689891, acc: 0.640625] [adversarial loss: 2.194778, acc: 0.000000]\n",
      "1294: [discriminator loss: 0.567740, acc: 0.687500] [adversarial loss: 0.862986, acc: 0.484375]\n",
      "1295: [discriminator loss: 0.499025, acc: 0.750000] [adversarial loss: 1.774380, acc: 0.062500]\n",
      "1296: [discriminator loss: 0.493669, acc: 0.742188] [adversarial loss: 0.950609, acc: 0.296875]\n",
      "1297: [discriminator loss: 0.470479, acc: 0.789062] [adversarial loss: 1.549290, acc: 0.046875]\n",
      "1298: [discriminator loss: 0.500730, acc: 0.726562] [adversarial loss: 0.858920, acc: 0.328125]\n",
      "1299: [discriminator loss: 0.481762, acc: 0.757812] [adversarial loss: 1.638059, acc: 0.031250]\n",
      "1300: [discriminator loss: 0.493157, acc: 0.789062] [adversarial loss: 1.292106, acc: 0.078125]\n",
      "1301: [discriminator loss: 0.387641, acc: 0.828125] [adversarial loss: 1.172975, acc: 0.125000]\n",
      "1302: [discriminator loss: 0.488365, acc: 0.781250] [adversarial loss: 1.746784, acc: 0.078125]\n",
      "1303: [discriminator loss: 0.462240, acc: 0.773438] [adversarial loss: 0.810777, acc: 0.421875]\n",
      "1304: [discriminator loss: 0.540464, acc: 0.656250] [adversarial loss: 2.386725, acc: 0.015625]\n",
      "1305: [discriminator loss: 0.618552, acc: 0.687500] [adversarial loss: 0.871504, acc: 0.359375]\n",
      "1306: [discriminator loss: 0.500082, acc: 0.773438] [adversarial loss: 1.542624, acc: 0.093750]\n",
      "1307: [discriminator loss: 0.434133, acc: 0.820312] [adversarial loss: 0.941441, acc: 0.343750]\n",
      "1308: [discriminator loss: 0.481727, acc: 0.773438] [adversarial loss: 1.866996, acc: 0.031250]\n",
      "1309: [discriminator loss: 0.437689, acc: 0.804688] [adversarial loss: 1.038681, acc: 0.203125]\n",
      "1310: [discriminator loss: 0.371476, acc: 0.867188] [adversarial loss: 1.812832, acc: 0.031250]\n",
      "1311: [discriminator loss: 0.486442, acc: 0.765625] [adversarial loss: 0.922815, acc: 0.406250]\n",
      "1312: [discriminator loss: 0.407970, acc: 0.851562] [adversarial loss: 1.157231, acc: 0.187500]\n",
      "1313: [discriminator loss: 0.383125, acc: 0.867188] [adversarial loss: 0.894682, acc: 0.406250]\n",
      "1314: [discriminator loss: 0.531285, acc: 0.671875] [adversarial loss: 1.889396, acc: 0.046875]\n",
      "1315: [discriminator loss: 0.462590, acc: 0.757812] [adversarial loss: 0.740124, acc: 0.531250]\n",
      "1316: [discriminator loss: 0.590927, acc: 0.632812] [adversarial loss: 2.453945, acc: 0.015625]\n",
      "1317: [discriminator loss: 0.697846, acc: 0.648438] [adversarial loss: 0.679559, acc: 0.562500]\n",
      "1318: [discriminator loss: 0.536016, acc: 0.679688] [adversarial loss: 1.983038, acc: 0.031250]\n",
      "1319: [discriminator loss: 0.520058, acc: 0.750000] [adversarial loss: 0.961333, acc: 0.406250]\n",
      "1320: [discriminator loss: 0.448678, acc: 0.843750] [adversarial loss: 1.237122, acc: 0.140625]\n",
      "1321: [discriminator loss: 0.424524, acc: 0.828125] [adversarial loss: 1.179677, acc: 0.187500]\n",
      "1322: [discriminator loss: 0.464949, acc: 0.773438] [adversarial loss: 1.174683, acc: 0.187500]\n",
      "1323: [discriminator loss: 0.475655, acc: 0.781250] [adversarial loss: 1.382228, acc: 0.109375]\n",
      "1324: [discriminator loss: 0.439186, acc: 0.828125] [adversarial loss: 1.282311, acc: 0.203125]\n",
      "1325: [discriminator loss: 0.479912, acc: 0.804688] [adversarial loss: 1.568655, acc: 0.031250]\n",
      "1326: [discriminator loss: 0.411148, acc: 0.828125] [adversarial loss: 0.783062, acc: 0.500000]\n",
      "1327: [discriminator loss: 0.472851, acc: 0.757812] [adversarial loss: 2.095753, acc: 0.031250]\n",
      "1328: [discriminator loss: 0.575798, acc: 0.750000] [adversarial loss: 0.657889, acc: 0.625000]\n",
      "1329: [discriminator loss: 0.577453, acc: 0.625000] [adversarial loss: 2.202315, acc: 0.015625]\n",
      "1330: [discriminator loss: 0.499250, acc: 0.726562] [adversarial loss: 1.024896, acc: 0.234375]\n",
      "1331: [discriminator loss: 0.425004, acc: 0.812500] [adversarial loss: 1.978426, acc: 0.000000]\n",
      "1332: [discriminator loss: 0.433648, acc: 0.773438] [adversarial loss: 1.120658, acc: 0.250000]\n",
      "1333: [discriminator loss: 0.361875, acc: 0.867188] [adversarial loss: 1.401936, acc: 0.062500]\n",
      "1334: [discriminator loss: 0.413982, acc: 0.796875] [adversarial loss: 1.259478, acc: 0.156250]\n",
      "1335: [discriminator loss: 0.444755, acc: 0.828125] [adversarial loss: 1.493131, acc: 0.031250]\n",
      "1336: [discriminator loss: 0.470867, acc: 0.820312] [adversarial loss: 1.092148, acc: 0.203125]\n",
      "1337: [discriminator loss: 0.457674, acc: 0.750000] [adversarial loss: 1.977923, acc: 0.000000]\n",
      "1338: [discriminator loss: 0.464214, acc: 0.765625] [adversarial loss: 0.995598, acc: 0.375000]\n",
      "1339: [discriminator loss: 0.497521, acc: 0.781250] [adversarial loss: 2.132709, acc: 0.062500]\n",
      "1340: [discriminator loss: 0.516817, acc: 0.718750] [adversarial loss: 0.659783, acc: 0.500000]\n",
      "1341: [discriminator loss: 0.556674, acc: 0.687500] [adversarial loss: 1.881444, acc: 0.015625]\n",
      "1342: [discriminator loss: 0.518286, acc: 0.765625] [adversarial loss: 0.761382, acc: 0.468750]\n",
      "1343: [discriminator loss: 0.505619, acc: 0.718750] [adversarial loss: 2.037625, acc: 0.015625]\n",
      "1344: [discriminator loss: 0.467786, acc: 0.750000] [adversarial loss: 0.934969, acc: 0.312500]\n",
      "1345: [discriminator loss: 0.438881, acc: 0.789062] [adversarial loss: 1.635831, acc: 0.125000]\n",
      "1346: [discriminator loss: 0.429223, acc: 0.789062] [adversarial loss: 1.185673, acc: 0.171875]\n",
      "1347: [discriminator loss: 0.443731, acc: 0.812500] [adversarial loss: 1.517538, acc: 0.062500]\n",
      "1348: [discriminator loss: 0.373585, acc: 0.867188] [adversarial loss: 1.328415, acc: 0.140625]\n",
      "1349: [discriminator loss: 0.498458, acc: 0.765625] [adversarial loss: 1.369042, acc: 0.093750]\n",
      "1350: [discriminator loss: 0.417961, acc: 0.804688] [adversarial loss: 1.142867, acc: 0.187500]\n",
      "1351: [discriminator loss: 0.442864, acc: 0.804688] [adversarial loss: 1.726457, acc: 0.046875]\n",
      "1352: [discriminator loss: 0.400478, acc: 0.820312] [adversarial loss: 0.899263, acc: 0.390625]\n",
      "1353: [discriminator loss: 0.490835, acc: 0.773438] [adversarial loss: 2.039448, acc: 0.015625]\n",
      "1354: [discriminator loss: 0.535278, acc: 0.710938] [adversarial loss: 0.623925, acc: 0.687500]\n",
      "1355: [discriminator loss: 0.590686, acc: 0.664062] [adversarial loss: 2.239555, acc: 0.000000]\n",
      "1356: [discriminator loss: 0.520150, acc: 0.718750] [adversarial loss: 0.729775, acc: 0.625000]\n",
      "1357: [discriminator loss: 0.474592, acc: 0.734375] [adversarial loss: 1.885724, acc: 0.015625]\n",
      "1358: [discriminator loss: 0.514547, acc: 0.734375] [adversarial loss: 0.932143, acc: 0.421875]\n",
      "1359: [discriminator loss: 0.511283, acc: 0.726562] [adversarial loss: 1.917534, acc: 0.000000]\n",
      "1360: [discriminator loss: 0.445619, acc: 0.773438] [adversarial loss: 0.783344, acc: 0.468750]\n",
      "1361: [discriminator loss: 0.551014, acc: 0.703125] [adversarial loss: 1.764523, acc: 0.015625]\n",
      "1362: [discriminator loss: 0.476080, acc: 0.765625] [adversarial loss: 1.039823, acc: 0.218750]\n",
      "1363: [discriminator loss: 0.403011, acc: 0.828125] [adversarial loss: 1.602443, acc: 0.093750]\n",
      "1364: [discriminator loss: 0.454146, acc: 0.757812] [adversarial loss: 1.213707, acc: 0.218750]\n",
      "1365: [discriminator loss: 0.487656, acc: 0.773438] [adversarial loss: 1.592271, acc: 0.046875]\n",
      "1366: [discriminator loss: 0.462152, acc: 0.773438] [adversarial loss: 0.833659, acc: 0.437500]\n",
      "1367: [discriminator loss: 0.451085, acc: 0.812500] [adversarial loss: 1.885922, acc: 0.000000]\n",
      "1368: [discriminator loss: 0.427332, acc: 0.796875] [adversarial loss: 0.989629, acc: 0.328125]\n",
      "1369: [discriminator loss: 0.490590, acc: 0.710938] [adversarial loss: 2.012057, acc: 0.062500]\n",
      "1370: [discriminator loss: 0.474368, acc: 0.789062] [adversarial loss: 0.740283, acc: 0.546875]\n",
      "1371: [discriminator loss: 0.492798, acc: 0.750000] [adversarial loss: 2.254456, acc: 0.046875]\n",
      "1372: [discriminator loss: 0.492811, acc: 0.726562] [adversarial loss: 1.014132, acc: 0.250000]\n",
      "1373: [discriminator loss: 0.480015, acc: 0.789062] [adversarial loss: 2.097807, acc: 0.031250]\n",
      "1374: [discriminator loss: 0.465846, acc: 0.789062] [adversarial loss: 0.821230, acc: 0.421875]\n",
      "1375: [discriminator loss: 0.501111, acc: 0.710938] [adversarial loss: 2.228704, acc: 0.031250]\n",
      "1376: [discriminator loss: 0.510951, acc: 0.718750] [adversarial loss: 0.992379, acc: 0.281250]\n",
      "1377: [discriminator loss: 0.524144, acc: 0.703125] [adversarial loss: 1.932801, acc: 0.000000]\n",
      "1378: [discriminator loss: 0.398204, acc: 0.789062] [adversarial loss: 1.265732, acc: 0.171875]\n",
      "1379: [discriminator loss: 0.392155, acc: 0.835938] [adversarial loss: 1.454587, acc: 0.156250]\n",
      "1380: [discriminator loss: 0.416122, acc: 0.843750] [adversarial loss: 1.161044, acc: 0.296875]\n",
      "1381: [discriminator loss: 0.500128, acc: 0.757812] [adversarial loss: 1.254080, acc: 0.140625]\n",
      "1382: [discriminator loss: 0.425133, acc: 0.820312] [adversarial loss: 1.587991, acc: 0.093750]\n",
      "1383: [discriminator loss: 0.463587, acc: 0.796875] [adversarial loss: 1.042524, acc: 0.265625]\n",
      "1384: [discriminator loss: 0.400246, acc: 0.835938] [adversarial loss: 1.835568, acc: 0.031250]\n",
      "1385: [discriminator loss: 0.452489, acc: 0.804688] [adversarial loss: 1.071696, acc: 0.265625]\n",
      "1386: [discriminator loss: 0.549645, acc: 0.742188] [adversarial loss: 2.223341, acc: 0.000000]\n",
      "1387: [discriminator loss: 0.514160, acc: 0.773438] [adversarial loss: 0.894380, acc: 0.453125]\n",
      "1388: [discriminator loss: 0.493414, acc: 0.742188] [adversarial loss: 2.149688, acc: 0.000000]\n",
      "1389: [discriminator loss: 0.492238, acc: 0.726562] [adversarial loss: 0.758848, acc: 0.468750]\n",
      "1390: [discriminator loss: 0.497384, acc: 0.773438] [adversarial loss: 2.105117, acc: 0.015625]\n",
      "1391: [discriminator loss: 0.437880, acc: 0.734375] [adversarial loss: 1.076084, acc: 0.312500]\n",
      "1392: [discriminator loss: 0.486615, acc: 0.734375] [adversarial loss: 1.878526, acc: 0.062500]\n",
      "1393: [discriminator loss: 0.478815, acc: 0.742188] [adversarial loss: 0.985155, acc: 0.265625]\n",
      "1394: [discriminator loss: 0.444458, acc: 0.796875] [adversarial loss: 1.880761, acc: 0.062500]\n",
      "1395: [discriminator loss: 0.392139, acc: 0.859375] [adversarial loss: 1.103809, acc: 0.281250]\n",
      "1396: [discriminator loss: 0.422586, acc: 0.781250] [adversarial loss: 2.040268, acc: 0.015625]\n",
      "1397: [discriminator loss: 0.449823, acc: 0.726562] [adversarial loss: 0.813241, acc: 0.453125]\n",
      "1398: [discriminator loss: 0.480682, acc: 0.750000] [adversarial loss: 2.018215, acc: 0.062500]\n",
      "1399: [discriminator loss: 0.486545, acc: 0.734375] [adversarial loss: 0.919252, acc: 0.328125]\n",
      "1400: [discriminator loss: 0.502460, acc: 0.750000] [adversarial loss: 1.964138, acc: 0.015625]\n",
      "1401: [discriminator loss: 0.465391, acc: 0.773438] [adversarial loss: 0.865202, acc: 0.453125]\n",
      "1402: [discriminator loss: 0.516182, acc: 0.734375] [adversarial loss: 1.970901, acc: 0.015625]\n",
      "1403: [discriminator loss: 0.540190, acc: 0.679688] [adversarial loss: 0.964318, acc: 0.375000]\n",
      "1404: [discriminator loss: 0.488057, acc: 0.703125] [adversarial loss: 1.862712, acc: 0.062500]\n",
      "1405: [discriminator loss: 0.478325, acc: 0.734375] [adversarial loss: 0.825213, acc: 0.390625]\n",
      "1406: [discriminator loss: 0.496409, acc: 0.734375] [adversarial loss: 2.049612, acc: 0.000000]\n",
      "1407: [discriminator loss: 0.430835, acc: 0.804688] [adversarial loss: 1.140489, acc: 0.187500]\n",
      "1408: [discriminator loss: 0.484895, acc: 0.789062] [adversarial loss: 1.527385, acc: 0.140625]\n",
      "1409: [discriminator loss: 0.368448, acc: 0.851562] [adversarial loss: 1.383158, acc: 0.109375]\n",
      "1410: [discriminator loss: 0.438820, acc: 0.757812] [adversarial loss: 1.359884, acc: 0.125000]\n",
      "1411: [discriminator loss: 0.375523, acc: 0.851562] [adversarial loss: 1.601785, acc: 0.109375]\n",
      "1412: [discriminator loss: 0.405727, acc: 0.828125] [adversarial loss: 1.223613, acc: 0.218750]\n",
      "1413: [discriminator loss: 0.450138, acc: 0.781250] [adversarial loss: 1.773021, acc: 0.046875]\n",
      "1414: [discriminator loss: 0.397822, acc: 0.812500] [adversarial loss: 0.824425, acc: 0.515625]\n",
      "1415: [discriminator loss: 0.529795, acc: 0.742188] [adversarial loss: 2.584115, acc: 0.015625]\n",
      "1416: [discriminator loss: 0.603921, acc: 0.632812] [adversarial loss: 0.744666, acc: 0.531250]\n",
      "1417: [discriminator loss: 0.508439, acc: 0.734375] [adversarial loss: 1.926355, acc: 0.015625]\n",
      "1418: [discriminator loss: 0.479383, acc: 0.757812] [adversarial loss: 1.075365, acc: 0.218750]\n",
      "1419: [discriminator loss: 0.449873, acc: 0.804688] [adversarial loss: 1.797183, acc: 0.062500]\n",
      "1420: [discriminator loss: 0.462022, acc: 0.812500] [adversarial loss: 0.961695, acc: 0.343750]\n",
      "1421: [discriminator loss: 0.448391, acc: 0.828125] [adversarial loss: 2.135198, acc: 0.000000]\n",
      "1422: [discriminator loss: 0.497795, acc: 0.750000] [adversarial loss: 0.921435, acc: 0.375000]\n",
      "1423: [discriminator loss: 0.498670, acc: 0.773438] [adversarial loss: 1.743522, acc: 0.046875]\n",
      "1424: [discriminator loss: 0.473500, acc: 0.742188] [adversarial loss: 1.049404, acc: 0.281250]\n",
      "1425: [discriminator loss: 0.446939, acc: 0.804688] [adversarial loss: 1.713192, acc: 0.062500]\n",
      "1426: [discriminator loss: 0.477483, acc: 0.742188] [adversarial loss: 0.717887, acc: 0.562500]\n",
      "1427: [discriminator loss: 0.519195, acc: 0.695312] [adversarial loss: 2.071274, acc: 0.031250]\n",
      "1428: [discriminator loss: 0.514523, acc: 0.726562] [adversarial loss: 0.923993, acc: 0.406250]\n",
      "1429: [discriminator loss: 0.471178, acc: 0.765625] [adversarial loss: 1.327369, acc: 0.156250]\n",
      "1430: [discriminator loss: 0.419630, acc: 0.789062] [adversarial loss: 1.075928, acc: 0.296875]\n",
      "1431: [discriminator loss: 0.451150, acc: 0.828125] [adversarial loss: 1.848726, acc: 0.015625]\n",
      "1432: [discriminator loss: 0.425102, acc: 0.796875] [adversarial loss: 1.088671, acc: 0.265625]\n",
      "1433: [discriminator loss: 0.347853, acc: 0.914062] [adversarial loss: 1.820129, acc: 0.031250]\n",
      "1434: [discriminator loss: 0.381489, acc: 0.820312] [adversarial loss: 1.222259, acc: 0.250000]\n",
      "1435: [discriminator loss: 0.458567, acc: 0.820312] [adversarial loss: 1.786257, acc: 0.046875]\n",
      "1436: [discriminator loss: 0.421127, acc: 0.773438] [adversarial loss: 1.055813, acc: 0.281250]\n",
      "1437: [discriminator loss: 0.459610, acc: 0.781250] [adversarial loss: 1.952666, acc: 0.015625]\n",
      "1438: [discriminator loss: 0.506325, acc: 0.742188] [adversarial loss: 0.497573, acc: 0.718750]\n",
      "1439: [discriminator loss: 0.608842, acc: 0.656250] [adversarial loss: 2.291931, acc: 0.015625]\n",
      "1440: [discriminator loss: 0.576893, acc: 0.679688] [adversarial loss: 0.946216, acc: 0.250000]\n",
      "1441: [discriminator loss: 0.557551, acc: 0.640625] [adversarial loss: 1.769026, acc: 0.031250]\n",
      "1442: [discriminator loss: 0.424152, acc: 0.796875] [adversarial loss: 1.681299, acc: 0.078125]\n",
      "1443: [discriminator loss: 0.429377, acc: 0.835938] [adversarial loss: 1.209097, acc: 0.125000]\n",
      "1444: [discriminator loss: 0.437150, acc: 0.828125] [adversarial loss: 1.480300, acc: 0.171875]\n",
      "1445: [discriminator loss: 0.407139, acc: 0.851562] [adversarial loss: 1.344483, acc: 0.203125]\n",
      "1446: [discriminator loss: 0.379733, acc: 0.867188] [adversarial loss: 1.610980, acc: 0.046875]\n",
      "1447: [discriminator loss: 0.419124, acc: 0.773438] [adversarial loss: 1.367211, acc: 0.218750]\n",
      "1448: [discriminator loss: 0.426951, acc: 0.789062] [adversarial loss: 1.089865, acc: 0.296875]\n",
      "1449: [discriminator loss: 0.469310, acc: 0.796875] [adversarial loss: 2.073795, acc: 0.015625]\n",
      "1450: [discriminator loss: 0.472005, acc: 0.781250] [adversarial loss: 0.697109, acc: 0.531250]\n",
      "1451: [discriminator loss: 0.576080, acc: 0.671875] [adversarial loss: 2.460069, acc: 0.015625]\n",
      "1452: [discriminator loss: 0.625721, acc: 0.695312] [adversarial loss: 0.915471, acc: 0.453125]\n",
      "1453: [discriminator loss: 0.488156, acc: 0.765625] [adversarial loss: 1.662863, acc: 0.015625]\n",
      "1454: [discriminator loss: 0.395176, acc: 0.835938] [adversarial loss: 1.139189, acc: 0.234375]\n",
      "1455: [discriminator loss: 0.380257, acc: 0.875000] [adversarial loss: 1.796167, acc: 0.031250]\n",
      "1456: [discriminator loss: 0.428592, acc: 0.765625] [adversarial loss: 1.039020, acc: 0.296875]\n",
      "1457: [discriminator loss: 0.504410, acc: 0.796875] [adversarial loss: 2.043751, acc: 0.031250]\n",
      "1458: [discriminator loss: 0.468115, acc: 0.796875] [adversarial loss: 1.044173, acc: 0.281250]\n",
      "1459: [discriminator loss: 0.444015, acc: 0.804688] [adversarial loss: 1.836392, acc: 0.031250]\n",
      "1460: [discriminator loss: 0.404488, acc: 0.804688] [adversarial loss: 1.132896, acc: 0.265625]\n",
      "1461: [discriminator loss: 0.404815, acc: 0.820312] [adversarial loss: 1.510056, acc: 0.171875]\n",
      "1462: [discriminator loss: 0.416598, acc: 0.820312] [adversarial loss: 1.030357, acc: 0.296875]\n",
      "1463: [discriminator loss: 0.457744, acc: 0.765625] [adversarial loss: 1.776947, acc: 0.109375]\n",
      "1464: [discriminator loss: 0.459064, acc: 0.781250] [adversarial loss: 0.910187, acc: 0.437500]\n",
      "1465: [discriminator loss: 0.498054, acc: 0.773438] [adversarial loss: 2.320704, acc: 0.000000]\n",
      "1466: [discriminator loss: 0.555932, acc: 0.687500] [adversarial loss: 0.716840, acc: 0.578125]\n",
      "1467: [discriminator loss: 0.557649, acc: 0.656250] [adversarial loss: 2.076455, acc: 0.046875]\n",
      "1468: [discriminator loss: 0.642408, acc: 0.664062] [adversarial loss: 0.822746, acc: 0.437500]\n",
      "1469: [discriminator loss: 0.456585, acc: 0.734375] [adversarial loss: 1.766634, acc: 0.062500]\n",
      "1470: [discriminator loss: 0.452450, acc: 0.750000] [adversarial loss: 1.195229, acc: 0.187500]\n",
      "1471: [discriminator loss: 0.468950, acc: 0.828125] [adversarial loss: 1.363531, acc: 0.125000]\n",
      "1472: [discriminator loss: 0.447811, acc: 0.789062] [adversarial loss: 1.136376, acc: 0.218750]\n",
      "1473: [discriminator loss: 0.442342, acc: 0.796875] [adversarial loss: 1.601011, acc: 0.109375]\n",
      "1474: [discriminator loss: 0.419564, acc: 0.820312] [adversarial loss: 1.373959, acc: 0.140625]\n",
      "1475: [discriminator loss: 0.429457, acc: 0.828125] [adversarial loss: 1.489920, acc: 0.140625]\n",
      "1476: [discriminator loss: 0.453378, acc: 0.773438] [adversarial loss: 1.090313, acc: 0.328125]\n",
      "1477: [discriminator loss: 0.467760, acc: 0.796875] [adversarial loss: 1.363973, acc: 0.078125]\n",
      "1478: [discriminator loss: 0.455771, acc: 0.773438] [adversarial loss: 1.254397, acc: 0.156250]\n",
      "1479: [discriminator loss: 0.469906, acc: 0.781250] [adversarial loss: 1.443744, acc: 0.140625]\n",
      "1480: [discriminator loss: 0.454061, acc: 0.804688] [adversarial loss: 1.068253, acc: 0.281250]\n",
      "1481: [discriminator loss: 0.417594, acc: 0.828125] [adversarial loss: 1.811913, acc: 0.000000]\n",
      "1482: [discriminator loss: 0.480935, acc: 0.757812] [adversarial loss: 0.907138, acc: 0.375000]\n",
      "1483: [discriminator loss: 0.470402, acc: 0.804688] [adversarial loss: 2.116910, acc: 0.031250]\n",
      "1484: [discriminator loss: 0.521553, acc: 0.734375] [adversarial loss: 0.668453, acc: 0.609375]\n",
      "1485: [discriminator loss: 0.614393, acc: 0.671875] [adversarial loss: 2.429226, acc: 0.062500]\n",
      "1486: [discriminator loss: 0.648419, acc: 0.671875] [adversarial loss: 0.845102, acc: 0.468750]\n",
      "1487: [discriminator loss: 0.515113, acc: 0.726562] [adversarial loss: 1.226302, acc: 0.187500]\n",
      "1488: [discriminator loss: 0.423501, acc: 0.804688] [adversarial loss: 1.306880, acc: 0.171875]\n",
      "1489: [discriminator loss: 0.416031, acc: 0.835938] [adversarial loss: 1.176731, acc: 0.187500]\n",
      "1490: [discriminator loss: 0.402170, acc: 0.851562] [adversarial loss: 1.321696, acc: 0.140625]\n",
      "1491: [discriminator loss: 0.512782, acc: 0.765625] [adversarial loss: 1.446586, acc: 0.125000]\n",
      "1492: [discriminator loss: 0.450956, acc: 0.796875] [adversarial loss: 1.172468, acc: 0.140625]\n",
      "1493: [discriminator loss: 0.413344, acc: 0.789062] [adversarial loss: 1.244635, acc: 0.171875]\n",
      "1494: [discriminator loss: 0.484914, acc: 0.726562] [adversarial loss: 1.036678, acc: 0.359375]\n",
      "1495: [discriminator loss: 0.434456, acc: 0.789062] [adversarial loss: 1.953371, acc: 0.046875]\n",
      "1496: [discriminator loss: 0.544189, acc: 0.750000] [adversarial loss: 0.588523, acc: 0.671875]\n",
      "1497: [discriminator loss: 0.515935, acc: 0.679688] [adversarial loss: 2.382751, acc: 0.000000]\n",
      "1498: [discriminator loss: 0.607845, acc: 0.734375] [adversarial loss: 0.934225, acc: 0.312500]\n",
      "1499: [discriminator loss: 0.449646, acc: 0.796875] [adversarial loss: 1.301696, acc: 0.093750]\n",
      "1500: [discriminator loss: 0.396515, acc: 0.820312] [adversarial loss: 0.972752, acc: 0.296875]\n",
      "1501: [discriminator loss: 0.545309, acc: 0.695312] [adversarial loss: 1.899319, acc: 0.078125]\n",
      "1502: [discriminator loss: 0.561191, acc: 0.703125] [adversarial loss: 0.860853, acc: 0.375000]\n",
      "1503: [discriminator loss: 0.438205, acc: 0.804688] [adversarial loss: 1.556587, acc: 0.093750]\n",
      "1504: [discriminator loss: 0.511822, acc: 0.734375] [adversarial loss: 1.058018, acc: 0.234375]\n",
      "1505: [discriminator loss: 0.503004, acc: 0.757812] [adversarial loss: 1.806995, acc: 0.046875]\n",
      "1506: [discriminator loss: 0.559363, acc: 0.703125] [adversarial loss: 0.952060, acc: 0.375000]\n",
      "1507: [discriminator loss: 0.479445, acc: 0.765625] [adversarial loss: 1.725074, acc: 0.031250]\n",
      "1508: [discriminator loss: 0.506006, acc: 0.734375] [adversarial loss: 0.771253, acc: 0.406250]\n",
      "1509: [discriminator loss: 0.551392, acc: 0.695312] [adversarial loss: 1.848742, acc: 0.031250]\n",
      "1510: [discriminator loss: 0.538221, acc: 0.710938] [adversarial loss: 0.907477, acc: 0.390625]\n",
      "1511: [discriminator loss: 0.465728, acc: 0.742188] [adversarial loss: 1.520313, acc: 0.062500]\n",
      "1512: [discriminator loss: 0.405940, acc: 0.828125] [adversarial loss: 1.437285, acc: 0.062500]\n",
      "1513: [discriminator loss: 0.504138, acc: 0.742188] [adversarial loss: 1.293032, acc: 0.187500]\n",
      "1514: [discriminator loss: 0.525631, acc: 0.710938] [adversarial loss: 0.980057, acc: 0.265625]\n",
      "1515: [discriminator loss: 0.445459, acc: 0.820312] [adversarial loss: 1.643996, acc: 0.093750]\n",
      "1516: [discriminator loss: 0.492235, acc: 0.742188] [adversarial loss: 1.128134, acc: 0.234375]\n",
      "1517: [discriminator loss: 0.443476, acc: 0.812500] [adversarial loss: 1.565888, acc: 0.078125]\n",
      "1518: [discriminator loss: 0.421615, acc: 0.820312] [adversarial loss: 0.881982, acc: 0.375000]\n",
      "1519: [discriminator loss: 0.441095, acc: 0.789062] [adversarial loss: 2.139351, acc: 0.046875]\n",
      "1520: [discriminator loss: 0.564434, acc: 0.726562] [adversarial loss: 0.812946, acc: 0.437500]\n",
      "1521: [discriminator loss: 0.505673, acc: 0.750000] [adversarial loss: 1.798820, acc: 0.015625]\n",
      "1522: [discriminator loss: 0.547801, acc: 0.734375] [adversarial loss: 0.923584, acc: 0.406250]\n",
      "1523: [discriminator loss: 0.405300, acc: 0.843750] [adversarial loss: 1.469653, acc: 0.125000]\n",
      "1524: [discriminator loss: 0.417712, acc: 0.859375] [adversarial loss: 1.292239, acc: 0.171875]\n",
      "1525: [discriminator loss: 0.355495, acc: 0.882812] [adversarial loss: 1.428201, acc: 0.187500]\n",
      "1526: [discriminator loss: 0.512467, acc: 0.734375] [adversarial loss: 0.999861, acc: 0.312500]\n",
      "1527: [discriminator loss: 0.405430, acc: 0.812500] [adversarial loss: 1.514763, acc: 0.062500]\n",
      "1528: [discriminator loss: 0.516685, acc: 0.710938] [adversarial loss: 0.802790, acc: 0.515625]\n",
      "1529: [discriminator loss: 0.540364, acc: 0.695312] [adversarial loss: 2.292501, acc: 0.000000]\n",
      "1530: [discriminator loss: 0.592340, acc: 0.648438] [adversarial loss: 0.734589, acc: 0.609375]\n",
      "1531: [discriminator loss: 0.532393, acc: 0.726562] [adversarial loss: 2.228011, acc: 0.062500]\n",
      "1532: [discriminator loss: 0.620473, acc: 0.671875] [adversarial loss: 0.889534, acc: 0.406250]\n",
      "1533: [discriminator loss: 0.480691, acc: 0.757812] [adversarial loss: 1.386781, acc: 0.140625]\n",
      "1534: [discriminator loss: 0.432736, acc: 0.796875] [adversarial loss: 1.008876, acc: 0.265625]\n",
      "1535: [discriminator loss: 0.474488, acc: 0.789062] [adversarial loss: 1.413225, acc: 0.078125]\n",
      "1536: [discriminator loss: 0.445231, acc: 0.804688] [adversarial loss: 1.266462, acc: 0.171875]\n",
      "1537: [discriminator loss: 0.463280, acc: 0.773438] [adversarial loss: 1.177757, acc: 0.234375]\n",
      "1538: [discriminator loss: 0.455620, acc: 0.750000] [adversarial loss: 1.727546, acc: 0.093750]\n",
      "1539: [discriminator loss: 0.536210, acc: 0.742188] [adversarial loss: 0.790877, acc: 0.515625]\n",
      "1540: [discriminator loss: 0.508201, acc: 0.742188] [adversarial loss: 1.936648, acc: 0.031250]\n",
      "1541: [discriminator loss: 0.511846, acc: 0.718750] [adversarial loss: 0.930932, acc: 0.328125]\n",
      "1542: [discriminator loss: 0.494476, acc: 0.734375] [adversarial loss: 1.604435, acc: 0.062500]\n",
      "1543: [discriminator loss: 0.503053, acc: 0.757812] [adversarial loss: 1.152419, acc: 0.265625]\n",
      "1544: [discriminator loss: 0.524899, acc: 0.773438] [adversarial loss: 1.487662, acc: 0.093750]\n",
      "1545: [discriminator loss: 0.533079, acc: 0.726562] [adversarial loss: 0.816515, acc: 0.468750]\n",
      "1546: [discriminator loss: 0.497932, acc: 0.750000] [adversarial loss: 1.722691, acc: 0.062500]\n",
      "1547: [discriminator loss: 0.515197, acc: 0.718750] [adversarial loss: 1.025916, acc: 0.265625]\n",
      "1548: [discriminator loss: 0.479417, acc: 0.750000] [adversarial loss: 1.532725, acc: 0.156250]\n",
      "1549: [discriminator loss: 0.562056, acc: 0.765625] [adversarial loss: 0.737163, acc: 0.531250]\n",
      "1550: [discriminator loss: 0.553060, acc: 0.703125] [adversarial loss: 1.842782, acc: 0.046875]\n",
      "1551: [discriminator loss: 0.579726, acc: 0.671875] [adversarial loss: 0.712334, acc: 0.546875]\n",
      "1552: [discriminator loss: 0.431840, acc: 0.789062] [adversarial loss: 1.623944, acc: 0.046875]\n",
      "1553: [discriminator loss: 0.477781, acc: 0.757812] [adversarial loss: 0.981631, acc: 0.328125]\n",
      "1554: [discriminator loss: 0.536091, acc: 0.742188] [adversarial loss: 1.709742, acc: 0.031250]\n",
      "1555: [discriminator loss: 0.450573, acc: 0.789062] [adversarial loss: 1.019792, acc: 0.312500]\n",
      "1556: [discriminator loss: 0.499167, acc: 0.796875] [adversarial loss: 1.722203, acc: 0.062500]\n",
      "1557: [discriminator loss: 0.541130, acc: 0.734375] [adversarial loss: 0.800246, acc: 0.406250]\n",
      "1558: [discriminator loss: 0.502947, acc: 0.773438] [adversarial loss: 1.912420, acc: 0.046875]\n",
      "1559: [discriminator loss: 0.498618, acc: 0.726562] [adversarial loss: 0.888752, acc: 0.359375]\n",
      "1560: [discriminator loss: 0.540208, acc: 0.781250] [adversarial loss: 1.572969, acc: 0.062500]\n",
      "1561: [discriminator loss: 0.467961, acc: 0.781250] [adversarial loss: 1.238110, acc: 0.234375]\n",
      "1562: [discriminator loss: 0.423816, acc: 0.835938] [adversarial loss: 1.269216, acc: 0.203125]\n",
      "1563: [discriminator loss: 0.481118, acc: 0.765625] [adversarial loss: 0.887318, acc: 0.437500]\n",
      "1564: [discriminator loss: 0.434238, acc: 0.812500] [adversarial loss: 1.618681, acc: 0.031250]\n",
      "1565: [discriminator loss: 0.513848, acc: 0.750000] [adversarial loss: 1.025612, acc: 0.234375]\n",
      "1566: [discriminator loss: 0.471472, acc: 0.804688] [adversarial loss: 1.777565, acc: 0.062500]\n",
      "1567: [discriminator loss: 0.519496, acc: 0.781250] [adversarial loss: 0.936782, acc: 0.296875]\n",
      "1568: [discriminator loss: 0.509804, acc: 0.742188] [adversarial loss: 1.619102, acc: 0.046875]\n",
      "1569: [discriminator loss: 0.591614, acc: 0.656250] [adversarial loss: 0.618262, acc: 0.625000]\n",
      "1570: [discriminator loss: 0.571924, acc: 0.679688] [adversarial loss: 2.021212, acc: 0.046875]\n",
      "1571: [discriminator loss: 0.644867, acc: 0.609375] [adversarial loss: 0.934181, acc: 0.421875]\n",
      "1572: [discriminator loss: 0.544519, acc: 0.750000] [adversarial loss: 1.661528, acc: 0.031250]\n",
      "1573: [discriminator loss: 0.547909, acc: 0.750000] [adversarial loss: 0.880879, acc: 0.328125]\n",
      "1574: [discriminator loss: 0.528618, acc: 0.718750] [adversarial loss: 1.423377, acc: 0.156250]\n",
      "1575: [discriminator loss: 0.507830, acc: 0.773438] [adversarial loss: 0.964785, acc: 0.328125]\n",
      "1576: [discriminator loss: 0.478806, acc: 0.835938] [adversarial loss: 1.212910, acc: 0.234375]\n",
      "1577: [discriminator loss: 0.498875, acc: 0.781250] [adversarial loss: 1.213250, acc: 0.250000]\n",
      "1578: [discriminator loss: 0.520503, acc: 0.726562] [adversarial loss: 1.414813, acc: 0.109375]\n",
      "1579: [discriminator loss: 0.455269, acc: 0.781250] [adversarial loss: 1.065249, acc: 0.296875]\n",
      "1580: [discriminator loss: 0.522343, acc: 0.718750] [adversarial loss: 1.753609, acc: 0.015625]\n",
      "1581: [discriminator loss: 0.620170, acc: 0.648438] [adversarial loss: 0.800372, acc: 0.484375]\n",
      "1582: [discriminator loss: 0.565378, acc: 0.664062] [adversarial loss: 1.413479, acc: 0.125000]\n",
      "1583: [discriminator loss: 0.568172, acc: 0.734375] [adversarial loss: 0.849477, acc: 0.421875]\n",
      "1584: [discriminator loss: 0.508941, acc: 0.734375] [adversarial loss: 2.081514, acc: 0.015625]\n",
      "1585: [discriminator loss: 0.599654, acc: 0.671875] [adversarial loss: 0.749416, acc: 0.578125]\n",
      "1586: [discriminator loss: 0.577253, acc: 0.718750] [adversarial loss: 1.781388, acc: 0.031250]\n",
      "1587: [discriminator loss: 0.539096, acc: 0.703125] [adversarial loss: 0.826662, acc: 0.421875]\n",
      "1588: [discriminator loss: 0.556190, acc: 0.718750] [adversarial loss: 1.580423, acc: 0.078125]\n",
      "1589: [discriminator loss: 0.475713, acc: 0.781250] [adversarial loss: 0.980137, acc: 0.281250]\n",
      "1590: [discriminator loss: 0.474054, acc: 0.750000] [adversarial loss: 1.290690, acc: 0.203125]\n",
      "1591: [discriminator loss: 0.505940, acc: 0.765625] [adversarial loss: 0.924071, acc: 0.312500]\n",
      "1592: [discriminator loss: 0.480743, acc: 0.757812] [adversarial loss: 1.635421, acc: 0.062500]\n",
      "1593: [discriminator loss: 0.563307, acc: 0.664062] [adversarial loss: 0.879784, acc: 0.437500]\n",
      "1594: [discriminator loss: 0.629592, acc: 0.640625] [adversarial loss: 1.572976, acc: 0.109375]\n",
      "1595: [discriminator loss: 0.591003, acc: 0.773438] [adversarial loss: 0.875934, acc: 0.390625]\n",
      "1596: [discriminator loss: 0.474124, acc: 0.789062] [adversarial loss: 1.429148, acc: 0.125000]\n",
      "1597: [discriminator loss: 0.421991, acc: 0.843750] [adversarial loss: 1.238377, acc: 0.171875]\n",
      "1598: [discriminator loss: 0.455689, acc: 0.804688] [adversarial loss: 1.410395, acc: 0.125000]\n",
      "1599: [discriminator loss: 0.489390, acc: 0.773438] [adversarial loss: 0.904835, acc: 0.375000]\n",
      "1600: [discriminator loss: 0.523884, acc: 0.742188] [adversarial loss: 1.695165, acc: 0.031250]\n",
      "1601: [discriminator loss: 0.575706, acc: 0.679688] [adversarial loss: 0.760359, acc: 0.453125]\n",
      "1602: [discriminator loss: 0.587232, acc: 0.687500] [adversarial loss: 1.915759, acc: 0.062500]\n",
      "1603: [discriminator loss: 0.569126, acc: 0.726562] [adversarial loss: 0.938621, acc: 0.312500]\n",
      "1604: [discriminator loss: 0.483779, acc: 0.773438] [adversarial loss: 1.464236, acc: 0.109375]\n",
      "1605: [discriminator loss: 0.452828, acc: 0.773438] [adversarial loss: 1.147670, acc: 0.218750]\n",
      "1606: [discriminator loss: 0.524062, acc: 0.710938] [adversarial loss: 1.563359, acc: 0.078125]\n",
      "1607: [discriminator loss: 0.505879, acc: 0.757812] [adversarial loss: 0.888459, acc: 0.265625]\n",
      "1608: [discriminator loss: 0.562274, acc: 0.679688] [adversarial loss: 2.077930, acc: 0.046875]\n",
      "1609: [discriminator loss: 0.561615, acc: 0.679688] [adversarial loss: 0.841219, acc: 0.390625]\n",
      "1610: [discriminator loss: 0.463462, acc: 0.796875] [adversarial loss: 1.541298, acc: 0.125000]\n",
      "1611: [discriminator loss: 0.578864, acc: 0.687500] [adversarial loss: 0.938741, acc: 0.343750]\n",
      "1612: [discriminator loss: 0.545407, acc: 0.671875] [adversarial loss: 1.332557, acc: 0.140625]\n",
      "1613: [discriminator loss: 0.504036, acc: 0.750000] [adversarial loss: 1.069955, acc: 0.234375]\n",
      "1614: [discriminator loss: 0.525934, acc: 0.750000] [adversarial loss: 1.259418, acc: 0.140625]\n",
      "1615: [discriminator loss: 0.521280, acc: 0.789062] [adversarial loss: 1.040620, acc: 0.250000]\n",
      "1616: [discriminator loss: 0.432875, acc: 0.835938] [adversarial loss: 1.376527, acc: 0.125000]\n",
      "1617: [discriminator loss: 0.574255, acc: 0.671875] [adversarial loss: 0.832647, acc: 0.390625]\n",
      "1618: [discriminator loss: 0.571692, acc: 0.679688] [adversarial loss: 2.043872, acc: 0.046875]\n",
      "1619: [discriminator loss: 0.610337, acc: 0.687500] [adversarial loss: 0.730205, acc: 0.484375]\n",
      "1620: [discriminator loss: 0.560799, acc: 0.718750] [adversarial loss: 1.647668, acc: 0.046875]\n",
      "1621: [discriminator loss: 0.618188, acc: 0.632812] [adversarial loss: 0.830442, acc: 0.390625]\n",
      "1622: [discriminator loss: 0.531095, acc: 0.742188] [adversarial loss: 1.510747, acc: 0.156250]\n",
      "1623: [discriminator loss: 0.531397, acc: 0.757812] [adversarial loss: 0.962268, acc: 0.343750]\n",
      "1624: [discriminator loss: 0.498931, acc: 0.734375] [adversarial loss: 1.478356, acc: 0.109375]\n",
      "1625: [discriminator loss: 0.528886, acc: 0.734375] [adversarial loss: 1.105983, acc: 0.203125]\n",
      "1626: [discriminator loss: 0.483986, acc: 0.773438] [adversarial loss: 1.474309, acc: 0.078125]\n",
      "1627: [discriminator loss: 0.563605, acc: 0.726562] [adversarial loss: 1.300781, acc: 0.171875]\n",
      "1628: [discriminator loss: 0.501276, acc: 0.757812] [adversarial loss: 1.459484, acc: 0.031250]\n",
      "1629: [discriminator loss: 0.456435, acc: 0.781250] [adversarial loss: 0.952259, acc: 0.312500]\n",
      "1630: [discriminator loss: 0.506439, acc: 0.718750] [adversarial loss: 1.489735, acc: 0.031250]\n",
      "1631: [discriminator loss: 0.487260, acc: 0.750000] [adversarial loss: 1.007269, acc: 0.328125]\n",
      "1632: [discriminator loss: 0.470180, acc: 0.765625] [adversarial loss: 1.533206, acc: 0.109375]\n",
      "1633: [discriminator loss: 0.458230, acc: 0.781250] [adversarial loss: 0.919153, acc: 0.281250]\n",
      "1634: [discriminator loss: 0.578718, acc: 0.695312] [adversarial loss: 1.928726, acc: 0.000000]\n",
      "1635: [discriminator loss: 0.588871, acc: 0.687500] [adversarial loss: 0.742458, acc: 0.593750]\n",
      "1636: [discriminator loss: 0.565245, acc: 0.679688] [adversarial loss: 1.831582, acc: 0.046875]\n",
      "1637: [discriminator loss: 0.636418, acc: 0.664062] [adversarial loss: 0.856266, acc: 0.406250]\n",
      "1638: [discriminator loss: 0.523958, acc: 0.703125] [adversarial loss: 1.541014, acc: 0.078125]\n",
      "1639: [discriminator loss: 0.560567, acc: 0.703125] [adversarial loss: 0.771241, acc: 0.468750]\n",
      "1640: [discriminator loss: 0.452389, acc: 0.804688] [adversarial loss: 1.323141, acc: 0.171875]\n",
      "1641: [discriminator loss: 0.492293, acc: 0.750000] [adversarial loss: 1.034254, acc: 0.281250]\n",
      "1642: [discriminator loss: 0.465206, acc: 0.789062] [adversarial loss: 1.319470, acc: 0.109375]\n",
      "1643: [discriminator loss: 0.558245, acc: 0.679688] [adversarial loss: 1.124282, acc: 0.187500]\n",
      "1644: [discriminator loss: 0.487857, acc: 0.781250] [adversarial loss: 1.493478, acc: 0.109375]\n",
      "1645: [discriminator loss: 0.520603, acc: 0.757812] [adversarial loss: 0.705523, acc: 0.625000]\n",
      "1646: [discriminator loss: 0.562450, acc: 0.734375] [adversarial loss: 1.980264, acc: 0.031250]\n",
      "1647: [discriminator loss: 0.518501, acc: 0.789062] [adversarial loss: 0.943592, acc: 0.390625]\n",
      "1648: [discriminator loss: 0.552879, acc: 0.710938] [adversarial loss: 1.574817, acc: 0.078125]\n",
      "1649: [discriminator loss: 0.665755, acc: 0.632812] [adversarial loss: 0.679155, acc: 0.546875]\n",
      "1650: [discriminator loss: 0.512668, acc: 0.726562] [adversarial loss: 1.656068, acc: 0.031250]\n",
      "1651: [discriminator loss: 0.547141, acc: 0.734375] [adversarial loss: 0.768892, acc: 0.421875]\n",
      "1652: [discriminator loss: 0.455789, acc: 0.843750] [adversarial loss: 1.390637, acc: 0.140625]\n",
      "1653: [discriminator loss: 0.498966, acc: 0.789062] [adversarial loss: 0.968738, acc: 0.281250]\n",
      "1654: [discriminator loss: 0.486549, acc: 0.750000] [adversarial loss: 1.601827, acc: 0.046875]\n",
      "1655: [discriminator loss: 0.527004, acc: 0.765625] [adversarial loss: 1.060896, acc: 0.250000]\n",
      "1656: [discriminator loss: 0.559967, acc: 0.679688] [adversarial loss: 1.214286, acc: 0.187500]\n",
      "1657: [discriminator loss: 0.525831, acc: 0.750000] [adversarial loss: 1.160311, acc: 0.125000]\n",
      "1658: [discriminator loss: 0.445732, acc: 0.835938] [adversarial loss: 1.236615, acc: 0.171875]\n",
      "1659: [discriminator loss: 0.454114, acc: 0.820312] [adversarial loss: 1.368577, acc: 0.078125]\n",
      "1660: [discriminator loss: 0.530478, acc: 0.750000] [adversarial loss: 1.112394, acc: 0.234375]\n",
      "1661: [discriminator loss: 0.427560, acc: 0.843750] [adversarial loss: 1.520528, acc: 0.109375]\n",
      "1662: [discriminator loss: 0.546447, acc: 0.726562] [adversarial loss: 1.061725, acc: 0.328125]\n",
      "1663: [discriminator loss: 0.568518, acc: 0.718750] [adversarial loss: 1.838650, acc: 0.031250]\n",
      "1664: [discriminator loss: 0.552635, acc: 0.710938] [adversarial loss: 0.800986, acc: 0.437500]\n",
      "1665: [discriminator loss: 0.510338, acc: 0.718750] [adversarial loss: 1.994232, acc: 0.046875]\n",
      "1666: [discriminator loss: 0.650923, acc: 0.648438] [adversarial loss: 0.908250, acc: 0.343750]\n",
      "1667: [discriminator loss: 0.533050, acc: 0.726562] [adversarial loss: 1.624165, acc: 0.046875]\n",
      "1668: [discriminator loss: 0.493929, acc: 0.757812] [adversarial loss: 0.949730, acc: 0.343750]\n",
      "1669: [discriminator loss: 0.437523, acc: 0.804688] [adversarial loss: 1.605715, acc: 0.093750]\n",
      "1670: [discriminator loss: 0.567478, acc: 0.695312] [adversarial loss: 0.963338, acc: 0.265625]\n",
      "1671: [discriminator loss: 0.499301, acc: 0.734375] [adversarial loss: 1.658092, acc: 0.062500]\n",
      "1672: [discriminator loss: 0.512073, acc: 0.710938] [adversarial loss: 0.954102, acc: 0.359375]\n",
      "1673: [discriminator loss: 0.458512, acc: 0.804688] [adversarial loss: 1.572589, acc: 0.078125]\n",
      "1674: [discriminator loss: 0.501491, acc: 0.765625] [adversarial loss: 1.069730, acc: 0.281250]\n",
      "1675: [discriminator loss: 0.443085, acc: 0.843750] [adversarial loss: 1.385012, acc: 0.109375]\n",
      "1676: [discriminator loss: 0.563886, acc: 0.695312] [adversarial loss: 0.896441, acc: 0.515625]\n",
      "1677: [discriminator loss: 0.492415, acc: 0.718750] [adversarial loss: 1.531713, acc: 0.078125]\n",
      "1678: [discriminator loss: 0.493367, acc: 0.710938] [adversarial loss: 1.015055, acc: 0.375000]\n",
      "1679: [discriminator loss: 0.481012, acc: 0.773438] [adversarial loss: 1.751877, acc: 0.062500]\n",
      "1680: [discriminator loss: 0.577724, acc: 0.671875] [adversarial loss: 0.719509, acc: 0.531250]\n",
      "1681: [discriminator loss: 0.562634, acc: 0.695312] [adversarial loss: 1.897834, acc: 0.000000]\n",
      "1682: [discriminator loss: 0.491537, acc: 0.734375] [adversarial loss: 1.028621, acc: 0.250000]\n",
      "1683: [discriminator loss: 0.524481, acc: 0.820312] [adversarial loss: 1.347480, acc: 0.093750]\n",
      "1684: [discriminator loss: 0.442825, acc: 0.820312] [adversarial loss: 1.057066, acc: 0.250000]\n",
      "1685: [discriminator loss: 0.425783, acc: 0.828125] [adversarial loss: 1.519400, acc: 0.062500]\n",
      "1686: [discriminator loss: 0.454270, acc: 0.804688] [adversarial loss: 0.915856, acc: 0.328125]\n",
      "1687: [discriminator loss: 0.591368, acc: 0.656250] [adversarial loss: 2.249485, acc: 0.031250]\n",
      "1688: [discriminator loss: 0.566405, acc: 0.671875] [adversarial loss: 0.908796, acc: 0.390625]\n",
      "1689: [discriminator loss: 0.563601, acc: 0.687500] [adversarial loss: 1.578510, acc: 0.078125]\n",
      "1690: [discriminator loss: 0.501119, acc: 0.773438] [adversarial loss: 0.964085, acc: 0.312500]\n",
      "1691: [discriminator loss: 0.511676, acc: 0.734375] [adversarial loss: 1.798524, acc: 0.062500]\n",
      "1692: [discriminator loss: 0.495364, acc: 0.765625] [adversarial loss: 0.738485, acc: 0.515625]\n",
      "1693: [discriminator loss: 0.544360, acc: 0.710938] [adversarial loss: 1.784265, acc: 0.046875]\n",
      "1694: [discriminator loss: 0.585954, acc: 0.695312] [adversarial loss: 0.811523, acc: 0.484375]\n",
      "1695: [discriminator loss: 0.553384, acc: 0.695312] [adversarial loss: 1.555920, acc: 0.109375]\n",
      "1696: [discriminator loss: 0.457824, acc: 0.789062] [adversarial loss: 0.885424, acc: 0.421875]\n",
      "1697: [discriminator loss: 0.501314, acc: 0.742188] [adversarial loss: 1.422784, acc: 0.140625]\n",
      "1698: [discriminator loss: 0.466153, acc: 0.812500] [adversarial loss: 1.067044, acc: 0.234375]\n",
      "1699: [discriminator loss: 0.440136, acc: 0.859375] [adversarial loss: 1.572564, acc: 0.078125]\n",
      "1700: [discriminator loss: 0.504706, acc: 0.757812] [adversarial loss: 0.959892, acc: 0.296875]\n",
      "1701: [discriminator loss: 0.446160, acc: 0.789062] [adversarial loss: 1.651619, acc: 0.046875]\n",
      "1702: [discriminator loss: 0.532129, acc: 0.679688] [adversarial loss: 0.786829, acc: 0.484375]\n",
      "1703: [discriminator loss: 0.505674, acc: 0.750000] [adversarial loss: 2.011127, acc: 0.031250]\n",
      "1704: [discriminator loss: 0.509645, acc: 0.757812] [adversarial loss: 0.881683, acc: 0.421875]\n",
      "1705: [discriminator loss: 0.496301, acc: 0.789062] [adversarial loss: 1.516396, acc: 0.093750]\n",
      "1706: [discriminator loss: 0.544186, acc: 0.703125] [adversarial loss: 0.753980, acc: 0.437500]\n",
      "1707: [discriminator loss: 0.549639, acc: 0.710938] [adversarial loss: 1.502403, acc: 0.156250]\n",
      "1708: [discriminator loss: 0.533506, acc: 0.718750] [adversarial loss: 0.825271, acc: 0.453125]\n",
      "1709: [discriminator loss: 0.521763, acc: 0.726562] [adversarial loss: 1.759914, acc: 0.031250]\n",
      "1710: [discriminator loss: 0.550378, acc: 0.718750] [adversarial loss: 0.632314, acc: 0.656250]\n",
      "1711: [discriminator loss: 0.634279, acc: 0.617188] [adversarial loss: 2.070497, acc: 0.015625]\n",
      "1712: [discriminator loss: 0.534495, acc: 0.734375] [adversarial loss: 1.037974, acc: 0.296875]\n",
      "1713: [discriminator loss: 0.455160, acc: 0.843750] [adversarial loss: 1.550725, acc: 0.140625]\n",
      "1714: [discriminator loss: 0.394201, acc: 0.843750] [adversarial loss: 1.087637, acc: 0.312500]\n",
      "1715: [discriminator loss: 0.452410, acc: 0.781250] [adversarial loss: 1.133402, acc: 0.171875]\n",
      "1716: [discriminator loss: 0.447601, acc: 0.789062] [adversarial loss: 1.470277, acc: 0.140625]\n",
      "1717: [discriminator loss: 0.430763, acc: 0.843750] [adversarial loss: 0.889378, acc: 0.406250]\n",
      "1718: [discriminator loss: 0.473509, acc: 0.765625] [adversarial loss: 1.917080, acc: 0.062500]\n",
      "1719: [discriminator loss: 0.563002, acc: 0.679688] [adversarial loss: 0.823096, acc: 0.421875]\n",
      "1720: [discriminator loss: 0.506595, acc: 0.765625] [adversarial loss: 1.608962, acc: 0.062500]\n",
      "1721: [discriminator loss: 0.487234, acc: 0.757812] [adversarial loss: 0.993849, acc: 0.359375]\n",
      "1722: [discriminator loss: 0.498024, acc: 0.750000] [adversarial loss: 1.541220, acc: 0.078125]\n",
      "1723: [discriminator loss: 0.490570, acc: 0.765625] [adversarial loss: 0.936715, acc: 0.343750]\n",
      "1724: [discriminator loss: 0.473425, acc: 0.781250] [adversarial loss: 1.745728, acc: 0.109375]\n",
      "1725: [discriminator loss: 0.559191, acc: 0.671875] [adversarial loss: 0.773697, acc: 0.453125]\n",
      "1726: [discriminator loss: 0.500487, acc: 0.703125] [adversarial loss: 1.742349, acc: 0.109375]\n",
      "1727: [discriminator loss: 0.560780, acc: 0.726562] [adversarial loss: 0.764932, acc: 0.484375]\n",
      "1728: [discriminator loss: 0.501119, acc: 0.765625] [adversarial loss: 1.576049, acc: 0.093750]\n",
      "1729: [discriminator loss: 0.471958, acc: 0.796875] [adversarial loss: 1.001019, acc: 0.250000]\n",
      "1730: [discriminator loss: 0.461425, acc: 0.781250] [adversarial loss: 1.675511, acc: 0.062500]\n",
      "1731: [discriminator loss: 0.481635, acc: 0.757812] [adversarial loss: 1.034253, acc: 0.375000]\n",
      "1732: [discriminator loss: 0.483738, acc: 0.734375] [adversarial loss: 1.605531, acc: 0.093750]\n",
      "1733: [discriminator loss: 0.480052, acc: 0.781250] [adversarial loss: 1.201754, acc: 0.234375]\n",
      "1734: [discriminator loss: 0.434343, acc: 0.804688] [adversarial loss: 1.262552, acc: 0.234375]\n",
      "1735: [discriminator loss: 0.479699, acc: 0.781250] [adversarial loss: 1.222550, acc: 0.156250]\n",
      "1736: [discriminator loss: 0.475642, acc: 0.773438] [adversarial loss: 1.246161, acc: 0.203125]\n",
      "1737: [discriminator loss: 0.447872, acc: 0.796875] [adversarial loss: 0.824934, acc: 0.468750]\n",
      "1738: [discriminator loss: 0.503168, acc: 0.750000] [adversarial loss: 1.922585, acc: 0.078125]\n",
      "1739: [discriminator loss: 0.535858, acc: 0.695312] [adversarial loss: 0.575716, acc: 0.718750]\n",
      "1740: [discriminator loss: 0.606899, acc: 0.632812] [adversarial loss: 1.988887, acc: 0.062500]\n",
      "1741: [discriminator loss: 0.665577, acc: 0.671875] [adversarial loss: 0.938799, acc: 0.343750]\n",
      "1742: [discriminator loss: 0.525294, acc: 0.718750] [adversarial loss: 1.622684, acc: 0.109375]\n",
      "1743: [discriminator loss: 0.534709, acc: 0.687500] [adversarial loss: 1.067089, acc: 0.328125]\n",
      "1744: [discriminator loss: 0.444898, acc: 0.820312] [adversarial loss: 1.621283, acc: 0.046875]\n",
      "1745: [discriminator loss: 0.477319, acc: 0.750000] [adversarial loss: 0.810289, acc: 0.453125]\n",
      "1746: [discriminator loss: 0.481589, acc: 0.734375] [adversarial loss: 1.464065, acc: 0.156250]\n",
      "1747: [discriminator loss: 0.476786, acc: 0.773438] [adversarial loss: 1.241129, acc: 0.203125]\n",
      "1748: [discriminator loss: 0.510077, acc: 0.750000] [adversarial loss: 1.449017, acc: 0.156250]\n",
      "1749: [discriminator loss: 0.454775, acc: 0.796875] [adversarial loss: 1.131790, acc: 0.281250]\n",
      "1750: [discriminator loss: 0.447260, acc: 0.828125] [adversarial loss: 1.492809, acc: 0.140625]\n",
      "1751: [discriminator loss: 0.505868, acc: 0.726562] [adversarial loss: 0.808305, acc: 0.484375]\n",
      "1752: [discriminator loss: 0.546305, acc: 0.757812] [adversarial loss: 1.779658, acc: 0.046875]\n",
      "1753: [discriminator loss: 0.538334, acc: 0.710938] [adversarial loss: 0.739361, acc: 0.531250]\n",
      "1754: [discriminator loss: 0.609923, acc: 0.617188] [adversarial loss: 1.800717, acc: 0.062500]\n",
      "1755: [discriminator loss: 0.484577, acc: 0.773438] [adversarial loss: 0.871299, acc: 0.453125]\n",
      "1756: [discriminator loss: 0.529581, acc: 0.757812] [adversarial loss: 1.586116, acc: 0.140625]\n",
      "1757: [discriminator loss: 0.521388, acc: 0.750000] [adversarial loss: 1.123926, acc: 0.234375]\n",
      "1758: [discriminator loss: 0.454854, acc: 0.796875] [adversarial loss: 1.378872, acc: 0.156250]\n",
      "1759: [discriminator loss: 0.563420, acc: 0.695312] [adversarial loss: 1.315819, acc: 0.156250]\n",
      "1760: [discriminator loss: 0.471023, acc: 0.820312] [adversarial loss: 1.330198, acc: 0.140625]\n",
      "1761: [discriminator loss: 0.475259, acc: 0.781250] [adversarial loss: 1.136798, acc: 0.250000]\n",
      "1762: [discriminator loss: 0.470262, acc: 0.804688] [adversarial loss: 1.406663, acc: 0.109375]\n",
      "1763: [discriminator loss: 0.486716, acc: 0.757812] [adversarial loss: 1.298496, acc: 0.140625]\n",
      "1764: [discriminator loss: 0.489195, acc: 0.773438] [adversarial loss: 1.556636, acc: 0.078125]\n",
      "1765: [discriminator loss: 0.428088, acc: 0.796875] [adversarial loss: 1.024712, acc: 0.281250]\n",
      "1766: [discriminator loss: 0.610051, acc: 0.640625] [adversarial loss: 1.727622, acc: 0.093750]\n",
      "1767: [discriminator loss: 0.497941, acc: 0.726562] [adversarial loss: 0.541426, acc: 0.718750]\n",
      "1768: [discriminator loss: 0.689877, acc: 0.601562] [adversarial loss: 1.893070, acc: 0.046875]\n",
      "1769: [discriminator loss: 0.647946, acc: 0.671875] [adversarial loss: 0.948262, acc: 0.406250]\n",
      "1770: [discriminator loss: 0.594382, acc: 0.671875] [adversarial loss: 1.716967, acc: 0.093750]\n",
      "1771: [discriminator loss: 0.488066, acc: 0.812500] [adversarial loss: 1.096760, acc: 0.296875]\n",
      "1772: [discriminator loss: 0.464576, acc: 0.812500] [adversarial loss: 1.440287, acc: 0.125000]\n",
      "1773: [discriminator loss: 0.493262, acc: 0.757812] [adversarial loss: 1.183472, acc: 0.218750]\n",
      "1774: [discriminator loss: 0.508021, acc: 0.757812] [adversarial loss: 1.515478, acc: 0.093750]\n",
      "1775: [discriminator loss: 0.406282, acc: 0.835938] [adversarial loss: 0.890827, acc: 0.484375]\n",
      "1776: [discriminator loss: 0.592904, acc: 0.648438] [adversarial loss: 2.045904, acc: 0.046875]\n",
      "1777: [discriminator loss: 0.526777, acc: 0.734375] [adversarial loss: 0.670377, acc: 0.500000]\n",
      "1778: [discriminator loss: 0.625907, acc: 0.640625] [adversarial loss: 1.987076, acc: 0.078125]\n",
      "1779: [discriminator loss: 0.598319, acc: 0.695312] [adversarial loss: 0.767909, acc: 0.546875]\n",
      "1780: [discriminator loss: 0.506302, acc: 0.757812] [adversarial loss: 1.733647, acc: 0.078125]\n",
      "1781: [discriminator loss: 0.513318, acc: 0.773438] [adversarial loss: 0.825084, acc: 0.390625]\n",
      "1782: [discriminator loss: 0.518426, acc: 0.734375] [adversarial loss: 1.337616, acc: 0.203125]\n",
      "1783: [discriminator loss: 0.473841, acc: 0.796875] [adversarial loss: 1.071030, acc: 0.234375]\n",
      "1784: [discriminator loss: 0.494302, acc: 0.773438] [adversarial loss: 0.875899, acc: 0.375000]\n",
      "1785: [discriminator loss: 0.504924, acc: 0.742188] [adversarial loss: 1.215918, acc: 0.265625]\n",
      "1786: [discriminator loss: 0.531367, acc: 0.757812] [adversarial loss: 1.206242, acc: 0.156250]\n",
      "1787: [discriminator loss: 0.513520, acc: 0.789062] [adversarial loss: 1.049010, acc: 0.234375]\n",
      "1788: [discriminator loss: 0.436515, acc: 0.828125] [adversarial loss: 1.585583, acc: 0.078125]\n",
      "1789: [discriminator loss: 0.421215, acc: 0.835938] [adversarial loss: 0.979458, acc: 0.359375]\n",
      "1790: [discriminator loss: 0.519466, acc: 0.773438] [adversarial loss: 1.611057, acc: 0.093750]\n",
      "1791: [discriminator loss: 0.488177, acc: 0.773438] [adversarial loss: 0.754338, acc: 0.578125]\n",
      "1792: [discriminator loss: 0.565742, acc: 0.679688] [adversarial loss: 1.850996, acc: 0.062500]\n",
      "1793: [discriminator loss: 0.578481, acc: 0.695312] [adversarial loss: 0.723362, acc: 0.593750]\n",
      "1794: [discriminator loss: 0.541255, acc: 0.695312] [adversarial loss: 1.914553, acc: 0.000000]\n",
      "1795: [discriminator loss: 0.536647, acc: 0.679688] [adversarial loss: 0.998951, acc: 0.390625]\n",
      "1796: [discriminator loss: 0.452372, acc: 0.820312] [adversarial loss: 1.302861, acc: 0.125000]\n",
      "1797: [discriminator loss: 0.514058, acc: 0.757812] [adversarial loss: 1.390525, acc: 0.171875]\n",
      "1798: [discriminator loss: 0.487838, acc: 0.789062] [adversarial loss: 0.933911, acc: 0.437500]\n",
      "1799: [discriminator loss: 0.494616, acc: 0.742188] [adversarial loss: 1.184947, acc: 0.218750]\n",
      "1800: [discriminator loss: 0.490868, acc: 0.789062] [adversarial loss: 0.955179, acc: 0.359375]\n",
      "1801: [discriminator loss: 0.500183, acc: 0.781250] [adversarial loss: 1.560039, acc: 0.062500]\n",
      "1802: [discriminator loss: 0.483264, acc: 0.765625] [adversarial loss: 1.226521, acc: 0.140625]\n",
      "1803: [discriminator loss: 0.545954, acc: 0.726562] [adversarial loss: 1.491281, acc: 0.140625]\n",
      "1804: [discriminator loss: 0.547951, acc: 0.734375] [adversarial loss: 1.234830, acc: 0.281250]\n",
      "1805: [discriminator loss: 0.424574, acc: 0.859375] [adversarial loss: 1.344111, acc: 0.156250]\n",
      "1806: [discriminator loss: 0.423729, acc: 0.796875] [adversarial loss: 1.276100, acc: 0.171875]\n",
      "1807: [discriminator loss: 0.444885, acc: 0.796875] [adversarial loss: 1.206102, acc: 0.203125]\n",
      "1808: [discriminator loss: 0.498291, acc: 0.789062] [adversarial loss: 1.642550, acc: 0.078125]\n",
      "1809: [discriminator loss: 0.501504, acc: 0.765625] [adversarial loss: 0.789965, acc: 0.484375]\n",
      "1810: [discriminator loss: 0.537845, acc: 0.734375] [adversarial loss: 1.794767, acc: 0.078125]\n",
      "1811: [discriminator loss: 0.549586, acc: 0.671875] [adversarial loss: 0.673275, acc: 0.625000]\n",
      "1812: [discriminator loss: 0.547992, acc: 0.710938] [adversarial loss: 2.162709, acc: 0.031250]\n",
      "1813: [discriminator loss: 0.521535, acc: 0.726562] [adversarial loss: 0.874332, acc: 0.406250]\n",
      "1814: [discriminator loss: 0.545109, acc: 0.726562] [adversarial loss: 1.670250, acc: 0.031250]\n",
      "1815: [discriminator loss: 0.580512, acc: 0.718750] [adversarial loss: 1.051684, acc: 0.250000]\n",
      "1816: [discriminator loss: 0.534768, acc: 0.742188] [adversarial loss: 1.672734, acc: 0.062500]\n",
      "1817: [discriminator loss: 0.515210, acc: 0.757812] [adversarial loss: 0.942528, acc: 0.343750]\n",
      "1818: [discriminator loss: 0.564905, acc: 0.687500] [adversarial loss: 1.504354, acc: 0.125000]\n",
      "1819: [discriminator loss: 0.509809, acc: 0.765625] [adversarial loss: 0.951986, acc: 0.375000]\n",
      "1820: [discriminator loss: 0.515071, acc: 0.718750] [adversarial loss: 1.862814, acc: 0.046875]\n",
      "1821: [discriminator loss: 0.594832, acc: 0.671875] [adversarial loss: 0.715769, acc: 0.578125]\n",
      "1822: [discriminator loss: 0.566125, acc: 0.703125] [adversarial loss: 1.558291, acc: 0.078125]\n",
      "1823: [discriminator loss: 0.504873, acc: 0.734375] [adversarial loss: 0.896445, acc: 0.406250]\n",
      "1824: [discriminator loss: 0.515414, acc: 0.734375] [adversarial loss: 1.773797, acc: 0.046875]\n",
      "1825: [discriminator loss: 0.519496, acc: 0.687500] [adversarial loss: 1.001765, acc: 0.312500]\n",
      "1826: [discriminator loss: 0.476958, acc: 0.773438] [adversarial loss: 1.588521, acc: 0.046875]\n",
      "1827: [discriminator loss: 0.496228, acc: 0.765625] [adversarial loss: 1.233823, acc: 0.171875]\n",
      "1828: [discriminator loss: 0.442663, acc: 0.796875] [adversarial loss: 1.110152, acc: 0.281250]\n",
      "1829: [discriminator loss: 0.461170, acc: 0.804688] [adversarial loss: 1.235254, acc: 0.171875]\n",
      "1830: [discriminator loss: 0.498756, acc: 0.734375] [adversarial loss: 1.417036, acc: 0.140625]\n",
      "1831: [discriminator loss: 0.515838, acc: 0.757812] [adversarial loss: 0.908411, acc: 0.453125]\n",
      "1832: [discriminator loss: 0.504801, acc: 0.710938] [adversarial loss: 1.963178, acc: 0.046875]\n",
      "1833: [discriminator loss: 0.615488, acc: 0.656250] [adversarial loss: 0.614176, acc: 0.625000]\n",
      "1834: [discriminator loss: 0.539253, acc: 0.687500] [adversarial loss: 1.916666, acc: 0.015625]\n",
      "1835: [discriminator loss: 0.605367, acc: 0.687500] [adversarial loss: 0.750569, acc: 0.515625]\n",
      "1836: [discriminator loss: 0.514710, acc: 0.742188] [adversarial loss: 1.378622, acc: 0.187500]\n",
      "1837: [discriminator loss: 0.469068, acc: 0.812500] [adversarial loss: 1.205788, acc: 0.265625]\n",
      "1838: [discriminator loss: 0.439137, acc: 0.820312] [adversarial loss: 1.130863, acc: 0.265625]\n",
      "1839: [discriminator loss: 0.502002, acc: 0.804688] [adversarial loss: 1.093920, acc: 0.281250]\n",
      "1840: [discriminator loss: 0.452079, acc: 0.781250] [adversarial loss: 1.175541, acc: 0.156250]\n",
      "1841: [discriminator loss: 0.449535, acc: 0.804688] [adversarial loss: 1.557257, acc: 0.187500]\n",
      "1842: [discriminator loss: 0.531542, acc: 0.750000] [adversarial loss: 0.926213, acc: 0.375000]\n",
      "1843: [discriminator loss: 0.440599, acc: 0.757812] [adversarial loss: 1.885005, acc: 0.031250]\n",
      "1844: [discriminator loss: 0.488159, acc: 0.773438] [adversarial loss: 0.720727, acc: 0.515625]\n",
      "1845: [discriminator loss: 0.528721, acc: 0.703125] [adversarial loss: 2.004397, acc: 0.046875]\n",
      "1846: [discriminator loss: 0.569430, acc: 0.687500] [adversarial loss: 0.725618, acc: 0.531250]\n",
      "1847: [discriminator loss: 0.564860, acc: 0.687500] [adversarial loss: 1.694453, acc: 0.046875]\n",
      "1848: [discriminator loss: 0.449832, acc: 0.765625] [adversarial loss: 1.123281, acc: 0.281250]\n",
      "1849: [discriminator loss: 0.524746, acc: 0.695312] [adversarial loss: 1.346640, acc: 0.140625]\n",
      "1850: [discriminator loss: 0.503758, acc: 0.726562] [adversarial loss: 0.915937, acc: 0.468750]\n",
      "1851: [discriminator loss: 0.534681, acc: 0.734375] [adversarial loss: 1.847203, acc: 0.093750]\n",
      "1852: [discriminator loss: 0.507082, acc: 0.726562] [adversarial loss: 0.800533, acc: 0.453125]\n",
      "1853: [discriminator loss: 0.456811, acc: 0.789062] [adversarial loss: 1.538910, acc: 0.109375]\n",
      "1854: [discriminator loss: 0.543773, acc: 0.750000] [adversarial loss: 0.829375, acc: 0.421875]\n",
      "1855: [discriminator loss: 0.495974, acc: 0.734375] [adversarial loss: 1.272052, acc: 0.125000]\n",
      "1856: [discriminator loss: 0.394230, acc: 0.851562] [adversarial loss: 1.141688, acc: 0.312500]\n",
      "1857: [discriminator loss: 0.444610, acc: 0.804688] [adversarial loss: 1.328218, acc: 0.203125]\n",
      "1858: [discriminator loss: 0.554849, acc: 0.765625] [adversarial loss: 1.915653, acc: 0.062500]\n",
      "1859: [discriminator loss: 0.574177, acc: 0.710938] [adversarial loss: 0.651717, acc: 0.656250]\n",
      "1860: [discriminator loss: 0.629078, acc: 0.625000] [adversarial loss: 2.156175, acc: 0.046875]\n",
      "1861: [discriminator loss: 0.638184, acc: 0.648438] [adversarial loss: 0.670606, acc: 0.640625]\n",
      "1862: [discriminator loss: 0.523407, acc: 0.718750] [adversarial loss: 1.670148, acc: 0.078125]\n",
      "1863: [discriminator loss: 0.524007, acc: 0.726562] [adversarial loss: 0.939375, acc: 0.328125]\n",
      "1864: [discriminator loss: 0.441298, acc: 0.828125] [adversarial loss: 1.510171, acc: 0.062500]\n",
      "1865: [discriminator loss: 0.496695, acc: 0.789062] [adversarial loss: 1.041911, acc: 0.296875]\n",
      "1866: [discriminator loss: 0.493618, acc: 0.765625] [adversarial loss: 1.458023, acc: 0.015625]\n",
      "1867: [discriminator loss: 0.471303, acc: 0.773438] [adversarial loss: 1.123220, acc: 0.203125]\n",
      "1868: [discriminator loss: 0.424264, acc: 0.835938] [adversarial loss: 1.305781, acc: 0.109375]\n",
      "1869: [discriminator loss: 0.483386, acc: 0.773438] [adversarial loss: 1.171419, acc: 0.234375]\n",
      "1870: [discriminator loss: 0.509679, acc: 0.773438] [adversarial loss: 1.410041, acc: 0.140625]\n",
      "1871: [discriminator loss: 0.444679, acc: 0.804688] [adversarial loss: 0.808569, acc: 0.515625]\n",
      "1872: [discriminator loss: 0.505762, acc: 0.765625] [adversarial loss: 1.376427, acc: 0.140625]\n",
      "1873: [discriminator loss: 0.540021, acc: 0.718750] [adversarial loss: 0.756991, acc: 0.531250]\n",
      "1874: [discriminator loss: 0.572414, acc: 0.703125] [adversarial loss: 1.682443, acc: 0.046875]\n",
      "1875: [discriminator loss: 0.542241, acc: 0.695312] [adversarial loss: 0.893952, acc: 0.406250]\n",
      "1876: [discriminator loss: 0.494450, acc: 0.789062] [adversarial loss: 1.437272, acc: 0.156250]\n",
      "1877: [discriminator loss: 0.495161, acc: 0.757812] [adversarial loss: 1.262865, acc: 0.140625]\n",
      "1878: [discriminator loss: 0.465117, acc: 0.796875] [adversarial loss: 1.255351, acc: 0.171875]\n",
      "1879: [discriminator loss: 0.485693, acc: 0.750000] [adversarial loss: 1.196589, acc: 0.218750]\n",
      "1880: [discriminator loss: 0.426983, acc: 0.859375] [adversarial loss: 1.114232, acc: 0.296875]\n",
      "1881: [discriminator loss: 0.456270, acc: 0.789062] [adversarial loss: 1.491809, acc: 0.093750]\n",
      "1882: [discriminator loss: 0.460796, acc: 0.781250] [adversarial loss: 0.937354, acc: 0.375000]\n",
      "1883: [discriminator loss: 0.506276, acc: 0.789062] [adversarial loss: 1.517067, acc: 0.125000]\n",
      "1884: [discriminator loss: 0.478714, acc: 0.804688] [adversarial loss: 0.781993, acc: 0.546875]\n",
      "1885: [discriminator loss: 0.604029, acc: 0.671875] [adversarial loss: 2.277999, acc: 0.015625]\n",
      "1886: [discriminator loss: 0.695711, acc: 0.632812] [adversarial loss: 0.706372, acc: 0.578125]\n",
      "1887: [discriminator loss: 0.594715, acc: 0.664062] [adversarial loss: 1.853077, acc: 0.109375]\n",
      "1888: [discriminator loss: 0.581167, acc: 0.703125] [adversarial loss: 0.778346, acc: 0.484375]\n",
      "1889: [discriminator loss: 0.599774, acc: 0.679688] [adversarial loss: 1.239660, acc: 0.187500]\n",
      "1890: [discriminator loss: 0.477065, acc: 0.765625] [adversarial loss: 1.510627, acc: 0.078125]\n",
      "1891: [discriminator loss: 0.541556, acc: 0.687500] [adversarial loss: 0.921939, acc: 0.375000]\n",
      "1892: [discriminator loss: 0.551278, acc: 0.750000] [adversarial loss: 1.542709, acc: 0.125000]\n",
      "1893: [discriminator loss: 0.459897, acc: 0.796875] [adversarial loss: 1.097584, acc: 0.265625]\n",
      "1894: [discriminator loss: 0.608603, acc: 0.640625] [adversarial loss: 1.543603, acc: 0.093750]\n",
      "1895: [discriminator loss: 0.481054, acc: 0.773438] [adversarial loss: 1.014502, acc: 0.328125]\n",
      "1896: [discriminator loss: 0.503837, acc: 0.726562] [adversarial loss: 1.339692, acc: 0.218750]\n",
      "1897: [discriminator loss: 0.536184, acc: 0.687500] [adversarial loss: 1.474433, acc: 0.125000]\n",
      "1898: [discriminator loss: 0.447631, acc: 0.773438] [adversarial loss: 1.222624, acc: 0.281250]\n",
      "1899: [discriminator loss: 0.451040, acc: 0.789062] [adversarial loss: 1.242270, acc: 0.203125]\n",
      "1900: [discriminator loss: 0.447559, acc: 0.789062] [adversarial loss: 1.762703, acc: 0.093750]\n",
      "1901: [discriminator loss: 0.507453, acc: 0.757812] [adversarial loss: 0.795190, acc: 0.453125]\n",
      "1902: [discriminator loss: 0.567298, acc: 0.710938] [adversarial loss: 1.696261, acc: 0.062500]\n",
      "1903: [discriminator loss: 0.596004, acc: 0.687500] [adversarial loss: 0.905648, acc: 0.390625]\n",
      "1904: [discriminator loss: 0.540568, acc: 0.695312] [adversarial loss: 1.594570, acc: 0.093750]\n",
      "1905: [discriminator loss: 0.510068, acc: 0.796875] [adversarial loss: 1.025486, acc: 0.296875]\n",
      "1906: [discriminator loss: 0.493040, acc: 0.757812] [adversarial loss: 1.337103, acc: 0.109375]\n",
      "1907: [discriminator loss: 0.499638, acc: 0.843750] [adversarial loss: 1.381716, acc: 0.078125]\n",
      "1908: [discriminator loss: 0.476521, acc: 0.804688] [adversarial loss: 1.229902, acc: 0.250000]\n",
      "1909: [discriminator loss: 0.459254, acc: 0.773438] [adversarial loss: 1.024472, acc: 0.359375]\n",
      "1910: [discriminator loss: 0.468053, acc: 0.789062] [adversarial loss: 1.686475, acc: 0.046875]\n",
      "1911: [discriminator loss: 0.549035, acc: 0.687500] [adversarial loss: 0.959528, acc: 0.359375]\n",
      "1912: [discriminator loss: 0.466429, acc: 0.781250] [adversarial loss: 1.651955, acc: 0.125000]\n",
      "1913: [discriminator loss: 0.488843, acc: 0.742188] [adversarial loss: 0.937018, acc: 0.390625]\n",
      "1914: [discriminator loss: 0.466650, acc: 0.765625] [adversarial loss: 1.775066, acc: 0.046875]\n",
      "1915: [discriminator loss: 0.530567, acc: 0.726562] [adversarial loss: 1.064735, acc: 0.281250]\n",
      "1916: [discriminator loss: 0.501630, acc: 0.796875] [adversarial loss: 1.653394, acc: 0.125000]\n",
      "1917: [discriminator loss: 0.472867, acc: 0.796875] [adversarial loss: 1.063107, acc: 0.312500]\n",
      "1918: [discriminator loss: 0.562187, acc: 0.726562] [adversarial loss: 1.574667, acc: 0.125000]\n",
      "1919: [discriminator loss: 0.479144, acc: 0.789062] [adversarial loss: 0.884492, acc: 0.406250]\n",
      "1920: [discriminator loss: 0.472164, acc: 0.796875] [adversarial loss: 1.484363, acc: 0.109375]\n",
      "1921: [discriminator loss: 0.529349, acc: 0.718750] [adversarial loss: 0.765791, acc: 0.562500]\n",
      "1922: [discriminator loss: 0.507827, acc: 0.695312] [adversarial loss: 1.971957, acc: 0.046875]\n",
      "1923: [discriminator loss: 0.583699, acc: 0.750000] [adversarial loss: 0.925018, acc: 0.375000]\n",
      "1924: [discriminator loss: 0.582871, acc: 0.664062] [adversarial loss: 1.668139, acc: 0.093750]\n",
      "1925: [discriminator loss: 0.529819, acc: 0.757812] [adversarial loss: 1.028115, acc: 0.296875]\n",
      "1926: [discriminator loss: 0.490343, acc: 0.742188] [adversarial loss: 1.585247, acc: 0.109375]\n",
      "1927: [discriminator loss: 0.415440, acc: 0.851562] [adversarial loss: 1.033142, acc: 0.328125]\n",
      "1928: [discriminator loss: 0.541920, acc: 0.734375] [adversarial loss: 1.569600, acc: 0.093750]\n",
      "1929: [discriminator loss: 0.527600, acc: 0.734375] [adversarial loss: 1.065602, acc: 0.281250]\n",
      "1930: [discriminator loss: 0.521874, acc: 0.726562] [adversarial loss: 1.723516, acc: 0.078125]\n",
      "1931: [discriminator loss: 0.556173, acc: 0.726562] [adversarial loss: 0.806849, acc: 0.484375]\n",
      "1932: [discriminator loss: 0.491665, acc: 0.695312] [adversarial loss: 1.799511, acc: 0.062500]\n",
      "1933: [discriminator loss: 0.489867, acc: 0.789062] [adversarial loss: 1.147024, acc: 0.265625]\n",
      "1934: [discriminator loss: 0.538226, acc: 0.750000] [adversarial loss: 1.557111, acc: 0.109375]\n",
      "1935: [discriminator loss: 0.499654, acc: 0.742188] [adversarial loss: 1.005440, acc: 0.343750]\n",
      "1936: [discriminator loss: 0.495126, acc: 0.750000] [adversarial loss: 1.764875, acc: 0.062500]\n",
      "1937: [discriminator loss: 0.486118, acc: 0.742188] [adversarial loss: 0.980127, acc: 0.328125]\n",
      "1938: [discriminator loss: 0.499701, acc: 0.750000] [adversarial loss: 1.203060, acc: 0.281250]\n",
      "1939: [discriminator loss: 0.471920, acc: 0.796875] [adversarial loss: 1.297657, acc: 0.187500]\n",
      "1940: [discriminator loss: 0.541273, acc: 0.718750] [adversarial loss: 1.200577, acc: 0.234375]\n",
      "1941: [discriminator loss: 0.536319, acc: 0.750000] [adversarial loss: 1.598675, acc: 0.109375]\n",
      "1942: [discriminator loss: 0.492316, acc: 0.773438] [adversarial loss: 1.063673, acc: 0.296875]\n",
      "1943: [discriminator loss: 0.459047, acc: 0.804688] [adversarial loss: 1.575401, acc: 0.078125]\n",
      "1944: [discriminator loss: 0.460920, acc: 0.757812] [adversarial loss: 0.976865, acc: 0.312500]\n",
      "1945: [discriminator loss: 0.514472, acc: 0.742188] [adversarial loss: 1.390959, acc: 0.109375]\n",
      "1946: [discriminator loss: 0.454252, acc: 0.796875] [adversarial loss: 1.045774, acc: 0.328125]\n",
      "1947: [discriminator loss: 0.591669, acc: 0.664062] [adversarial loss: 2.059247, acc: 0.078125]\n",
      "1948: [discriminator loss: 0.571403, acc: 0.671875] [adversarial loss: 0.631913, acc: 0.546875]\n",
      "1949: [discriminator loss: 0.560420, acc: 0.656250] [adversarial loss: 1.713899, acc: 0.078125]\n",
      "1950: [discriminator loss: 0.457857, acc: 0.789062] [adversarial loss: 1.116646, acc: 0.296875]\n",
      "1951: [discriminator loss: 0.529650, acc: 0.734375] [adversarial loss: 1.523483, acc: 0.109375]\n",
      "1952: [discriminator loss: 0.481072, acc: 0.820312] [adversarial loss: 1.255456, acc: 0.312500]\n",
      "1953: [discriminator loss: 0.493520, acc: 0.750000] [adversarial loss: 1.218403, acc: 0.171875]\n",
      "1954: [discriminator loss: 0.517308, acc: 0.750000] [adversarial loss: 1.384126, acc: 0.171875]\n",
      "1955: [discriminator loss: 0.509674, acc: 0.742188] [adversarial loss: 0.782672, acc: 0.531250]\n",
      "1956: [discriminator loss: 0.529468, acc: 0.695312] [adversarial loss: 1.890276, acc: 0.015625]\n",
      "1957: [discriminator loss: 0.596901, acc: 0.687500] [adversarial loss: 0.806422, acc: 0.500000]\n",
      "1958: [discriminator loss: 0.592923, acc: 0.679688] [adversarial loss: 1.944633, acc: 0.000000]\n",
      "1959: [discriminator loss: 0.508858, acc: 0.734375] [adversarial loss: 0.970476, acc: 0.328125]\n",
      "1960: [discriminator loss: 0.494678, acc: 0.812500] [adversarial loss: 1.588991, acc: 0.140625]\n",
      "1961: [discriminator loss: 0.542036, acc: 0.710938] [adversarial loss: 1.033144, acc: 0.375000]\n",
      "1962: [discriminator loss: 0.499075, acc: 0.773438] [adversarial loss: 1.649333, acc: 0.062500]\n",
      "1963: [discriminator loss: 0.449719, acc: 0.804688] [adversarial loss: 1.120746, acc: 0.218750]\n",
      "1964: [discriminator loss: 0.449598, acc: 0.789062] [adversarial loss: 1.646321, acc: 0.140625]\n",
      "1965: [discriminator loss: 0.467716, acc: 0.789062] [adversarial loss: 1.313286, acc: 0.281250]\n",
      "1966: [discriminator loss: 0.507702, acc: 0.718750] [adversarial loss: 1.629222, acc: 0.140625]\n",
      "1967: [discriminator loss: 0.427569, acc: 0.804688] [adversarial loss: 0.954935, acc: 0.359375]\n",
      "1968: [discriminator loss: 0.470423, acc: 0.750000] [adversarial loss: 1.501859, acc: 0.140625]\n",
      "1969: [discriminator loss: 0.511052, acc: 0.734375] [adversarial loss: 1.294640, acc: 0.171875]\n",
      "1970: [discriminator loss: 0.551729, acc: 0.718750] [adversarial loss: 1.239776, acc: 0.156250]\n",
      "1971: [discriminator loss: 0.545845, acc: 0.687500] [adversarial loss: 1.522222, acc: 0.125000]\n",
      "1972: [discriminator loss: 0.456759, acc: 0.742188] [adversarial loss: 1.503981, acc: 0.171875]\n",
      "1973: [discriminator loss: 0.494200, acc: 0.750000] [adversarial loss: 0.806481, acc: 0.468750]\n",
      "1974: [discriminator loss: 0.538662, acc: 0.734375] [adversarial loss: 2.058074, acc: 0.031250]\n",
      "1975: [discriminator loss: 0.569154, acc: 0.703125] [adversarial loss: 0.647151, acc: 0.593750]\n",
      "1976: [discriminator loss: 0.515348, acc: 0.750000] [adversarial loss: 1.773616, acc: 0.046875]\n",
      "1977: [discriminator loss: 0.549409, acc: 0.695312] [adversarial loss: 0.809845, acc: 0.500000]\n",
      "1978: [discriminator loss: 0.551381, acc: 0.687500] [adversarial loss: 1.762245, acc: 0.046875]\n",
      "1979: [discriminator loss: 0.516643, acc: 0.757812] [adversarial loss: 0.988993, acc: 0.296875]\n",
      "1980: [discriminator loss: 0.470475, acc: 0.773438] [adversarial loss: 1.506333, acc: 0.171875]\n",
      "1981: [discriminator loss: 0.513809, acc: 0.726562] [adversarial loss: 1.031947, acc: 0.296875]\n",
      "1982: [discriminator loss: 0.513394, acc: 0.765625] [adversarial loss: 1.324836, acc: 0.171875]\n",
      "1983: [discriminator loss: 0.491156, acc: 0.757812] [adversarial loss: 1.358513, acc: 0.187500]\n",
      "1984: [discriminator loss: 0.460095, acc: 0.757812] [adversarial loss: 1.455480, acc: 0.093750]\n",
      "1985: [discriminator loss: 0.409208, acc: 0.875000] [adversarial loss: 1.211515, acc: 0.171875]\n",
      "1986: [discriminator loss: 0.494209, acc: 0.718750] [adversarial loss: 1.615150, acc: 0.093750]\n",
      "1987: [discriminator loss: 0.484206, acc: 0.757812] [adversarial loss: 1.119526, acc: 0.281250]\n",
      "1988: [discriminator loss: 0.510562, acc: 0.710938] [adversarial loss: 1.891999, acc: 0.062500]\n",
      "1989: [discriminator loss: 0.553527, acc: 0.726562] [adversarial loss: 0.935176, acc: 0.437500]\n",
      "1990: [discriminator loss: 0.526371, acc: 0.750000] [adversarial loss: 1.777149, acc: 0.062500]\n",
      "1991: [discriminator loss: 0.509556, acc: 0.742188] [adversarial loss: 0.915743, acc: 0.421875]\n",
      "1992: [discriminator loss: 0.504748, acc: 0.742188] [adversarial loss: 1.620373, acc: 0.078125]\n",
      "1993: [discriminator loss: 0.501731, acc: 0.765625] [adversarial loss: 0.909424, acc: 0.406250]\n",
      "1994: [discriminator loss: 0.554031, acc: 0.734375] [adversarial loss: 1.729133, acc: 0.125000]\n",
      "1995: [discriminator loss: 0.558321, acc: 0.695312] [adversarial loss: 0.814956, acc: 0.531250]\n",
      "1996: [discriminator loss: 0.655803, acc: 0.656250] [adversarial loss: 1.823319, acc: 0.031250]\n",
      "1997: [discriminator loss: 0.528043, acc: 0.687500] [adversarial loss: 0.892443, acc: 0.375000]\n",
      "1998: [discriminator loss: 0.514538, acc: 0.695312] [adversarial loss: 1.495250, acc: 0.093750]\n",
      "1999: [discriminator loss: 0.557991, acc: 0.742188] [adversarial loss: 0.949654, acc: 0.281250]\n",
      "2000: [discriminator loss: 0.472124, acc: 0.781250] [adversarial loss: 1.479587, acc: 0.109375]\n",
      "2001: [discriminator loss: 0.429409, acc: 0.804688] [adversarial loss: 1.086616, acc: 0.328125]\n",
      "2002: [discriminator loss: 0.412492, acc: 0.875000] [adversarial loss: 1.420716, acc: 0.171875]\n",
      "2003: [discriminator loss: 0.550697, acc: 0.718750] [adversarial loss: 0.800050, acc: 0.515625]\n",
      "2004: [discriminator loss: 0.457551, acc: 0.765625] [adversarial loss: 1.712361, acc: 0.093750]\n",
      "2005: [discriminator loss: 0.466881, acc: 0.804688] [adversarial loss: 0.997005, acc: 0.375000]\n",
      "2006: [discriminator loss: 0.568077, acc: 0.679688] [adversarial loss: 1.528716, acc: 0.093750]\n",
      "2007: [discriminator loss: 0.516285, acc: 0.750000] [adversarial loss: 0.994285, acc: 0.312500]\n",
      "2008: [discriminator loss: 0.435295, acc: 0.820312] [adversarial loss: 1.424633, acc: 0.140625]\n",
      "2009: [discriminator loss: 0.451585, acc: 0.812500] [adversarial loss: 1.219263, acc: 0.140625]\n",
      "2010: [discriminator loss: 0.466851, acc: 0.765625] [adversarial loss: 1.351763, acc: 0.203125]\n",
      "2011: [discriminator loss: 0.506943, acc: 0.757812] [adversarial loss: 1.025670, acc: 0.312500]\n",
      "2012: [discriminator loss: 0.491766, acc: 0.812500] [adversarial loss: 1.354871, acc: 0.171875]\n",
      "2013: [discriminator loss: 0.467236, acc: 0.796875] [adversarial loss: 1.279224, acc: 0.250000]\n",
      "2014: [discriminator loss: 0.531795, acc: 0.742188] [adversarial loss: 1.364692, acc: 0.140625]\n",
      "2015: [discriminator loss: 0.513343, acc: 0.773438] [adversarial loss: 1.322464, acc: 0.171875]\n",
      "2016: [discriminator loss: 0.514802, acc: 0.757812] [adversarial loss: 1.438173, acc: 0.187500]\n",
      "2017: [discriminator loss: 0.478815, acc: 0.718750] [adversarial loss: 1.144865, acc: 0.234375]\n",
      "2018: [discriminator loss: 0.398324, acc: 0.828125] [adversarial loss: 1.541094, acc: 0.140625]\n",
      "2019: [discriminator loss: 0.506034, acc: 0.734375] [adversarial loss: 0.882892, acc: 0.437500]\n",
      "2020: [discriminator loss: 0.556106, acc: 0.687500] [adversarial loss: 2.135593, acc: 0.062500]\n",
      "2021: [discriminator loss: 0.640122, acc: 0.671875] [adversarial loss: 0.617056, acc: 0.687500]\n",
      "2022: [discriminator loss: 0.517193, acc: 0.687500] [adversarial loss: 1.711639, acc: 0.093750]\n",
      "2023: [discriminator loss: 0.534406, acc: 0.710938] [adversarial loss: 0.969327, acc: 0.421875]\n",
      "2024: [discriminator loss: 0.447896, acc: 0.820312] [adversarial loss: 1.189522, acc: 0.234375]\n",
      "2025: [discriminator loss: 0.473146, acc: 0.789062] [adversarial loss: 1.329636, acc: 0.187500]\n",
      "2026: [discriminator loss: 0.516667, acc: 0.710938] [adversarial loss: 1.389310, acc: 0.171875]\n",
      "2027: [discriminator loss: 0.482883, acc: 0.765625] [adversarial loss: 1.528756, acc: 0.078125]\n",
      "2028: [discriminator loss: 0.561695, acc: 0.710938] [adversarial loss: 1.084558, acc: 0.312500]\n",
      "2029: [discriminator loss: 0.472980, acc: 0.796875] [adversarial loss: 1.515293, acc: 0.171875]\n",
      "2030: [discriminator loss: 0.472240, acc: 0.773438] [adversarial loss: 1.334863, acc: 0.203125]\n",
      "2031: [discriminator loss: 0.482662, acc: 0.796875] [adversarial loss: 1.823259, acc: 0.078125]\n",
      "2032: [discriminator loss: 0.527773, acc: 0.757812] [adversarial loss: 0.801168, acc: 0.500000]\n",
      "2033: [discriminator loss: 0.549325, acc: 0.726562] [adversarial loss: 2.003642, acc: 0.031250]\n",
      "2034: [discriminator loss: 0.540633, acc: 0.726562] [adversarial loss: 0.968882, acc: 0.343750]\n",
      "2035: [discriminator loss: 0.456918, acc: 0.757812] [adversarial loss: 1.492990, acc: 0.156250]\n",
      "2036: [discriminator loss: 0.544937, acc: 0.726562] [adversarial loss: 1.146934, acc: 0.265625]\n",
      "2037: [discriminator loss: 0.556446, acc: 0.710938] [adversarial loss: 1.857372, acc: 0.015625]\n",
      "2038: [discriminator loss: 0.484497, acc: 0.781250] [adversarial loss: 1.012372, acc: 0.343750]\n",
      "2039: [discriminator loss: 0.543321, acc: 0.710938] [adversarial loss: 1.815270, acc: 0.062500]\n",
      "2040: [discriminator loss: 0.585790, acc: 0.710938] [adversarial loss: 0.958746, acc: 0.359375]\n",
      "2041: [discriminator loss: 0.442233, acc: 0.812500] [adversarial loss: 1.161892, acc: 0.296875]\n",
      "2042: [discriminator loss: 0.462603, acc: 0.781250] [adversarial loss: 1.299129, acc: 0.234375]\n",
      "2043: [discriminator loss: 0.429645, acc: 0.812500] [adversarial loss: 1.111325, acc: 0.250000]\n",
      "2044: [discriminator loss: 0.496783, acc: 0.796875] [adversarial loss: 1.520558, acc: 0.125000]\n",
      "2045: [discriminator loss: 0.539907, acc: 0.757812] [adversarial loss: 1.191392, acc: 0.281250]\n",
      "2046: [discriminator loss: 0.553763, acc: 0.687500] [adversarial loss: 1.298517, acc: 0.187500]\n",
      "2047: [discriminator loss: 0.546027, acc: 0.703125] [adversarial loss: 1.316453, acc: 0.093750]\n",
      "2048: [discriminator loss: 0.511876, acc: 0.781250] [adversarial loss: 1.259225, acc: 0.203125]\n",
      "2049: [discriminator loss: 0.474733, acc: 0.804688] [adversarial loss: 1.149158, acc: 0.296875]\n",
      "2050: [discriminator loss: 0.499783, acc: 0.765625] [adversarial loss: 1.475591, acc: 0.093750]\n",
      "2051: [discriminator loss: 0.510691, acc: 0.765625] [adversarial loss: 1.172607, acc: 0.359375]\n",
      "2052: [discriminator loss: 0.578241, acc: 0.695312] [adversarial loss: 1.680739, acc: 0.109375]\n",
      "2053: [discriminator loss: 0.549734, acc: 0.757812] [adversarial loss: 0.919078, acc: 0.359375]\n",
      "2054: [discriminator loss: 0.518151, acc: 0.710938] [adversarial loss: 2.147591, acc: 0.000000]\n",
      "2055: [discriminator loss: 0.628030, acc: 0.648438] [adversarial loss: 0.713480, acc: 0.531250]\n",
      "2056: [discriminator loss: 0.652728, acc: 0.648438] [adversarial loss: 2.177320, acc: 0.046875]\n",
      "2057: [discriminator loss: 0.509810, acc: 0.757812] [adversarial loss: 0.883244, acc: 0.437500]\n",
      "2058: [discriminator loss: 0.495415, acc: 0.789062] [adversarial loss: 1.552993, acc: 0.093750]\n",
      "2059: [discriminator loss: 0.516232, acc: 0.773438] [adversarial loss: 0.888867, acc: 0.421875]\n",
      "2060: [discriminator loss: 0.540157, acc: 0.718750] [adversarial loss: 1.477161, acc: 0.156250]\n",
      "2061: [discriminator loss: 0.452874, acc: 0.828125] [adversarial loss: 0.947877, acc: 0.375000]\n",
      "2062: [discriminator loss: 0.466456, acc: 0.804688] [adversarial loss: 1.734936, acc: 0.046875]\n",
      "2063: [discriminator loss: 0.543055, acc: 0.710938] [adversarial loss: 0.815278, acc: 0.500000]\n",
      "2064: [discriminator loss: 0.531183, acc: 0.726562] [adversarial loss: 1.685896, acc: 0.031250]\n",
      "2065: [discriminator loss: 0.499989, acc: 0.750000] [adversarial loss: 1.196490, acc: 0.218750]\n",
      "2066: [discriminator loss: 0.415215, acc: 0.851562] [adversarial loss: 1.224972, acc: 0.203125]\n",
      "2067: [discriminator loss: 0.461037, acc: 0.820312] [adversarial loss: 1.393305, acc: 0.171875]\n",
      "2068: [discriminator loss: 0.464246, acc: 0.789062] [adversarial loss: 1.371287, acc: 0.187500]\n",
      "2069: [discriminator loss: 0.508689, acc: 0.750000] [adversarial loss: 1.003873, acc: 0.312500]\n",
      "2070: [discriminator loss: 0.485228, acc: 0.734375] [adversarial loss: 1.981344, acc: 0.093750]\n",
      "2071: [discriminator loss: 0.569639, acc: 0.718750] [adversarial loss: 0.930615, acc: 0.406250]\n",
      "2072: [discriminator loss: 0.502210, acc: 0.742188] [adversarial loss: 1.532291, acc: 0.078125]\n",
      "2073: [discriminator loss: 0.466180, acc: 0.820312] [adversarial loss: 1.284511, acc: 0.156250]\n",
      "2074: [discriminator loss: 0.536573, acc: 0.726562] [adversarial loss: 1.068067, acc: 0.328125]\n",
      "2075: [discriminator loss: 0.546994, acc: 0.710938] [adversarial loss: 1.598818, acc: 0.046875]\n",
      "2076: [discriminator loss: 0.510046, acc: 0.765625] [adversarial loss: 0.912487, acc: 0.375000]\n",
      "2077: [discriminator loss: 0.566902, acc: 0.742188] [adversarial loss: 1.784155, acc: 0.046875]\n",
      "2078: [discriminator loss: 0.551028, acc: 0.750000] [adversarial loss: 1.054819, acc: 0.328125]\n",
      "2079: [discriminator loss: 0.550672, acc: 0.679688] [adversarial loss: 1.951558, acc: 0.078125]\n",
      "2080: [discriminator loss: 0.578250, acc: 0.773438] [adversarial loss: 1.043274, acc: 0.359375]\n",
      "2081: [discriminator loss: 0.500532, acc: 0.757812] [adversarial loss: 1.503874, acc: 0.093750]\n",
      "2082: [discriminator loss: 0.533440, acc: 0.757812] [adversarial loss: 1.286717, acc: 0.093750]\n",
      "2083: [discriminator loss: 0.487963, acc: 0.789062] [adversarial loss: 1.195449, acc: 0.296875]\n",
      "2084: [discriminator loss: 0.488836, acc: 0.765625] [adversarial loss: 1.365986, acc: 0.171875]\n",
      "2085: [discriminator loss: 0.473254, acc: 0.812500] [adversarial loss: 1.286617, acc: 0.265625]\n",
      "2086: [discriminator loss: 0.412098, acc: 0.843750] [adversarial loss: 1.536796, acc: 0.125000]\n",
      "2087: [discriminator loss: 0.459637, acc: 0.734375] [adversarial loss: 1.267252, acc: 0.156250]\n",
      "2088: [discriminator loss: 0.532725, acc: 0.734375] [adversarial loss: 1.056383, acc: 0.390625]\n",
      "2089: [discriminator loss: 0.594245, acc: 0.679688] [adversarial loss: 1.609655, acc: 0.078125]\n",
      "2090: [discriminator loss: 0.454707, acc: 0.812500] [adversarial loss: 1.003011, acc: 0.406250]\n",
      "2091: [discriminator loss: 0.518173, acc: 0.718750] [adversarial loss: 1.786767, acc: 0.062500]\n",
      "2092: [discriminator loss: 0.507422, acc: 0.781250] [adversarial loss: 1.036986, acc: 0.296875]\n",
      "2093: [discriminator loss: 0.509023, acc: 0.750000] [adversarial loss: 1.424267, acc: 0.171875]\n",
      "2094: [discriminator loss: 0.495196, acc: 0.742188] [adversarial loss: 0.969310, acc: 0.437500]\n",
      "2095: [discriminator loss: 0.498096, acc: 0.781250] [adversarial loss: 1.941009, acc: 0.015625]\n",
      "2096: [discriminator loss: 0.573687, acc: 0.703125] [adversarial loss: 0.775565, acc: 0.515625]\n",
      "2097: [discriminator loss: 0.543796, acc: 0.703125] [adversarial loss: 1.911225, acc: 0.046875]\n",
      "2098: [discriminator loss: 0.484097, acc: 0.742188] [adversarial loss: 0.948796, acc: 0.406250]\n",
      "2099: [discriminator loss: 0.471654, acc: 0.757812] [adversarial loss: 1.588606, acc: 0.093750]\n",
      "2100: [discriminator loss: 0.485591, acc: 0.789062] [adversarial loss: 1.056806, acc: 0.328125]\n",
      "2101: [discriminator loss: 0.501608, acc: 0.726562] [adversarial loss: 1.607511, acc: 0.140625]\n",
      "2102: [discriminator loss: 0.494291, acc: 0.757812] [adversarial loss: 1.089290, acc: 0.250000]\n",
      "2103: [discriminator loss: 0.571298, acc: 0.703125] [adversarial loss: 1.591414, acc: 0.125000]\n",
      "2104: [discriminator loss: 0.591760, acc: 0.710938] [adversarial loss: 0.839385, acc: 0.453125]\n",
      "2105: [discriminator loss: 0.600835, acc: 0.695312] [adversarial loss: 1.806973, acc: 0.031250]\n",
      "2106: [discriminator loss: 0.470686, acc: 0.765625] [adversarial loss: 1.055214, acc: 0.328125]\n",
      "2107: [discriminator loss: 0.468811, acc: 0.789062] [adversarial loss: 1.643864, acc: 0.046875]\n",
      "2108: [discriminator loss: 0.497390, acc: 0.718750] [adversarial loss: 1.167554, acc: 0.203125]\n",
      "2109: [discriminator loss: 0.386412, acc: 0.867188] [adversarial loss: 1.235231, acc: 0.265625]\n",
      "2110: [discriminator loss: 0.516019, acc: 0.742188] [adversarial loss: 1.177951, acc: 0.250000]\n",
      "2111: [discriminator loss: 0.493627, acc: 0.781250] [adversarial loss: 1.387743, acc: 0.140625]\n",
      "2112: [discriminator loss: 0.443042, acc: 0.804688] [adversarial loss: 1.188031, acc: 0.312500]\n",
      "2113: [discriminator loss: 0.471167, acc: 0.820312] [adversarial loss: 1.471391, acc: 0.171875]\n",
      "2114: [discriminator loss: 0.487014, acc: 0.781250] [adversarial loss: 1.438364, acc: 0.078125]\n",
      "2115: [discriminator loss: 0.533567, acc: 0.710938] [adversarial loss: 1.243546, acc: 0.234375]\n",
      "2116: [discriminator loss: 0.412799, acc: 0.843750] [adversarial loss: 1.470680, acc: 0.109375]\n",
      "2117: [discriminator loss: 0.512268, acc: 0.734375] [adversarial loss: 1.036631, acc: 0.296875]\n",
      "2118: [discriminator loss: 0.532365, acc: 0.757812] [adversarial loss: 2.044942, acc: 0.046875]\n",
      "2119: [discriminator loss: 0.514596, acc: 0.757812] [adversarial loss: 0.883090, acc: 0.437500]\n",
      "2120: [discriminator loss: 0.548355, acc: 0.710938] [adversarial loss: 1.757341, acc: 0.093750]\n",
      "2121: [discriminator loss: 0.589628, acc: 0.695312] [adversarial loss: 0.817542, acc: 0.546875]\n",
      "2122: [discriminator loss: 0.590473, acc: 0.609375] [adversarial loss: 1.988447, acc: 0.093750]\n",
      "2123: [discriminator loss: 0.491654, acc: 0.742188] [adversarial loss: 1.150018, acc: 0.312500]\n",
      "2124: [discriminator loss: 0.536133, acc: 0.742188] [adversarial loss: 1.522467, acc: 0.109375]\n",
      "2125: [discriminator loss: 0.508076, acc: 0.789062] [adversarial loss: 0.838418, acc: 0.421875]\n",
      "2126: [discriminator loss: 0.545269, acc: 0.757812] [adversarial loss: 1.599573, acc: 0.093750]\n",
      "2127: [discriminator loss: 0.437836, acc: 0.789062] [adversarial loss: 1.102970, acc: 0.328125]\n",
      "2128: [discriminator loss: 0.387832, acc: 0.851562] [adversarial loss: 1.191969, acc: 0.187500]\n",
      "2129: [discriminator loss: 0.493180, acc: 0.781250] [adversarial loss: 1.296032, acc: 0.203125]\n",
      "2130: [discriminator loss: 0.520512, acc: 0.750000] [adversarial loss: 1.613696, acc: 0.078125]\n",
      "2131: [discriminator loss: 0.531517, acc: 0.742188] [adversarial loss: 1.436139, acc: 0.156250]\n",
      "2132: [discriminator loss: 0.484447, acc: 0.804688] [adversarial loss: 1.670553, acc: 0.109375]\n",
      "2133: [discriminator loss: 0.511717, acc: 0.734375] [adversarial loss: 0.980175, acc: 0.375000]\n",
      "2134: [discriminator loss: 0.495350, acc: 0.750000] [adversarial loss: 2.069215, acc: 0.031250]\n",
      "2135: [discriminator loss: 0.514338, acc: 0.734375] [adversarial loss: 0.984683, acc: 0.359375]\n",
      "2136: [discriminator loss: 0.505930, acc: 0.718750] [adversarial loss: 1.707234, acc: 0.078125]\n",
      "2137: [discriminator loss: 0.467168, acc: 0.757812] [adversarial loss: 0.844337, acc: 0.500000]\n",
      "2138: [discriminator loss: 0.550776, acc: 0.695312] [adversarial loss: 1.914779, acc: 0.031250]\n",
      "2139: [discriminator loss: 0.537853, acc: 0.742188] [adversarial loss: 0.880971, acc: 0.390625]\n",
      "2140: [discriminator loss: 0.544017, acc: 0.718750] [adversarial loss: 1.744644, acc: 0.140625]\n",
      "2141: [discriminator loss: 0.512048, acc: 0.726562] [adversarial loss: 1.030860, acc: 0.265625]\n",
      "2142: [discriminator loss: 0.495726, acc: 0.703125] [adversarial loss: 1.628654, acc: 0.109375]\n",
      "2143: [discriminator loss: 0.506174, acc: 0.781250] [adversarial loss: 0.940105, acc: 0.390625]\n",
      "2144: [discriminator loss: 0.527876, acc: 0.742188] [adversarial loss: 1.368329, acc: 0.156250]\n",
      "2145: [discriminator loss: 0.493128, acc: 0.781250] [adversarial loss: 1.071112, acc: 0.343750]\n",
      "2146: [discriminator loss: 0.531681, acc: 0.750000] [adversarial loss: 1.604353, acc: 0.093750]\n",
      "2147: [discriminator loss: 0.475388, acc: 0.757812] [adversarial loss: 1.244418, acc: 0.250000]\n",
      "2148: [discriminator loss: 0.412455, acc: 0.828125] [adversarial loss: 1.109064, acc: 0.281250]\n",
      "2149: [discriminator loss: 0.522286, acc: 0.757812] [adversarial loss: 1.670462, acc: 0.109375]\n",
      "2150: [discriminator loss: 0.440658, acc: 0.796875] [adversarial loss: 1.314969, acc: 0.171875]\n",
      "2151: [discriminator loss: 0.529745, acc: 0.750000] [adversarial loss: 1.972015, acc: 0.062500]\n",
      "2152: [discriminator loss: 0.491014, acc: 0.750000] [adversarial loss: 1.102016, acc: 0.281250]\n",
      "2153: [discriminator loss: 0.457006, acc: 0.781250] [adversarial loss: 1.433562, acc: 0.156250]\n",
      "2154: [discriminator loss: 0.480601, acc: 0.773438] [adversarial loss: 1.237014, acc: 0.171875]\n",
      "2155: [discriminator loss: 0.499708, acc: 0.750000] [adversarial loss: 1.258464, acc: 0.140625]\n",
      "2156: [discriminator loss: 0.484669, acc: 0.773438] [adversarial loss: 1.354936, acc: 0.171875]\n",
      "2157: [discriminator loss: 0.542359, acc: 0.742188] [adversarial loss: 1.370477, acc: 0.203125]\n",
      "2158: [discriminator loss: 0.501072, acc: 0.742188] [adversarial loss: 1.394412, acc: 0.218750]\n",
      "2159: [discriminator loss: 0.489166, acc: 0.757812] [adversarial loss: 1.266954, acc: 0.156250]\n",
      "2160: [discriminator loss: 0.486757, acc: 0.757812] [adversarial loss: 1.302997, acc: 0.265625]\n",
      "2161: [discriminator loss: 0.512537, acc: 0.757812] [adversarial loss: 1.664157, acc: 0.093750]\n",
      "2162: [discriminator loss: 0.451687, acc: 0.796875] [adversarial loss: 1.274340, acc: 0.234375]\n",
      "2163: [discriminator loss: 0.448934, acc: 0.781250] [adversarial loss: 1.637419, acc: 0.156250]\n",
      "2164: [discriminator loss: 0.435438, acc: 0.796875] [adversarial loss: 1.441228, acc: 0.218750]\n",
      "2165: [discriminator loss: 0.494224, acc: 0.710938] [adversarial loss: 1.424961, acc: 0.140625]\n",
      "2166: [discriminator loss: 0.442666, acc: 0.812500] [adversarial loss: 0.884603, acc: 0.453125]\n",
      "2167: [discriminator loss: 0.513912, acc: 0.789062] [adversarial loss: 2.159175, acc: 0.046875]\n",
      "2168: [discriminator loss: 0.589377, acc: 0.710938] [adversarial loss: 0.630610, acc: 0.656250]\n",
      "2169: [discriminator loss: 0.557489, acc: 0.687500] [adversarial loss: 2.199437, acc: 0.062500]\n",
      "2170: [discriminator loss: 0.640331, acc: 0.632812] [adversarial loss: 0.806729, acc: 0.468750]\n",
      "2171: [discriminator loss: 0.592972, acc: 0.679688] [adversarial loss: 1.574052, acc: 0.093750]\n",
      "2172: [discriminator loss: 0.539117, acc: 0.734375] [adversarial loss: 1.244002, acc: 0.203125]\n",
      "2173: [discriminator loss: 0.510026, acc: 0.773438] [adversarial loss: 1.317903, acc: 0.140625]\n",
      "2174: [discriminator loss: 0.513838, acc: 0.773438] [adversarial loss: 1.354650, acc: 0.187500]\n",
      "2175: [discriminator loss: 0.528099, acc: 0.734375] [adversarial loss: 1.359093, acc: 0.203125]\n",
      "2176: [discriminator loss: 0.531026, acc: 0.726562] [adversarial loss: 0.882282, acc: 0.484375]\n",
      "2177: [discriminator loss: 0.526625, acc: 0.710938] [adversarial loss: 1.964923, acc: 0.031250]\n",
      "2178: [discriminator loss: 0.563156, acc: 0.664062] [adversarial loss: 0.958791, acc: 0.406250]\n",
      "2179: [discriminator loss: 0.499382, acc: 0.757812] [adversarial loss: 1.605228, acc: 0.109375]\n",
      "2180: [discriminator loss: 0.483833, acc: 0.765625] [adversarial loss: 0.931681, acc: 0.312500]\n",
      "2181: [discriminator loss: 0.465791, acc: 0.765625] [adversarial loss: 1.717558, acc: 0.125000]\n",
      "2182: [discriminator loss: 0.475842, acc: 0.796875] [adversarial loss: 1.192047, acc: 0.218750]\n",
      "2183: [discriminator loss: 0.474765, acc: 0.804688] [adversarial loss: 1.507932, acc: 0.140625]\n",
      "2184: [discriminator loss: 0.517020, acc: 0.757812] [adversarial loss: 1.660887, acc: 0.093750]\n",
      "2185: [discriminator loss: 0.481212, acc: 0.789062] [adversarial loss: 1.156373, acc: 0.281250]\n",
      "2186: [discriminator loss: 0.518083, acc: 0.726562] [adversarial loss: 1.480881, acc: 0.125000]\n",
      "2187: [discriminator loss: 0.429074, acc: 0.804688] [adversarial loss: 0.921655, acc: 0.468750]\n",
      "2188: [discriminator loss: 0.518037, acc: 0.726562] [adversarial loss: 1.891220, acc: 0.109375]\n",
      "2189: [discriminator loss: 0.491810, acc: 0.765625] [adversarial loss: 0.980470, acc: 0.375000]\n",
      "2190: [discriminator loss: 0.519570, acc: 0.742188] [adversarial loss: 1.538927, acc: 0.125000]\n",
      "2191: [discriminator loss: 0.491726, acc: 0.765625] [adversarial loss: 1.069526, acc: 0.343750]\n",
      "2192: [discriminator loss: 0.507385, acc: 0.742188] [adversarial loss: 1.684641, acc: 0.125000]\n",
      "2193: [discriminator loss: 0.520417, acc: 0.757812] [adversarial loss: 1.054347, acc: 0.375000]\n",
      "2194: [discriminator loss: 0.511469, acc: 0.773438] [adversarial loss: 1.505837, acc: 0.093750]\n",
      "2195: [discriminator loss: 0.498560, acc: 0.750000] [adversarial loss: 0.975340, acc: 0.375000]\n",
      "2196: [discriminator loss: 0.528282, acc: 0.765625] [adversarial loss: 1.431919, acc: 0.203125]\n",
      "2197: [discriminator loss: 0.519763, acc: 0.734375] [adversarial loss: 0.776647, acc: 0.515625]\n",
      "2198: [discriminator loss: 0.475877, acc: 0.742188] [adversarial loss: 2.035868, acc: 0.000000]\n",
      "2199: [discriminator loss: 0.590116, acc: 0.695312] [adversarial loss: 0.693613, acc: 0.578125]\n",
      "2200: [discriminator loss: 0.563007, acc: 0.671875] [adversarial loss: 1.747092, acc: 0.062500]\n",
      "2201: [discriminator loss: 0.480883, acc: 0.773438] [adversarial loss: 1.134629, acc: 0.281250]\n",
      "2202: [discriminator loss: 0.454140, acc: 0.781250] [adversarial loss: 1.279571, acc: 0.171875]\n",
      "2203: [discriminator loss: 0.444384, acc: 0.796875] [adversarial loss: 1.373357, acc: 0.093750]\n",
      "2204: [discriminator loss: 0.474975, acc: 0.765625] [adversarial loss: 1.296739, acc: 0.203125]\n",
      "2205: [discriminator loss: 0.500297, acc: 0.757812] [adversarial loss: 1.430467, acc: 0.156250]\n",
      "2206: [discriminator loss: 0.433806, acc: 0.789062] [adversarial loss: 1.429524, acc: 0.140625]\n",
      "2207: [discriminator loss: 0.436568, acc: 0.796875] [adversarial loss: 1.592039, acc: 0.156250]\n",
      "2208: [discriminator loss: 0.525688, acc: 0.773438] [adversarial loss: 1.143085, acc: 0.296875]\n",
      "2209: [discriminator loss: 0.482203, acc: 0.804688] [adversarial loss: 1.371376, acc: 0.156250]\n",
      "2210: [discriminator loss: 0.440282, acc: 0.812500] [adversarial loss: 1.265806, acc: 0.171875]\n",
      "2211: [discriminator loss: 0.532325, acc: 0.773438] [adversarial loss: 0.958676, acc: 0.453125]\n",
      "2212: [discriminator loss: 0.510360, acc: 0.695312] [adversarial loss: 1.849166, acc: 0.046875]\n",
      "2213: [discriminator loss: 0.569324, acc: 0.734375] [adversarial loss: 0.875388, acc: 0.421875]\n",
      "2214: [discriminator loss: 0.507069, acc: 0.742188] [adversarial loss: 1.770846, acc: 0.140625]\n",
      "2215: [discriminator loss: 0.497676, acc: 0.789062] [adversarial loss: 0.993293, acc: 0.328125]\n",
      "2216: [discriminator loss: 0.510949, acc: 0.750000] [adversarial loss: 1.684350, acc: 0.093750]\n",
      "2217: [discriminator loss: 0.608269, acc: 0.695312] [adversarial loss: 0.738718, acc: 0.531250]\n",
      "2218: [discriminator loss: 0.659734, acc: 0.617188] [adversarial loss: 2.034599, acc: 0.046875]\n",
      "2219: [discriminator loss: 0.558797, acc: 0.718750] [adversarial loss: 0.924058, acc: 0.343750]\n",
      "2220: [discriminator loss: 0.478706, acc: 0.734375] [adversarial loss: 1.691509, acc: 0.109375]\n",
      "2221: [discriminator loss: 0.473691, acc: 0.773438] [adversarial loss: 1.122163, acc: 0.265625]\n",
      "2222: [discriminator loss: 0.489197, acc: 0.773438] [adversarial loss: 1.301741, acc: 0.156250]\n",
      "2223: [discriminator loss: 0.445596, acc: 0.796875] [adversarial loss: 1.375080, acc: 0.109375]\n",
      "2224: [discriminator loss: 0.498815, acc: 0.765625] [adversarial loss: 1.275505, acc: 0.218750]\n",
      "2225: [discriminator loss: 0.534272, acc: 0.718750] [adversarial loss: 1.548551, acc: 0.093750]\n",
      "2226: [discriminator loss: 0.457921, acc: 0.757812] [adversarial loss: 1.018665, acc: 0.359375]\n",
      "2227: [discriminator loss: 0.500731, acc: 0.726562] [adversarial loss: 1.924208, acc: 0.015625]\n",
      "2228: [discriminator loss: 0.547159, acc: 0.742188] [adversarial loss: 1.112172, acc: 0.265625]\n",
      "2229: [discriminator loss: 0.583171, acc: 0.726562] [adversarial loss: 1.874459, acc: 0.078125]\n",
      "2230: [discriminator loss: 0.499600, acc: 0.757812] [adversarial loss: 0.945349, acc: 0.406250]\n",
      "2231: [discriminator loss: 0.461647, acc: 0.804688] [adversarial loss: 1.608515, acc: 0.140625]\n",
      "2232: [discriminator loss: 0.487533, acc: 0.812500] [adversarial loss: 1.057274, acc: 0.296875]\n",
      "2233: [discriminator loss: 0.505491, acc: 0.757812] [adversarial loss: 1.716023, acc: 0.078125]\n",
      "2234: [discriminator loss: 0.502758, acc: 0.789062] [adversarial loss: 1.009404, acc: 0.406250]\n",
      "2235: [discriminator loss: 0.479403, acc: 0.773438] [adversarial loss: 1.681520, acc: 0.140625]\n",
      "2236: [discriminator loss: 0.520868, acc: 0.710938] [adversarial loss: 1.121436, acc: 0.265625]\n",
      "2237: [discriminator loss: 0.477444, acc: 0.796875] [adversarial loss: 1.284233, acc: 0.203125]\n",
      "2238: [discriminator loss: 0.511361, acc: 0.742188] [adversarial loss: 1.120241, acc: 0.250000]\n",
      "2239: [discriminator loss: 0.454549, acc: 0.781250] [adversarial loss: 1.657993, acc: 0.109375]\n",
      "2240: [discriminator loss: 0.520669, acc: 0.734375] [adversarial loss: 0.957282, acc: 0.359375]\n",
      "2241: [discriminator loss: 0.453734, acc: 0.789062] [adversarial loss: 1.950724, acc: 0.046875]\n",
      "2242: [discriminator loss: 0.535589, acc: 0.757812] [adversarial loss: 0.887646, acc: 0.421875]\n",
      "2243: [discriminator loss: 0.557256, acc: 0.695312] [adversarial loss: 1.833890, acc: 0.062500]\n",
      "2244: [discriminator loss: 0.528716, acc: 0.757812] [adversarial loss: 0.922114, acc: 0.437500]\n",
      "2245: [discriminator loss: 0.508608, acc: 0.734375] [adversarial loss: 1.768236, acc: 0.140625]\n",
      "2246: [discriminator loss: 0.485633, acc: 0.742188] [adversarial loss: 1.183541, acc: 0.281250]\n",
      "2247: [discriminator loss: 0.550661, acc: 0.695312] [adversarial loss: 1.709368, acc: 0.078125]\n",
      "2248: [discriminator loss: 0.417389, acc: 0.835938] [adversarial loss: 1.363064, acc: 0.109375]\n",
      "2249: [discriminator loss: 0.485272, acc: 0.773438] [adversarial loss: 1.204005, acc: 0.265625]\n",
      "2250: [discriminator loss: 0.501573, acc: 0.757812] [adversarial loss: 1.585623, acc: 0.171875]\n",
      "2251: [discriminator loss: 0.464514, acc: 0.773438] [adversarial loss: 1.374857, acc: 0.218750]\n",
      "2252: [discriminator loss: 0.578439, acc: 0.734375] [adversarial loss: 1.779301, acc: 0.078125]\n",
      "2253: [discriminator loss: 0.483472, acc: 0.750000] [adversarial loss: 1.065842, acc: 0.312500]\n",
      "2254: [discriminator loss: 0.417621, acc: 0.851562] [adversarial loss: 1.646969, acc: 0.187500]\n",
      "2255: [discriminator loss: 0.560381, acc: 0.703125] [adversarial loss: 1.169595, acc: 0.218750]\n",
      "2256: [discriminator loss: 0.486138, acc: 0.773438] [adversarial loss: 1.912316, acc: 0.093750]\n",
      "2257: [discriminator loss: 0.559481, acc: 0.695312] [adversarial loss: 0.954432, acc: 0.359375]\n",
      "2258: [discriminator loss: 0.469690, acc: 0.757812] [adversarial loss: 1.737271, acc: 0.125000]\n",
      "2259: [discriminator loss: 0.508969, acc: 0.710938] [adversarial loss: 1.030174, acc: 0.359375]\n",
      "2260: [discriminator loss: 0.554226, acc: 0.695312] [adversarial loss: 1.817266, acc: 0.046875]\n",
      "2261: [discriminator loss: 0.510008, acc: 0.742188] [adversarial loss: 1.080255, acc: 0.281250]\n",
      "2262: [discriminator loss: 0.454612, acc: 0.843750] [adversarial loss: 1.533113, acc: 0.140625]\n",
      "2263: [discriminator loss: 0.493466, acc: 0.734375] [adversarial loss: 1.138494, acc: 0.328125]\n",
      "2264: [discriminator loss: 0.472687, acc: 0.789062] [adversarial loss: 1.617102, acc: 0.093750]\n",
      "2265: [discriminator loss: 0.503927, acc: 0.765625] [adversarial loss: 1.043800, acc: 0.265625]\n",
      "2266: [discriminator loss: 0.456564, acc: 0.812500] [adversarial loss: 1.572794, acc: 0.187500]\n",
      "2267: [discriminator loss: 0.480183, acc: 0.789062] [adversarial loss: 1.211018, acc: 0.296875]\n",
      "2268: [discriminator loss: 0.514729, acc: 0.773438] [adversarial loss: 1.497933, acc: 0.125000]\n",
      "2269: [discriminator loss: 0.488185, acc: 0.773438] [adversarial loss: 1.132663, acc: 0.406250]\n",
      "2270: [discriminator loss: 0.474890, acc: 0.796875] [adversarial loss: 1.657137, acc: 0.078125]\n",
      "2271: [discriminator loss: 0.560469, acc: 0.757812] [adversarial loss: 0.657653, acc: 0.515625]\n",
      "2272: [discriminator loss: 0.512818, acc: 0.718750] [adversarial loss: 1.823981, acc: 0.125000]\n",
      "2273: [discriminator loss: 0.576137, acc: 0.750000] [adversarial loss: 0.960233, acc: 0.406250]\n",
      "2274: [discriminator loss: 0.514778, acc: 0.750000] [adversarial loss: 1.839921, acc: 0.046875]\n",
      "2275: [discriminator loss: 0.498090, acc: 0.796875] [adversarial loss: 0.867300, acc: 0.468750]\n",
      "2276: [discriminator loss: 0.507173, acc: 0.734375] [adversarial loss: 1.600471, acc: 0.140625]\n",
      "2277: [discriminator loss: 0.600798, acc: 0.695312] [adversarial loss: 0.950595, acc: 0.421875]\n",
      "2278: [discriminator loss: 0.464677, acc: 0.757812] [adversarial loss: 1.880429, acc: 0.078125]\n",
      "2279: [discriminator loss: 0.525374, acc: 0.781250] [adversarial loss: 1.067606, acc: 0.312500]\n",
      "2280: [discriminator loss: 0.536372, acc: 0.710938] [adversarial loss: 1.613708, acc: 0.156250]\n",
      "2281: [discriminator loss: 0.502257, acc: 0.765625] [adversarial loss: 1.021297, acc: 0.406250]\n",
      "2282: [discriminator loss: 0.387839, acc: 0.828125] [adversarial loss: 1.146431, acc: 0.281250]\n",
      "2283: [discriminator loss: 0.480043, acc: 0.796875] [adversarial loss: 1.564825, acc: 0.093750]\n",
      "2284: [discriminator loss: 0.512753, acc: 0.726562] [adversarial loss: 1.701493, acc: 0.046875]\n",
      "2285: [discriminator loss: 0.496041, acc: 0.734375] [adversarial loss: 1.311414, acc: 0.203125]\n",
      "2286: [discriminator loss: 0.449780, acc: 0.828125] [adversarial loss: 1.065885, acc: 0.359375]\n",
      "2287: [discriminator loss: 0.566074, acc: 0.718750] [adversarial loss: 1.503694, acc: 0.125000]\n",
      "2288: [discriminator loss: 0.484609, acc: 0.796875] [adversarial loss: 0.808576, acc: 0.468750]\n",
      "2289: [discriminator loss: 0.520430, acc: 0.773438] [adversarial loss: 1.840974, acc: 0.078125]\n",
      "2290: [discriminator loss: 0.576923, acc: 0.742188] [adversarial loss: 0.921916, acc: 0.406250]\n",
      "2291: [discriminator loss: 0.526163, acc: 0.710938] [adversarial loss: 1.742954, acc: 0.109375]\n",
      "2292: [discriminator loss: 0.477699, acc: 0.742188] [adversarial loss: 0.870856, acc: 0.390625]\n",
      "2293: [discriminator loss: 0.593116, acc: 0.687500] [adversarial loss: 1.541769, acc: 0.203125]\n",
      "2294: [discriminator loss: 0.445893, acc: 0.796875] [adversarial loss: 0.863963, acc: 0.406250]\n",
      "2295: [discriminator loss: 0.514951, acc: 0.742188] [adversarial loss: 1.869672, acc: 0.078125]\n",
      "2296: [discriminator loss: 0.503588, acc: 0.734375] [adversarial loss: 1.073432, acc: 0.281250]\n",
      "2297: [discriminator loss: 0.420946, acc: 0.812500] [adversarial loss: 1.321075, acc: 0.187500]\n",
      "2298: [discriminator loss: 0.458223, acc: 0.789062] [adversarial loss: 1.520851, acc: 0.218750]\n",
      "2299: [discriminator loss: 0.565606, acc: 0.726562] [adversarial loss: 1.341044, acc: 0.171875]\n",
      "2300: [discriminator loss: 0.512333, acc: 0.734375] [adversarial loss: 1.414262, acc: 0.093750]\n",
      "2301: [discriminator loss: 0.445874, acc: 0.789062] [adversarial loss: 1.213012, acc: 0.281250]\n",
      "2302: [discriminator loss: 0.479038, acc: 0.781250] [adversarial loss: 1.597449, acc: 0.125000]\n",
      "2303: [discriminator loss: 0.467483, acc: 0.789062] [adversarial loss: 1.388759, acc: 0.171875]\n",
      "2304: [discriminator loss: 0.403244, acc: 0.812500] [adversarial loss: 1.307897, acc: 0.218750]\n",
      "2305: [discriminator loss: 0.421122, acc: 0.851562] [adversarial loss: 1.570884, acc: 0.171875]\n",
      "2306: [discriminator loss: 0.518005, acc: 0.796875] [adversarial loss: 1.105256, acc: 0.312500]\n",
      "2307: [discriminator loss: 0.423205, acc: 0.796875] [adversarial loss: 1.203445, acc: 0.343750]\n",
      "2308: [discriminator loss: 0.470269, acc: 0.820312] [adversarial loss: 1.367812, acc: 0.234375]\n",
      "2309: [discriminator loss: 0.459947, acc: 0.796875] [adversarial loss: 1.066118, acc: 0.296875]\n",
      "2310: [discriminator loss: 0.576984, acc: 0.656250] [adversarial loss: 2.548902, acc: 0.062500]\n",
      "2311: [discriminator loss: 0.548205, acc: 0.734375] [adversarial loss: 0.792204, acc: 0.500000]\n",
      "2312: [discriminator loss: 0.626831, acc: 0.687500] [adversarial loss: 1.914572, acc: 0.015625]\n",
      "2313: [discriminator loss: 0.552377, acc: 0.757812] [adversarial loss: 0.806002, acc: 0.484375]\n",
      "2314: [discriminator loss: 0.580102, acc: 0.703125] [adversarial loss: 1.472356, acc: 0.125000]\n",
      "2315: [discriminator loss: 0.514781, acc: 0.742188] [adversarial loss: 0.920993, acc: 0.390625]\n",
      "2316: [discriminator loss: 0.554041, acc: 0.750000] [adversarial loss: 1.608506, acc: 0.156250]\n",
      "2317: [discriminator loss: 0.474131, acc: 0.765625] [adversarial loss: 1.067785, acc: 0.359375]\n",
      "2318: [discriminator loss: 0.439593, acc: 0.781250] [adversarial loss: 1.317825, acc: 0.234375]\n",
      "2319: [discriminator loss: 0.521895, acc: 0.734375] [adversarial loss: 1.151982, acc: 0.312500]\n",
      "2320: [discriminator loss: 0.476365, acc: 0.773438] [adversarial loss: 1.398378, acc: 0.203125]\n",
      "2321: [discriminator loss: 0.478198, acc: 0.757812] [adversarial loss: 1.249446, acc: 0.281250]\n",
      "2322: [discriminator loss: 0.537201, acc: 0.726562] [adversarial loss: 1.056333, acc: 0.359375]\n",
      "2323: [discriminator loss: 0.417208, acc: 0.804688] [adversarial loss: 1.851240, acc: 0.093750]\n",
      "2324: [discriminator loss: 0.475125, acc: 0.804688] [adversarial loss: 1.036226, acc: 0.296875]\n",
      "2325: [discriminator loss: 0.492071, acc: 0.796875] [adversarial loss: 1.333697, acc: 0.187500]\n",
      "2326: [discriminator loss: 0.475506, acc: 0.757812] [adversarial loss: 1.197469, acc: 0.187500]\n",
      "2327: [discriminator loss: 0.522539, acc: 0.750000] [adversarial loss: 1.702133, acc: 0.046875]\n",
      "2328: [discriminator loss: 0.449708, acc: 0.750000] [adversarial loss: 0.811260, acc: 0.484375]\n",
      "2329: [discriminator loss: 0.514145, acc: 0.726562] [adversarial loss: 2.222093, acc: 0.078125]\n",
      "2330: [discriminator loss: 0.483212, acc: 0.796875] [adversarial loss: 1.062659, acc: 0.359375]\n",
      "2331: [discriminator loss: 0.465467, acc: 0.757812] [adversarial loss: 1.519410, acc: 0.156250]\n",
      "2332: [discriminator loss: 0.466884, acc: 0.804688] [adversarial loss: 0.923717, acc: 0.390625]\n",
      "2333: [discriminator loss: 0.540504, acc: 0.742188] [adversarial loss: 1.498404, acc: 0.140625]\n",
      "2334: [discriminator loss: 0.503046, acc: 0.781250] [adversarial loss: 1.152134, acc: 0.250000]\n",
      "2335: [discriminator loss: 0.581087, acc: 0.632812] [adversarial loss: 2.019887, acc: 0.062500]\n",
      "2336: [discriminator loss: 0.544631, acc: 0.726562] [adversarial loss: 0.780762, acc: 0.515625]\n",
      "2337: [discriminator loss: 0.610833, acc: 0.695312] [adversarial loss: 2.041424, acc: 0.046875]\n",
      "2338: [discriminator loss: 0.610165, acc: 0.648438] [adversarial loss: 1.038344, acc: 0.312500]\n",
      "2339: [discriminator loss: 0.467209, acc: 0.773438] [adversarial loss: 1.630040, acc: 0.062500]\n",
      "2340: [discriminator loss: 0.525442, acc: 0.742188] [adversarial loss: 0.884940, acc: 0.437500]\n",
      "2341: [discriminator loss: 0.543055, acc: 0.695312] [adversarial loss: 1.807986, acc: 0.062500]\n",
      "2342: [discriminator loss: 0.503244, acc: 0.750000] [adversarial loss: 0.991518, acc: 0.343750]\n",
      "2343: [discriminator loss: 0.537153, acc: 0.757812] [adversarial loss: 1.363209, acc: 0.156250]\n",
      "2344: [discriminator loss: 0.421047, acc: 0.812500] [adversarial loss: 1.117866, acc: 0.203125]\n",
      "2345: [discriminator loss: 0.490315, acc: 0.804688] [adversarial loss: 1.331111, acc: 0.218750]\n",
      "2346: [discriminator loss: 0.535064, acc: 0.718750] [adversarial loss: 1.623018, acc: 0.125000]\n",
      "2347: [discriminator loss: 0.433544, acc: 0.789062] [adversarial loss: 1.292972, acc: 0.203125]\n",
      "2348: [discriminator loss: 0.456381, acc: 0.828125] [adversarial loss: 1.402870, acc: 0.187500]\n",
      "2349: [discriminator loss: 0.458769, acc: 0.820312] [adversarial loss: 1.376713, acc: 0.109375]\n",
      "2350: [discriminator loss: 0.467775, acc: 0.804688] [adversarial loss: 1.427712, acc: 0.203125]\n",
      "2351: [discriminator loss: 0.495199, acc: 0.781250] [adversarial loss: 1.395812, acc: 0.156250]\n",
      "2352: [discriminator loss: 0.435841, acc: 0.796875] [adversarial loss: 1.588591, acc: 0.140625]\n",
      "2353: [discriminator loss: 0.482645, acc: 0.773438] [adversarial loss: 0.996858, acc: 0.375000]\n",
      "2354: [discriminator loss: 0.511823, acc: 0.718750] [adversarial loss: 1.570192, acc: 0.156250]\n",
      "2355: [discriminator loss: 0.455981, acc: 0.804688] [adversarial loss: 1.192071, acc: 0.265625]\n",
      "2356: [discriminator loss: 0.492072, acc: 0.734375] [adversarial loss: 1.612532, acc: 0.156250]\n",
      "2357: [discriminator loss: 0.485965, acc: 0.726562] [adversarial loss: 1.325275, acc: 0.281250]\n",
      "2358: [discriminator loss: 0.470292, acc: 0.796875] [adversarial loss: 1.972614, acc: 0.078125]\n",
      "2359: [discriminator loss: 0.504590, acc: 0.765625] [adversarial loss: 1.039515, acc: 0.328125]\n",
      "2360: [discriminator loss: 0.480044, acc: 0.773438] [adversarial loss: 1.973108, acc: 0.062500]\n",
      "2361: [discriminator loss: 0.658071, acc: 0.671875] [adversarial loss: 0.731801, acc: 0.437500]\n",
      "2362: [discriminator loss: 0.593316, acc: 0.671875] [adversarial loss: 2.099275, acc: 0.046875]\n",
      "2363: [discriminator loss: 0.528420, acc: 0.710938] [adversarial loss: 1.135721, acc: 0.296875]\n",
      "2364: [discriminator loss: 0.436527, acc: 0.835938] [adversarial loss: 1.660023, acc: 0.062500]\n",
      "2365: [discriminator loss: 0.472152, acc: 0.804688] [adversarial loss: 1.201327, acc: 0.218750]\n",
      "2366: [discriminator loss: 0.449537, acc: 0.812500] [adversarial loss: 1.352926, acc: 0.171875]\n",
      "2367: [discriminator loss: 0.504025, acc: 0.789062] [adversarial loss: 1.473163, acc: 0.125000]\n",
      "2368: [discriminator loss: 0.522287, acc: 0.718750] [adversarial loss: 1.410250, acc: 0.187500]\n",
      "2369: [discriminator loss: 0.536282, acc: 0.718750] [adversarial loss: 1.127445, acc: 0.312500]\n",
      "2370: [discriminator loss: 0.475276, acc: 0.804688] [adversarial loss: 1.734318, acc: 0.093750]\n",
      "2371: [discriminator loss: 0.450142, acc: 0.781250] [adversarial loss: 0.936568, acc: 0.406250]\n",
      "2372: [discriminator loss: 0.533683, acc: 0.734375] [adversarial loss: 1.726127, acc: 0.125000]\n",
      "2373: [discriminator loss: 0.510031, acc: 0.750000] [adversarial loss: 0.886515, acc: 0.406250]\n",
      "2374: [discriminator loss: 0.463799, acc: 0.757812] [adversarial loss: 1.803406, acc: 0.109375]\n",
      "2375: [discriminator loss: 0.579104, acc: 0.726562] [adversarial loss: 0.950240, acc: 0.265625]\n",
      "2376: [discriminator loss: 0.535429, acc: 0.718750] [adversarial loss: 1.904196, acc: 0.031250]\n",
      "2377: [discriminator loss: 0.545709, acc: 0.726562] [adversarial loss: 0.969299, acc: 0.375000]\n",
      "2378: [discriminator loss: 0.541774, acc: 0.726562] [adversarial loss: 1.546727, acc: 0.109375]\n",
      "2379: [discriminator loss: 0.492048, acc: 0.750000] [adversarial loss: 1.540139, acc: 0.125000]\n",
      "2380: [discriminator loss: 0.439961, acc: 0.773438] [adversarial loss: 1.462913, acc: 0.125000]\n",
      "2381: [discriminator loss: 0.543444, acc: 0.757812] [adversarial loss: 1.177861, acc: 0.343750]\n",
      "2382: [discriminator loss: 0.485618, acc: 0.757812] [adversarial loss: 1.739127, acc: 0.062500]\n",
      "2383: [discriminator loss: 0.496180, acc: 0.773438] [adversarial loss: 1.033031, acc: 0.359375]\n",
      "2384: [discriminator loss: 0.618482, acc: 0.695312] [adversarial loss: 1.837119, acc: 0.093750]\n",
      "2385: [discriminator loss: 0.578122, acc: 0.687500] [adversarial loss: 1.066210, acc: 0.265625]\n",
      "2386: [discriminator loss: 0.431198, acc: 0.812500] [adversarial loss: 1.949771, acc: 0.062500]\n",
      "2387: [discriminator loss: 0.549950, acc: 0.718750] [adversarial loss: 0.890899, acc: 0.406250]\n",
      "2388: [discriminator loss: 0.514519, acc: 0.703125] [adversarial loss: 1.639403, acc: 0.125000]\n",
      "2389: [discriminator loss: 0.560401, acc: 0.750000] [adversarial loss: 0.979034, acc: 0.359375]\n",
      "2390: [discriminator loss: 0.520706, acc: 0.781250] [adversarial loss: 1.700259, acc: 0.109375]\n",
      "2391: [discriminator loss: 0.474228, acc: 0.765625] [adversarial loss: 1.044061, acc: 0.390625]\n",
      "2392: [discriminator loss: 0.465766, acc: 0.804688] [adversarial loss: 1.526944, acc: 0.171875]\n",
      "2393: [discriminator loss: 0.467075, acc: 0.796875] [adversarial loss: 1.294801, acc: 0.296875]\n",
      "2394: [discriminator loss: 0.489631, acc: 0.812500] [adversarial loss: 1.766609, acc: 0.046875]\n",
      "2395: [discriminator loss: 0.513957, acc: 0.781250] [adversarial loss: 0.986215, acc: 0.359375]\n",
      "2396: [discriminator loss: 0.593924, acc: 0.703125] [adversarial loss: 1.707743, acc: 0.093750]\n",
      "2397: [discriminator loss: 0.460997, acc: 0.820312] [adversarial loss: 1.050493, acc: 0.328125]\n",
      "2398: [discriminator loss: 0.426101, acc: 0.812500] [adversarial loss: 1.548583, acc: 0.078125]\n",
      "2399: [discriminator loss: 0.515261, acc: 0.765625] [adversarial loss: 1.152617, acc: 0.218750]\n",
      "2400: [discriminator loss: 0.458797, acc: 0.789062] [adversarial loss: 1.463569, acc: 0.140625]\n",
      "2401: [discriminator loss: 0.481332, acc: 0.773438] [adversarial loss: 1.398370, acc: 0.234375]\n",
      "2402: [discriminator loss: 0.549867, acc: 0.695312] [adversarial loss: 1.456337, acc: 0.203125]\n",
      "2403: [discriminator loss: 0.472753, acc: 0.773438] [adversarial loss: 1.327586, acc: 0.218750]\n",
      "2404: [discriminator loss: 0.513679, acc: 0.726562] [adversarial loss: 0.997680, acc: 0.359375]\n",
      "2405: [discriminator loss: 0.444056, acc: 0.820312] [adversarial loss: 1.609013, acc: 0.156250]\n",
      "2406: [discriminator loss: 0.487751, acc: 0.781250] [adversarial loss: 0.875201, acc: 0.437500]\n",
      "2407: [discriminator loss: 0.609265, acc: 0.687500] [adversarial loss: 2.183112, acc: 0.015625]\n",
      "2408: [discriminator loss: 0.504987, acc: 0.750000] [adversarial loss: 0.872359, acc: 0.375000]\n",
      "2409: [discriminator loss: 0.523715, acc: 0.726562] [adversarial loss: 1.759320, acc: 0.109375]\n",
      "2410: [discriminator loss: 0.499491, acc: 0.726562] [adversarial loss: 1.122849, acc: 0.203125]\n",
      "2411: [discriminator loss: 0.436363, acc: 0.828125] [adversarial loss: 1.544301, acc: 0.140625]\n",
      "2412: [discriminator loss: 0.489480, acc: 0.765625] [adversarial loss: 1.109624, acc: 0.265625]\n",
      "2413: [discriminator loss: 0.456394, acc: 0.781250] [adversarial loss: 1.526825, acc: 0.187500]\n",
      "2414: [discriminator loss: 0.455227, acc: 0.789062] [adversarial loss: 1.287062, acc: 0.203125]\n",
      "2415: [discriminator loss: 0.410650, acc: 0.804688] [adversarial loss: 1.520651, acc: 0.109375]\n",
      "2416: [discriminator loss: 0.517829, acc: 0.726562] [adversarial loss: 0.994172, acc: 0.437500]\n",
      "2417: [discriminator loss: 0.619541, acc: 0.656250] [adversarial loss: 1.476256, acc: 0.171875]\n",
      "2418: [discriminator loss: 0.549446, acc: 0.695312] [adversarial loss: 1.020434, acc: 0.312500]\n",
      "2419: [discriminator loss: 0.483541, acc: 0.742188] [adversarial loss: 1.281387, acc: 0.187500]\n",
      "2420: [discriminator loss: 0.466466, acc: 0.796875] [adversarial loss: 1.311843, acc: 0.171875]\n",
      "2421: [discriminator loss: 0.431905, acc: 0.781250] [adversarial loss: 1.582419, acc: 0.156250]\n",
      "2422: [discriminator loss: 0.485843, acc: 0.773438] [adversarial loss: 0.932132, acc: 0.406250]\n",
      "2423: [discriminator loss: 0.512584, acc: 0.757812] [adversarial loss: 1.580884, acc: 0.125000]\n",
      "2424: [discriminator loss: 0.477793, acc: 0.773438] [adversarial loss: 1.115478, acc: 0.296875]\n",
      "2425: [discriminator loss: 0.521478, acc: 0.679688] [adversarial loss: 1.976933, acc: 0.078125]\n",
      "2426: [discriminator loss: 0.515576, acc: 0.726562] [adversarial loss: 0.873487, acc: 0.500000]\n",
      "2427: [discriminator loss: 0.498551, acc: 0.750000] [adversarial loss: 1.875087, acc: 0.078125]\n",
      "2428: [discriminator loss: 0.536670, acc: 0.726562] [adversarial loss: 0.812542, acc: 0.546875]\n",
      "2429: [discriminator loss: 0.471381, acc: 0.796875] [adversarial loss: 1.523333, acc: 0.109375]\n",
      "2430: [discriminator loss: 0.523141, acc: 0.750000] [adversarial loss: 1.048927, acc: 0.390625]\n",
      "2431: [discriminator loss: 0.506588, acc: 0.757812] [adversarial loss: 1.588223, acc: 0.171875]\n",
      "2432: [discriminator loss: 0.529104, acc: 0.718750] [adversarial loss: 1.449273, acc: 0.171875]\n",
      "2433: [discriminator loss: 0.472656, acc: 0.773438] [adversarial loss: 0.996636, acc: 0.375000]\n",
      "2434: [discriminator loss: 0.474508, acc: 0.781250] [adversarial loss: 1.712657, acc: 0.078125]\n",
      "2435: [discriminator loss: 0.470058, acc: 0.804688] [adversarial loss: 1.237380, acc: 0.265625]\n",
      "2436: [discriminator loss: 0.554955, acc: 0.718750] [adversarial loss: 1.672601, acc: 0.062500]\n",
      "2437: [discriminator loss: 0.496306, acc: 0.750000] [adversarial loss: 0.903483, acc: 0.500000]\n",
      "2438: [discriminator loss: 0.449550, acc: 0.765625] [adversarial loss: 1.336681, acc: 0.218750]\n",
      "2439: [discriminator loss: 0.508216, acc: 0.773438] [adversarial loss: 1.390224, acc: 0.171875]\n",
      "2440: [discriminator loss: 0.461070, acc: 0.804688] [adversarial loss: 1.398989, acc: 0.250000]\n",
      "2441: [discriminator loss: 0.411068, acc: 0.851562] [adversarial loss: 1.562583, acc: 0.125000]\n",
      "2442: [discriminator loss: 0.499304, acc: 0.726562] [adversarial loss: 0.947367, acc: 0.390625]\n",
      "2443: [discriminator loss: 0.513027, acc: 0.742188] [adversarial loss: 1.740416, acc: 0.140625]\n",
      "2444: [discriminator loss: 0.550515, acc: 0.750000] [adversarial loss: 1.231969, acc: 0.296875]\n",
      "2445: [discriminator loss: 0.558239, acc: 0.757812] [adversarial loss: 1.956273, acc: 0.078125]\n",
      "2446: [discriminator loss: 0.513320, acc: 0.726562] [adversarial loss: 0.960567, acc: 0.421875]\n",
      "2447: [discriminator loss: 0.540370, acc: 0.679688] [adversarial loss: 2.045656, acc: 0.015625]\n",
      "2448: [discriminator loss: 0.461758, acc: 0.812500] [adversarial loss: 1.024789, acc: 0.484375]\n",
      "2449: [discriminator loss: 0.519861, acc: 0.734375] [adversarial loss: 1.299523, acc: 0.250000]\n",
      "2450: [discriminator loss: 0.460281, acc: 0.796875] [adversarial loss: 1.163499, acc: 0.281250]\n",
      "2451: [discriminator loss: 0.497213, acc: 0.765625] [adversarial loss: 1.268998, acc: 0.156250]\n",
      "2452: [discriminator loss: 0.412589, acc: 0.828125] [adversarial loss: 1.301252, acc: 0.203125]\n",
      "2453: [discriminator loss: 0.467091, acc: 0.804688] [adversarial loss: 1.547116, acc: 0.187500]\n",
      "2454: [discriminator loss: 0.479315, acc: 0.773438] [adversarial loss: 1.314271, acc: 0.187500]\n",
      "2455: [discriminator loss: 0.473358, acc: 0.789062] [adversarial loss: 1.151468, acc: 0.187500]\n",
      "2456: [discriminator loss: 0.566256, acc: 0.695312] [adversarial loss: 1.618089, acc: 0.140625]\n",
      "2457: [discriminator loss: 0.540689, acc: 0.726562] [adversarial loss: 1.265658, acc: 0.171875]\n",
      "2458: [discriminator loss: 0.375570, acc: 0.851562] [adversarial loss: 1.523462, acc: 0.125000]\n",
      "2459: [discriminator loss: 0.447671, acc: 0.796875] [adversarial loss: 1.341262, acc: 0.203125]\n",
      "2460: [discriminator loss: 0.494804, acc: 0.781250] [adversarial loss: 1.619358, acc: 0.125000]\n",
      "2461: [discriminator loss: 0.450982, acc: 0.781250] [adversarial loss: 1.204216, acc: 0.250000]\n",
      "2462: [discriminator loss: 0.450215, acc: 0.804688] [adversarial loss: 1.509339, acc: 0.187500]\n",
      "2463: [discriminator loss: 0.567292, acc: 0.734375] [adversarial loss: 0.917587, acc: 0.453125]\n",
      "2464: [discriminator loss: 0.608563, acc: 0.695312] [adversarial loss: 2.043740, acc: 0.078125]\n",
      "2465: [discriminator loss: 0.533320, acc: 0.742188] [adversarial loss: 1.054589, acc: 0.390625]\n",
      "2466: [discriminator loss: 0.493024, acc: 0.750000] [adversarial loss: 1.534818, acc: 0.171875]\n",
      "2467: [discriminator loss: 0.444650, acc: 0.812500] [adversarial loss: 0.942193, acc: 0.375000]\n",
      "2468: [discriminator loss: 0.532630, acc: 0.710938] [adversarial loss: 1.666364, acc: 0.093750]\n",
      "2469: [discriminator loss: 0.476677, acc: 0.757812] [adversarial loss: 1.044092, acc: 0.281250]\n",
      "2470: [discriminator loss: 0.587119, acc: 0.703125] [adversarial loss: 2.024260, acc: 0.062500]\n",
      "2471: [discriminator loss: 0.479179, acc: 0.757812] [adversarial loss: 0.952154, acc: 0.421875]\n",
      "2472: [discriminator loss: 0.521350, acc: 0.710938] [adversarial loss: 1.968293, acc: 0.093750]\n",
      "2473: [discriminator loss: 0.552338, acc: 0.726562] [adversarial loss: 0.826465, acc: 0.468750]\n",
      "2474: [discriminator loss: 0.548927, acc: 0.710938] [adversarial loss: 1.792815, acc: 0.046875]\n",
      "2475: [discriminator loss: 0.516151, acc: 0.726562] [adversarial loss: 1.175769, acc: 0.218750]\n",
      "2476: [discriminator loss: 0.446038, acc: 0.828125] [adversarial loss: 1.395242, acc: 0.140625]\n",
      "2477: [discriminator loss: 0.530530, acc: 0.726562] [adversarial loss: 1.208223, acc: 0.218750]\n",
      "2478: [discriminator loss: 0.461552, acc: 0.812500] [adversarial loss: 1.115159, acc: 0.312500]\n",
      "2479: [discriminator loss: 0.482998, acc: 0.742188] [adversarial loss: 1.557259, acc: 0.171875]\n",
      "2480: [discriminator loss: 0.459212, acc: 0.773438] [adversarial loss: 1.017782, acc: 0.359375]\n",
      "2481: [discriminator loss: 0.469283, acc: 0.765625] [adversarial loss: 1.695549, acc: 0.093750]\n",
      "2482: [discriminator loss: 0.483755, acc: 0.765625] [adversarial loss: 0.981143, acc: 0.375000]\n",
      "2483: [discriminator loss: 0.469495, acc: 0.789062] [adversarial loss: 1.480829, acc: 0.171875]\n",
      "2484: [discriminator loss: 0.486430, acc: 0.734375] [adversarial loss: 1.529185, acc: 0.093750]\n",
      "2485: [discriminator loss: 0.533061, acc: 0.710938] [adversarial loss: 0.983747, acc: 0.437500]\n",
      "2486: [discriminator loss: 0.435815, acc: 0.789062] [adversarial loss: 1.651450, acc: 0.156250]\n",
      "2487: [discriminator loss: 0.415741, acc: 0.820312] [adversarial loss: 1.291718, acc: 0.234375]\n",
      "2488: [discriminator loss: 0.521539, acc: 0.710938] [adversarial loss: 1.809022, acc: 0.078125]\n",
      "2489: [discriminator loss: 0.482417, acc: 0.781250] [adversarial loss: 0.761300, acc: 0.515625]\n",
      "2490: [discriminator loss: 0.597058, acc: 0.656250] [adversarial loss: 2.132732, acc: 0.031250]\n",
      "2491: [discriminator loss: 0.537846, acc: 0.773438] [adversarial loss: 0.857839, acc: 0.421875]\n",
      "2492: [discriminator loss: 0.530906, acc: 0.750000] [adversarial loss: 1.939277, acc: 0.078125]\n",
      "2493: [discriminator loss: 0.447856, acc: 0.750000] [adversarial loss: 1.034720, acc: 0.406250]\n",
      "2494: [discriminator loss: 0.528265, acc: 0.734375] [adversarial loss: 1.341462, acc: 0.187500]\n",
      "2495: [discriminator loss: 0.514027, acc: 0.781250] [adversarial loss: 1.609514, acc: 0.078125]\n",
      "2496: [discriminator loss: 0.539840, acc: 0.757812] [adversarial loss: 1.095926, acc: 0.281250]\n",
      "2497: [discriminator loss: 0.467337, acc: 0.804688] [adversarial loss: 1.475395, acc: 0.125000]\n",
      "2498: [discriminator loss: 0.525622, acc: 0.742188] [adversarial loss: 1.058547, acc: 0.421875]\n",
      "2499: [discriminator loss: 0.436815, acc: 0.804688] [adversarial loss: 1.519205, acc: 0.187500]\n",
      "2500: [discriminator loss: 0.523315, acc: 0.718750] [adversarial loss: 1.257951, acc: 0.234375]\n",
      "2501: [discriminator loss: 0.489982, acc: 0.804688] [adversarial loss: 1.514985, acc: 0.171875]\n",
      "2502: [discriminator loss: 0.551508, acc: 0.703125] [adversarial loss: 0.964540, acc: 0.328125]\n",
      "2503: [discriminator loss: 0.449467, acc: 0.820312] [adversarial loss: 1.765947, acc: 0.093750]\n",
      "2504: [discriminator loss: 0.545060, acc: 0.750000] [adversarial loss: 1.161293, acc: 0.281250]\n",
      "2505: [discriminator loss: 0.444259, acc: 0.820312] [adversarial loss: 1.367436, acc: 0.171875]\n",
      "2506: [discriminator loss: 0.431198, acc: 0.835938] [adversarial loss: 1.589106, acc: 0.093750]\n",
      "2507: [discriminator loss: 0.406757, acc: 0.820312] [adversarial loss: 1.368118, acc: 0.156250]\n",
      "2508: [discriminator loss: 0.522394, acc: 0.734375] [adversarial loss: 1.343578, acc: 0.265625]\n",
      "2509: [discriminator loss: 0.444801, acc: 0.812500] [adversarial loss: 1.215749, acc: 0.265625]\n",
      "2510: [discriminator loss: 0.496114, acc: 0.726562] [adversarial loss: 1.288266, acc: 0.312500]\n",
      "2511: [discriminator loss: 0.512040, acc: 0.765625] [adversarial loss: 1.457137, acc: 0.125000]\n",
      "2512: [discriminator loss: 0.442937, acc: 0.812500] [adversarial loss: 1.700611, acc: 0.093750]\n",
      "2513: [discriminator loss: 0.604863, acc: 0.664062] [adversarial loss: 1.481090, acc: 0.156250]\n",
      "2514: [discriminator loss: 0.408344, acc: 0.828125] [adversarial loss: 1.350106, acc: 0.171875]\n",
      "2515: [discriminator loss: 0.429879, acc: 0.804688] [adversarial loss: 1.819694, acc: 0.109375]\n",
      "2516: [discriminator loss: 0.426508, acc: 0.804688] [adversarial loss: 1.258877, acc: 0.234375]\n",
      "2517: [discriminator loss: 0.551064, acc: 0.734375] [adversarial loss: 2.246588, acc: 0.078125]\n",
      "2518: [discriminator loss: 0.442270, acc: 0.789062] [adversarial loss: 0.989693, acc: 0.390625]\n",
      "2519: [discriminator loss: 0.544928, acc: 0.734375] [adversarial loss: 1.873191, acc: 0.109375]\n",
      "2520: [discriminator loss: 0.508934, acc: 0.726562] [adversarial loss: 1.125450, acc: 0.359375]\n",
      "2521: [discriminator loss: 0.479584, acc: 0.773438] [adversarial loss: 1.688440, acc: 0.156250]\n",
      "2522: [discriminator loss: 0.467742, acc: 0.789062] [adversarial loss: 1.244576, acc: 0.234375]\n",
      "2523: [discriminator loss: 0.458789, acc: 0.757812] [adversarial loss: 1.567056, acc: 0.093750]\n",
      "2524: [discriminator loss: 0.486076, acc: 0.765625] [adversarial loss: 0.994479, acc: 0.375000]\n",
      "2525: [discriminator loss: 0.458856, acc: 0.750000] [adversarial loss: 1.938053, acc: 0.078125]\n",
      "2526: [discriminator loss: 0.534808, acc: 0.718750] [adversarial loss: 0.737095, acc: 0.593750]\n",
      "2527: [discriminator loss: 0.593398, acc: 0.664062] [adversarial loss: 1.988915, acc: 0.031250]\n",
      "2528: [discriminator loss: 0.524331, acc: 0.734375] [adversarial loss: 0.934782, acc: 0.437500]\n",
      "2529: [discriminator loss: 0.472098, acc: 0.765625] [adversarial loss: 1.832396, acc: 0.078125]\n",
      "2530: [discriminator loss: 0.462759, acc: 0.765625] [adversarial loss: 1.067387, acc: 0.359375]\n",
      "2531: [discriminator loss: 0.408522, acc: 0.828125] [adversarial loss: 1.359916, acc: 0.171875]\n",
      "2532: [discriminator loss: 0.487734, acc: 0.765625] [adversarial loss: 2.033958, acc: 0.062500]\n",
      "2533: [discriminator loss: 0.521981, acc: 0.726562] [adversarial loss: 0.979116, acc: 0.328125]\n",
      "2534: [discriminator loss: 0.611621, acc: 0.679688] [adversarial loss: 2.071884, acc: 0.078125]\n",
      "2535: [discriminator loss: 0.556020, acc: 0.718750] [adversarial loss: 0.903193, acc: 0.421875]\n",
      "2536: [discriminator loss: 0.520765, acc: 0.750000] [adversarial loss: 1.517964, acc: 0.109375]\n",
      "2537: [discriminator loss: 0.476480, acc: 0.773438] [adversarial loss: 1.502157, acc: 0.203125]\n",
      "2538: [discriminator loss: 0.492492, acc: 0.796875] [adversarial loss: 0.946089, acc: 0.406250]\n",
      "2539: [discriminator loss: 0.525714, acc: 0.773438] [adversarial loss: 1.423333, acc: 0.125000]\n",
      "2540: [discriminator loss: 0.497635, acc: 0.734375] [adversarial loss: 1.330227, acc: 0.203125]\n",
      "2541: [discriminator loss: 0.503690, acc: 0.781250] [adversarial loss: 1.422119, acc: 0.171875]\n",
      "2542: [discriminator loss: 0.486330, acc: 0.750000] [adversarial loss: 1.430549, acc: 0.156250]\n",
      "2543: [discriminator loss: 0.482838, acc: 0.781250] [adversarial loss: 1.172503, acc: 0.296875]\n",
      "2544: [discriminator loss: 0.532187, acc: 0.703125] [adversarial loss: 1.252449, acc: 0.187500]\n",
      "2545: [discriminator loss: 0.488272, acc: 0.765625] [adversarial loss: 0.975157, acc: 0.375000]\n",
      "2546: [discriminator loss: 0.415105, acc: 0.851562] [adversarial loss: 1.579940, acc: 0.171875]\n",
      "2547: [discriminator loss: 0.505986, acc: 0.757812] [adversarial loss: 1.072191, acc: 0.375000]\n",
      "2548: [discriminator loss: 0.612410, acc: 0.703125] [adversarial loss: 1.809124, acc: 0.156250]\n",
      "2549: [discriminator loss: 0.514169, acc: 0.726562] [adversarial loss: 0.947456, acc: 0.421875]\n",
      "2550: [discriminator loss: 0.562128, acc: 0.742188] [adversarial loss: 2.006060, acc: 0.031250]\n",
      "2551: [discriminator loss: 0.481503, acc: 0.750000] [adversarial loss: 0.954574, acc: 0.390625]\n",
      "2552: [discriminator loss: 0.549080, acc: 0.718750] [adversarial loss: 2.041020, acc: 0.031250]\n",
      "2553: [discriminator loss: 0.446876, acc: 0.781250] [adversarial loss: 1.082254, acc: 0.296875]\n",
      "2554: [discriminator loss: 0.479249, acc: 0.757812] [adversarial loss: 2.005315, acc: 0.031250]\n",
      "2555: [discriminator loss: 0.531284, acc: 0.757812] [adversarial loss: 0.875069, acc: 0.375000]\n",
      "2556: [discriminator loss: 0.517839, acc: 0.710938] [adversarial loss: 1.816100, acc: 0.125000]\n",
      "2557: [discriminator loss: 0.477610, acc: 0.757812] [adversarial loss: 1.017962, acc: 0.359375]\n",
      "2558: [discriminator loss: 0.502408, acc: 0.789062] [adversarial loss: 1.591133, acc: 0.156250]\n",
      "2559: [discriminator loss: 0.574357, acc: 0.679688] [adversarial loss: 1.479335, acc: 0.171875]\n",
      "2560: [discriminator loss: 0.530483, acc: 0.750000] [adversarial loss: 1.121279, acc: 0.265625]\n",
      "2561: [discriminator loss: 0.445700, acc: 0.789062] [adversarial loss: 1.594256, acc: 0.125000]\n",
      "2562: [discriminator loss: 0.475744, acc: 0.789062] [adversarial loss: 1.570690, acc: 0.187500]\n",
      "2563: [discriminator loss: 0.523724, acc: 0.750000] [adversarial loss: 1.304090, acc: 0.250000]\n",
      "2564: [discriminator loss: 0.482582, acc: 0.750000] [adversarial loss: 1.484353, acc: 0.125000]\n",
      "2565: [discriminator loss: 0.564880, acc: 0.703125] [adversarial loss: 1.347478, acc: 0.171875]\n",
      "2566: [discriminator loss: 0.449065, acc: 0.796875] [adversarial loss: 1.281052, acc: 0.218750]\n",
      "2567: [discriminator loss: 0.549177, acc: 0.734375] [adversarial loss: 1.662991, acc: 0.109375]\n",
      "2568: [discriminator loss: 0.517028, acc: 0.695312] [adversarial loss: 1.426366, acc: 0.234375]\n",
      "2569: [discriminator loss: 0.539186, acc: 0.750000] [adversarial loss: 1.044995, acc: 0.343750]\n",
      "2570: [discriminator loss: 0.474192, acc: 0.765625] [adversarial loss: 1.283918, acc: 0.281250]\n",
      "2571: [discriminator loss: 0.479131, acc: 0.789062] [adversarial loss: 1.256806, acc: 0.218750]\n",
      "2572: [discriminator loss: 0.482856, acc: 0.804688] [adversarial loss: 1.193969, acc: 0.281250]\n",
      "2573: [discriminator loss: 0.487518, acc: 0.781250] [adversarial loss: 1.506073, acc: 0.140625]\n",
      "2574: [discriminator loss: 0.490792, acc: 0.773438] [adversarial loss: 1.148609, acc: 0.281250]\n",
      "2575: [discriminator loss: 0.490037, acc: 0.789062] [adversarial loss: 1.113439, acc: 0.250000]\n",
      "2576: [discriminator loss: 0.450448, acc: 0.781250] [adversarial loss: 1.845370, acc: 0.125000]\n",
      "2577: [discriminator loss: 0.471069, acc: 0.781250] [adversarial loss: 0.914801, acc: 0.453125]\n",
      "2578: [discriminator loss: 0.572696, acc: 0.687500] [adversarial loss: 2.172813, acc: 0.046875]\n",
      "2579: [discriminator loss: 0.538577, acc: 0.710938] [adversarial loss: 0.816212, acc: 0.468750]\n",
      "2580: [discriminator loss: 0.487733, acc: 0.781250] [adversarial loss: 2.016211, acc: 0.015625]\n",
      "2581: [discriminator loss: 0.486636, acc: 0.765625] [adversarial loss: 0.994313, acc: 0.375000]\n",
      "2582: [discriminator loss: 0.515850, acc: 0.703125] [adversarial loss: 1.670506, acc: 0.140625]\n",
      "2583: [discriminator loss: 0.556868, acc: 0.710938] [adversarial loss: 1.047837, acc: 0.312500]\n",
      "2584: [discriminator loss: 0.518249, acc: 0.750000] [adversarial loss: 1.601340, acc: 0.203125]\n",
      "2585: [discriminator loss: 0.464435, acc: 0.796875] [adversarial loss: 1.051337, acc: 0.281250]\n",
      "2586: [discriminator loss: 0.559598, acc: 0.750000] [adversarial loss: 1.429997, acc: 0.187500]\n",
      "2587: [discriminator loss: 0.424846, acc: 0.820312] [adversarial loss: 1.493896, acc: 0.078125]\n",
      "2588: [discriminator loss: 0.459418, acc: 0.796875] [adversarial loss: 1.771282, acc: 0.109375]\n",
      "2589: [discriminator loss: 0.547257, acc: 0.734375] [adversarial loss: 0.968904, acc: 0.484375]\n",
      "2590: [discriminator loss: 0.466712, acc: 0.757812] [adversarial loss: 2.008783, acc: 0.078125]\n",
      "2591: [discriminator loss: 0.453951, acc: 0.781250] [adversarial loss: 1.079456, acc: 0.390625]\n",
      "2592: [discriminator loss: 0.551212, acc: 0.726562] [adversarial loss: 1.667170, acc: 0.125000]\n",
      "2593: [discriminator loss: 0.480730, acc: 0.789062] [adversarial loss: 1.179070, acc: 0.234375]\n",
      "2594: [discriminator loss: 0.488015, acc: 0.781250] [adversarial loss: 1.403365, acc: 0.218750]\n",
      "2595: [discriminator loss: 0.454535, acc: 0.812500] [adversarial loss: 1.178363, acc: 0.218750]\n",
      "2596: [discriminator loss: 0.481641, acc: 0.773438] [adversarial loss: 1.804017, acc: 0.031250]\n",
      "2597: [discriminator loss: 0.521100, acc: 0.750000] [adversarial loss: 1.001488, acc: 0.390625]\n",
      "2598: [discriminator loss: 0.577162, acc: 0.742188] [adversarial loss: 1.691917, acc: 0.093750]\n",
      "2599: [discriminator loss: 0.495284, acc: 0.757812] [adversarial loss: 0.858723, acc: 0.515625]\n",
      "2600: [discriminator loss: 0.503500, acc: 0.750000] [adversarial loss: 2.042858, acc: 0.031250]\n",
      "2601: [discriminator loss: 0.582301, acc: 0.718750] [adversarial loss: 0.916916, acc: 0.421875]\n",
      "2602: [discriminator loss: 0.618824, acc: 0.632812] [adversarial loss: 1.921996, acc: 0.062500]\n",
      "2603: [discriminator loss: 0.517055, acc: 0.734375] [adversarial loss: 0.886110, acc: 0.515625]\n",
      "2604: [discriminator loss: 0.483553, acc: 0.804688] [adversarial loss: 1.495858, acc: 0.125000]\n",
      "2605: [discriminator loss: 0.477980, acc: 0.765625] [adversarial loss: 1.290456, acc: 0.140625]\n",
      "2606: [discriminator loss: 0.526099, acc: 0.718750] [adversarial loss: 1.451606, acc: 0.093750]\n",
      "2607: [discriminator loss: 0.575897, acc: 0.687500] [adversarial loss: 1.515024, acc: 0.140625]\n",
      "2608: [discriminator loss: 0.506893, acc: 0.726562] [adversarial loss: 1.556946, acc: 0.125000]\n",
      "2609: [discriminator loss: 0.508110, acc: 0.742188] [adversarial loss: 0.913228, acc: 0.375000]\n",
      "2610: [discriminator loss: 0.511848, acc: 0.718750] [adversarial loss: 2.034510, acc: 0.093750]\n",
      "2611: [discriminator loss: 0.469417, acc: 0.757812] [adversarial loss: 0.968657, acc: 0.328125]\n",
      "2612: [discriminator loss: 0.556278, acc: 0.710938] [adversarial loss: 1.541221, acc: 0.062500]\n",
      "2613: [discriminator loss: 0.433174, acc: 0.796875] [adversarial loss: 1.235968, acc: 0.187500]\n",
      "2614: [discriminator loss: 0.454596, acc: 0.796875] [adversarial loss: 1.343253, acc: 0.156250]\n",
      "2615: [discriminator loss: 0.435197, acc: 0.835938] [adversarial loss: 1.163436, acc: 0.234375]\n",
      "2616: [discriminator loss: 0.482304, acc: 0.773438] [adversarial loss: 1.464790, acc: 0.156250]\n",
      "2617: [discriminator loss: 0.543722, acc: 0.703125] [adversarial loss: 1.040007, acc: 0.375000]\n",
      "2618: [discriminator loss: 0.506516, acc: 0.765625] [adversarial loss: 1.616790, acc: 0.078125]\n",
      "2619: [discriminator loss: 0.463984, acc: 0.828125] [adversarial loss: 1.303396, acc: 0.250000]\n",
      "2620: [discriminator loss: 0.546288, acc: 0.742188] [adversarial loss: 1.507559, acc: 0.171875]\n",
      "2621: [discriminator loss: 0.494833, acc: 0.757812] [adversarial loss: 0.800181, acc: 0.484375]\n",
      "2622: [discriminator loss: 0.551678, acc: 0.726562] [adversarial loss: 1.864184, acc: 0.078125]\n",
      "2623: [discriminator loss: 0.559924, acc: 0.726562] [adversarial loss: 0.867069, acc: 0.484375]\n",
      "2624: [discriminator loss: 0.523745, acc: 0.750000] [adversarial loss: 1.625018, acc: 0.140625]\n",
      "2625: [discriminator loss: 0.523421, acc: 0.781250] [adversarial loss: 1.050407, acc: 0.375000]\n",
      "2626: [discriminator loss: 0.501075, acc: 0.734375] [adversarial loss: 1.648273, acc: 0.062500]\n",
      "2627: [discriminator loss: 0.503276, acc: 0.757812] [adversarial loss: 1.180048, acc: 0.281250]\n",
      "2628: [discriminator loss: 0.494630, acc: 0.773438] [adversarial loss: 1.598782, acc: 0.140625]\n",
      "2629: [discriminator loss: 0.497935, acc: 0.742188] [adversarial loss: 1.453701, acc: 0.171875]\n",
      "2630: [discriminator loss: 0.434931, acc: 0.812500] [adversarial loss: 1.235617, acc: 0.234375]\n",
      "2631: [discriminator loss: 0.455354, acc: 0.773438] [adversarial loss: 1.100146, acc: 0.281250]\n",
      "2632: [discriminator loss: 0.524892, acc: 0.734375] [adversarial loss: 2.018646, acc: 0.015625]\n",
      "2633: [discriminator loss: 0.475022, acc: 0.765625] [adversarial loss: 0.884417, acc: 0.421875]\n",
      "2634: [discriminator loss: 0.481395, acc: 0.750000] [adversarial loss: 1.873098, acc: 0.140625]\n",
      "2635: [discriminator loss: 0.511489, acc: 0.734375] [adversarial loss: 1.147315, acc: 0.265625]\n",
      "2636: [discriminator loss: 0.504917, acc: 0.718750] [adversarial loss: 1.791302, acc: 0.093750]\n",
      "2637: [discriminator loss: 0.557951, acc: 0.718750] [adversarial loss: 1.207938, acc: 0.250000]\n",
      "2638: [discriminator loss: 0.452307, acc: 0.796875] [adversarial loss: 1.611947, acc: 0.187500]\n",
      "2639: [discriminator loss: 0.466761, acc: 0.804688] [adversarial loss: 1.031913, acc: 0.312500]\n",
      "2640: [discriminator loss: 0.482198, acc: 0.742188] [adversarial loss: 1.841448, acc: 0.031250]\n",
      "2641: [discriminator loss: 0.436917, acc: 0.789062] [adversarial loss: 1.247164, acc: 0.265625]\n",
      "2642: [discriminator loss: 0.445602, acc: 0.781250] [adversarial loss: 1.841365, acc: 0.109375]\n",
      "2643: [discriminator loss: 0.535117, acc: 0.703125] [adversarial loss: 1.314608, acc: 0.265625]\n",
      "2644: [discriminator loss: 0.507480, acc: 0.757812] [adversarial loss: 1.491010, acc: 0.140625]\n",
      "2645: [discriminator loss: 0.489635, acc: 0.757812] [adversarial loss: 1.260810, acc: 0.140625]\n",
      "2646: [discriminator loss: 0.456551, acc: 0.789062] [adversarial loss: 1.254053, acc: 0.296875]\n",
      "2647: [discriminator loss: 0.642394, acc: 0.648438] [adversarial loss: 1.470030, acc: 0.234375]\n",
      "2648: [discriminator loss: 0.480122, acc: 0.796875] [adversarial loss: 1.330734, acc: 0.234375]\n",
      "2649: [discriminator loss: 0.542659, acc: 0.742188] [adversarial loss: 1.465112, acc: 0.156250]\n",
      "2650: [discriminator loss: 0.430046, acc: 0.812500] [adversarial loss: 1.437146, acc: 0.140625]\n",
      "2651: [discriminator loss: 0.440345, acc: 0.781250] [adversarial loss: 1.743139, acc: 0.078125]\n",
      "2652: [discriminator loss: 0.435650, acc: 0.781250] [adversarial loss: 1.394284, acc: 0.109375]\n",
      "2653: [discriminator loss: 0.483443, acc: 0.757812] [adversarial loss: 1.443058, acc: 0.140625]\n",
      "2654: [discriminator loss: 0.528783, acc: 0.703125] [adversarial loss: 1.193658, acc: 0.390625]\n",
      "2655: [discriminator loss: 0.546992, acc: 0.750000] [adversarial loss: 1.671384, acc: 0.109375]\n",
      "2656: [discriminator loss: 0.483955, acc: 0.757812] [adversarial loss: 1.137544, acc: 0.281250]\n",
      "2657: [discriminator loss: 0.514925, acc: 0.757812] [adversarial loss: 1.399257, acc: 0.125000]\n",
      "2658: [discriminator loss: 0.474479, acc: 0.812500] [adversarial loss: 1.279711, acc: 0.171875]\n",
      "2659: [discriminator loss: 0.511630, acc: 0.742188] [adversarial loss: 1.186356, acc: 0.187500]\n",
      "2660: [discriminator loss: 0.507089, acc: 0.789062] [adversarial loss: 1.193536, acc: 0.234375]\n",
      "2661: [discriminator loss: 0.396428, acc: 0.789062] [adversarial loss: 1.549026, acc: 0.171875]\n",
      "2662: [discriminator loss: 0.551150, acc: 0.726562] [adversarial loss: 1.949592, acc: 0.140625]\n",
      "2663: [discriminator loss: 0.511209, acc: 0.757812] [adversarial loss: 0.804813, acc: 0.531250]\n",
      "2664: [discriminator loss: 0.560550, acc: 0.710938] [adversarial loss: 2.173945, acc: 0.031250]\n",
      "2665: [discriminator loss: 0.605126, acc: 0.710938] [adversarial loss: 0.945412, acc: 0.406250]\n",
      "2666: [discriminator loss: 0.586907, acc: 0.695312] [adversarial loss: 2.073049, acc: 0.109375]\n",
      "2667: [discriminator loss: 0.542359, acc: 0.695312] [adversarial loss: 1.005309, acc: 0.390625]\n",
      "2668: [discriminator loss: 0.495440, acc: 0.750000] [adversarial loss: 1.354944, acc: 0.156250]\n",
      "2669: [discriminator loss: 0.457233, acc: 0.796875] [adversarial loss: 1.104082, acc: 0.281250]\n",
      "2670: [discriminator loss: 0.459094, acc: 0.750000] [adversarial loss: 1.672557, acc: 0.109375]\n",
      "2671: [discriminator loss: 0.468671, acc: 0.796875] [adversarial loss: 1.121707, acc: 0.265625]\n",
      "2672: [discriminator loss: 0.490279, acc: 0.765625] [adversarial loss: 1.808153, acc: 0.062500]\n",
      "2673: [discriminator loss: 0.483781, acc: 0.742188] [adversarial loss: 1.087382, acc: 0.203125]\n",
      "2674: [discriminator loss: 0.533859, acc: 0.718750] [adversarial loss: 1.942498, acc: 0.078125]\n",
      "2675: [discriminator loss: 0.547903, acc: 0.734375] [adversarial loss: 0.792209, acc: 0.578125]\n",
      "2676: [discriminator loss: 0.663753, acc: 0.648438] [adversarial loss: 2.064914, acc: 0.046875]\n",
      "2677: [discriminator loss: 0.510165, acc: 0.734375] [adversarial loss: 1.163569, acc: 0.265625]\n",
      "2678: [discriminator loss: 0.492856, acc: 0.757812] [adversarial loss: 1.261507, acc: 0.250000]\n",
      "2679: [discriminator loss: 0.460925, acc: 0.789062] [adversarial loss: 1.240608, acc: 0.171875]\n",
      "2680: [discriminator loss: 0.514358, acc: 0.734375] [adversarial loss: 1.373611, acc: 0.218750]\n",
      "2681: [discriminator loss: 0.393881, acc: 0.828125] [adversarial loss: 1.509619, acc: 0.156250]\n",
      "2682: [discriminator loss: 0.515329, acc: 0.710938] [adversarial loss: 1.104555, acc: 0.359375]\n",
      "2683: [discriminator loss: 0.512201, acc: 0.750000] [adversarial loss: 1.774858, acc: 0.078125]\n",
      "2684: [discriminator loss: 0.487003, acc: 0.757812] [adversarial loss: 1.156256, acc: 0.312500]\n",
      "2685: [discriminator loss: 0.547477, acc: 0.734375] [adversarial loss: 1.472795, acc: 0.125000]\n",
      "2686: [discriminator loss: 0.466005, acc: 0.789062] [adversarial loss: 1.315212, acc: 0.171875]\n",
      "2687: [discriminator loss: 0.455318, acc: 0.828125] [adversarial loss: 1.371236, acc: 0.187500]\n",
      "2688: [discriminator loss: 0.448748, acc: 0.820312] [adversarial loss: 1.047894, acc: 0.343750]\n",
      "2689: [discriminator loss: 0.456405, acc: 0.789062] [adversarial loss: 1.671524, acc: 0.062500]\n",
      "2690: [discriminator loss: 0.449996, acc: 0.804688] [adversarial loss: 1.332938, acc: 0.234375]\n",
      "2691: [discriminator loss: 0.527845, acc: 0.742188] [adversarial loss: 1.512192, acc: 0.250000]\n",
      "2692: [discriminator loss: 0.483361, acc: 0.796875] [adversarial loss: 1.275507, acc: 0.281250]\n",
      "2693: [discriminator loss: 0.507283, acc: 0.773438] [adversarial loss: 1.829917, acc: 0.093750]\n",
      "2694: [discriminator loss: 0.543925, acc: 0.742188] [adversarial loss: 1.183331, acc: 0.265625]\n",
      "2695: [discriminator loss: 0.508767, acc: 0.765625] [adversarial loss: 1.830703, acc: 0.125000]\n",
      "2696: [discriminator loss: 0.608773, acc: 0.656250] [adversarial loss: 1.070526, acc: 0.328125]\n",
      "2697: [discriminator loss: 0.533342, acc: 0.734375] [adversarial loss: 1.707677, acc: 0.140625]\n",
      "2698: [discriminator loss: 0.506576, acc: 0.773438] [adversarial loss: 1.232641, acc: 0.218750]\n",
      "2699: [discriminator loss: 0.471253, acc: 0.781250] [adversarial loss: 1.574787, acc: 0.093750]\n",
      "2700: [discriminator loss: 0.494466, acc: 0.796875] [adversarial loss: 1.213954, acc: 0.218750]\n",
      "2701: [discriminator loss: 0.453416, acc: 0.765625] [adversarial loss: 1.843399, acc: 0.140625]\n",
      "2702: [discriminator loss: 0.487934, acc: 0.773438] [adversarial loss: 1.159676, acc: 0.265625]\n",
      "2703: [discriminator loss: 0.523444, acc: 0.734375] [adversarial loss: 1.699717, acc: 0.171875]\n",
      "2704: [discriminator loss: 0.461105, acc: 0.781250] [adversarial loss: 1.095174, acc: 0.296875]\n",
      "2705: [discriminator loss: 0.488697, acc: 0.726562] [adversarial loss: 1.636629, acc: 0.125000]\n",
      "2706: [discriminator loss: 0.485481, acc: 0.757812] [adversarial loss: 1.148437, acc: 0.296875]\n",
      "2707: [discriminator loss: 0.598361, acc: 0.765625] [adversarial loss: 2.079392, acc: 0.046875]\n",
      "2708: [discriminator loss: 0.580593, acc: 0.742188] [adversarial loss: 0.928252, acc: 0.421875]\n",
      "2709: [discriminator loss: 0.632614, acc: 0.585938] [adversarial loss: 1.896164, acc: 0.093750]\n",
      "2710: [discriminator loss: 0.539437, acc: 0.726562] [adversarial loss: 0.987608, acc: 0.406250]\n",
      "2711: [discriminator loss: 0.436837, acc: 0.812500] [adversarial loss: 1.634641, acc: 0.125000]\n",
      "2712: [discriminator loss: 0.459236, acc: 0.773438] [adversarial loss: 0.944156, acc: 0.406250]\n",
      "2713: [discriminator loss: 0.456564, acc: 0.789062] [adversarial loss: 1.633263, acc: 0.109375]\n",
      "2714: [discriminator loss: 0.494516, acc: 0.734375] [adversarial loss: 1.126387, acc: 0.281250]\n",
      "2715: [discriminator loss: 0.443971, acc: 0.812500] [adversarial loss: 1.754421, acc: 0.140625]\n",
      "2716: [discriminator loss: 0.507084, acc: 0.734375] [adversarial loss: 1.283180, acc: 0.218750]\n",
      "2717: [discriminator loss: 0.444281, acc: 0.781250] [adversarial loss: 1.514196, acc: 0.140625]\n",
      "2718: [discriminator loss: 0.465576, acc: 0.734375] [adversarial loss: 1.204782, acc: 0.203125]\n",
      "2719: [discriminator loss: 0.394129, acc: 0.843750] [adversarial loss: 1.566722, acc: 0.140625]\n",
      "2720: [discriminator loss: 0.392377, acc: 0.843750] [adversarial loss: 1.406367, acc: 0.156250]\n",
      "2721: [discriminator loss: 0.515599, acc: 0.718750] [adversarial loss: 1.239121, acc: 0.171875]\n",
      "2722: [discriminator loss: 0.459859, acc: 0.781250] [adversarial loss: 1.054140, acc: 0.343750]\n",
      "2723: [discriminator loss: 0.542145, acc: 0.703125] [adversarial loss: 1.618661, acc: 0.171875]\n",
      "2724: [discriminator loss: 0.491818, acc: 0.757812] [adversarial loss: 0.943555, acc: 0.375000]\n",
      "2725: [discriminator loss: 0.597232, acc: 0.679688] [adversarial loss: 2.215022, acc: 0.046875]\n",
      "2726: [discriminator loss: 0.679002, acc: 0.632812] [adversarial loss: 0.878383, acc: 0.484375]\n",
      "2727: [discriminator loss: 0.613171, acc: 0.640625] [adversarial loss: 2.211725, acc: 0.062500]\n",
      "2728: [discriminator loss: 0.524893, acc: 0.710938] [adversarial loss: 1.334087, acc: 0.218750]\n",
      "2729: [discriminator loss: 0.408195, acc: 0.820312] [adversarial loss: 1.469511, acc: 0.093750]\n",
      "2730: [discriminator loss: 0.429908, acc: 0.796875] [adversarial loss: 1.255716, acc: 0.250000]\n",
      "2731: [discriminator loss: 0.454385, acc: 0.812500] [adversarial loss: 1.372997, acc: 0.218750]\n",
      "2732: [discriminator loss: 0.487989, acc: 0.734375] [adversarial loss: 0.965986, acc: 0.421875]\n",
      "2733: [discriminator loss: 0.509684, acc: 0.757812] [adversarial loss: 1.717462, acc: 0.125000]\n",
      "2734: [discriminator loss: 0.533206, acc: 0.726562] [adversarial loss: 1.216557, acc: 0.203125]\n",
      "2735: [discriminator loss: 0.575667, acc: 0.726562] [adversarial loss: 1.422591, acc: 0.109375]\n",
      "2736: [discriminator loss: 0.474778, acc: 0.750000] [adversarial loss: 1.286419, acc: 0.218750]\n",
      "2737: [discriminator loss: 0.454783, acc: 0.796875] [adversarial loss: 1.619077, acc: 0.093750]\n",
      "2738: [discriminator loss: 0.443204, acc: 0.804688] [adversarial loss: 1.225886, acc: 0.250000]\n",
      "2739: [discriminator loss: 0.538422, acc: 0.695312] [adversarial loss: 1.845649, acc: 0.062500]\n",
      "2740: [discriminator loss: 0.474469, acc: 0.750000] [adversarial loss: 1.309307, acc: 0.234375]\n",
      "2741: [discriminator loss: 0.455233, acc: 0.750000] [adversarial loss: 1.546091, acc: 0.203125]\n",
      "2742: [discriminator loss: 0.429400, acc: 0.789062] [adversarial loss: 1.377738, acc: 0.234375]\n",
      "2743: [discriminator loss: 0.409656, acc: 0.843750] [adversarial loss: 1.419432, acc: 0.156250]\n",
      "2744: [discriminator loss: 0.433475, acc: 0.789062] [adversarial loss: 1.588097, acc: 0.171875]\n",
      "2745: [discriminator loss: 0.462826, acc: 0.773438] [adversarial loss: 1.562230, acc: 0.171875]\n",
      "2746: [discriminator loss: 0.532193, acc: 0.734375] [adversarial loss: 2.072372, acc: 0.062500]\n",
      "2747: [discriminator loss: 0.503370, acc: 0.742188] [adversarial loss: 1.271188, acc: 0.218750]\n",
      "2748: [discriminator loss: 0.459354, acc: 0.734375] [adversarial loss: 1.862309, acc: 0.062500]\n",
      "2749: [discriminator loss: 0.555217, acc: 0.742188] [adversarial loss: 1.294496, acc: 0.281250]\n",
      "2750: [discriminator loss: 0.446133, acc: 0.789062] [adversarial loss: 1.679010, acc: 0.093750]\n",
      "2751: [discriminator loss: 0.458802, acc: 0.750000] [adversarial loss: 1.140270, acc: 0.312500]\n",
      "2752: [discriminator loss: 0.475758, acc: 0.757812] [adversarial loss: 1.885219, acc: 0.031250]\n",
      "2753: [discriminator loss: 0.482385, acc: 0.750000] [adversarial loss: 0.714047, acc: 0.546875]\n",
      "2754: [discriminator loss: 0.725684, acc: 0.687500] [adversarial loss: 1.989746, acc: 0.046875]\n",
      "2755: [discriminator loss: 0.553883, acc: 0.687500] [adversarial loss: 1.326295, acc: 0.234375]\n",
      "2756: [discriminator loss: 0.522476, acc: 0.757812] [adversarial loss: 1.300423, acc: 0.281250]\n",
      "2757: [discriminator loss: 0.466594, acc: 0.710938] [adversarial loss: 1.294537, acc: 0.234375]\n",
      "2758: [discriminator loss: 0.441219, acc: 0.804688] [adversarial loss: 1.857968, acc: 0.046875]\n",
      "2759: [discriminator loss: 0.431853, acc: 0.812500] [adversarial loss: 0.924883, acc: 0.515625]\n",
      "2760: [discriminator loss: 0.401945, acc: 0.843750] [adversarial loss: 1.605780, acc: 0.171875]\n",
      "2761: [discriminator loss: 0.482315, acc: 0.750000] [adversarial loss: 1.302933, acc: 0.234375]\n",
      "2762: [discriminator loss: 0.511577, acc: 0.765625] [adversarial loss: 1.866749, acc: 0.093750]\n",
      "2763: [discriminator loss: 0.577510, acc: 0.726562] [adversarial loss: 0.906714, acc: 0.406250]\n",
      "2764: [discriminator loss: 0.536769, acc: 0.710938] [adversarial loss: 1.669667, acc: 0.125000]\n",
      "2765: [discriminator loss: 0.483653, acc: 0.757812] [adversarial loss: 1.189511, acc: 0.281250]\n",
      "2766: [discriminator loss: 0.441425, acc: 0.796875] [adversarial loss: 1.833822, acc: 0.078125]\n",
      "2767: [discriminator loss: 0.585154, acc: 0.687500] [adversarial loss: 1.160221, acc: 0.250000]\n",
      "2768: [discriminator loss: 0.486362, acc: 0.796875] [adversarial loss: 1.679238, acc: 0.109375]\n",
      "2769: [discriminator loss: 0.528916, acc: 0.781250] [adversarial loss: 1.149352, acc: 0.312500]\n",
      "2770: [discriminator loss: 0.468577, acc: 0.804688] [adversarial loss: 1.556763, acc: 0.125000]\n",
      "2771: [discriminator loss: 0.540751, acc: 0.710938] [adversarial loss: 1.579649, acc: 0.171875]\n",
      "2772: [discriminator loss: 0.521619, acc: 0.718750] [adversarial loss: 1.137550, acc: 0.296875]\n",
      "2773: [discriminator loss: 0.488689, acc: 0.781250] [adversarial loss: 1.457989, acc: 0.203125]\n",
      "2774: [discriminator loss: 0.517586, acc: 0.757812] [adversarial loss: 0.836608, acc: 0.468750]\n",
      "2775: [discriminator loss: 0.502253, acc: 0.742188] [adversarial loss: 1.890039, acc: 0.062500]\n",
      "2776: [discriminator loss: 0.583012, acc: 0.679688] [adversarial loss: 1.053175, acc: 0.390625]\n",
      "2777: [discriminator loss: 0.543433, acc: 0.718750] [adversarial loss: 1.774068, acc: 0.093750]\n",
      "2778: [discriminator loss: 0.491749, acc: 0.750000] [adversarial loss: 1.215052, acc: 0.218750]\n",
      "2779: [discriminator loss: 0.474389, acc: 0.781250] [adversarial loss: 1.547680, acc: 0.125000]\n",
      "2780: [discriminator loss: 0.488011, acc: 0.742188] [adversarial loss: 1.416595, acc: 0.140625]\n",
      "2781: [discriminator loss: 0.555820, acc: 0.765625] [adversarial loss: 1.454785, acc: 0.171875]\n",
      "2782: [discriminator loss: 0.475205, acc: 0.781250] [adversarial loss: 2.059921, acc: 0.031250]\n",
      "2783: [discriminator loss: 0.483938, acc: 0.781250] [adversarial loss: 0.781075, acc: 0.468750]\n",
      "2784: [discriminator loss: 0.635467, acc: 0.671875] [adversarial loss: 2.211798, acc: 0.031250]\n",
      "2785: [discriminator loss: 0.574427, acc: 0.710938] [adversarial loss: 1.136952, acc: 0.296875]\n",
      "2786: [discriminator loss: 0.527541, acc: 0.757812] [adversarial loss: 1.893106, acc: 0.046875]\n",
      "2787: [discriminator loss: 0.560823, acc: 0.718750] [adversarial loss: 1.101810, acc: 0.312500]\n",
      "2788: [discriminator loss: 0.449398, acc: 0.789062] [adversarial loss: 1.391668, acc: 0.187500]\n",
      "2789: [discriminator loss: 0.488197, acc: 0.796875] [adversarial loss: 1.171501, acc: 0.234375]\n",
      "2790: [discriminator loss: 0.477463, acc: 0.781250] [adversarial loss: 1.462181, acc: 0.187500]\n",
      "2791: [discriminator loss: 0.487082, acc: 0.750000] [adversarial loss: 1.310534, acc: 0.265625]\n",
      "2792: [discriminator loss: 0.438505, acc: 0.781250] [adversarial loss: 1.832038, acc: 0.093750]\n",
      "2793: [discriminator loss: 0.581736, acc: 0.703125] [adversarial loss: 0.947455, acc: 0.343750]\n",
      "2794: [discriminator loss: 0.534058, acc: 0.679688] [adversarial loss: 1.781936, acc: 0.078125]\n",
      "2795: [discriminator loss: 0.536401, acc: 0.750000] [adversarial loss: 1.281736, acc: 0.234375]\n",
      "2796: [discriminator loss: 0.531381, acc: 0.750000] [adversarial loss: 1.468934, acc: 0.125000]\n",
      "2797: [discriminator loss: 0.541820, acc: 0.726562] [adversarial loss: 1.289326, acc: 0.171875]\n",
      "2798: [discriminator loss: 0.486246, acc: 0.820312] [adversarial loss: 1.595477, acc: 0.156250]\n",
      "2799: [discriminator loss: 0.568366, acc: 0.679688] [adversarial loss: 1.848671, acc: 0.187500]\n",
      "2800: [discriminator loss: 0.539887, acc: 0.742188] [adversarial loss: 0.996711, acc: 0.296875]\n",
      "2801: [discriminator loss: 0.532390, acc: 0.718750] [adversarial loss: 1.791969, acc: 0.062500]\n",
      "2802: [discriminator loss: 0.569134, acc: 0.726562] [adversarial loss: 0.883290, acc: 0.406250]\n",
      "2803: [discriminator loss: 0.543833, acc: 0.773438] [adversarial loss: 1.742213, acc: 0.078125]\n",
      "2804: [discriminator loss: 0.543420, acc: 0.710938] [adversarial loss: 1.350941, acc: 0.250000]\n",
      "2805: [discriminator loss: 0.460124, acc: 0.773438] [adversarial loss: 1.236129, acc: 0.265625]\n",
      "2806: [discriminator loss: 0.406470, acc: 0.820312] [adversarial loss: 1.176455, acc: 0.281250]\n",
      "2807: [discriminator loss: 0.470661, acc: 0.789062] [adversarial loss: 1.443074, acc: 0.140625]\n",
      "2808: [discriminator loss: 0.500127, acc: 0.765625] [adversarial loss: 1.400636, acc: 0.125000]\n",
      "2809: [discriminator loss: 0.549056, acc: 0.718750] [adversarial loss: 1.554429, acc: 0.171875]\n",
      "2810: [discriminator loss: 0.499562, acc: 0.750000] [adversarial loss: 0.863837, acc: 0.390625]\n",
      "2811: [discriminator loss: 0.490777, acc: 0.750000] [adversarial loss: 2.073851, acc: 0.031250]\n",
      "2812: [discriminator loss: 0.457924, acc: 0.789062] [adversarial loss: 1.022473, acc: 0.359375]\n",
      "2813: [discriminator loss: 0.621287, acc: 0.617188] [adversarial loss: 1.988112, acc: 0.031250]\n",
      "2814: [discriminator loss: 0.530635, acc: 0.710938] [adversarial loss: 1.212031, acc: 0.296875]\n",
      "2815: [discriminator loss: 0.528604, acc: 0.726562] [adversarial loss: 1.517049, acc: 0.140625]\n",
      "2816: [discriminator loss: 0.510634, acc: 0.734375] [adversarial loss: 1.336873, acc: 0.203125]\n",
      "2817: [discriminator loss: 0.459223, acc: 0.773438] [adversarial loss: 1.106055, acc: 0.281250]\n",
      "2818: [discriminator loss: 0.442699, acc: 0.804688] [adversarial loss: 1.265291, acc: 0.171875]\n",
      "2819: [discriminator loss: 0.493392, acc: 0.750000] [adversarial loss: 1.630993, acc: 0.093750]\n",
      "2820: [discriminator loss: 0.509694, acc: 0.765625] [adversarial loss: 1.143682, acc: 0.218750]\n",
      "2821: [discriminator loss: 0.490310, acc: 0.781250] [adversarial loss: 1.834817, acc: 0.093750]\n",
      "2822: [discriminator loss: 0.529291, acc: 0.750000] [adversarial loss: 0.918208, acc: 0.390625]\n",
      "2823: [discriminator loss: 0.578178, acc: 0.710938] [adversarial loss: 1.788188, acc: 0.046875]\n",
      "2824: [discriminator loss: 0.542966, acc: 0.726562] [adversarial loss: 1.150279, acc: 0.296875]\n",
      "2825: [discriminator loss: 0.477464, acc: 0.757812] [adversarial loss: 1.421681, acc: 0.171875]\n",
      "2826: [discriminator loss: 0.510666, acc: 0.781250] [adversarial loss: 1.105171, acc: 0.312500]\n",
      "2827: [discriminator loss: 0.512855, acc: 0.757812] [adversarial loss: 1.668262, acc: 0.140625]\n",
      "2828: [discriminator loss: 0.584220, acc: 0.664062] [adversarial loss: 1.084930, acc: 0.250000]\n",
      "2829: [discriminator loss: 0.446494, acc: 0.828125] [adversarial loss: 1.891734, acc: 0.031250]\n",
      "2830: [discriminator loss: 0.478549, acc: 0.750000] [adversarial loss: 1.261897, acc: 0.328125]\n",
      "2831: [discriminator loss: 0.511753, acc: 0.765625] [adversarial loss: 1.775920, acc: 0.140625]\n",
      "2832: [discriminator loss: 0.524174, acc: 0.750000] [adversarial loss: 0.943991, acc: 0.500000]\n",
      "2833: [discriminator loss: 0.488114, acc: 0.773438] [adversarial loss: 1.500165, acc: 0.171875]\n",
      "2834: [discriminator loss: 0.437844, acc: 0.804688] [adversarial loss: 1.529198, acc: 0.125000]\n",
      "2835: [discriminator loss: 0.462304, acc: 0.750000] [adversarial loss: 1.244547, acc: 0.265625]\n",
      "2836: [discriminator loss: 0.544737, acc: 0.718750] [adversarial loss: 1.631205, acc: 0.156250]\n",
      "2837: [discriminator loss: 0.517938, acc: 0.757812] [adversarial loss: 1.343014, acc: 0.250000]\n",
      "2838: [discriminator loss: 0.463059, acc: 0.789062] [adversarial loss: 1.681404, acc: 0.093750]\n",
      "2839: [discriminator loss: 0.521375, acc: 0.734375] [adversarial loss: 1.040336, acc: 0.343750]\n",
      "2840: [discriminator loss: 0.417108, acc: 0.796875] [adversarial loss: 1.488064, acc: 0.156250]\n",
      "2841: [discriminator loss: 0.505780, acc: 0.773438] [adversarial loss: 1.131973, acc: 0.312500]\n",
      "2842: [discriminator loss: 0.477401, acc: 0.781250] [adversarial loss: 1.579715, acc: 0.125000]\n",
      "2843: [discriminator loss: 0.575909, acc: 0.718750] [adversarial loss: 1.064734, acc: 0.296875]\n",
      "2844: [discriminator loss: 0.560645, acc: 0.710938] [adversarial loss: 1.782366, acc: 0.078125]\n",
      "2845: [discriminator loss: 0.528484, acc: 0.773438] [adversarial loss: 0.873248, acc: 0.500000]\n",
      "2846: [discriminator loss: 0.489211, acc: 0.804688] [adversarial loss: 1.758231, acc: 0.093750]\n",
      "2847: [discriminator loss: 0.514387, acc: 0.742188] [adversarial loss: 1.240721, acc: 0.281250]\n",
      "2848: [discriminator loss: 0.522839, acc: 0.765625] [adversarial loss: 1.552882, acc: 0.078125]\n",
      "2849: [discriminator loss: 0.428786, acc: 0.789062] [adversarial loss: 1.305658, acc: 0.171875]\n",
      "2850: [discriminator loss: 0.435420, acc: 0.820312] [adversarial loss: 1.302306, acc: 0.187500]\n",
      "2851: [discriminator loss: 0.526596, acc: 0.742188] [adversarial loss: 1.426594, acc: 0.093750]\n",
      "2852: [discriminator loss: 0.450578, acc: 0.828125] [adversarial loss: 1.307316, acc: 0.265625]\n",
      "2853: [discriminator loss: 0.434147, acc: 0.765625] [adversarial loss: 1.528368, acc: 0.093750]\n",
      "2854: [discriminator loss: 0.443221, acc: 0.796875] [adversarial loss: 1.469663, acc: 0.187500]\n",
      "2855: [discriminator loss: 0.421977, acc: 0.781250] [adversarial loss: 1.375127, acc: 0.187500]\n",
      "2856: [discriminator loss: 0.476744, acc: 0.773438] [adversarial loss: 0.993320, acc: 0.375000]\n",
      "2857: [discriminator loss: 0.553438, acc: 0.757812] [adversarial loss: 2.051206, acc: 0.109375]\n",
      "2858: [discriminator loss: 0.540738, acc: 0.781250] [adversarial loss: 0.900953, acc: 0.468750]\n",
      "2859: [discriminator loss: 0.560124, acc: 0.695312] [adversarial loss: 1.880816, acc: 0.046875]\n",
      "2860: [discriminator loss: 0.381394, acc: 0.843750] [adversarial loss: 1.114402, acc: 0.390625]\n",
      "2861: [discriminator loss: 0.484409, acc: 0.773438] [adversarial loss: 1.278216, acc: 0.218750]\n",
      "2862: [discriminator loss: 0.399850, acc: 0.812500] [adversarial loss: 1.788464, acc: 0.109375]\n",
      "2863: [discriminator loss: 0.581631, acc: 0.695312] [adversarial loss: 1.069282, acc: 0.312500]\n",
      "2864: [discriminator loss: 0.540376, acc: 0.695312] [adversarial loss: 2.051032, acc: 0.015625]\n",
      "2865: [discriminator loss: 0.496627, acc: 0.773438] [adversarial loss: 0.991826, acc: 0.328125]\n",
      "2866: [discriminator loss: 0.490073, acc: 0.765625] [adversarial loss: 1.607271, acc: 0.125000]\n",
      "2867: [discriminator loss: 0.528352, acc: 0.742188] [adversarial loss: 1.193046, acc: 0.328125]\n",
      "2868: [discriminator loss: 0.466223, acc: 0.789062] [adversarial loss: 1.239533, acc: 0.250000]\n",
      "2869: [discriminator loss: 0.535422, acc: 0.718750] [adversarial loss: 1.475155, acc: 0.125000]\n",
      "2870: [discriminator loss: 0.456652, acc: 0.820312] [adversarial loss: 1.785890, acc: 0.109375]\n",
      "2871: [discriminator loss: 0.519649, acc: 0.757812] [adversarial loss: 1.437480, acc: 0.203125]\n",
      "2872: [discriminator loss: 0.485923, acc: 0.750000] [adversarial loss: 1.279630, acc: 0.250000]\n",
      "2873: [discriminator loss: 0.478257, acc: 0.773438] [adversarial loss: 1.391792, acc: 0.265625]\n",
      "2874: [discriminator loss: 0.487077, acc: 0.734375] [adversarial loss: 1.479291, acc: 0.140625]\n",
      "2875: [discriminator loss: 0.519934, acc: 0.726562] [adversarial loss: 1.411483, acc: 0.109375]\n",
      "2876: [discriminator loss: 0.493713, acc: 0.750000] [adversarial loss: 1.640721, acc: 0.078125]\n",
      "2877: [discriminator loss: 0.490641, acc: 0.765625] [adversarial loss: 0.846362, acc: 0.500000]\n",
      "2878: [discriminator loss: 0.502694, acc: 0.718750] [adversarial loss: 1.781553, acc: 0.140625]\n",
      "2879: [discriminator loss: 0.602315, acc: 0.695312] [adversarial loss: 1.067899, acc: 0.359375]\n",
      "2880: [discriminator loss: 0.498814, acc: 0.734375] [adversarial loss: 1.796251, acc: 0.062500]\n",
      "2881: [discriminator loss: 0.541984, acc: 0.757812] [adversarial loss: 1.203345, acc: 0.250000]\n",
      "2882: [discriminator loss: 0.549610, acc: 0.734375] [adversarial loss: 1.828334, acc: 0.140625]\n",
      "2883: [discriminator loss: 0.563075, acc: 0.710938] [adversarial loss: 1.025260, acc: 0.328125]\n",
      "2884: [discriminator loss: 0.520645, acc: 0.726562] [adversarial loss: 1.788325, acc: 0.062500]\n",
      "2885: [discriminator loss: 0.478556, acc: 0.726562] [adversarial loss: 1.008071, acc: 0.328125]\n",
      "2886: [discriminator loss: 0.471579, acc: 0.828125] [adversarial loss: 1.790993, acc: 0.156250]\n",
      "2887: [discriminator loss: 0.458271, acc: 0.820312] [adversarial loss: 1.338847, acc: 0.203125]\n",
      "2888: [discriminator loss: 0.491015, acc: 0.726562] [adversarial loss: 1.385286, acc: 0.250000]\n",
      "2889: [discriminator loss: 0.355750, acc: 0.843750] [adversarial loss: 1.404696, acc: 0.171875]\n",
      "2890: [discriminator loss: 0.445421, acc: 0.789062] [adversarial loss: 1.395286, acc: 0.203125]\n",
      "2891: [discriminator loss: 0.525087, acc: 0.742188] [adversarial loss: 1.063119, acc: 0.375000]\n",
      "2892: [discriminator loss: 0.456762, acc: 0.828125] [adversarial loss: 1.261879, acc: 0.203125]\n",
      "2893: [discriminator loss: 0.457473, acc: 0.812500] [adversarial loss: 1.290182, acc: 0.234375]\n",
      "2894: [discriminator loss: 0.453410, acc: 0.789062] [adversarial loss: 1.313409, acc: 0.140625]\n",
      "2895: [discriminator loss: 0.602947, acc: 0.687500] [adversarial loss: 1.599925, acc: 0.078125]\n",
      "2896: [discriminator loss: 0.449981, acc: 0.828125] [adversarial loss: 1.142814, acc: 0.265625]\n",
      "2897: [discriminator loss: 0.455945, acc: 0.781250] [adversarial loss: 1.616951, acc: 0.156250]\n",
      "2898: [discriminator loss: 0.513152, acc: 0.742188] [adversarial loss: 1.185470, acc: 0.296875]\n",
      "2899: [discriminator loss: 0.504317, acc: 0.742188] [adversarial loss: 1.945762, acc: 0.093750]\n",
      "2900: [discriminator loss: 0.533977, acc: 0.695312] [adversarial loss: 0.954466, acc: 0.437500]\n",
      "2901: [discriminator loss: 0.491990, acc: 0.765625] [adversarial loss: 1.700019, acc: 0.140625]\n",
      "2902: [discriminator loss: 0.444985, acc: 0.742188] [adversarial loss: 0.811259, acc: 0.437500]\n",
      "2903: [discriminator loss: 0.494098, acc: 0.765625] [adversarial loss: 1.887856, acc: 0.062500]\n",
      "2904: [discriminator loss: 0.516135, acc: 0.718750] [adversarial loss: 1.005550, acc: 0.312500]\n",
      "2905: [discriminator loss: 0.489010, acc: 0.781250] [adversarial loss: 2.095719, acc: 0.062500]\n",
      "2906: [discriminator loss: 0.612253, acc: 0.679688] [adversarial loss: 0.813097, acc: 0.468750]\n",
      "2907: [discriminator loss: 0.511819, acc: 0.773438] [adversarial loss: 1.910255, acc: 0.078125]\n",
      "2908: [discriminator loss: 0.475819, acc: 0.773438] [adversarial loss: 1.102536, acc: 0.343750]\n",
      "2909: [discriminator loss: 0.464268, acc: 0.804688] [adversarial loss: 1.420029, acc: 0.187500]\n",
      "2910: [discriminator loss: 0.469187, acc: 0.773438] [adversarial loss: 1.289795, acc: 0.281250]\n",
      "2911: [discriminator loss: 0.418490, acc: 0.843750] [adversarial loss: 1.399275, acc: 0.187500]\n",
      "2912: [discriminator loss: 0.486882, acc: 0.789062] [adversarial loss: 1.524035, acc: 0.187500]\n",
      "2913: [discriminator loss: 0.428406, acc: 0.796875] [adversarial loss: 1.326034, acc: 0.265625]\n",
      "2914: [discriminator loss: 0.465478, acc: 0.789062] [adversarial loss: 1.736393, acc: 0.093750]\n",
      "2915: [discriminator loss: 0.544287, acc: 0.710938] [adversarial loss: 1.159714, acc: 0.265625]\n",
      "2916: [discriminator loss: 0.404318, acc: 0.804688] [adversarial loss: 1.393538, acc: 0.203125]\n",
      "2917: [discriminator loss: 0.524095, acc: 0.695312] [adversarial loss: 1.299778, acc: 0.156250]\n",
      "2918: [discriminator loss: 0.490392, acc: 0.781250] [adversarial loss: 1.465520, acc: 0.156250]\n",
      "2919: [discriminator loss: 0.460612, acc: 0.781250] [adversarial loss: 1.017218, acc: 0.437500]\n",
      "2920: [discriminator loss: 0.487522, acc: 0.750000] [adversarial loss: 1.784335, acc: 0.093750]\n",
      "2921: [discriminator loss: 0.423020, acc: 0.773438] [adversarial loss: 1.516966, acc: 0.203125]\n",
      "2922: [discriminator loss: 0.540270, acc: 0.687500] [adversarial loss: 1.288083, acc: 0.296875]\n",
      "2923: [discriminator loss: 0.468221, acc: 0.781250] [adversarial loss: 1.532215, acc: 0.140625]\n",
      "2924: [discriminator loss: 0.542637, acc: 0.726562] [adversarial loss: 1.696845, acc: 0.218750]\n",
      "2925: [discriminator loss: 0.539381, acc: 0.710938] [adversarial loss: 1.085843, acc: 0.265625]\n",
      "2926: [discriminator loss: 0.480166, acc: 0.773438] [adversarial loss: 1.441843, acc: 0.250000]\n",
      "2927: [discriminator loss: 0.517340, acc: 0.750000] [adversarial loss: 1.218324, acc: 0.312500]\n",
      "2928: [discriminator loss: 0.563405, acc: 0.718750] [adversarial loss: 2.060213, acc: 0.031250]\n",
      "2929: [discriminator loss: 0.535206, acc: 0.695312] [adversarial loss: 1.040421, acc: 0.421875]\n",
      "2930: [discriminator loss: 0.567410, acc: 0.750000] [adversarial loss: 1.999169, acc: 0.109375]\n",
      "2931: [discriminator loss: 0.564756, acc: 0.718750] [adversarial loss: 1.082062, acc: 0.343750]\n",
      "2932: [discriminator loss: 0.507271, acc: 0.726562] [adversarial loss: 1.730432, acc: 0.062500]\n",
      "2933: [discriminator loss: 0.450527, acc: 0.820312] [adversarial loss: 1.249315, acc: 0.265625]\n",
      "2934: [discriminator loss: 0.431210, acc: 0.812500] [adversarial loss: 1.375976, acc: 0.218750]\n",
      "2935: [discriminator loss: 0.511272, acc: 0.718750] [adversarial loss: 1.526675, acc: 0.156250]\n",
      "2936: [discriminator loss: 0.409823, acc: 0.859375] [adversarial loss: 1.499468, acc: 0.203125]\n",
      "2937: [discriminator loss: 0.501433, acc: 0.789062] [adversarial loss: 1.678869, acc: 0.062500]\n",
      "2938: [discriminator loss: 0.477330, acc: 0.765625] [adversarial loss: 1.237385, acc: 0.250000]\n",
      "2939: [discriminator loss: 0.516092, acc: 0.718750] [adversarial loss: 1.779358, acc: 0.125000]\n",
      "2940: [discriminator loss: 0.480828, acc: 0.812500] [adversarial loss: 1.137724, acc: 0.375000]\n",
      "2941: [discriminator loss: 0.504333, acc: 0.750000] [adversarial loss: 1.394385, acc: 0.218750]\n",
      "2942: [discriminator loss: 0.419569, acc: 0.789062] [adversarial loss: 1.406824, acc: 0.250000]\n",
      "2943: [discriminator loss: 0.520330, acc: 0.757812] [adversarial loss: 1.657753, acc: 0.109375]\n",
      "2944: [discriminator loss: 0.486508, acc: 0.773438] [adversarial loss: 1.009543, acc: 0.375000]\n",
      "2945: [discriminator loss: 0.506846, acc: 0.718750] [adversarial loss: 1.982392, acc: 0.140625]\n",
      "2946: [discriminator loss: 0.453852, acc: 0.757812] [adversarial loss: 1.145824, acc: 0.343750]\n",
      "2947: [discriminator loss: 0.654329, acc: 0.632812] [adversarial loss: 2.117872, acc: 0.062500]\n",
      "2948: [discriminator loss: 0.611170, acc: 0.687500] [adversarial loss: 0.844503, acc: 0.515625]\n",
      "2949: [discriminator loss: 0.492413, acc: 0.742188] [adversarial loss: 1.970070, acc: 0.109375]\n",
      "2950: [discriminator loss: 0.584783, acc: 0.703125] [adversarial loss: 1.168047, acc: 0.328125]\n",
      "2951: [discriminator loss: 0.485289, acc: 0.750000] [adversarial loss: 1.497619, acc: 0.156250]\n",
      "2952: [discriminator loss: 0.548429, acc: 0.742188] [adversarial loss: 1.651065, acc: 0.140625]\n",
      "2953: [discriminator loss: 0.550487, acc: 0.726562] [adversarial loss: 0.992046, acc: 0.406250]\n",
      "2954: [discriminator loss: 0.508367, acc: 0.742188] [adversarial loss: 1.731475, acc: 0.156250]\n",
      "2955: [discriminator loss: 0.504945, acc: 0.789062] [adversarial loss: 1.180390, acc: 0.265625]\n",
      "2956: [discriminator loss: 0.547596, acc: 0.679688] [adversarial loss: 1.817234, acc: 0.062500]\n",
      "2957: [discriminator loss: 0.570731, acc: 0.687500] [adversarial loss: 0.859939, acc: 0.437500]\n",
      "2958: [discriminator loss: 0.496474, acc: 0.750000] [adversarial loss: 1.607507, acc: 0.093750]\n",
      "2959: [discriminator loss: 0.429833, acc: 0.789062] [adversarial loss: 1.206043, acc: 0.250000]\n",
      "2960: [discriminator loss: 0.589780, acc: 0.648438] [adversarial loss: 1.870498, acc: 0.046875]\n",
      "2961: [discriminator loss: 0.432734, acc: 0.804688] [adversarial loss: 1.302472, acc: 0.281250]\n",
      "2962: [discriminator loss: 0.478022, acc: 0.804688] [adversarial loss: 1.208969, acc: 0.203125]\n",
      "2963: [discriminator loss: 0.444073, acc: 0.781250] [adversarial loss: 1.386686, acc: 0.187500]\n",
      "2964: [discriminator loss: 0.538989, acc: 0.710938] [adversarial loss: 1.297860, acc: 0.203125]\n",
      "2965: [discriminator loss: 0.528727, acc: 0.757812] [adversarial loss: 1.279760, acc: 0.250000]\n",
      "2966: [discriminator loss: 0.488344, acc: 0.750000] [adversarial loss: 1.625061, acc: 0.140625]\n",
      "2967: [discriminator loss: 0.446533, acc: 0.765625] [adversarial loss: 1.405689, acc: 0.171875]\n",
      "2968: [discriminator loss: 0.524682, acc: 0.750000] [adversarial loss: 1.339567, acc: 0.312500]\n",
      "2969: [discriminator loss: 0.474338, acc: 0.765625] [adversarial loss: 1.406373, acc: 0.171875]\n",
      "2970: [discriminator loss: 0.470867, acc: 0.796875] [adversarial loss: 1.469791, acc: 0.171875]\n",
      "2971: [discriminator loss: 0.527210, acc: 0.757812] [adversarial loss: 1.519114, acc: 0.125000]\n",
      "2972: [discriminator loss: 0.481296, acc: 0.812500] [adversarial loss: 1.834902, acc: 0.125000]\n",
      "2973: [discriminator loss: 0.545663, acc: 0.734375] [adversarial loss: 0.852398, acc: 0.515625]\n",
      "2974: [discriminator loss: 0.528390, acc: 0.742188] [adversarial loss: 1.852582, acc: 0.125000]\n",
      "2975: [discriminator loss: 0.507313, acc: 0.789062] [adversarial loss: 0.859268, acc: 0.406250]\n",
      "2976: [discriminator loss: 0.518208, acc: 0.726562] [adversarial loss: 1.830376, acc: 0.093750]\n",
      "2977: [discriminator loss: 0.628692, acc: 0.703125] [adversarial loss: 0.752749, acc: 0.484375]\n",
      "2978: [discriminator loss: 0.553456, acc: 0.742188] [adversarial loss: 1.812271, acc: 0.062500]\n",
      "2979: [discriminator loss: 0.524361, acc: 0.718750] [adversarial loss: 1.299257, acc: 0.250000]\n",
      "2980: [discriminator loss: 0.464288, acc: 0.804688] [adversarial loss: 1.994429, acc: 0.140625]\n",
      "2981: [discriminator loss: 0.600457, acc: 0.750000] [adversarial loss: 0.931667, acc: 0.296875]\n",
      "2982: [discriminator loss: 0.500898, acc: 0.710938] [adversarial loss: 1.743712, acc: 0.031250]\n",
      "2983: [discriminator loss: 0.484008, acc: 0.757812] [adversarial loss: 1.326807, acc: 0.250000]\n",
      "2984: [discriminator loss: 0.468205, acc: 0.796875] [adversarial loss: 1.450110, acc: 0.218750]\n",
      "2985: [discriminator loss: 0.480038, acc: 0.781250] [adversarial loss: 1.129341, acc: 0.343750]\n",
      "2986: [discriminator loss: 0.571165, acc: 0.742188] [adversarial loss: 1.892899, acc: 0.093750]\n",
      "2987: [discriminator loss: 0.517233, acc: 0.734375] [adversarial loss: 1.133468, acc: 0.312500]\n",
      "2988: [discriminator loss: 0.568543, acc: 0.703125] [adversarial loss: 1.950173, acc: 0.093750]\n",
      "2989: [discriminator loss: 0.492419, acc: 0.765625] [adversarial loss: 0.887064, acc: 0.484375]\n",
      "2990: [discriminator loss: 0.488338, acc: 0.789062] [adversarial loss: 2.017632, acc: 0.140625]\n",
      "2991: [discriminator loss: 0.597243, acc: 0.656250] [adversarial loss: 1.296879, acc: 0.171875]\n",
      "2992: [discriminator loss: 0.472410, acc: 0.742188] [adversarial loss: 1.414201, acc: 0.203125]\n",
      "2993: [discriminator loss: 0.592763, acc: 0.718750] [adversarial loss: 1.103562, acc: 0.359375]\n",
      "2994: [discriminator loss: 0.492051, acc: 0.781250] [adversarial loss: 1.581307, acc: 0.171875]\n",
      "2995: [discriminator loss: 0.497305, acc: 0.789062] [adversarial loss: 1.372466, acc: 0.250000]\n",
      "2996: [discriminator loss: 0.448943, acc: 0.773438] [adversarial loss: 1.387422, acc: 0.187500]\n",
      "2997: [discriminator loss: 0.519373, acc: 0.750000] [adversarial loss: 1.419119, acc: 0.156250]\n",
      "2998: [discriminator loss: 0.396527, acc: 0.820312] [adversarial loss: 1.028021, acc: 0.296875]\n",
      "2999: [discriminator loss: 0.394192, acc: 0.859375] [adversarial loss: 1.509311, acc: 0.187500]\n",
      "3000: [discriminator loss: 0.602079, acc: 0.710938] [adversarial loss: 1.910406, acc: 0.140625]\n",
      "3001: [discriminator loss: 0.534555, acc: 0.750000] [adversarial loss: 1.077258, acc: 0.343750]\n",
      "3002: [discriminator loss: 0.528099, acc: 0.734375] [adversarial loss: 1.839163, acc: 0.125000]\n",
      "3003: [discriminator loss: 0.511213, acc: 0.765625] [adversarial loss: 1.157877, acc: 0.312500]\n",
      "3004: [discriminator loss: 0.424464, acc: 0.796875] [adversarial loss: 1.396622, acc: 0.140625]\n",
      "3005: [discriminator loss: 0.456624, acc: 0.796875] [adversarial loss: 1.274181, acc: 0.187500]\n",
      "3006: [discriminator loss: 0.447570, acc: 0.789062] [adversarial loss: 1.433622, acc: 0.156250]\n",
      "3007: [discriminator loss: 0.473570, acc: 0.750000] [adversarial loss: 1.383186, acc: 0.187500]\n",
      "3008: [discriminator loss: 0.504982, acc: 0.750000] [adversarial loss: 1.583955, acc: 0.203125]\n",
      "3009: [discriminator loss: 0.526066, acc: 0.757812] [adversarial loss: 1.137460, acc: 0.250000]\n",
      "3010: [discriminator loss: 0.434628, acc: 0.828125] [adversarial loss: 1.833598, acc: 0.062500]\n",
      "3011: [discriminator loss: 0.423889, acc: 0.781250] [adversarial loss: 0.965139, acc: 0.406250]\n",
      "3012: [discriminator loss: 0.540378, acc: 0.757812] [adversarial loss: 1.917007, acc: 0.031250]\n",
      "3013: [discriminator loss: 0.600528, acc: 0.710938] [adversarial loss: 0.859288, acc: 0.531250]\n",
      "3014: [discriminator loss: 0.641141, acc: 0.648438] [adversarial loss: 2.014200, acc: 0.078125]\n",
      "3015: [discriminator loss: 0.580260, acc: 0.710938] [adversarial loss: 1.067677, acc: 0.437500]\n",
      "3016: [discriminator loss: 0.470607, acc: 0.789062] [adversarial loss: 1.766848, acc: 0.109375]\n",
      "3017: [discriminator loss: 0.503498, acc: 0.757812] [adversarial loss: 1.150692, acc: 0.203125]\n",
      "3018: [discriminator loss: 0.463276, acc: 0.781250] [adversarial loss: 1.704539, acc: 0.156250]\n",
      "3019: [discriminator loss: 0.517345, acc: 0.781250] [adversarial loss: 1.181114, acc: 0.281250]\n",
      "3020: [discriminator loss: 0.426833, acc: 0.757812] [adversarial loss: 1.152624, acc: 0.312500]\n",
      "3021: [discriminator loss: 0.507936, acc: 0.718750] [adversarial loss: 1.526776, acc: 0.140625]\n",
      "3022: [discriminator loss: 0.509467, acc: 0.781250] [adversarial loss: 0.998450, acc: 0.359375]\n",
      "3023: [discriminator loss: 0.499929, acc: 0.765625] [adversarial loss: 1.805062, acc: 0.109375]\n",
      "3024: [discriminator loss: 0.457745, acc: 0.773438] [adversarial loss: 1.177542, acc: 0.281250]\n",
      "3025: [discriminator loss: 0.431135, acc: 0.757812] [adversarial loss: 1.552578, acc: 0.187500]\n",
      "3026: [discriminator loss: 0.472929, acc: 0.812500] [adversarial loss: 1.413144, acc: 0.234375]\n",
      "3027: [discriminator loss: 0.445946, acc: 0.789062] [adversarial loss: 1.463193, acc: 0.171875]\n",
      "3028: [discriminator loss: 0.465502, acc: 0.789062] [adversarial loss: 1.498724, acc: 0.171875]\n",
      "3029: [discriminator loss: 0.492818, acc: 0.789062] [adversarial loss: 1.641676, acc: 0.156250]\n",
      "3030: [discriminator loss: 0.494376, acc: 0.765625] [adversarial loss: 1.060536, acc: 0.296875]\n",
      "3031: [discriminator loss: 0.635531, acc: 0.703125] [adversarial loss: 1.840662, acc: 0.078125]\n",
      "3032: [discriminator loss: 0.523686, acc: 0.726562] [adversarial loss: 0.858122, acc: 0.375000]\n",
      "3033: [discriminator loss: 0.587000, acc: 0.664062] [adversarial loss: 2.208692, acc: 0.062500]\n",
      "3034: [discriminator loss: 0.517285, acc: 0.726562] [adversarial loss: 0.963195, acc: 0.265625]\n",
      "3035: [discriminator loss: 0.500417, acc: 0.765625] [adversarial loss: 1.811431, acc: 0.125000]\n",
      "3036: [discriminator loss: 0.482133, acc: 0.726562] [adversarial loss: 1.231109, acc: 0.156250]\n",
      "3037: [discriminator loss: 0.457179, acc: 0.812500] [adversarial loss: 1.451908, acc: 0.125000]\n",
      "3038: [discriminator loss: 0.518447, acc: 0.750000] [adversarial loss: 1.292911, acc: 0.250000]\n",
      "3039: [discriminator loss: 0.558232, acc: 0.664062] [adversarial loss: 1.203156, acc: 0.328125]\n",
      "3040: [discriminator loss: 0.456549, acc: 0.796875] [adversarial loss: 1.521305, acc: 0.187500]\n",
      "3041: [discriminator loss: 0.442881, acc: 0.765625] [adversarial loss: 1.592879, acc: 0.125000]\n",
      "3042: [discriminator loss: 0.496104, acc: 0.734375] [adversarial loss: 1.353069, acc: 0.171875]\n",
      "3043: [discriminator loss: 0.499347, acc: 0.781250] [adversarial loss: 1.290074, acc: 0.218750]\n",
      "3044: [discriminator loss: 0.468105, acc: 0.796875] [adversarial loss: 1.461531, acc: 0.156250]\n",
      "3045: [discriminator loss: 0.405853, acc: 0.851562] [adversarial loss: 1.086078, acc: 0.328125]\n",
      "3046: [discriminator loss: 0.476275, acc: 0.796875] [adversarial loss: 1.679636, acc: 0.171875]\n",
      "3047: [discriminator loss: 0.477758, acc: 0.750000] [adversarial loss: 1.863627, acc: 0.093750]\n",
      "3048: [discriminator loss: 0.527216, acc: 0.710938] [adversarial loss: 1.313309, acc: 0.281250]\n",
      "3049: [discriminator loss: 0.457527, acc: 0.796875] [adversarial loss: 1.730678, acc: 0.078125]\n",
      "3050: [discriminator loss: 0.536757, acc: 0.750000] [adversarial loss: 1.120628, acc: 0.359375]\n",
      "3051: [discriminator loss: 0.481893, acc: 0.773438] [adversarial loss: 1.839046, acc: 0.187500]\n",
      "3052: [discriminator loss: 0.430610, acc: 0.789062] [adversarial loss: 1.191211, acc: 0.296875]\n",
      "3053: [discriminator loss: 0.409780, acc: 0.835938] [adversarial loss: 1.895618, acc: 0.109375]\n",
      "3054: [discriminator loss: 0.504793, acc: 0.757812] [adversarial loss: 1.148167, acc: 0.312500]\n",
      "3055: [discriminator loss: 0.505740, acc: 0.757812] [adversarial loss: 1.926156, acc: 0.156250]\n",
      "3056: [discriminator loss: 0.628441, acc: 0.664062] [adversarial loss: 0.859121, acc: 0.421875]\n",
      "3057: [discriminator loss: 0.583990, acc: 0.671875] [adversarial loss: 1.848567, acc: 0.093750]\n",
      "3058: [discriminator loss: 0.558340, acc: 0.679688] [adversarial loss: 1.101718, acc: 0.250000]\n",
      "3059: [discriminator loss: 0.533555, acc: 0.765625] [adversarial loss: 1.946664, acc: 0.062500]\n",
      "3060: [discriminator loss: 0.505900, acc: 0.757812] [adversarial loss: 1.181557, acc: 0.328125]\n",
      "3061: [discriminator loss: 0.494245, acc: 0.742188] [adversarial loss: 1.687506, acc: 0.171875]\n",
      "3062: [discriminator loss: 0.483492, acc: 0.781250] [adversarial loss: 1.244891, acc: 0.218750]\n",
      "3063: [discriminator loss: 0.508878, acc: 0.781250] [adversarial loss: 1.620186, acc: 0.062500]\n",
      "3064: [discriminator loss: 0.517511, acc: 0.734375] [adversarial loss: 1.568989, acc: 0.187500]\n",
      "3065: [discriminator loss: 0.481108, acc: 0.718750] [adversarial loss: 1.475584, acc: 0.140625]\n",
      "3066: [discriminator loss: 0.455392, acc: 0.812500] [adversarial loss: 1.209004, acc: 0.296875]\n",
      "3067: [discriminator loss: 0.489900, acc: 0.757812] [adversarial loss: 1.663360, acc: 0.093750]\n",
      "3068: [discriminator loss: 0.531173, acc: 0.718750] [adversarial loss: 0.867802, acc: 0.375000]\n",
      "3069: [discriminator loss: 0.509284, acc: 0.718750] [adversarial loss: 1.686436, acc: 0.109375]\n",
      "3070: [discriminator loss: 0.472207, acc: 0.773438] [adversarial loss: 1.045244, acc: 0.343750]\n",
      "3071: [discriminator loss: 0.478997, acc: 0.742188] [adversarial loss: 1.252168, acc: 0.296875]\n",
      "3072: [discriminator loss: 0.510524, acc: 0.726562] [adversarial loss: 1.528341, acc: 0.109375]\n",
      "3073: [discriminator loss: 0.445518, acc: 0.781250] [adversarial loss: 1.305993, acc: 0.265625]\n",
      "3074: [discriminator loss: 0.474614, acc: 0.726562] [adversarial loss: 1.455564, acc: 0.171875]\n",
      "3075: [discriminator loss: 0.498764, acc: 0.773438] [adversarial loss: 1.208520, acc: 0.250000]\n",
      "3076: [discriminator loss: 0.537094, acc: 0.750000] [adversarial loss: 1.781020, acc: 0.109375]\n",
      "3077: [discriminator loss: 0.435243, acc: 0.796875] [adversarial loss: 1.359375, acc: 0.265625]\n",
      "3078: [discriminator loss: 0.467665, acc: 0.812500] [adversarial loss: 1.326977, acc: 0.234375]\n",
      "3079: [discriminator loss: 0.463755, acc: 0.812500] [adversarial loss: 1.262349, acc: 0.187500]\n",
      "3080: [discriminator loss: 0.459003, acc: 0.765625] [adversarial loss: 1.305400, acc: 0.265625]\n",
      "3081: [discriminator loss: 0.544982, acc: 0.703125] [adversarial loss: 1.393473, acc: 0.265625]\n",
      "3082: [discriminator loss: 0.523321, acc: 0.718750] [adversarial loss: 1.401520, acc: 0.203125]\n",
      "3083: [discriminator loss: 0.464456, acc: 0.812500] [adversarial loss: 1.606806, acc: 0.125000]\n",
      "3084: [discriminator loss: 0.473757, acc: 0.796875] [adversarial loss: 0.939605, acc: 0.421875]\n",
      "3085: [discriminator loss: 0.485383, acc: 0.765625] [adversarial loss: 1.634070, acc: 0.109375]\n",
      "3086: [discriminator loss: 0.392022, acc: 0.851562] [adversarial loss: 1.041087, acc: 0.281250]\n",
      "3087: [discriminator loss: 0.476427, acc: 0.726562] [adversarial loss: 2.463130, acc: 0.031250]\n",
      "3088: [discriminator loss: 0.756842, acc: 0.640625] [adversarial loss: 0.862586, acc: 0.406250]\n",
      "3089: [discriminator loss: 0.595804, acc: 0.710938] [adversarial loss: 2.019933, acc: 0.031250]\n",
      "3090: [discriminator loss: 0.467051, acc: 0.750000] [adversarial loss: 1.299446, acc: 0.171875]\n",
      "3091: [discriminator loss: 0.503143, acc: 0.757812] [adversarial loss: 1.345293, acc: 0.265625]\n",
      "3092: [discriminator loss: 0.557362, acc: 0.726562] [adversarial loss: 1.024744, acc: 0.359375]\n",
      "3093: [discriminator loss: 0.515284, acc: 0.734375] [adversarial loss: 1.523260, acc: 0.140625]\n",
      "3094: [discriminator loss: 0.489386, acc: 0.765625] [adversarial loss: 1.078641, acc: 0.343750]\n",
      "3095: [discriminator loss: 0.481286, acc: 0.812500] [adversarial loss: 1.215738, acc: 0.281250]\n",
      "3096: [discriminator loss: 0.578489, acc: 0.687500] [adversarial loss: 1.462148, acc: 0.171875]\n",
      "3097: [discriminator loss: 0.501551, acc: 0.750000] [adversarial loss: 1.049138, acc: 0.421875]\n",
      "3098: [discriminator loss: 0.419752, acc: 0.820312] [adversarial loss: 1.933817, acc: 0.062500]\n",
      "3099: [discriminator loss: 0.527981, acc: 0.734375] [adversarial loss: 0.983933, acc: 0.359375]\n",
      "3100: [discriminator loss: 0.612531, acc: 0.726562] [adversarial loss: 1.961717, acc: 0.078125]\n",
      "3101: [discriminator loss: 0.516260, acc: 0.710938] [adversarial loss: 1.209015, acc: 0.218750]\n",
      "3102: [discriminator loss: 0.596159, acc: 0.679688] [adversarial loss: 1.758793, acc: 0.109375]\n",
      "3103: [discriminator loss: 0.526006, acc: 0.695312] [adversarial loss: 1.009292, acc: 0.375000]\n",
      "3104: [discriminator loss: 0.495262, acc: 0.750000] [adversarial loss: 1.819344, acc: 0.140625]\n",
      "3105: [discriminator loss: 0.481898, acc: 0.757812] [adversarial loss: 1.188359, acc: 0.281250]\n",
      "3106: [discriminator loss: 0.413083, acc: 0.843750] [adversarial loss: 1.740755, acc: 0.062500]\n",
      "3107: [discriminator loss: 0.513337, acc: 0.765625] [adversarial loss: 1.101931, acc: 0.281250]\n",
      "3108: [discriminator loss: 0.547558, acc: 0.765625] [adversarial loss: 2.099295, acc: 0.000000]\n",
      "3109: [discriminator loss: 0.508017, acc: 0.750000] [adversarial loss: 1.068012, acc: 0.328125]\n",
      "3110: [discriminator loss: 0.491415, acc: 0.773438] [adversarial loss: 1.662669, acc: 0.125000]\n",
      "3111: [discriminator loss: 0.476750, acc: 0.757812] [adversarial loss: 1.437155, acc: 0.218750]\n",
      "3112: [discriminator loss: 0.483932, acc: 0.750000] [adversarial loss: 1.213333, acc: 0.250000]\n",
      "3113: [discriminator loss: 0.512505, acc: 0.718750] [adversarial loss: 1.647011, acc: 0.125000]\n",
      "3114: [discriminator loss: 0.505210, acc: 0.734375] [adversarial loss: 1.085171, acc: 0.296875]\n",
      "3115: [discriminator loss: 0.457491, acc: 0.789062] [adversarial loss: 1.544889, acc: 0.109375]\n",
      "3116: [discriminator loss: 0.524286, acc: 0.773438] [adversarial loss: 1.357620, acc: 0.218750]\n",
      "3117: [discriminator loss: 0.407020, acc: 0.835938] [adversarial loss: 1.426820, acc: 0.171875]\n",
      "3118: [discriminator loss: 0.413028, acc: 0.835938] [adversarial loss: 1.305470, acc: 0.265625]\n",
      "3119: [discriminator loss: 0.467068, acc: 0.781250] [adversarial loss: 1.428226, acc: 0.171875]\n",
      "3120: [discriminator loss: 0.424527, acc: 0.789062] [adversarial loss: 1.538462, acc: 0.156250]\n",
      "3121: [discriminator loss: 0.469925, acc: 0.804688] [adversarial loss: 1.397540, acc: 0.265625]\n",
      "3122: [discriminator loss: 0.435566, acc: 0.765625] [adversarial loss: 1.631531, acc: 0.109375]\n",
      "3123: [discriminator loss: 0.545814, acc: 0.742188] [adversarial loss: 1.156471, acc: 0.234375]\n",
      "3124: [discriminator loss: 0.640625, acc: 0.671875] [adversarial loss: 1.745862, acc: 0.078125]\n",
      "3125: [discriminator loss: 0.485909, acc: 0.765625] [adversarial loss: 0.729270, acc: 0.562500]\n",
      "3126: [discriminator loss: 0.519210, acc: 0.710938] [adversarial loss: 1.628703, acc: 0.156250]\n",
      "3127: [discriminator loss: 0.534384, acc: 0.765625] [adversarial loss: 0.951990, acc: 0.390625]\n",
      "3128: [discriminator loss: 0.678460, acc: 0.632812] [adversarial loss: 2.077055, acc: 0.062500]\n",
      "3129: [discriminator loss: 0.562674, acc: 0.734375] [adversarial loss: 1.056078, acc: 0.390625]\n",
      "3130: [discriminator loss: 0.438455, acc: 0.773438] [adversarial loss: 1.689217, acc: 0.140625]\n",
      "3131: [discriminator loss: 0.605768, acc: 0.718750] [adversarial loss: 0.924239, acc: 0.406250]\n",
      "3132: [discriminator loss: 0.514246, acc: 0.765625] [adversarial loss: 1.640196, acc: 0.062500]\n",
      "3133: [discriminator loss: 0.440807, acc: 0.781250] [adversarial loss: 1.211720, acc: 0.187500]\n",
      "3134: [discriminator loss: 0.405753, acc: 0.804688] [adversarial loss: 1.360014, acc: 0.125000]\n",
      "3135: [discriminator loss: 0.517828, acc: 0.687500] [adversarial loss: 1.020769, acc: 0.406250]\n",
      "3136: [discriminator loss: 0.456868, acc: 0.812500] [adversarial loss: 1.482691, acc: 0.140625]\n",
      "3137: [discriminator loss: 0.574645, acc: 0.687500] [adversarial loss: 1.210499, acc: 0.203125]\n",
      "3138: [discriminator loss: 0.501714, acc: 0.718750] [adversarial loss: 1.863478, acc: 0.078125]\n",
      "3139: [discriminator loss: 0.501259, acc: 0.750000] [adversarial loss: 0.947291, acc: 0.390625]\n",
      "3140: [discriminator loss: 0.504419, acc: 0.710938] [adversarial loss: 1.707778, acc: 0.093750]\n",
      "3141: [discriminator loss: 0.447227, acc: 0.796875] [adversarial loss: 1.261658, acc: 0.359375]\n",
      "3142: [discriminator loss: 0.579022, acc: 0.734375] [adversarial loss: 1.287333, acc: 0.203125]\n",
      "3143: [discriminator loss: 0.491712, acc: 0.804688] [adversarial loss: 1.423848, acc: 0.250000]\n",
      "3144: [discriminator loss: 0.490147, acc: 0.789062] [adversarial loss: 0.925647, acc: 0.437500]\n",
      "3145: [discriminator loss: 0.459740, acc: 0.742188] [adversarial loss: 1.816423, acc: 0.078125]\n",
      "3146: [discriminator loss: 0.516315, acc: 0.726562] [adversarial loss: 1.219701, acc: 0.265625]\n",
      "3147: [discriminator loss: 0.439534, acc: 0.804688] [adversarial loss: 1.267662, acc: 0.296875]\n",
      "3148: [discriminator loss: 0.490729, acc: 0.773438] [adversarial loss: 1.787704, acc: 0.125000]\n",
      "3149: [discriminator loss: 0.504919, acc: 0.718750] [adversarial loss: 1.034104, acc: 0.390625]\n",
      "3150: [discriminator loss: 0.526137, acc: 0.726562] [adversarial loss: 2.153072, acc: 0.015625]\n",
      "3151: [discriminator loss: 0.508472, acc: 0.726562] [adversarial loss: 1.056581, acc: 0.343750]\n",
      "3152: [discriminator loss: 0.465589, acc: 0.789062] [adversarial loss: 1.668758, acc: 0.140625]\n",
      "3153: [discriminator loss: 0.470419, acc: 0.742188] [adversarial loss: 1.188146, acc: 0.296875]\n",
      "3154: [discriminator loss: 0.510632, acc: 0.750000] [adversarial loss: 1.435939, acc: 0.203125]\n",
      "3155: [discriminator loss: 0.523203, acc: 0.679688] [adversarial loss: 0.845169, acc: 0.453125]\n",
      "3156: [discriminator loss: 0.573163, acc: 0.656250] [adversarial loss: 2.122240, acc: 0.093750]\n",
      "3157: [discriminator loss: 0.467203, acc: 0.804688] [adversarial loss: 1.021845, acc: 0.359375]\n",
      "3158: [discriminator loss: 0.537219, acc: 0.726562] [adversarial loss: 1.600844, acc: 0.156250]\n",
      "3159: [discriminator loss: 0.490040, acc: 0.773438] [adversarial loss: 0.997686, acc: 0.328125]\n",
      "3160: [discriminator loss: 0.465250, acc: 0.773438] [adversarial loss: 1.493453, acc: 0.156250]\n",
      "3161: [discriminator loss: 0.477030, acc: 0.781250] [adversarial loss: 1.153357, acc: 0.250000]\n",
      "3162: [discriminator loss: 0.520050, acc: 0.750000] [adversarial loss: 1.597908, acc: 0.109375]\n",
      "3163: [discriminator loss: 0.496429, acc: 0.734375] [adversarial loss: 1.203246, acc: 0.218750]\n",
      "3164: [discriminator loss: 0.471449, acc: 0.765625] [adversarial loss: 1.889054, acc: 0.125000]\n",
      "3165: [discriminator loss: 0.526895, acc: 0.718750] [adversarial loss: 1.016483, acc: 0.265625]\n",
      "3166: [discriminator loss: 0.485458, acc: 0.750000] [adversarial loss: 1.820310, acc: 0.078125]\n",
      "3167: [discriminator loss: 0.456700, acc: 0.804688] [adversarial loss: 1.076293, acc: 0.281250]\n",
      "3168: [discriminator loss: 0.487347, acc: 0.765625] [adversarial loss: 1.756955, acc: 0.031250]\n",
      "3169: [discriminator loss: 0.428929, acc: 0.796875] [adversarial loss: 1.132748, acc: 0.281250]\n",
      "3170: [discriminator loss: 0.465284, acc: 0.765625] [adversarial loss: 1.658175, acc: 0.093750]\n",
      "3171: [discriminator loss: 0.547608, acc: 0.726562] [adversarial loss: 1.072902, acc: 0.296875]\n",
      "3172: [discriminator loss: 0.544292, acc: 0.695312] [adversarial loss: 1.839042, acc: 0.078125]\n",
      "3173: [discriminator loss: 0.582450, acc: 0.679688] [adversarial loss: 1.047593, acc: 0.328125]\n",
      "3174: [discriminator loss: 0.477871, acc: 0.804688] [adversarial loss: 1.270277, acc: 0.265625]\n",
      "3175: [discriminator loss: 0.465664, acc: 0.781250] [adversarial loss: 1.371481, acc: 0.171875]\n",
      "3176: [discriminator loss: 0.530536, acc: 0.726562] [adversarial loss: 1.455962, acc: 0.109375]\n",
      "3177: [discriminator loss: 0.489519, acc: 0.757812] [adversarial loss: 1.395513, acc: 0.234375]\n",
      "3178: [discriminator loss: 0.472823, acc: 0.773438] [adversarial loss: 1.376514, acc: 0.125000]\n",
      "3179: [discriminator loss: 0.449792, acc: 0.820312] [adversarial loss: 0.992892, acc: 0.343750]\n",
      "3180: [discriminator loss: 0.548057, acc: 0.710938] [adversarial loss: 1.952907, acc: 0.046875]\n",
      "3181: [discriminator loss: 0.692007, acc: 0.648438] [adversarial loss: 1.008889, acc: 0.406250]\n",
      "3182: [discriminator loss: 0.570072, acc: 0.734375] [adversarial loss: 1.792306, acc: 0.187500]\n",
      "3183: [discriminator loss: 0.617250, acc: 0.695312] [adversarial loss: 1.018449, acc: 0.375000]\n",
      "3184: [discriminator loss: 0.504190, acc: 0.796875] [adversarial loss: 1.681548, acc: 0.140625]\n",
      "3185: [discriminator loss: 0.459045, acc: 0.789062] [adversarial loss: 1.457895, acc: 0.093750]\n",
      "3186: [discriminator loss: 0.426207, acc: 0.781250] [adversarial loss: 1.362695, acc: 0.171875]\n",
      "3187: [discriminator loss: 0.460899, acc: 0.789062] [adversarial loss: 1.428890, acc: 0.234375]\n",
      "3188: [discriminator loss: 0.543102, acc: 0.703125] [adversarial loss: 1.529887, acc: 0.203125]\n",
      "3189: [discriminator loss: 0.441837, acc: 0.773438] [adversarial loss: 1.718229, acc: 0.093750]\n",
      "3190: [discriminator loss: 0.492175, acc: 0.734375] [adversarial loss: 1.251172, acc: 0.203125]\n",
      "3191: [discriminator loss: 0.482105, acc: 0.765625] [adversarial loss: 1.542802, acc: 0.187500]\n",
      "3192: [discriminator loss: 0.430669, acc: 0.789062] [adversarial loss: 1.206237, acc: 0.312500]\n",
      "3193: [discriminator loss: 0.501519, acc: 0.789062] [adversarial loss: 1.840655, acc: 0.093750]\n",
      "3194: [discriminator loss: 0.574731, acc: 0.664062] [adversarial loss: 0.806739, acc: 0.437500]\n",
      "3195: [discriminator loss: 0.475173, acc: 0.773438] [adversarial loss: 1.239490, acc: 0.328125]\n",
      "3196: [discriminator loss: 0.453017, acc: 0.804688] [adversarial loss: 1.355601, acc: 0.250000]\n",
      "3197: [discriminator loss: 0.473462, acc: 0.820312] [adversarial loss: 1.500208, acc: 0.140625]\n",
      "3198: [discriminator loss: 0.439962, acc: 0.796875] [adversarial loss: 1.639080, acc: 0.156250]\n",
      "3199: [discriminator loss: 0.523955, acc: 0.765625] [adversarial loss: 1.047626, acc: 0.343750]\n",
      "3200: [discriminator loss: 0.405475, acc: 0.835938] [adversarial loss: 1.537669, acc: 0.125000]\n",
      "3201: [discriminator loss: 0.492530, acc: 0.757812] [adversarial loss: 1.171137, acc: 0.312500]\n",
      "3202: [discriminator loss: 0.518164, acc: 0.742188] [adversarial loss: 1.137304, acc: 0.203125]\n",
      "3203: [discriminator loss: 0.502131, acc: 0.812500] [adversarial loss: 1.675409, acc: 0.109375]\n",
      "3204: [discriminator loss: 0.525286, acc: 0.710938] [adversarial loss: 1.242998, acc: 0.296875]\n",
      "3205: [discriminator loss: 0.434122, acc: 0.812500] [adversarial loss: 1.498035, acc: 0.171875]\n",
      "3206: [discriminator loss: 0.429894, acc: 0.796875] [adversarial loss: 1.234464, acc: 0.359375]\n",
      "3207: [discriminator loss: 0.523031, acc: 0.750000] [adversarial loss: 1.474625, acc: 0.156250]\n",
      "3208: [discriminator loss: 0.544202, acc: 0.750000] [adversarial loss: 1.097866, acc: 0.281250]\n",
      "3209: [discriminator loss: 0.536389, acc: 0.734375] [adversarial loss: 1.594575, acc: 0.093750]\n",
      "3210: [discriminator loss: 0.500002, acc: 0.757812] [adversarial loss: 0.990700, acc: 0.265625]\n",
      "3211: [discriminator loss: 0.445783, acc: 0.804688] [adversarial loss: 2.149969, acc: 0.093750]\n",
      "3212: [discriminator loss: 0.458683, acc: 0.804688] [adversarial loss: 1.250163, acc: 0.203125]\n",
      "3213: [discriminator loss: 0.608289, acc: 0.695312] [adversarial loss: 1.927524, acc: 0.046875]\n",
      "3214: [discriminator loss: 0.585750, acc: 0.710938] [adversarial loss: 0.834187, acc: 0.468750]\n",
      "3215: [discriminator loss: 0.613740, acc: 0.640625] [adversarial loss: 1.853921, acc: 0.078125]\n",
      "3216: [discriminator loss: 0.506815, acc: 0.742188] [adversarial loss: 1.301904, acc: 0.250000]\n",
      "3217: [discriminator loss: 0.448547, acc: 0.789062] [adversarial loss: 1.964512, acc: 0.078125]\n",
      "3218: [discriminator loss: 0.566355, acc: 0.718750] [adversarial loss: 1.194178, acc: 0.328125]\n",
      "3219: [discriminator loss: 0.500260, acc: 0.750000] [adversarial loss: 1.533373, acc: 0.140625]\n",
      "3220: [discriminator loss: 0.384022, acc: 0.867188] [adversarial loss: 1.363026, acc: 0.156250]\n",
      "3221: [discriminator loss: 0.553108, acc: 0.718750] [adversarial loss: 1.142244, acc: 0.281250]\n",
      "3222: [discriminator loss: 0.469053, acc: 0.804688] [adversarial loss: 1.297026, acc: 0.156250]\n",
      "3223: [discriminator loss: 0.457799, acc: 0.757812] [adversarial loss: 1.045887, acc: 0.312500]\n",
      "3224: [discriminator loss: 0.522235, acc: 0.742188] [adversarial loss: 1.707772, acc: 0.078125]\n",
      "3225: [discriminator loss: 0.508036, acc: 0.695312] [adversarial loss: 0.907030, acc: 0.531250]\n",
      "3226: [discriminator loss: 0.588770, acc: 0.718750] [adversarial loss: 1.812675, acc: 0.015625]\n",
      "3227: [discriminator loss: 0.571859, acc: 0.718750] [adversarial loss: 1.160881, acc: 0.281250]\n",
      "3228: [discriminator loss: 0.448014, acc: 0.789062] [adversarial loss: 1.269195, acc: 0.203125]\n",
      "3229: [discriminator loss: 0.455031, acc: 0.789062] [adversarial loss: 1.626837, acc: 0.187500]\n",
      "3230: [discriminator loss: 0.574043, acc: 0.750000] [adversarial loss: 1.223132, acc: 0.281250]\n",
      "3231: [discriminator loss: 0.485914, acc: 0.765625] [adversarial loss: 1.301394, acc: 0.187500]\n",
      "3232: [discriminator loss: 0.463058, acc: 0.781250] [adversarial loss: 1.192754, acc: 0.281250]\n",
      "3233: [discriminator loss: 0.376806, acc: 0.867188] [adversarial loss: 1.743572, acc: 0.125000]\n",
      "3234: [discriminator loss: 0.605009, acc: 0.703125] [adversarial loss: 0.931664, acc: 0.453125]\n",
      "3235: [discriminator loss: 0.557710, acc: 0.671875] [adversarial loss: 1.849419, acc: 0.062500]\n",
      "3236: [discriminator loss: 0.599176, acc: 0.734375] [adversarial loss: 0.986349, acc: 0.375000]\n",
      "3237: [discriminator loss: 0.520000, acc: 0.750000] [adversarial loss: 1.741891, acc: 0.140625]\n",
      "3238: [discriminator loss: 0.492005, acc: 0.765625] [adversarial loss: 1.299649, acc: 0.203125]\n",
      "3239: [discriminator loss: 0.518596, acc: 0.773438] [adversarial loss: 1.524910, acc: 0.109375]\n",
      "3240: [discriminator loss: 0.454247, acc: 0.789062] [adversarial loss: 1.192229, acc: 0.156250]\n",
      "3241: [discriminator loss: 0.469456, acc: 0.789062] [adversarial loss: 1.340453, acc: 0.234375]\n",
      "3242: [discriminator loss: 0.439722, acc: 0.796875] [adversarial loss: 1.476762, acc: 0.062500]\n",
      "3243: [discriminator loss: 0.510508, acc: 0.757812] [adversarial loss: 1.010705, acc: 0.421875]\n",
      "3244: [discriminator loss: 0.473901, acc: 0.765625] [adversarial loss: 1.813621, acc: 0.046875]\n",
      "3245: [discriminator loss: 0.448849, acc: 0.781250] [adversarial loss: 1.109184, acc: 0.250000]\n",
      "3246: [discriminator loss: 0.514188, acc: 0.734375] [adversarial loss: 1.651630, acc: 0.062500]\n",
      "3247: [discriminator loss: 0.461968, acc: 0.796875] [adversarial loss: 1.189506, acc: 0.312500]\n",
      "3248: [discriminator loss: 0.452159, acc: 0.789062] [adversarial loss: 1.622294, acc: 0.171875]\n",
      "3249: [discriminator loss: 0.484154, acc: 0.781250] [adversarial loss: 1.122072, acc: 0.281250]\n",
      "3250: [discriminator loss: 0.454060, acc: 0.820312] [adversarial loss: 1.366170, acc: 0.218750]\n",
      "3251: [discriminator loss: 0.457069, acc: 0.789062] [adversarial loss: 1.541155, acc: 0.156250]\n",
      "3252: [discriminator loss: 0.476358, acc: 0.781250] [adversarial loss: 1.288768, acc: 0.171875]\n",
      "3253: [discriminator loss: 0.481316, acc: 0.757812] [adversarial loss: 1.155250, acc: 0.328125]\n",
      "3254: [discriminator loss: 0.465056, acc: 0.765625] [adversarial loss: 2.227407, acc: 0.046875]\n",
      "3255: [discriminator loss: 0.578494, acc: 0.726562] [adversarial loss: 0.985694, acc: 0.359375]\n",
      "3256: [discriminator loss: 0.478593, acc: 0.757812] [adversarial loss: 1.742003, acc: 0.109375]\n",
      "3257: [discriminator loss: 0.575328, acc: 0.734375] [adversarial loss: 0.923114, acc: 0.453125]\n",
      "3258: [discriminator loss: 0.555895, acc: 0.718750] [adversarial loss: 1.987907, acc: 0.062500]\n",
      "3259: [discriminator loss: 0.510605, acc: 0.718750] [adversarial loss: 1.063577, acc: 0.328125]\n",
      "3260: [discriminator loss: 0.629957, acc: 0.679688] [adversarial loss: 1.786175, acc: 0.078125]\n",
      "3261: [discriminator loss: 0.559926, acc: 0.726562] [adversarial loss: 1.201524, acc: 0.234375]\n",
      "3262: [discriminator loss: 0.551077, acc: 0.695312] [adversarial loss: 1.374822, acc: 0.203125]\n",
      "3263: [discriminator loss: 0.456011, acc: 0.765625] [adversarial loss: 1.087505, acc: 0.312500]\n",
      "3264: [discriminator loss: 0.482364, acc: 0.789062] [adversarial loss: 1.447808, acc: 0.156250]\n",
      "3265: [discriminator loss: 0.379637, acc: 0.867188] [adversarial loss: 1.147312, acc: 0.265625]\n",
      "3266: [discriminator loss: 0.472105, acc: 0.773438] [adversarial loss: 1.471285, acc: 0.156250]\n",
      "3267: [discriminator loss: 0.469551, acc: 0.726562] [adversarial loss: 1.459912, acc: 0.187500]\n",
      "3268: [discriminator loss: 0.523977, acc: 0.757812] [adversarial loss: 1.501477, acc: 0.156250]\n",
      "3269: [discriminator loss: 0.576978, acc: 0.695312] [adversarial loss: 1.002533, acc: 0.343750]\n",
      "3270: [discriminator loss: 0.515593, acc: 0.742188] [adversarial loss: 1.718346, acc: 0.140625]\n",
      "3271: [discriminator loss: 0.510387, acc: 0.726562] [adversarial loss: 1.038305, acc: 0.328125]\n",
      "3272: [discriminator loss: 0.550640, acc: 0.742188] [adversarial loss: 1.952134, acc: 0.062500]\n",
      "3273: [discriminator loss: 0.550063, acc: 0.710938] [adversarial loss: 1.105491, acc: 0.406250]\n",
      "3274: [discriminator loss: 0.497531, acc: 0.765625] [adversarial loss: 1.668664, acc: 0.078125]\n",
      "3275: [discriminator loss: 0.455914, acc: 0.773438] [adversarial loss: 1.342417, acc: 0.203125]\n",
      "3276: [discriminator loss: 0.513250, acc: 0.710938] [adversarial loss: 1.493485, acc: 0.093750]\n",
      "3277: [discriminator loss: 0.474418, acc: 0.765625] [adversarial loss: 1.116907, acc: 0.281250]\n",
      "3278: [discriminator loss: 0.480816, acc: 0.734375] [adversarial loss: 1.573294, acc: 0.093750]\n",
      "3279: [discriminator loss: 0.495577, acc: 0.742188] [adversarial loss: 1.122360, acc: 0.218750]\n",
      "3280: [discriminator loss: 0.478852, acc: 0.789062] [adversarial loss: 1.785292, acc: 0.078125]\n",
      "3281: [discriminator loss: 0.572214, acc: 0.718750] [adversarial loss: 1.404262, acc: 0.156250]\n",
      "3282: [discriminator loss: 0.463607, acc: 0.773438] [adversarial loss: 1.618034, acc: 0.125000]\n",
      "3283: [discriminator loss: 0.502312, acc: 0.750000] [adversarial loss: 1.264257, acc: 0.296875]\n",
      "3284: [discriminator loss: 0.492207, acc: 0.742188] [adversarial loss: 1.677450, acc: 0.078125]\n",
      "3285: [discriminator loss: 0.531030, acc: 0.742188] [adversarial loss: 1.204240, acc: 0.203125]\n",
      "3286: [discriminator loss: 0.560962, acc: 0.718750] [adversarial loss: 2.172669, acc: 0.031250]\n",
      "3287: [discriminator loss: 0.487853, acc: 0.757812] [adversarial loss: 1.156393, acc: 0.343750]\n",
      "3288: [discriminator loss: 0.553851, acc: 0.742188] [adversarial loss: 1.755786, acc: 0.078125]\n",
      "3289: [discriminator loss: 0.520904, acc: 0.726562] [adversarial loss: 1.137242, acc: 0.312500]\n",
      "3290: [discriminator loss: 0.516467, acc: 0.781250] [adversarial loss: 1.616140, acc: 0.109375]\n",
      "3291: [discriminator loss: 0.480507, acc: 0.789062] [adversarial loss: 1.082034, acc: 0.312500]\n",
      "3292: [discriminator loss: 0.528080, acc: 0.757812] [adversarial loss: 1.912099, acc: 0.109375]\n",
      "3293: [discriminator loss: 0.482110, acc: 0.750000] [adversarial loss: 1.319721, acc: 0.156250]\n",
      "3294: [discriminator loss: 0.468448, acc: 0.750000] [adversarial loss: 1.429037, acc: 0.171875]\n",
      "3295: [discriminator loss: 0.415549, acc: 0.812500] [adversarial loss: 1.325361, acc: 0.187500]\n",
      "3296: [discriminator loss: 0.461042, acc: 0.796875] [adversarial loss: 1.285442, acc: 0.234375]\n",
      "3297: [discriminator loss: 0.519420, acc: 0.789062] [adversarial loss: 1.747537, acc: 0.125000]\n",
      "3298: [discriminator loss: 0.519418, acc: 0.710938] [adversarial loss: 0.996949, acc: 0.375000]\n",
      "3299: [discriminator loss: 0.405376, acc: 0.812500] [adversarial loss: 1.683860, acc: 0.156250]\n",
      "3300: [discriminator loss: 0.438285, acc: 0.765625] [adversarial loss: 0.844586, acc: 0.468750]\n",
      "3301: [discriminator loss: 0.616075, acc: 0.640625] [adversarial loss: 2.064802, acc: 0.078125]\n",
      "3302: [discriminator loss: 0.560351, acc: 0.648438] [adversarial loss: 1.122127, acc: 0.328125]\n",
      "3303: [discriminator loss: 0.475964, acc: 0.757812] [adversarial loss: 1.278501, acc: 0.250000]\n",
      "3304: [discriminator loss: 0.614048, acc: 0.664062] [adversarial loss: 1.676853, acc: 0.078125]\n",
      "3305: [discriminator loss: 0.548637, acc: 0.734375] [adversarial loss: 1.292531, acc: 0.171875]\n",
      "3306: [discriminator loss: 0.472896, acc: 0.742188] [adversarial loss: 1.354275, acc: 0.203125]\n",
      "3307: [discriminator loss: 0.435120, acc: 0.812500] [adversarial loss: 1.634987, acc: 0.140625]\n",
      "3308: [discriminator loss: 0.447820, acc: 0.804688] [adversarial loss: 1.366253, acc: 0.218750]\n",
      "3309: [discriminator loss: 0.480254, acc: 0.804688] [adversarial loss: 1.231574, acc: 0.250000]\n",
      "3310: [discriminator loss: 0.520343, acc: 0.710938] [adversarial loss: 1.470784, acc: 0.187500]\n",
      "3311: [discriminator loss: 0.573598, acc: 0.695312] [adversarial loss: 1.052646, acc: 0.359375]\n",
      "3312: [discriminator loss: 0.452441, acc: 0.828125] [adversarial loss: 1.627117, acc: 0.140625]\n",
      "3313: [discriminator loss: 0.525743, acc: 0.765625] [adversarial loss: 1.490056, acc: 0.203125]\n",
      "3314: [discriminator loss: 0.425873, acc: 0.796875] [adversarial loss: 1.496593, acc: 0.140625]\n",
      "3315: [discriminator loss: 0.457812, acc: 0.789062] [adversarial loss: 1.427587, acc: 0.171875]\n",
      "3316: [discriminator loss: 0.478310, acc: 0.781250] [adversarial loss: 1.440628, acc: 0.171875]\n",
      "3317: [discriminator loss: 0.433602, acc: 0.796875] [adversarial loss: 1.298393, acc: 0.250000]\n",
      "3318: [discriminator loss: 0.453761, acc: 0.804688] [adversarial loss: 1.109957, acc: 0.312500]\n",
      "3319: [discriminator loss: 0.468369, acc: 0.789062] [adversarial loss: 1.850869, acc: 0.093750]\n",
      "3320: [discriminator loss: 0.618622, acc: 0.687500] [adversarial loss: 0.968911, acc: 0.406250]\n",
      "3321: [discriminator loss: 0.557294, acc: 0.718750] [adversarial loss: 2.172396, acc: 0.031250]\n",
      "3322: [discriminator loss: 0.647579, acc: 0.656250] [adversarial loss: 0.844194, acc: 0.515625]\n",
      "3323: [discriminator loss: 0.630292, acc: 0.671875] [adversarial loss: 1.850165, acc: 0.093750]\n",
      "3324: [discriminator loss: 0.516936, acc: 0.742188] [adversarial loss: 0.860768, acc: 0.421875]\n",
      "3325: [discriminator loss: 0.546498, acc: 0.734375] [adversarial loss: 1.824522, acc: 0.125000]\n",
      "3326: [discriminator loss: 0.531134, acc: 0.726562] [adversarial loss: 1.050614, acc: 0.359375]\n",
      "3327: [discriminator loss: 0.519261, acc: 0.750000] [adversarial loss: 1.762982, acc: 0.125000]\n",
      "3328: [discriminator loss: 0.454115, acc: 0.781250] [adversarial loss: 1.474838, acc: 0.203125]\n",
      "3329: [discriminator loss: 0.499937, acc: 0.804688] [adversarial loss: 1.323788, acc: 0.171875]\n",
      "3330: [discriminator loss: 0.456578, acc: 0.820312] [adversarial loss: 1.396396, acc: 0.218750]\n",
      "3331: [discriminator loss: 0.359961, acc: 0.875000] [adversarial loss: 1.551217, acc: 0.140625]\n",
      "3332: [discriminator loss: 0.453419, acc: 0.781250] [adversarial loss: 1.041029, acc: 0.296875]\n",
      "3333: [discriminator loss: 0.550959, acc: 0.773438] [adversarial loss: 1.354777, acc: 0.281250]\n",
      "3334: [discriminator loss: 0.519607, acc: 0.718750] [adversarial loss: 1.329053, acc: 0.265625]\n",
      "3335: [discriminator loss: 0.430116, acc: 0.804688] [adversarial loss: 1.474365, acc: 0.187500]\n",
      "3336: [discriminator loss: 0.468894, acc: 0.804688] [adversarial loss: 1.188129, acc: 0.296875]\n",
      "3337: [discriminator loss: 0.510531, acc: 0.742188] [adversarial loss: 1.207486, acc: 0.234375]\n",
      "3338: [discriminator loss: 0.516821, acc: 0.726562] [adversarial loss: 1.164743, acc: 0.328125]\n",
      "3339: [discriminator loss: 0.444284, acc: 0.781250] [adversarial loss: 1.457886, acc: 0.203125]\n",
      "3340: [discriminator loss: 0.433441, acc: 0.789062] [adversarial loss: 1.344807, acc: 0.281250]\n",
      "3341: [discriminator loss: 0.429481, acc: 0.781250] [adversarial loss: 1.384949, acc: 0.171875]\n",
      "3342: [discriminator loss: 0.532779, acc: 0.718750] [adversarial loss: 1.535252, acc: 0.203125]\n",
      "3343: [discriminator loss: 0.425007, acc: 0.820312] [adversarial loss: 1.346991, acc: 0.281250]\n",
      "3344: [discriminator loss: 0.521342, acc: 0.710938] [adversarial loss: 1.840281, acc: 0.046875]\n",
      "3345: [discriminator loss: 0.505132, acc: 0.726562] [adversarial loss: 0.834153, acc: 0.437500]\n",
      "3346: [discriminator loss: 0.482819, acc: 0.757812] [adversarial loss: 1.816554, acc: 0.046875]\n",
      "3347: [discriminator loss: 0.533393, acc: 0.710938] [adversarial loss: 0.813785, acc: 0.500000]\n",
      "3348: [discriminator loss: 0.457560, acc: 0.789062] [adversarial loss: 1.481187, acc: 0.250000]\n",
      "3349: [discriminator loss: 0.523516, acc: 0.726562] [adversarial loss: 1.190218, acc: 0.218750]\n",
      "3350: [discriminator loss: 0.443675, acc: 0.773438] [adversarial loss: 1.678006, acc: 0.078125]\n",
      "3351: [discriminator loss: 0.422989, acc: 0.789062] [adversarial loss: 1.380582, acc: 0.234375]\n",
      "3352: [discriminator loss: 0.550653, acc: 0.710938] [adversarial loss: 1.150976, acc: 0.328125]\n",
      "3353: [discriminator loss: 0.466582, acc: 0.750000] [adversarial loss: 1.640650, acc: 0.140625]\n",
      "3354: [discriminator loss: 0.539291, acc: 0.757812] [adversarial loss: 1.098805, acc: 0.312500]\n",
      "3355: [discriminator loss: 0.448690, acc: 0.812500] [adversarial loss: 1.581372, acc: 0.171875]\n",
      "3356: [discriminator loss: 0.421524, acc: 0.820312] [adversarial loss: 0.981659, acc: 0.421875]\n",
      "3357: [discriminator loss: 0.512174, acc: 0.726562] [adversarial loss: 2.254665, acc: 0.062500]\n",
      "3358: [discriminator loss: 0.584095, acc: 0.718750] [adversarial loss: 0.887241, acc: 0.468750]\n",
      "3359: [discriminator loss: 0.566255, acc: 0.718750] [adversarial loss: 1.905313, acc: 0.125000]\n",
      "3360: [discriminator loss: 0.559455, acc: 0.750000] [adversarial loss: 1.093855, acc: 0.234375]\n",
      "3361: [discriminator loss: 0.477624, acc: 0.742188] [adversarial loss: 1.624749, acc: 0.109375]\n",
      "3362: [discriminator loss: 0.499086, acc: 0.765625] [adversarial loss: 1.084637, acc: 0.359375]\n",
      "3363: [discriminator loss: 0.595864, acc: 0.687500] [adversarial loss: 1.954936, acc: 0.171875]\n",
      "3364: [discriminator loss: 0.503533, acc: 0.765625] [adversarial loss: 1.146353, acc: 0.343750]\n",
      "3365: [discriminator loss: 0.483167, acc: 0.765625] [adversarial loss: 1.759914, acc: 0.093750]\n",
      "3366: [discriminator loss: 0.525626, acc: 0.765625] [adversarial loss: 1.157221, acc: 0.250000]\n",
      "3367: [discriminator loss: 0.528198, acc: 0.687500] [adversarial loss: 1.783886, acc: 0.109375]\n",
      "3368: [discriminator loss: 0.619034, acc: 0.656250] [adversarial loss: 1.092427, acc: 0.328125]\n",
      "3369: [discriminator loss: 0.496527, acc: 0.757812] [adversarial loss: 1.402529, acc: 0.093750]\n",
      "3370: [discriminator loss: 0.437415, acc: 0.773438] [adversarial loss: 1.127888, acc: 0.343750]\n",
      "3371: [discriminator loss: 0.546816, acc: 0.695312] [adversarial loss: 1.288355, acc: 0.312500]\n",
      "3372: [discriminator loss: 0.570315, acc: 0.695312] [adversarial loss: 1.520906, acc: 0.250000]\n",
      "3373: [discriminator loss: 0.469044, acc: 0.757812] [adversarial loss: 1.461057, acc: 0.125000]\n",
      "3374: [discriminator loss: 0.507293, acc: 0.718750] [adversarial loss: 1.399280, acc: 0.140625]\n",
      "3375: [discriminator loss: 0.505775, acc: 0.750000] [adversarial loss: 1.763126, acc: 0.062500]\n",
      "3376: [discriminator loss: 0.462383, acc: 0.773438] [adversarial loss: 0.968418, acc: 0.375000]\n",
      "3377: [discriminator loss: 0.489311, acc: 0.781250] [adversarial loss: 1.508495, acc: 0.203125]\n",
      "3378: [discriminator loss: 0.414848, acc: 0.820312] [adversarial loss: 1.373445, acc: 0.187500]\n",
      "3379: [discriminator loss: 0.524117, acc: 0.757812] [adversarial loss: 1.144486, acc: 0.265625]\n",
      "3380: [discriminator loss: 0.464601, acc: 0.796875] [adversarial loss: 1.893567, acc: 0.093750]\n",
      "3381: [discriminator loss: 0.560017, acc: 0.710938] [adversarial loss: 0.753107, acc: 0.484375]\n",
      "3382: [discriminator loss: 0.661959, acc: 0.640625] [adversarial loss: 2.299484, acc: 0.046875]\n",
      "3383: [discriminator loss: 0.562480, acc: 0.703125] [adversarial loss: 0.930038, acc: 0.421875]\n",
      "3384: [discriminator loss: 0.577587, acc: 0.718750] [adversarial loss: 1.726268, acc: 0.171875]\n",
      "3385: [discriminator loss: 0.521838, acc: 0.781250] [adversarial loss: 1.204086, acc: 0.265625]\n",
      "3386: [discriminator loss: 0.414749, acc: 0.820312] [adversarial loss: 1.459257, acc: 0.171875]\n",
      "3387: [discriminator loss: 0.428921, acc: 0.828125] [adversarial loss: 1.741804, acc: 0.093750]\n",
      "3388: [discriminator loss: 0.480195, acc: 0.765625] [adversarial loss: 1.565795, acc: 0.125000]\n",
      "3389: [discriminator loss: 0.441768, acc: 0.750000] [adversarial loss: 1.470775, acc: 0.171875]\n",
      "3390: [discriminator loss: 0.516556, acc: 0.703125] [adversarial loss: 1.504708, acc: 0.156250]\n",
      "3391: [discriminator loss: 0.447386, acc: 0.796875] [adversarial loss: 1.276515, acc: 0.203125]\n",
      "3392: [discriminator loss: 0.470341, acc: 0.765625] [adversarial loss: 1.549008, acc: 0.187500]\n",
      "3393: [discriminator loss: 0.426427, acc: 0.820312] [adversarial loss: 1.251468, acc: 0.328125]\n",
      "3394: [discriminator loss: 0.469216, acc: 0.726562] [adversarial loss: 1.230111, acc: 0.234375]\n",
      "3395: [discriminator loss: 0.578860, acc: 0.718750] [adversarial loss: 1.804284, acc: 0.078125]\n",
      "3396: [discriminator loss: 0.507263, acc: 0.710938] [adversarial loss: 0.971094, acc: 0.296875]\n",
      "3397: [discriminator loss: 0.515683, acc: 0.726562] [adversarial loss: 1.876940, acc: 0.062500]\n",
      "3398: [discriminator loss: 0.480791, acc: 0.757812] [adversarial loss: 1.235932, acc: 0.281250]\n",
      "3399: [discriminator loss: 0.502345, acc: 0.757812] [adversarial loss: 1.410857, acc: 0.250000]\n",
      "3400: [discriminator loss: 0.512764, acc: 0.726562] [adversarial loss: 1.279627, acc: 0.234375]\n",
      "3401: [discriminator loss: 0.471710, acc: 0.781250] [adversarial loss: 1.146946, acc: 0.375000]\n",
      "3402: [discriminator loss: 0.485657, acc: 0.765625] [adversarial loss: 1.312129, acc: 0.250000]\n",
      "3403: [discriminator loss: 0.482334, acc: 0.742188] [adversarial loss: 1.675664, acc: 0.109375]\n",
      "3404: [discriminator loss: 0.549679, acc: 0.742188] [adversarial loss: 1.142155, acc: 0.265625]\n",
      "3405: [discriminator loss: 0.601379, acc: 0.695312] [adversarial loss: 2.104355, acc: 0.046875]\n",
      "3406: [discriminator loss: 0.467492, acc: 0.812500] [adversarial loss: 0.930218, acc: 0.421875]\n",
      "3407: [discriminator loss: 0.465602, acc: 0.765625] [adversarial loss: 1.908504, acc: 0.093750]\n",
      "3408: [discriminator loss: 0.450502, acc: 0.734375] [adversarial loss: 0.917045, acc: 0.406250]\n",
      "3409: [discriminator loss: 0.573859, acc: 0.703125] [adversarial loss: 1.922499, acc: 0.093750]\n",
      "3410: [discriminator loss: 0.610894, acc: 0.679688] [adversarial loss: 0.986249, acc: 0.468750]\n",
      "3411: [discriminator loss: 0.592516, acc: 0.695312] [adversarial loss: 1.469653, acc: 0.125000]\n",
      "3412: [discriminator loss: 0.506886, acc: 0.703125] [adversarial loss: 1.031544, acc: 0.359375]\n",
      "3413: [discriminator loss: 0.574308, acc: 0.695312] [adversarial loss: 1.610357, acc: 0.078125]\n",
      "3414: [discriminator loss: 0.554530, acc: 0.734375] [adversarial loss: 1.185884, acc: 0.203125]\n",
      "3415: [discriminator loss: 0.429232, acc: 0.820312] [adversarial loss: 1.896949, acc: 0.140625]\n",
      "3416: [discriminator loss: 0.481256, acc: 0.781250] [adversarial loss: 1.425338, acc: 0.125000]\n",
      "3417: [discriminator loss: 0.461975, acc: 0.765625] [adversarial loss: 1.474707, acc: 0.140625]\n",
      "3418: [discriminator loss: 0.476737, acc: 0.757812] [adversarial loss: 1.194023, acc: 0.281250]\n",
      "3419: [discriminator loss: 0.606826, acc: 0.664062] [adversarial loss: 2.050005, acc: 0.046875]\n",
      "3420: [discriminator loss: 0.565280, acc: 0.671875] [adversarial loss: 0.958591, acc: 0.406250]\n",
      "3421: [discriminator loss: 0.577936, acc: 0.640625] [adversarial loss: 1.737203, acc: 0.093750]\n",
      "3422: [discriminator loss: 0.603819, acc: 0.710938] [adversarial loss: 1.004426, acc: 0.328125]\n",
      "3423: [discriminator loss: 0.536008, acc: 0.695312] [adversarial loss: 1.380178, acc: 0.171875]\n",
      "3424: [discriminator loss: 0.497260, acc: 0.773438] [adversarial loss: 1.325450, acc: 0.281250]\n",
      "3425: [discriminator loss: 0.447461, acc: 0.757812] [adversarial loss: 1.175112, acc: 0.234375]\n",
      "3426: [discriminator loss: 0.508934, acc: 0.750000] [adversarial loss: 1.468859, acc: 0.171875]\n",
      "3427: [discriminator loss: 0.505117, acc: 0.765625] [adversarial loss: 1.175601, acc: 0.312500]\n",
      "3428: [discriminator loss: 0.488922, acc: 0.765625] [adversarial loss: 1.709342, acc: 0.125000]\n",
      "3429: [discriminator loss: 0.452398, acc: 0.765625] [adversarial loss: 0.981736, acc: 0.375000]\n",
      "3430: [discriminator loss: 0.466309, acc: 0.750000] [adversarial loss: 1.976634, acc: 0.062500]\n",
      "3431: [discriminator loss: 0.561824, acc: 0.710938] [adversarial loss: 0.867328, acc: 0.406250]\n",
      "3432: [discriminator loss: 0.560661, acc: 0.734375] [adversarial loss: 2.167386, acc: 0.031250]\n",
      "3433: [discriminator loss: 0.589790, acc: 0.703125] [adversarial loss: 1.008129, acc: 0.296875]\n",
      "3434: [discriminator loss: 0.609408, acc: 0.679688] [adversarial loss: 1.685824, acc: 0.156250]\n",
      "3435: [discriminator loss: 0.453215, acc: 0.812500] [adversarial loss: 1.312773, acc: 0.281250]\n",
      "3436: [discriminator loss: 0.458074, acc: 0.804688] [adversarial loss: 1.474357, acc: 0.187500]\n",
      "3437: [discriminator loss: 0.513526, acc: 0.757812] [adversarial loss: 1.361597, acc: 0.218750]\n",
      "3438: [discriminator loss: 0.480660, acc: 0.789062] [adversarial loss: 1.244740, acc: 0.296875]\n",
      "3439: [discriminator loss: 0.451756, acc: 0.796875] [adversarial loss: 1.393156, acc: 0.250000]\n",
      "3440: [discriminator loss: 0.455435, acc: 0.796875] [adversarial loss: 1.308825, acc: 0.250000]\n",
      "3441: [discriminator loss: 0.496317, acc: 0.750000] [adversarial loss: 1.507528, acc: 0.062500]\n",
      "3442: [discriminator loss: 0.559261, acc: 0.703125] [adversarial loss: 1.403301, acc: 0.156250]\n",
      "3443: [discriminator loss: 0.483180, acc: 0.765625] [adversarial loss: 1.143879, acc: 0.312500]\n",
      "3444: [discriminator loss: 0.484039, acc: 0.812500] [adversarial loss: 1.356857, acc: 0.250000]\n",
      "3445: [discriminator loss: 0.506617, acc: 0.750000] [adversarial loss: 1.635207, acc: 0.171875]\n",
      "3446: [discriminator loss: 0.573416, acc: 0.695312] [adversarial loss: 0.746063, acc: 0.562500]\n",
      "3447: [discriminator loss: 0.428734, acc: 0.804688] [adversarial loss: 1.403187, acc: 0.125000]\n",
      "3448: [discriminator loss: 0.500504, acc: 0.750000] [adversarial loss: 1.371272, acc: 0.140625]\n",
      "3449: [discriminator loss: 0.515621, acc: 0.718750] [adversarial loss: 1.543026, acc: 0.093750]\n",
      "3450: [discriminator loss: 0.467751, acc: 0.757812] [adversarial loss: 1.287135, acc: 0.203125]\n",
      "3451: [discriminator loss: 0.468519, acc: 0.765625] [adversarial loss: 1.311845, acc: 0.203125]\n",
      "3452: [discriminator loss: 0.483438, acc: 0.781250] [adversarial loss: 1.221771, acc: 0.265625]\n",
      "3453: [discriminator loss: 0.516244, acc: 0.734375] [adversarial loss: 1.187115, acc: 0.375000]\n",
      "3454: [discriminator loss: 0.533525, acc: 0.750000] [adversarial loss: 1.550630, acc: 0.109375]\n",
      "3455: [discriminator loss: 0.550985, acc: 0.703125] [adversarial loss: 1.243794, acc: 0.281250]\n",
      "3456: [discriminator loss: 0.455968, acc: 0.796875] [adversarial loss: 1.597630, acc: 0.156250]\n",
      "3457: [discriminator loss: 0.622625, acc: 0.687500] [adversarial loss: 1.398467, acc: 0.265625]\n",
      "3458: [discriminator loss: 0.555601, acc: 0.695312] [adversarial loss: 0.865246, acc: 0.453125]\n",
      "3459: [discriminator loss: 0.545908, acc: 0.718750] [adversarial loss: 2.331316, acc: 0.000000]\n",
      "3460: [discriminator loss: 0.616657, acc: 0.679688] [adversarial loss: 0.857280, acc: 0.437500]\n",
      "3461: [discriminator loss: 0.639567, acc: 0.648438] [adversarial loss: 1.869622, acc: 0.093750]\n",
      "3462: [discriminator loss: 0.533140, acc: 0.765625] [adversarial loss: 1.225981, acc: 0.218750]\n",
      "3463: [discriminator loss: 0.586474, acc: 0.671875] [adversarial loss: 1.355307, acc: 0.140625]\n",
      "3464: [discriminator loss: 0.436955, acc: 0.765625] [adversarial loss: 1.286089, acc: 0.218750]\n",
      "3465: [discriminator loss: 0.472955, acc: 0.789062] [adversarial loss: 1.362609, acc: 0.203125]\n",
      "3466: [discriminator loss: 0.463777, acc: 0.789062] [adversarial loss: 1.466907, acc: 0.171875]\n",
      "3467: [discriminator loss: 0.437115, acc: 0.820312] [adversarial loss: 1.256014, acc: 0.218750]\n",
      "3468: [discriminator loss: 0.518150, acc: 0.742188] [adversarial loss: 1.390594, acc: 0.187500]\n",
      "3469: [discriminator loss: 0.489475, acc: 0.750000] [adversarial loss: 1.726653, acc: 0.140625]\n",
      "3470: [discriminator loss: 0.423757, acc: 0.820312] [adversarial loss: 0.995398, acc: 0.468750]\n",
      "3471: [discriminator loss: 0.515978, acc: 0.710938] [adversarial loss: 1.660784, acc: 0.140625]\n",
      "3472: [discriminator loss: 0.521871, acc: 0.757812] [adversarial loss: 1.059228, acc: 0.359375]\n",
      "3473: [discriminator loss: 0.512746, acc: 0.765625] [adversarial loss: 1.640682, acc: 0.109375]\n",
      "3474: [discriminator loss: 0.517518, acc: 0.789062] [adversarial loss: 0.885567, acc: 0.359375]\n",
      "3475: [discriminator loss: 0.547673, acc: 0.750000] [adversarial loss: 2.281281, acc: 0.046875]\n",
      "3476: [discriminator loss: 0.565257, acc: 0.765625] [adversarial loss: 0.923255, acc: 0.437500]\n",
      "3477: [discriminator loss: 0.526415, acc: 0.695312] [adversarial loss: 1.646017, acc: 0.156250]\n",
      "3478: [discriminator loss: 0.434164, acc: 0.812500] [adversarial loss: 1.198734, acc: 0.281250]\n",
      "3479: [discriminator loss: 0.527413, acc: 0.703125] [adversarial loss: 1.723489, acc: 0.078125]\n",
      "3480: [discriminator loss: 0.626161, acc: 0.671875] [adversarial loss: 0.848379, acc: 0.421875]\n",
      "3481: [discriminator loss: 0.531130, acc: 0.734375] [adversarial loss: 1.770245, acc: 0.093750]\n",
      "3482: [discriminator loss: 0.528324, acc: 0.757812] [adversarial loss: 0.915093, acc: 0.390625]\n",
      "3483: [discriminator loss: 0.525972, acc: 0.734375] [adversarial loss: 1.702556, acc: 0.109375]\n",
      "3484: [discriminator loss: 0.493079, acc: 0.750000] [adversarial loss: 1.208712, acc: 0.234375]\n",
      "3485: [discriminator loss: 0.515516, acc: 0.765625] [adversarial loss: 1.372720, acc: 0.156250]\n",
      "3486: [discriminator loss: 0.504267, acc: 0.742188] [adversarial loss: 0.929574, acc: 0.375000]\n",
      "3487: [discriminator loss: 0.592225, acc: 0.703125] [adversarial loss: 1.836504, acc: 0.093750]\n",
      "3488: [discriminator loss: 0.466752, acc: 0.789062] [adversarial loss: 1.314062, acc: 0.265625]\n",
      "3489: [discriminator loss: 0.501425, acc: 0.703125] [adversarial loss: 1.252456, acc: 0.234375]\n",
      "3490: [discriminator loss: 0.507821, acc: 0.734375] [adversarial loss: 1.437285, acc: 0.218750]\n",
      "3491: [discriminator loss: 0.542917, acc: 0.742188] [adversarial loss: 1.265208, acc: 0.171875]\n",
      "3492: [discriminator loss: 0.518968, acc: 0.750000] [adversarial loss: 1.061090, acc: 0.406250]\n",
      "3493: [discriminator loss: 0.521564, acc: 0.726562] [adversarial loss: 1.269969, acc: 0.187500]\n",
      "3494: [discriminator loss: 0.475217, acc: 0.765625] [adversarial loss: 1.455619, acc: 0.125000]\n",
      "3495: [discriminator loss: 0.453391, acc: 0.781250] [adversarial loss: 1.409505, acc: 0.171875]\n",
      "3496: [discriminator loss: 0.523568, acc: 0.742188] [adversarial loss: 0.769148, acc: 0.484375]\n",
      "3497: [discriminator loss: 0.456595, acc: 0.781250] [adversarial loss: 1.697635, acc: 0.046875]\n",
      "3498: [discriminator loss: 0.532732, acc: 0.664062] [adversarial loss: 0.932143, acc: 0.390625]\n",
      "3499: [discriminator loss: 0.515106, acc: 0.789062] [adversarial loss: 1.652999, acc: 0.140625]\n",
      "3500: [discriminator loss: 0.449713, acc: 0.812500] [adversarial loss: 1.169674, acc: 0.187500]\n",
      "3501: [discriminator loss: 0.539016, acc: 0.687500] [adversarial loss: 2.053166, acc: 0.031250]\n",
      "3502: [discriminator loss: 0.523993, acc: 0.710938] [adversarial loss: 1.022511, acc: 0.328125]\n",
      "3503: [discriminator loss: 0.534968, acc: 0.671875] [adversarial loss: 1.713719, acc: 0.062500]\n",
      "3504: [discriminator loss: 0.494393, acc: 0.750000] [adversarial loss: 1.437297, acc: 0.203125]\n",
      "3505: [discriminator loss: 0.477955, acc: 0.750000] [adversarial loss: 1.292619, acc: 0.250000]\n",
      "3506: [discriminator loss: 0.505405, acc: 0.734375] [adversarial loss: 1.701568, acc: 0.171875]\n",
      "3507: [discriminator loss: 0.456737, acc: 0.773438] [adversarial loss: 1.124741, acc: 0.312500]\n",
      "3508: [discriminator loss: 0.577402, acc: 0.718750] [adversarial loss: 1.856477, acc: 0.093750]\n",
      "3509: [discriminator loss: 0.643629, acc: 0.656250] [adversarial loss: 0.953369, acc: 0.421875]\n",
      "3510: [discriminator loss: 0.469902, acc: 0.781250] [adversarial loss: 1.713493, acc: 0.078125]\n",
      "3511: [discriminator loss: 0.497623, acc: 0.742188] [adversarial loss: 1.075515, acc: 0.281250]\n",
      "3512: [discriminator loss: 0.433886, acc: 0.781250] [adversarial loss: 1.234416, acc: 0.312500]\n",
      "3513: [discriminator loss: 0.550290, acc: 0.750000] [adversarial loss: 1.398043, acc: 0.171875]\n",
      "3514: [discriminator loss: 0.535207, acc: 0.742188] [adversarial loss: 1.088831, acc: 0.296875]\n",
      "3515: [discriminator loss: 0.488779, acc: 0.781250] [adversarial loss: 1.350386, acc: 0.187500]\n",
      "3516: [discriminator loss: 0.516539, acc: 0.750000] [adversarial loss: 0.989887, acc: 0.296875]\n",
      "3517: [discriminator loss: 0.515215, acc: 0.726562] [adversarial loss: 1.874813, acc: 0.156250]\n",
      "3518: [discriminator loss: 0.477059, acc: 0.750000] [adversarial loss: 0.964497, acc: 0.359375]\n",
      "3519: [discriminator loss: 0.440827, acc: 0.812500] [adversarial loss: 1.871066, acc: 0.062500]\n",
      "3520: [discriminator loss: 0.474557, acc: 0.757812] [adversarial loss: 1.039391, acc: 0.312500]\n",
      "3521: [discriminator loss: 0.599523, acc: 0.710938] [adversarial loss: 2.000141, acc: 0.062500]\n",
      "3522: [discriminator loss: 0.610724, acc: 0.718750] [adversarial loss: 1.139460, acc: 0.375000]\n",
      "3523: [discriminator loss: 0.520491, acc: 0.773438] [adversarial loss: 1.294448, acc: 0.250000]\n",
      "3524: [discriminator loss: 0.492111, acc: 0.773438] [adversarial loss: 1.070994, acc: 0.406250]\n",
      "3525: [discriminator loss: 0.503289, acc: 0.726562] [adversarial loss: 1.431767, acc: 0.140625]\n",
      "3526: [discriminator loss: 0.527306, acc: 0.765625] [adversarial loss: 1.305911, acc: 0.187500]\n",
      "3527: [discriminator loss: 0.438679, acc: 0.781250] [adversarial loss: 1.280087, acc: 0.234375]\n",
      "3528: [discriminator loss: 0.486064, acc: 0.789062] [adversarial loss: 1.217940, acc: 0.218750]\n",
      "3529: [discriminator loss: 0.556162, acc: 0.718750] [adversarial loss: 1.476637, acc: 0.187500]\n",
      "3530: [discriminator loss: 0.545965, acc: 0.757812] [adversarial loss: 1.212259, acc: 0.234375]\n",
      "3531: [discriminator loss: 0.466998, acc: 0.773438] [adversarial loss: 1.144919, acc: 0.265625]\n",
      "3532: [discriminator loss: 0.457530, acc: 0.789062] [adversarial loss: 1.613795, acc: 0.093750]\n",
      "3533: [discriminator loss: 0.486605, acc: 0.796875] [adversarial loss: 1.229007, acc: 0.296875]\n",
      "3534: [discriminator loss: 0.477630, acc: 0.789062] [adversarial loss: 1.431086, acc: 0.156250]\n",
      "3535: [discriminator loss: 0.447349, acc: 0.773438] [adversarial loss: 0.991817, acc: 0.375000]\n",
      "3536: [discriminator loss: 0.446428, acc: 0.789062] [adversarial loss: 1.559452, acc: 0.140625]\n",
      "3537: [discriminator loss: 0.538546, acc: 0.718750] [adversarial loss: 0.952867, acc: 0.375000]\n",
      "3538: [discriminator loss: 0.545902, acc: 0.734375] [adversarial loss: 1.767826, acc: 0.062500]\n",
      "3539: [discriminator loss: 0.490433, acc: 0.742188] [adversarial loss: 1.002869, acc: 0.359375]\n",
      "3540: [discriminator loss: 0.472899, acc: 0.750000] [adversarial loss: 1.532272, acc: 0.156250]\n",
      "3541: [discriminator loss: 0.508521, acc: 0.734375] [adversarial loss: 0.903537, acc: 0.453125]\n",
      "3542: [discriminator loss: 0.558074, acc: 0.695312] [adversarial loss: 1.772623, acc: 0.078125]\n",
      "3543: [discriminator loss: 0.478536, acc: 0.773438] [adversarial loss: 1.026315, acc: 0.390625]\n",
      "3544: [discriminator loss: 0.503061, acc: 0.765625] [adversarial loss: 1.701125, acc: 0.140625]\n",
      "3545: [discriminator loss: 0.480856, acc: 0.773438] [adversarial loss: 1.192850, acc: 0.312500]\n",
      "3546: [discriminator loss: 0.494411, acc: 0.773438] [adversarial loss: 1.720345, acc: 0.109375]\n",
      "3547: [discriminator loss: 0.619666, acc: 0.640625] [adversarial loss: 1.075919, acc: 0.250000]\n",
      "3548: [discriminator loss: 0.521760, acc: 0.742188] [adversarial loss: 1.711596, acc: 0.156250]\n",
      "3549: [discriminator loss: 0.485598, acc: 0.773438] [adversarial loss: 1.653997, acc: 0.093750]\n",
      "3550: [discriminator loss: 0.445191, acc: 0.781250] [adversarial loss: 1.152864, acc: 0.234375]\n",
      "3551: [discriminator loss: 0.422998, acc: 0.812500] [adversarial loss: 1.708169, acc: 0.093750]\n",
      "3552: [discriminator loss: 0.545081, acc: 0.734375] [adversarial loss: 1.135786, acc: 0.250000]\n",
      "3553: [discriminator loss: 0.540714, acc: 0.703125] [adversarial loss: 1.580960, acc: 0.078125]\n",
      "3554: [discriminator loss: 0.553711, acc: 0.671875] [adversarial loss: 0.869917, acc: 0.375000]\n",
      "3555: [discriminator loss: 0.505445, acc: 0.757812] [adversarial loss: 1.534066, acc: 0.187500]\n",
      "3556: [discriminator loss: 0.532470, acc: 0.718750] [adversarial loss: 0.899943, acc: 0.406250]\n",
      "3557: [discriminator loss: 0.490684, acc: 0.757812] [adversarial loss: 1.853710, acc: 0.125000]\n",
      "3558: [discriminator loss: 0.428766, acc: 0.812500] [adversarial loss: 0.944338, acc: 0.406250]\n",
      "3559: [discriminator loss: 0.552533, acc: 0.757812] [adversarial loss: 1.653103, acc: 0.093750]\n",
      "3560: [discriminator loss: 0.582370, acc: 0.726562] [adversarial loss: 0.789917, acc: 0.531250]\n",
      "3561: [discriminator loss: 0.464570, acc: 0.781250] [adversarial loss: 1.600661, acc: 0.140625]\n",
      "3562: [discriminator loss: 0.532627, acc: 0.718750] [adversarial loss: 1.401393, acc: 0.140625]\n",
      "3563: [discriminator loss: 0.538575, acc: 0.734375] [adversarial loss: 1.507238, acc: 0.140625]\n",
      "3564: [discriminator loss: 0.494575, acc: 0.773438] [adversarial loss: 1.517009, acc: 0.125000]\n",
      "3565: [discriminator loss: 0.489349, acc: 0.796875] [adversarial loss: 1.472213, acc: 0.156250]\n",
      "3566: [discriminator loss: 0.367324, acc: 0.898438] [adversarial loss: 0.973869, acc: 0.375000]\n",
      "3567: [discriminator loss: 0.492058, acc: 0.710938] [adversarial loss: 1.874789, acc: 0.109375]\n",
      "3568: [discriminator loss: 0.499452, acc: 0.773438] [adversarial loss: 1.087217, acc: 0.359375]\n",
      "3569: [discriminator loss: 0.563699, acc: 0.664062] [adversarial loss: 1.865537, acc: 0.109375]\n",
      "3570: [discriminator loss: 0.486832, acc: 0.757812] [adversarial loss: 0.921131, acc: 0.453125]\n",
      "3571: [discriminator loss: 0.560842, acc: 0.710938] [adversarial loss: 2.028233, acc: 0.046875]\n",
      "3572: [discriminator loss: 0.728573, acc: 0.601562] [adversarial loss: 1.078610, acc: 0.312500]\n",
      "3573: [discriminator loss: 0.460325, acc: 0.804688] [adversarial loss: 1.491546, acc: 0.140625]\n",
      "3574: [discriminator loss: 0.528012, acc: 0.750000] [adversarial loss: 0.976440, acc: 0.390625]\n",
      "3575: [discriminator loss: 0.485401, acc: 0.789062] [adversarial loss: 1.547901, acc: 0.125000]\n",
      "3576: [discriminator loss: 0.596904, acc: 0.695312] [adversarial loss: 1.068211, acc: 0.343750]\n",
      "3577: [discriminator loss: 0.508615, acc: 0.773438] [adversarial loss: 1.555814, acc: 0.171875]\n",
      "3578: [discriminator loss: 0.452797, acc: 0.796875] [adversarial loss: 1.191314, acc: 0.250000]\n",
      "3579: [discriminator loss: 0.464444, acc: 0.750000] [adversarial loss: 1.257716, acc: 0.296875]\n",
      "3580: [discriminator loss: 0.452142, acc: 0.812500] [adversarial loss: 1.092544, acc: 0.328125]\n",
      "3581: [discriminator loss: 0.479849, acc: 0.742188] [adversarial loss: 1.652892, acc: 0.078125]\n",
      "3582: [discriminator loss: 0.484617, acc: 0.765625] [adversarial loss: 1.169675, acc: 0.218750]\n",
      "3583: [discriminator loss: 0.409467, acc: 0.867188] [adversarial loss: 1.477318, acc: 0.140625]\n",
      "3584: [discriminator loss: 0.395660, acc: 0.796875] [adversarial loss: 1.273326, acc: 0.187500]\n",
      "3585: [discriminator loss: 0.424742, acc: 0.820312] [adversarial loss: 1.716903, acc: 0.125000]\n",
      "3586: [discriminator loss: 0.563302, acc: 0.687500] [adversarial loss: 0.842166, acc: 0.484375]\n",
      "3587: [discriminator loss: 0.659728, acc: 0.625000] [adversarial loss: 1.844640, acc: 0.109375]\n",
      "3588: [discriminator loss: 0.469811, acc: 0.750000] [adversarial loss: 1.421130, acc: 0.171875]\n",
      "3589: [discriminator loss: 0.547357, acc: 0.718750] [adversarial loss: 1.414412, acc: 0.187500]\n",
      "3590: [discriminator loss: 0.474669, acc: 0.796875] [adversarial loss: 1.343740, acc: 0.234375]\n",
      "3591: [discriminator loss: 0.472320, acc: 0.757812] [adversarial loss: 1.533453, acc: 0.156250]\n",
      "3592: [discriminator loss: 0.458298, acc: 0.789062] [adversarial loss: 1.270899, acc: 0.234375]\n",
      "3593: [discriminator loss: 0.414459, acc: 0.843750] [adversarial loss: 1.735525, acc: 0.093750]\n",
      "3594: [discriminator loss: 0.424871, acc: 0.812500] [adversarial loss: 0.979966, acc: 0.421875]\n",
      "3595: [discriminator loss: 0.579086, acc: 0.703125] [adversarial loss: 1.921991, acc: 0.062500]\n",
      "3596: [discriminator loss: 0.605803, acc: 0.695312] [adversarial loss: 0.768030, acc: 0.468750]\n",
      "3597: [discriminator loss: 0.584879, acc: 0.679688] [adversarial loss: 1.975714, acc: 0.093750]\n",
      "3598: [discriminator loss: 0.540170, acc: 0.726562] [adversarial loss: 1.260291, acc: 0.203125]\n",
      "3599: [discriminator loss: 0.467398, acc: 0.757812] [adversarial loss: 1.760156, acc: 0.046875]\n",
      "3600: [discriminator loss: 0.542253, acc: 0.734375] [adversarial loss: 1.346013, acc: 0.234375]\n",
      "3601: [discriminator loss: 0.541344, acc: 0.742188] [adversarial loss: 1.352580, acc: 0.203125]\n",
      "3602: [discriminator loss: 0.495873, acc: 0.742188] [adversarial loss: 1.044631, acc: 0.312500]\n",
      "3603: [discriminator loss: 0.463690, acc: 0.820312] [adversarial loss: 1.704545, acc: 0.171875]\n",
      "3604: [discriminator loss: 0.445845, acc: 0.835938] [adversarial loss: 1.191441, acc: 0.265625]\n",
      "3605: [discriminator loss: 0.405544, acc: 0.773438] [adversarial loss: 1.775108, acc: 0.125000]\n",
      "3606: [discriminator loss: 0.584557, acc: 0.703125] [adversarial loss: 1.123363, acc: 0.312500]\n",
      "3607: [discriminator loss: 0.529377, acc: 0.742188] [adversarial loss: 1.567971, acc: 0.125000]\n",
      "3608: [discriminator loss: 0.417429, acc: 0.789062] [adversarial loss: 1.218991, acc: 0.187500]\n",
      "3609: [discriminator loss: 0.458878, acc: 0.820312] [adversarial loss: 1.471635, acc: 0.203125]\n",
      "3610: [discriminator loss: 0.468254, acc: 0.789062] [adversarial loss: 0.832975, acc: 0.531250]\n",
      "3611: [discriminator loss: 0.567517, acc: 0.710938] [adversarial loss: 2.327962, acc: 0.046875]\n",
      "3612: [discriminator loss: 0.536016, acc: 0.679688] [adversarial loss: 0.910240, acc: 0.406250]\n",
      "3613: [discriminator loss: 0.536896, acc: 0.750000] [adversarial loss: 1.630066, acc: 0.140625]\n",
      "3614: [discriminator loss: 0.529780, acc: 0.742188] [adversarial loss: 1.050984, acc: 0.375000]\n",
      "3615: [discriminator loss: 0.534994, acc: 0.742188] [adversarial loss: 2.052336, acc: 0.015625]\n",
      "3616: [discriminator loss: 0.510461, acc: 0.742188] [adversarial loss: 0.932005, acc: 0.437500]\n",
      "3617: [discriminator loss: 0.519723, acc: 0.718750] [adversarial loss: 1.660615, acc: 0.046875]\n",
      "3618: [discriminator loss: 0.496599, acc: 0.742188] [adversarial loss: 1.025095, acc: 0.343750]\n",
      "3619: [discriminator loss: 0.440423, acc: 0.812500] [adversarial loss: 1.293739, acc: 0.250000]\n",
      "3620: [discriminator loss: 0.517368, acc: 0.695312] [adversarial loss: 1.530766, acc: 0.171875]\n",
      "3621: [discriminator loss: 0.535475, acc: 0.742188] [adversarial loss: 1.237725, acc: 0.218750]\n",
      "3622: [discriminator loss: 0.528241, acc: 0.757812] [adversarial loss: 1.876503, acc: 0.046875]\n",
      "3623: [discriminator loss: 0.476105, acc: 0.812500] [adversarial loss: 1.274920, acc: 0.218750]\n",
      "3624: [discriminator loss: 0.491546, acc: 0.773438] [adversarial loss: 1.396891, acc: 0.187500]\n",
      "3625: [discriminator loss: 0.437482, acc: 0.789062] [adversarial loss: 1.775720, acc: 0.062500]\n",
      "3626: [discriminator loss: 0.462019, acc: 0.789062] [adversarial loss: 0.952356, acc: 0.437500]\n",
      "3627: [discriminator loss: 0.413490, acc: 0.812500] [adversarial loss: 1.540163, acc: 0.203125]\n",
      "3628: [discriminator loss: 0.447450, acc: 0.765625] [adversarial loss: 1.598880, acc: 0.156250]\n",
      "3629: [discriminator loss: 0.401763, acc: 0.843750] [adversarial loss: 1.297477, acc: 0.218750]\n",
      "3630: [discriminator loss: 0.536714, acc: 0.695312] [adversarial loss: 1.401278, acc: 0.156250]\n",
      "3631: [discriminator loss: 0.459987, acc: 0.750000] [adversarial loss: 1.460524, acc: 0.250000]\n",
      "3632: [discriminator loss: 0.538623, acc: 0.789062] [adversarial loss: 1.326526, acc: 0.234375]\n",
      "3633: [discriminator loss: 0.526976, acc: 0.718750] [adversarial loss: 2.173134, acc: 0.031250]\n",
      "3634: [discriminator loss: 0.529941, acc: 0.734375] [adversarial loss: 0.816151, acc: 0.531250]\n",
      "3635: [discriminator loss: 0.525567, acc: 0.710938] [adversarial loss: 2.044108, acc: 0.046875]\n",
      "3636: [discriminator loss: 0.558119, acc: 0.687500] [adversarial loss: 0.793332, acc: 0.531250]\n",
      "3637: [discriminator loss: 0.539904, acc: 0.695312] [adversarial loss: 1.673033, acc: 0.125000]\n",
      "3638: [discriminator loss: 0.548061, acc: 0.687500] [adversarial loss: 0.948002, acc: 0.359375]\n",
      "3639: [discriminator loss: 0.590401, acc: 0.703125] [adversarial loss: 1.561665, acc: 0.140625]\n",
      "3640: [discriminator loss: 0.404727, acc: 0.804688] [adversarial loss: 1.275962, acc: 0.234375]\n",
      "3641: [discriminator loss: 0.463510, acc: 0.796875] [adversarial loss: 1.394688, acc: 0.187500]\n",
      "3642: [discriminator loss: 0.491313, acc: 0.750000] [adversarial loss: 1.258949, acc: 0.234375]\n",
      "3643: [discriminator loss: 0.517235, acc: 0.695312] [adversarial loss: 1.398680, acc: 0.187500]\n",
      "3644: [discriminator loss: 0.465457, acc: 0.781250] [adversarial loss: 1.509654, acc: 0.140625]\n",
      "3645: [discriminator loss: 0.518769, acc: 0.734375] [adversarial loss: 1.278027, acc: 0.234375]\n",
      "3646: [discriminator loss: 0.409429, acc: 0.851562] [adversarial loss: 1.398814, acc: 0.187500]\n",
      "3647: [discriminator loss: 0.478108, acc: 0.781250] [adversarial loss: 0.980689, acc: 0.421875]\n",
      "3648: [discriminator loss: 0.486897, acc: 0.750000] [adversarial loss: 1.345281, acc: 0.234375]\n",
      "3649: [discriminator loss: 0.552517, acc: 0.750000] [adversarial loss: 1.044273, acc: 0.296875]\n",
      "3650: [discriminator loss: 0.456442, acc: 0.757812] [adversarial loss: 1.846115, acc: 0.078125]\n",
      "3651: [discriminator loss: 0.481912, acc: 0.773438] [adversarial loss: 1.239614, acc: 0.312500]\n",
      "3652: [discriminator loss: 0.516678, acc: 0.750000] [adversarial loss: 1.837201, acc: 0.125000]\n",
      "3653: [discriminator loss: 0.482173, acc: 0.773438] [adversarial loss: 1.155851, acc: 0.265625]\n",
      "3654: [discriminator loss: 0.498632, acc: 0.757812] [adversarial loss: 1.312526, acc: 0.218750]\n",
      "3655: [discriminator loss: 0.534304, acc: 0.750000] [adversarial loss: 1.748453, acc: 0.109375]\n",
      "3656: [discriminator loss: 0.575923, acc: 0.726562] [adversarial loss: 1.227272, acc: 0.343750]\n",
      "3657: [discriminator loss: 0.471360, acc: 0.804688] [adversarial loss: 1.507473, acc: 0.093750]\n",
      "3658: [discriminator loss: 0.453840, acc: 0.796875] [adversarial loss: 1.196622, acc: 0.375000]\n",
      "3659: [discriminator loss: 0.489491, acc: 0.757812] [adversarial loss: 2.079195, acc: 0.062500]\n",
      "3660: [discriminator loss: 0.433966, acc: 0.789062] [adversarial loss: 1.111060, acc: 0.281250]\n",
      "3661: [discriminator loss: 0.531546, acc: 0.710938] [adversarial loss: 1.561329, acc: 0.140625]\n",
      "3662: [discriminator loss: 0.442902, acc: 0.820312] [adversarial loss: 1.166474, acc: 0.250000]\n",
      "3663: [discriminator loss: 0.578915, acc: 0.703125] [adversarial loss: 1.024933, acc: 0.390625]\n",
      "3664: [discriminator loss: 0.508802, acc: 0.750000] [adversarial loss: 1.693120, acc: 0.078125]\n",
      "3665: [discriminator loss: 0.598071, acc: 0.679688] [adversarial loss: 1.059053, acc: 0.296875]\n",
      "3666: [discriminator loss: 0.593182, acc: 0.718750] [adversarial loss: 1.576899, acc: 0.140625]\n",
      "3667: [discriminator loss: 0.567730, acc: 0.695312] [adversarial loss: 0.828472, acc: 0.500000]\n",
      "3668: [discriminator loss: 0.475529, acc: 0.742188] [adversarial loss: 1.925546, acc: 0.062500]\n",
      "3669: [discriminator loss: 0.511476, acc: 0.734375] [adversarial loss: 0.862459, acc: 0.500000]\n",
      "3670: [discriminator loss: 0.549467, acc: 0.734375] [adversarial loss: 1.810677, acc: 0.078125]\n",
      "3671: [discriminator loss: 0.545166, acc: 0.718750] [adversarial loss: 0.976772, acc: 0.390625]\n",
      "3672: [discriminator loss: 0.504294, acc: 0.757812] [adversarial loss: 1.996528, acc: 0.062500]\n",
      "3673: [discriminator loss: 0.528474, acc: 0.687500] [adversarial loss: 0.997260, acc: 0.328125]\n",
      "3674: [discriminator loss: 0.553472, acc: 0.695312] [adversarial loss: 1.857039, acc: 0.093750]\n",
      "3675: [discriminator loss: 0.465097, acc: 0.804688] [adversarial loss: 1.220374, acc: 0.250000]\n",
      "3676: [discriminator loss: 0.489596, acc: 0.765625] [adversarial loss: 1.424285, acc: 0.218750]\n",
      "3677: [discriminator loss: 0.517497, acc: 0.742188] [adversarial loss: 1.104752, acc: 0.281250]\n",
      "3678: [discriminator loss: 0.451522, acc: 0.789062] [adversarial loss: 1.694535, acc: 0.109375]\n",
      "3679: [discriminator loss: 0.503206, acc: 0.726562] [adversarial loss: 0.963237, acc: 0.375000]\n",
      "3680: [discriminator loss: 0.561530, acc: 0.750000] [adversarial loss: 1.765559, acc: 0.125000]\n",
      "3681: [discriminator loss: 0.520959, acc: 0.742188] [adversarial loss: 1.060207, acc: 0.250000]\n",
      "3682: [discriminator loss: 0.564940, acc: 0.695312] [adversarial loss: 1.617803, acc: 0.140625]\n",
      "3683: [discriminator loss: 0.406126, acc: 0.843750] [adversarial loss: 0.996901, acc: 0.343750]\n",
      "3684: [discriminator loss: 0.524117, acc: 0.734375] [adversarial loss: 1.330284, acc: 0.250000]\n",
      "3685: [discriminator loss: 0.554839, acc: 0.671875] [adversarial loss: 1.036775, acc: 0.328125]\n",
      "3686: [discriminator loss: 0.485643, acc: 0.750000] [adversarial loss: 1.602123, acc: 0.109375]\n",
      "3687: [discriminator loss: 0.541565, acc: 0.734375] [adversarial loss: 0.894058, acc: 0.421875]\n",
      "3688: [discriminator loss: 0.525570, acc: 0.687500] [adversarial loss: 1.793644, acc: 0.109375]\n",
      "3689: [discriminator loss: 0.543329, acc: 0.718750] [adversarial loss: 0.976846, acc: 0.421875]\n",
      "3690: [discriminator loss: 0.543461, acc: 0.695312] [adversarial loss: 2.070642, acc: 0.031250]\n",
      "3691: [discriminator loss: 0.532754, acc: 0.726562] [adversarial loss: 1.045251, acc: 0.312500]\n",
      "3692: [discriminator loss: 0.443048, acc: 0.804688] [adversarial loss: 1.766762, acc: 0.125000]\n",
      "3693: [discriminator loss: 0.470001, acc: 0.773438] [adversarial loss: 1.489295, acc: 0.187500]\n",
      "3694: [discriminator loss: 0.471945, acc: 0.789062] [adversarial loss: 1.197456, acc: 0.296875]\n",
      "3695: [discriminator loss: 0.475510, acc: 0.750000] [adversarial loss: 1.303731, acc: 0.250000]\n",
      "3696: [discriminator loss: 0.453786, acc: 0.781250] [adversarial loss: 1.395770, acc: 0.125000]\n",
      "3697: [discriminator loss: 0.448796, acc: 0.781250] [adversarial loss: 1.239186, acc: 0.250000]\n",
      "3698: [discriminator loss: 0.510291, acc: 0.726562] [adversarial loss: 1.340333, acc: 0.187500]\n",
      "3699: [discriminator loss: 0.496037, acc: 0.742188] [adversarial loss: 1.037279, acc: 0.359375]\n",
      "3700: [discriminator loss: 0.465131, acc: 0.765625] [adversarial loss: 1.702571, acc: 0.125000]\n",
      "3701: [discriminator loss: 0.539806, acc: 0.734375] [adversarial loss: 1.061601, acc: 0.281250]\n",
      "3702: [discriminator loss: 0.454445, acc: 0.789062] [adversarial loss: 1.565152, acc: 0.156250]\n",
      "3703: [discriminator loss: 0.525707, acc: 0.757812] [adversarial loss: 0.855522, acc: 0.421875]\n",
      "3704: [discriminator loss: 0.480649, acc: 0.757812] [adversarial loss: 1.720842, acc: 0.078125]\n",
      "3705: [discriminator loss: 0.464356, acc: 0.765625] [adversarial loss: 0.999046, acc: 0.328125]\n",
      "3706: [discriminator loss: 0.414797, acc: 0.820312] [adversarial loss: 1.741623, acc: 0.140625]\n",
      "3707: [discriminator loss: 0.530497, acc: 0.695312] [adversarial loss: 1.212455, acc: 0.203125]\n",
      "3708: [discriminator loss: 0.521873, acc: 0.734375] [adversarial loss: 1.249625, acc: 0.281250]\n",
      "3709: [discriminator loss: 0.478245, acc: 0.843750] [adversarial loss: 1.321436, acc: 0.218750]\n",
      "3710: [discriminator loss: 0.533699, acc: 0.726562] [adversarial loss: 1.475239, acc: 0.171875]\n",
      "3711: [discriminator loss: 0.467487, acc: 0.757812] [adversarial loss: 1.160606, acc: 0.171875]\n",
      "3712: [discriminator loss: 0.496939, acc: 0.765625] [adversarial loss: 1.464644, acc: 0.234375]\n",
      "3713: [discriminator loss: 0.478963, acc: 0.773438] [adversarial loss: 0.938445, acc: 0.359375]\n",
      "3714: [discriminator loss: 0.526786, acc: 0.726562] [adversarial loss: 1.882569, acc: 0.062500]\n",
      "3715: [discriminator loss: 0.423940, acc: 0.828125] [adversarial loss: 1.260000, acc: 0.265625]\n",
      "3716: [discriminator loss: 0.485190, acc: 0.734375] [adversarial loss: 1.392366, acc: 0.218750]\n",
      "3717: [discriminator loss: 0.439173, acc: 0.804688] [adversarial loss: 1.404872, acc: 0.171875]\n",
      "3718: [discriminator loss: 0.440118, acc: 0.781250] [adversarial loss: 1.448019, acc: 0.156250]\n",
      "3719: [discriminator loss: 0.599161, acc: 0.695312] [adversarial loss: 0.823352, acc: 0.531250]\n",
      "3720: [discriminator loss: 0.590917, acc: 0.703125] [adversarial loss: 2.242697, acc: 0.031250]\n",
      "3721: [discriminator loss: 0.667949, acc: 0.640625] [adversarial loss: 0.771109, acc: 0.531250]\n",
      "3722: [discriminator loss: 0.621085, acc: 0.671875] [adversarial loss: 2.052226, acc: 0.062500]\n",
      "3723: [discriminator loss: 0.466520, acc: 0.781250] [adversarial loss: 1.453244, acc: 0.156250]\n",
      "3724: [discriminator loss: 0.460570, acc: 0.765625] [adversarial loss: 1.064486, acc: 0.328125]\n",
      "3725: [discriminator loss: 0.492440, acc: 0.734375] [adversarial loss: 1.366728, acc: 0.203125]\n",
      "3726: [discriminator loss: 0.486076, acc: 0.742188] [adversarial loss: 1.280060, acc: 0.296875]\n",
      "3727: [discriminator loss: 0.441402, acc: 0.789062] [adversarial loss: 1.512241, acc: 0.187500]\n",
      "3728: [discriminator loss: 0.447538, acc: 0.804688] [adversarial loss: 1.252739, acc: 0.250000]\n",
      "3729: [discriminator loss: 0.442614, acc: 0.789062] [adversarial loss: 1.601527, acc: 0.109375]\n",
      "3730: [discriminator loss: 0.406058, acc: 0.820312] [adversarial loss: 0.970150, acc: 0.375000]\n",
      "3731: [discriminator loss: 0.552780, acc: 0.710938] [adversarial loss: 1.842502, acc: 0.140625]\n",
      "3732: [discriminator loss: 0.535645, acc: 0.734375] [adversarial loss: 1.058862, acc: 0.312500]\n",
      "3733: [discriminator loss: 0.566769, acc: 0.664062] [adversarial loss: 1.906584, acc: 0.078125]\n",
      "3734: [discriminator loss: 0.559295, acc: 0.710938] [adversarial loss: 0.852990, acc: 0.500000]\n",
      "3735: [discriminator loss: 0.632142, acc: 0.632812] [adversarial loss: 2.146021, acc: 0.031250]\n",
      "3736: [discriminator loss: 0.584922, acc: 0.664062] [adversarial loss: 0.856375, acc: 0.500000]\n",
      "3737: [discriminator loss: 0.571368, acc: 0.703125] [adversarial loss: 1.859015, acc: 0.046875]\n",
      "3738: [discriminator loss: 0.467105, acc: 0.742188] [adversarial loss: 1.170892, acc: 0.203125]\n",
      "3739: [discriminator loss: 0.521992, acc: 0.695312] [adversarial loss: 1.206939, acc: 0.171875]\n",
      "3740: [discriminator loss: 0.525164, acc: 0.742188] [adversarial loss: 1.241774, acc: 0.265625]\n",
      "3741: [discriminator loss: 0.438013, acc: 0.765625] [adversarial loss: 1.392233, acc: 0.171875]\n",
      "3742: [discriminator loss: 0.461280, acc: 0.796875] [adversarial loss: 1.367195, acc: 0.156250]\n",
      "3743: [discriminator loss: 0.429386, acc: 0.804688] [adversarial loss: 1.303292, acc: 0.281250]\n",
      "3744: [discriminator loss: 0.484344, acc: 0.757812] [adversarial loss: 1.571061, acc: 0.203125]\n",
      "3745: [discriminator loss: 0.486143, acc: 0.781250] [adversarial loss: 1.716321, acc: 0.109375]\n",
      "3746: [discriminator loss: 0.529755, acc: 0.750000] [adversarial loss: 1.216625, acc: 0.328125]\n",
      "3747: [discriminator loss: 0.466174, acc: 0.757812] [adversarial loss: 1.680087, acc: 0.156250]\n",
      "3748: [discriminator loss: 0.429799, acc: 0.804688] [adversarial loss: 1.123280, acc: 0.265625]\n",
      "3749: [discriminator loss: 0.464871, acc: 0.804688] [adversarial loss: 1.632623, acc: 0.171875]\n",
      "3750: [discriminator loss: 0.482505, acc: 0.757812] [adversarial loss: 1.084368, acc: 0.328125]\n",
      "3751: [discriminator loss: 0.463561, acc: 0.789062] [adversarial loss: 1.658709, acc: 0.125000]\n",
      "3752: [discriminator loss: 0.429513, acc: 0.796875] [adversarial loss: 0.850551, acc: 0.437500]\n",
      "3753: [discriminator loss: 0.536216, acc: 0.679688] [adversarial loss: 1.848989, acc: 0.093750]\n",
      "3754: [discriminator loss: 0.471977, acc: 0.757812] [adversarial loss: 0.872403, acc: 0.484375]\n",
      "3755: [discriminator loss: 0.651141, acc: 0.640625] [adversarial loss: 2.072101, acc: 0.031250]\n",
      "3756: [discriminator loss: 0.576871, acc: 0.710938] [adversarial loss: 0.949764, acc: 0.359375]\n",
      "3757: [discriminator loss: 0.486536, acc: 0.789062] [adversarial loss: 1.509889, acc: 0.125000]\n",
      "3758: [discriminator loss: 0.468755, acc: 0.804688] [adversarial loss: 1.138923, acc: 0.343750]\n",
      "3759: [discriminator loss: 0.481044, acc: 0.773438] [adversarial loss: 1.186674, acc: 0.234375]\n",
      "3760: [discriminator loss: 0.501869, acc: 0.718750] [adversarial loss: 1.377732, acc: 0.187500]\n",
      "3761: [discriminator loss: 0.489743, acc: 0.773438] [adversarial loss: 1.810789, acc: 0.046875]\n",
      "3762: [discriminator loss: 0.452490, acc: 0.789062] [adversarial loss: 1.195190, acc: 0.281250]\n",
      "3763: [discriminator loss: 0.447556, acc: 0.789062] [adversarial loss: 1.504893, acc: 0.218750]\n",
      "3764: [discriminator loss: 0.419695, acc: 0.835938] [adversarial loss: 1.150236, acc: 0.265625]\n",
      "3765: [discriminator loss: 0.454331, acc: 0.789062] [adversarial loss: 1.568211, acc: 0.187500]\n",
      "3766: [discriminator loss: 0.415284, acc: 0.828125] [adversarial loss: 1.124368, acc: 0.343750]\n",
      "3767: [discriminator loss: 0.474401, acc: 0.726562] [adversarial loss: 2.010045, acc: 0.046875]\n",
      "3768: [discriminator loss: 0.535627, acc: 0.703125] [adversarial loss: 1.040493, acc: 0.296875]\n",
      "3769: [discriminator loss: 0.511035, acc: 0.687500] [adversarial loss: 1.384847, acc: 0.156250]\n",
      "3770: [discriminator loss: 0.451800, acc: 0.781250] [adversarial loss: 1.386838, acc: 0.187500]\n",
      "3771: [discriminator loss: 0.488503, acc: 0.781250] [adversarial loss: 1.439867, acc: 0.171875]\n",
      "3772: [discriminator loss: 0.503288, acc: 0.796875] [adversarial loss: 1.061578, acc: 0.265625]\n",
      "3773: [discriminator loss: 0.485421, acc: 0.765625] [adversarial loss: 1.623106, acc: 0.046875]\n",
      "3774: [discriminator loss: 0.382271, acc: 0.835938] [adversarial loss: 1.610332, acc: 0.125000]\n",
      "3775: [discriminator loss: 0.466830, acc: 0.781250] [adversarial loss: 1.054000, acc: 0.453125]\n",
      "3776: [discriminator loss: 0.517187, acc: 0.757812] [adversarial loss: 1.907429, acc: 0.078125]\n",
      "3777: [discriminator loss: 0.522813, acc: 0.734375] [adversarial loss: 1.075149, acc: 0.328125]\n",
      "3778: [discriminator loss: 0.453126, acc: 0.703125] [adversarial loss: 2.199439, acc: 0.046875]\n",
      "3779: [discriminator loss: 0.476496, acc: 0.828125] [adversarial loss: 1.045441, acc: 0.343750]\n",
      "3780: [discriminator loss: 0.556587, acc: 0.734375] [adversarial loss: 1.761149, acc: 0.046875]\n",
      "3781: [discriminator loss: 0.528451, acc: 0.734375] [adversarial loss: 0.810430, acc: 0.500000]\n",
      "3782: [discriminator loss: 0.557291, acc: 0.679688] [adversarial loss: 1.718579, acc: 0.109375]\n",
      "3783: [discriminator loss: 0.479178, acc: 0.757812] [adversarial loss: 1.062330, acc: 0.328125]\n",
      "3784: [discriminator loss: 0.483911, acc: 0.804688] [adversarial loss: 1.346666, acc: 0.265625]\n",
      "3785: [discriminator loss: 0.437908, acc: 0.796875] [adversarial loss: 1.394674, acc: 0.265625]\n",
      "3786: [discriminator loss: 0.573147, acc: 0.664062] [adversarial loss: 1.496612, acc: 0.109375]\n",
      "3787: [discriminator loss: 0.532271, acc: 0.703125] [adversarial loss: 0.930198, acc: 0.390625]\n",
      "3788: [discriminator loss: 0.526369, acc: 0.742188] [adversarial loss: 1.678950, acc: 0.125000]\n",
      "3789: [discriminator loss: 0.526143, acc: 0.742188] [adversarial loss: 1.215274, acc: 0.312500]\n",
      "3790: [discriminator loss: 0.544095, acc: 0.703125] [adversarial loss: 1.356983, acc: 0.250000]\n",
      "3791: [discriminator loss: 0.469283, acc: 0.734375] [adversarial loss: 1.756928, acc: 0.109375]\n",
      "3792: [discriminator loss: 0.507301, acc: 0.734375] [adversarial loss: 1.067999, acc: 0.343750]\n",
      "3793: [discriminator loss: 0.488430, acc: 0.757812] [adversarial loss: 1.533963, acc: 0.187500]\n",
      "3794: [discriminator loss: 0.471939, acc: 0.765625] [adversarial loss: 1.101807, acc: 0.312500]\n",
      "3795: [discriminator loss: 0.505248, acc: 0.773438] [adversarial loss: 1.653605, acc: 0.140625]\n",
      "3796: [discriminator loss: 0.446625, acc: 0.804688] [adversarial loss: 1.131338, acc: 0.328125]\n",
      "3797: [discriminator loss: 0.523464, acc: 0.765625] [adversarial loss: 1.591695, acc: 0.109375]\n",
      "3798: [discriminator loss: 0.474347, acc: 0.750000] [adversarial loss: 0.970387, acc: 0.421875]\n",
      "3799: [discriminator loss: 0.473208, acc: 0.789062] [adversarial loss: 1.543267, acc: 0.140625]\n",
      "3800: [discriminator loss: 0.487532, acc: 0.781250] [adversarial loss: 1.277904, acc: 0.265625]\n",
      "3801: [discriminator loss: 0.527925, acc: 0.734375] [adversarial loss: 1.479674, acc: 0.187500]\n",
      "3802: [discriminator loss: 0.461886, acc: 0.773438] [adversarial loss: 1.676661, acc: 0.093750]\n",
      "3803: [discriminator loss: 0.481262, acc: 0.765625] [adversarial loss: 1.260698, acc: 0.218750]\n",
      "3804: [discriminator loss: 0.552819, acc: 0.757812] [adversarial loss: 2.097979, acc: 0.062500]\n",
      "3805: [discriminator loss: 0.515492, acc: 0.742188] [adversarial loss: 1.021667, acc: 0.359375]\n",
      "3806: [discriminator loss: 0.454899, acc: 0.765625] [adversarial loss: 1.488788, acc: 0.109375]\n",
      "3807: [discriminator loss: 0.566259, acc: 0.695312] [adversarial loss: 1.014880, acc: 0.281250]\n",
      "3808: [discriminator loss: 0.590994, acc: 0.648438] [adversarial loss: 2.059399, acc: 0.062500]\n",
      "3809: [discriminator loss: 0.610338, acc: 0.703125] [adversarial loss: 0.975050, acc: 0.468750]\n",
      "3810: [discriminator loss: 0.585726, acc: 0.734375] [adversarial loss: 2.027992, acc: 0.078125]\n",
      "3811: [discriminator loss: 0.522431, acc: 0.773438] [adversarial loss: 1.226712, acc: 0.296875]\n",
      "3812: [discriminator loss: 0.476675, acc: 0.781250] [adversarial loss: 1.555947, acc: 0.078125]\n",
      "3813: [discriminator loss: 0.493637, acc: 0.765625] [adversarial loss: 1.177010, acc: 0.171875]\n",
      "3814: [discriminator loss: 0.494395, acc: 0.773438] [adversarial loss: 1.525892, acc: 0.140625]\n",
      "3815: [discriminator loss: 0.529891, acc: 0.773438] [adversarial loss: 1.303923, acc: 0.312500]\n",
      "3816: [discriminator loss: 0.556666, acc: 0.765625] [adversarial loss: 1.729302, acc: 0.125000]\n",
      "3817: [discriminator loss: 0.502847, acc: 0.781250] [adversarial loss: 1.179282, acc: 0.203125]\n",
      "3818: [discriminator loss: 0.434664, acc: 0.820312] [adversarial loss: 1.509808, acc: 0.109375]\n",
      "3819: [discriminator loss: 0.447400, acc: 0.765625] [adversarial loss: 1.180137, acc: 0.296875]\n",
      "3820: [discriminator loss: 0.500270, acc: 0.773438] [adversarial loss: 1.643724, acc: 0.156250]\n",
      "3821: [discriminator loss: 0.520879, acc: 0.765625] [adversarial loss: 1.818776, acc: 0.109375]\n",
      "3822: [discriminator loss: 0.501173, acc: 0.734375] [adversarial loss: 0.806225, acc: 0.562500]\n",
      "3823: [discriminator loss: 0.600950, acc: 0.710938] [adversarial loss: 1.646274, acc: 0.140625]\n",
      "3824: [discriminator loss: 0.537723, acc: 0.710938] [adversarial loss: 0.869455, acc: 0.468750]\n",
      "3825: [discriminator loss: 0.509747, acc: 0.726562] [adversarial loss: 1.771136, acc: 0.093750]\n",
      "3826: [discriminator loss: 0.553143, acc: 0.687500] [adversarial loss: 0.939529, acc: 0.468750]\n",
      "3827: [discriminator loss: 0.523392, acc: 0.734375] [adversarial loss: 2.116335, acc: 0.046875]\n",
      "3828: [discriminator loss: 0.588795, acc: 0.664062] [adversarial loss: 0.846119, acc: 0.453125]\n",
      "3829: [discriminator loss: 0.469685, acc: 0.757812] [adversarial loss: 1.659269, acc: 0.140625]\n",
      "3830: [discriminator loss: 0.464140, acc: 0.765625] [adversarial loss: 0.824226, acc: 0.546875]\n",
      "3831: [discriminator loss: 0.476547, acc: 0.765625] [adversarial loss: 1.142477, acc: 0.328125]\n",
      "3832: [discriminator loss: 0.432797, acc: 0.796875] [adversarial loss: 0.528418, acc: 0.750000]\n",
      "3833: [discriminator loss: 0.492801, acc: 0.757812] [adversarial loss: 1.709526, acc: 0.125000]\n",
      "3834: [discriminator loss: 0.596574, acc: 0.734375] [adversarial loss: 0.916212, acc: 0.437500]\n",
      "3835: [discriminator loss: 0.627745, acc: 0.710938] [adversarial loss: 1.999530, acc: 0.062500]\n",
      "3836: [discriminator loss: 0.502772, acc: 0.750000] [adversarial loss: 1.160630, acc: 0.250000]\n",
      "3837: [discriminator loss: 0.444128, acc: 0.781250] [adversarial loss: 1.768280, acc: 0.156250]\n",
      "3838: [discriminator loss: 0.433432, acc: 0.820312] [adversarial loss: 1.369835, acc: 0.203125]\n",
      "3839: [discriminator loss: 0.407125, acc: 0.820312] [adversarial loss: 1.543608, acc: 0.140625]\n",
      "3840: [discriminator loss: 0.516030, acc: 0.710938] [adversarial loss: 1.194506, acc: 0.296875]\n",
      "3841: [discriminator loss: 0.508959, acc: 0.757812] [adversarial loss: 1.259811, acc: 0.218750]\n",
      "3842: [discriminator loss: 0.498429, acc: 0.789062] [adversarial loss: 1.694935, acc: 0.062500]\n",
      "3843: [discriminator loss: 0.468419, acc: 0.781250] [adversarial loss: 1.057969, acc: 0.312500]\n",
      "3844: [discriminator loss: 0.470277, acc: 0.773438] [adversarial loss: 1.358451, acc: 0.203125]\n",
      "3845: [discriminator loss: 0.514828, acc: 0.750000] [adversarial loss: 1.524993, acc: 0.125000]\n",
      "3846: [discriminator loss: 0.517271, acc: 0.742188] [adversarial loss: 1.048291, acc: 0.343750]\n",
      "3847: [discriminator loss: 0.504978, acc: 0.695312] [adversarial loss: 1.483516, acc: 0.187500]\n",
      "3848: [discriminator loss: 0.510215, acc: 0.734375] [adversarial loss: 1.479000, acc: 0.156250]\n",
      "3849: [discriminator loss: 0.471345, acc: 0.789062] [adversarial loss: 1.178551, acc: 0.156250]\n",
      "3850: [discriminator loss: 0.533491, acc: 0.757812] [adversarial loss: 1.237924, acc: 0.218750]\n",
      "3851: [discriminator loss: 0.584534, acc: 0.695312] [adversarial loss: 1.721395, acc: 0.062500]\n",
      "3852: [discriminator loss: 0.569591, acc: 0.695312] [adversarial loss: 0.910892, acc: 0.390625]\n",
      "3853: [discriminator loss: 0.506371, acc: 0.773438] [adversarial loss: 1.670422, acc: 0.078125]\n",
      "3854: [discriminator loss: 0.513351, acc: 0.765625] [adversarial loss: 1.013192, acc: 0.250000]\n",
      "3855: [discriminator loss: 0.451560, acc: 0.796875] [adversarial loss: 1.400947, acc: 0.218750]\n",
      "3856: [discriminator loss: 0.545494, acc: 0.695312] [adversarial loss: 1.358437, acc: 0.281250]\n",
      "3857: [discriminator loss: 0.501869, acc: 0.757812] [adversarial loss: 1.462881, acc: 0.140625]\n",
      "3858: [discriminator loss: 0.466794, acc: 0.742188] [adversarial loss: 1.236708, acc: 0.234375]\n",
      "3859: [discriminator loss: 0.445118, acc: 0.820312] [adversarial loss: 1.153366, acc: 0.281250]\n",
      "3860: [discriminator loss: 0.577374, acc: 0.664062] [adversarial loss: 0.967395, acc: 0.406250]\n",
      "3861: [discriminator loss: 0.458676, acc: 0.773438] [adversarial loss: 1.757354, acc: 0.109375]\n",
      "3862: [discriminator loss: 0.519728, acc: 0.718750] [adversarial loss: 0.741909, acc: 0.562500]\n",
      "3863: [discriminator loss: 0.662078, acc: 0.648438] [adversarial loss: 2.258399, acc: 0.015625]\n",
      "3864: [discriminator loss: 0.728746, acc: 0.609375] [adversarial loss: 0.985536, acc: 0.359375]\n",
      "3865: [discriminator loss: 0.491339, acc: 0.742188] [adversarial loss: 1.487264, acc: 0.109375]\n",
      "3866: [discriminator loss: 0.501930, acc: 0.734375] [adversarial loss: 1.157914, acc: 0.281250]\n",
      "3867: [discriminator loss: 0.492566, acc: 0.742188] [adversarial loss: 1.285649, acc: 0.187500]\n",
      "3868: [discriminator loss: 0.459798, acc: 0.781250] [adversarial loss: 1.290392, acc: 0.218750]\n",
      "3869: [discriminator loss: 0.483779, acc: 0.757812] [adversarial loss: 1.328089, acc: 0.250000]\n",
      "3870: [discriminator loss: 0.499482, acc: 0.726562] [adversarial loss: 1.448741, acc: 0.125000]\n",
      "3871: [discriminator loss: 0.449793, acc: 0.796875] [adversarial loss: 1.444051, acc: 0.234375]\n",
      "3872: [discriminator loss: 0.523661, acc: 0.687500] [adversarial loss: 1.378423, acc: 0.156250]\n",
      "3873: [discriminator loss: 0.431060, acc: 0.820312] [adversarial loss: 1.213193, acc: 0.312500]\n",
      "3874: [discriminator loss: 0.464474, acc: 0.820312] [adversarial loss: 1.527819, acc: 0.156250]\n",
      "3875: [discriminator loss: 0.482287, acc: 0.781250] [adversarial loss: 0.958651, acc: 0.375000]\n",
      "3876: [discriminator loss: 0.546104, acc: 0.742188] [adversarial loss: 1.591501, acc: 0.171875]\n",
      "3877: [discriminator loss: 0.572455, acc: 0.703125] [adversarial loss: 1.201457, acc: 0.156250]\n",
      "3878: [discriminator loss: 0.517561, acc: 0.710938] [adversarial loss: 1.617360, acc: 0.093750]\n",
      "3879: [discriminator loss: 0.466758, acc: 0.773438] [adversarial loss: 1.128559, acc: 0.296875]\n",
      "3880: [discriminator loss: 0.516170, acc: 0.757812] [adversarial loss: 1.644412, acc: 0.125000]\n",
      "3881: [discriminator loss: 0.470194, acc: 0.765625] [adversarial loss: 1.217491, acc: 0.203125]\n",
      "3882: [discriminator loss: 0.478201, acc: 0.757812] [adversarial loss: 1.496404, acc: 0.203125]\n",
      "3883: [discriminator loss: 0.522431, acc: 0.757812] [adversarial loss: 1.376248, acc: 0.156250]\n",
      "3884: [discriminator loss: 0.417819, acc: 0.820312] [adversarial loss: 1.239949, acc: 0.203125]\n",
      "3885: [discriminator loss: 0.503073, acc: 0.773438] [adversarial loss: 1.061733, acc: 0.250000]\n",
      "3886: [discriminator loss: 0.503267, acc: 0.687500] [adversarial loss: 1.845206, acc: 0.046875]\n",
      "3887: [discriminator loss: 0.536999, acc: 0.726562] [adversarial loss: 0.752045, acc: 0.578125]\n",
      "3888: [discriminator loss: 0.533381, acc: 0.726562] [adversarial loss: 1.799846, acc: 0.093750]\n",
      "3889: [discriminator loss: 0.515121, acc: 0.765625] [adversarial loss: 1.155746, acc: 0.328125]\n",
      "3890: [discriminator loss: 0.521192, acc: 0.703125] [adversarial loss: 1.788619, acc: 0.046875]\n",
      "3891: [discriminator loss: 0.525752, acc: 0.687500] [adversarial loss: 0.941406, acc: 0.375000]\n",
      "3892: [discriminator loss: 0.531984, acc: 0.734375] [adversarial loss: 1.700932, acc: 0.125000]\n",
      "3893: [discriminator loss: 0.496688, acc: 0.734375] [adversarial loss: 1.166716, acc: 0.296875]\n",
      "3894: [discriminator loss: 0.567491, acc: 0.757812] [adversarial loss: 1.441728, acc: 0.203125]\n",
      "3895: [discriminator loss: 0.480796, acc: 0.757812] [adversarial loss: 1.025833, acc: 0.343750]\n",
      "3896: [discriminator loss: 0.492092, acc: 0.765625] [adversarial loss: 1.731334, acc: 0.140625]\n",
      "3897: [discriminator loss: 0.509095, acc: 0.734375] [adversarial loss: 0.943652, acc: 0.359375]\n",
      "3898: [discriminator loss: 0.469213, acc: 0.765625] [adversarial loss: 1.801584, acc: 0.046875]\n",
      "3899: [discriminator loss: 0.476205, acc: 0.765625] [adversarial loss: 1.326619, acc: 0.218750]\n",
      "3900: [discriminator loss: 0.470429, acc: 0.773438] [adversarial loss: 1.428925, acc: 0.187500]\n",
      "3901: [discriminator loss: 0.486508, acc: 0.742188] [adversarial loss: 1.556966, acc: 0.125000]\n",
      "3902: [discriminator loss: 0.545133, acc: 0.734375] [adversarial loss: 1.055503, acc: 0.312500]\n",
      "3903: [discriminator loss: 0.422944, acc: 0.804688] [adversarial loss: 1.499497, acc: 0.156250]\n",
      "3904: [discriminator loss: 0.467119, acc: 0.796875] [adversarial loss: 1.468838, acc: 0.093750]\n",
      "3905: [discriminator loss: 0.504552, acc: 0.742188] [adversarial loss: 1.200487, acc: 0.234375]\n",
      "3906: [discriminator loss: 0.477616, acc: 0.773438] [adversarial loss: 1.437309, acc: 0.125000]\n",
      "3907: [discriminator loss: 0.470795, acc: 0.750000] [adversarial loss: 1.355609, acc: 0.125000]\n",
      "3908: [discriminator loss: 0.583091, acc: 0.703125] [adversarial loss: 1.453327, acc: 0.203125]\n",
      "3909: [discriminator loss: 0.406381, acc: 0.796875] [adversarial loss: 1.202688, acc: 0.203125]\n",
      "3910: [discriminator loss: 0.580110, acc: 0.718750] [adversarial loss: 1.567478, acc: 0.156250]\n",
      "3911: [discriminator loss: 0.512267, acc: 0.718750] [adversarial loss: 1.052018, acc: 0.281250]\n",
      "3912: [discriminator loss: 0.503713, acc: 0.734375] [adversarial loss: 1.537790, acc: 0.203125]\n",
      "3913: [discriminator loss: 0.516968, acc: 0.726562] [adversarial loss: 1.485260, acc: 0.125000]\n",
      "3914: [discriminator loss: 0.476055, acc: 0.750000] [adversarial loss: 1.189513, acc: 0.250000]\n",
      "3915: [discriminator loss: 0.536116, acc: 0.765625] [adversarial loss: 1.710068, acc: 0.109375]\n",
      "3916: [discriminator loss: 0.456969, acc: 0.765625] [adversarial loss: 1.192147, acc: 0.250000]\n",
      "3917: [discriminator loss: 0.558184, acc: 0.726562] [adversarial loss: 1.458593, acc: 0.125000]\n",
      "3918: [discriminator loss: 0.486737, acc: 0.789062] [adversarial loss: 0.950871, acc: 0.406250]\n",
      "3919: [discriminator loss: 0.452416, acc: 0.804688] [adversarial loss: 2.241511, acc: 0.046875]\n",
      "3920: [discriminator loss: 0.548816, acc: 0.726562] [adversarial loss: 0.703306, acc: 0.578125]\n",
      "3921: [discriminator loss: 0.647592, acc: 0.617188] [adversarial loss: 2.129528, acc: 0.062500]\n",
      "3922: [discriminator loss: 0.518607, acc: 0.734375] [adversarial loss: 1.151184, acc: 0.375000]\n",
      "3923: [discriminator loss: 0.464489, acc: 0.804688] [adversarial loss: 1.650313, acc: 0.125000]\n",
      "3924: [discriminator loss: 0.470993, acc: 0.765625] [adversarial loss: 1.113400, acc: 0.234375]\n",
      "3925: [discriminator loss: 0.469199, acc: 0.773438] [adversarial loss: 1.682635, acc: 0.093750]\n",
      "3926: [discriminator loss: 0.473659, acc: 0.757812] [adversarial loss: 1.226230, acc: 0.218750]\n",
      "3927: [discriminator loss: 0.482850, acc: 0.734375] [adversarial loss: 1.179148, acc: 0.218750]\n",
      "3928: [discriminator loss: 0.438367, acc: 0.812500] [adversarial loss: 1.307539, acc: 0.171875]\n",
      "3929: [discriminator loss: 0.452376, acc: 0.781250] [adversarial loss: 1.214599, acc: 0.187500]\n",
      "3930: [discriminator loss: 0.526350, acc: 0.757812] [adversarial loss: 1.227320, acc: 0.218750]\n",
      "3931: [discriminator loss: 0.485052, acc: 0.765625] [adversarial loss: 1.194763, acc: 0.218750]\n",
      "3932: [discriminator loss: 0.464626, acc: 0.773438] [adversarial loss: 1.591616, acc: 0.125000]\n",
      "3933: [discriminator loss: 0.429076, acc: 0.812500] [adversarial loss: 1.194265, acc: 0.265625]\n",
      "3934: [discriminator loss: 0.493946, acc: 0.773438] [adversarial loss: 1.454954, acc: 0.187500]\n",
      "3935: [discriminator loss: 0.461391, acc: 0.765625] [adversarial loss: 1.468837, acc: 0.171875]\n",
      "3936: [discriminator loss: 0.550444, acc: 0.718750] [adversarial loss: 1.080936, acc: 0.296875]\n",
      "3937: [discriminator loss: 0.584439, acc: 0.734375] [adversarial loss: 2.030347, acc: 0.062500]\n",
      "3938: [discriminator loss: 0.608028, acc: 0.695312] [adversarial loss: 0.746883, acc: 0.484375]\n",
      "3939: [discriminator loss: 0.547477, acc: 0.742188] [adversarial loss: 1.753284, acc: 0.062500]\n",
      "3940: [discriminator loss: 0.469566, acc: 0.765625] [adversarial loss: 1.031196, acc: 0.312500]\n",
      "3941: [discriminator loss: 0.522948, acc: 0.742188] [adversarial loss: 1.274658, acc: 0.203125]\n",
      "3942: [discriminator loss: 0.485044, acc: 0.789062] [adversarial loss: 1.626065, acc: 0.093750]\n",
      "3943: [discriminator loss: 0.551585, acc: 0.726562] [adversarial loss: 0.941978, acc: 0.390625]\n",
      "3944: [discriminator loss: 0.578215, acc: 0.742188] [adversarial loss: 1.871551, acc: 0.078125]\n",
      "3945: [discriminator loss: 0.488440, acc: 0.773438] [adversarial loss: 1.200989, acc: 0.265625]\n",
      "3946: [discriminator loss: 0.508458, acc: 0.742188] [adversarial loss: 1.848792, acc: 0.093750]\n",
      "3947: [discriminator loss: 0.510931, acc: 0.765625] [adversarial loss: 1.114031, acc: 0.281250]\n",
      "3948: [discriminator loss: 0.593882, acc: 0.679688] [adversarial loss: 1.714539, acc: 0.140625]\n",
      "3949: [discriminator loss: 0.465217, acc: 0.812500] [adversarial loss: 1.106941, acc: 0.343750]\n",
      "3950: [discriminator loss: 0.504332, acc: 0.703125] [adversarial loss: 1.837392, acc: 0.109375]\n",
      "3951: [discriminator loss: 0.553859, acc: 0.703125] [adversarial loss: 0.841810, acc: 0.453125]\n",
      "3952: [discriminator loss: 0.555055, acc: 0.679688] [adversarial loss: 1.838016, acc: 0.078125]\n",
      "3953: [discriminator loss: 0.513006, acc: 0.726562] [adversarial loss: 1.316045, acc: 0.218750]\n",
      "3954: [discriminator loss: 0.502260, acc: 0.726562] [adversarial loss: 1.539664, acc: 0.203125]\n",
      "3955: [discriminator loss: 0.434063, acc: 0.820312] [adversarial loss: 1.440272, acc: 0.234375]\n",
      "3956: [discriminator loss: 0.448932, acc: 0.765625] [adversarial loss: 1.481011, acc: 0.218750]\n",
      "3957: [discriminator loss: 0.493866, acc: 0.757812] [adversarial loss: 1.557565, acc: 0.156250]\n",
      "3958: [discriminator loss: 0.432466, acc: 0.796875] [adversarial loss: 1.158247, acc: 0.250000]\n",
      "3959: [discriminator loss: 0.443795, acc: 0.812500] [adversarial loss: 1.616923, acc: 0.093750]\n",
      "3960: [discriminator loss: 0.492714, acc: 0.773438] [adversarial loss: 1.176990, acc: 0.218750]\n",
      "3961: [discriminator loss: 0.547295, acc: 0.742188] [adversarial loss: 1.656474, acc: 0.171875]\n",
      "3962: [discriminator loss: 0.485765, acc: 0.765625] [adversarial loss: 1.323961, acc: 0.250000]\n",
      "3963: [discriminator loss: 0.486163, acc: 0.773438] [adversarial loss: 1.382489, acc: 0.281250]\n",
      "3964: [discriminator loss: 0.420730, acc: 0.828125] [adversarial loss: 1.733851, acc: 0.140625]\n",
      "3965: [discriminator loss: 0.465984, acc: 0.765625] [adversarial loss: 1.248866, acc: 0.312500]\n",
      "3966: [discriminator loss: 0.467817, acc: 0.750000] [adversarial loss: 1.480304, acc: 0.187500]\n",
      "3967: [discriminator loss: 0.457853, acc: 0.804688] [adversarial loss: 0.982530, acc: 0.359375]\n",
      "3968: [discriminator loss: 0.478958, acc: 0.789062] [adversarial loss: 1.585382, acc: 0.140625]\n",
      "3969: [discriminator loss: 0.528388, acc: 0.757812] [adversarial loss: 1.440322, acc: 0.171875]\n",
      "3970: [discriminator loss: 0.514094, acc: 0.773438] [adversarial loss: 1.088004, acc: 0.265625]\n",
      "3971: [discriminator loss: 0.470489, acc: 0.781250] [adversarial loss: 1.791775, acc: 0.093750]\n",
      "3972: [discriminator loss: 0.467473, acc: 0.773438] [adversarial loss: 1.188055, acc: 0.359375]\n",
      "3973: [discriminator loss: 0.451914, acc: 0.781250] [adversarial loss: 1.893048, acc: 0.046875]\n",
      "3974: [discriminator loss: 0.493059, acc: 0.734375] [adversarial loss: 1.106229, acc: 0.281250]\n",
      "3975: [discriminator loss: 0.465309, acc: 0.796875] [adversarial loss: 1.700894, acc: 0.093750]\n",
      "3976: [discriminator loss: 0.485503, acc: 0.773438] [adversarial loss: 0.864313, acc: 0.500000]\n",
      "3977: [discriminator loss: 0.530618, acc: 0.710938] [adversarial loss: 1.770079, acc: 0.125000]\n",
      "3978: [discriminator loss: 0.572452, acc: 0.648438] [adversarial loss: 1.106376, acc: 0.281250]\n",
      "3979: [discriminator loss: 0.403063, acc: 0.789062] [adversarial loss: 1.559542, acc: 0.125000]\n",
      "3980: [discriminator loss: 0.531436, acc: 0.687500] [adversarial loss: 1.200292, acc: 0.312500]\n",
      "3981: [discriminator loss: 0.501965, acc: 0.757812] [adversarial loss: 1.786395, acc: 0.062500]\n",
      "3982: [discriminator loss: 0.498325, acc: 0.742188] [adversarial loss: 1.013954, acc: 0.359375]\n",
      "3983: [discriminator loss: 0.511332, acc: 0.726562] [adversarial loss: 1.622805, acc: 0.171875]\n",
      "3984: [discriminator loss: 0.480340, acc: 0.750000] [adversarial loss: 1.319197, acc: 0.312500]\n",
      "3985: [discriminator loss: 0.545808, acc: 0.718750] [adversarial loss: 1.710919, acc: 0.109375]\n",
      "3986: [discriminator loss: 0.476225, acc: 0.765625] [adversarial loss: 1.148025, acc: 0.296875]\n",
      "3987: [discriminator loss: 0.480325, acc: 0.789062] [adversarial loss: 1.322129, acc: 0.203125]\n",
      "3988: [discriminator loss: 0.477609, acc: 0.812500] [adversarial loss: 1.504359, acc: 0.171875]\n",
      "3989: [discriminator loss: 0.501219, acc: 0.757812] [adversarial loss: 1.184578, acc: 0.296875]\n",
      "3990: [discriminator loss: 0.514024, acc: 0.781250] [adversarial loss: 1.273746, acc: 0.265625]\n",
      "3991: [discriminator loss: 0.588122, acc: 0.750000] [adversarial loss: 1.482718, acc: 0.156250]\n",
      "3992: [discriminator loss: 0.411783, acc: 0.796875] [adversarial loss: 1.311528, acc: 0.265625]\n",
      "3993: [discriminator loss: 0.497417, acc: 0.742188] [adversarial loss: 1.570942, acc: 0.171875]\n",
      "3994: [discriminator loss: 0.483323, acc: 0.757812] [adversarial loss: 1.267538, acc: 0.234375]\n",
      "3995: [discriminator loss: 0.544579, acc: 0.695312] [adversarial loss: 1.637681, acc: 0.187500]\n",
      "3996: [discriminator loss: 0.485156, acc: 0.750000] [adversarial loss: 1.116436, acc: 0.265625]\n",
      "3997: [discriminator loss: 0.546337, acc: 0.718750] [adversarial loss: 2.320624, acc: 0.031250]\n",
      "3998: [discriminator loss: 0.710539, acc: 0.656250] [adversarial loss: 0.687085, acc: 0.625000]\n",
      "3999: [discriminator loss: 0.585936, acc: 0.687500] [adversarial loss: 1.835155, acc: 0.093750]\n",
      "4000: [discriminator loss: 0.428241, acc: 0.804688] [adversarial loss: 1.213937, acc: 0.218750]\n",
      "4001: [discriminator loss: 0.526610, acc: 0.742188] [adversarial loss: 1.183873, acc: 0.296875]\n",
      "4002: [discriminator loss: 0.539294, acc: 0.726562] [adversarial loss: 1.863244, acc: 0.046875]\n",
      "4003: [discriminator loss: 0.516538, acc: 0.765625] [adversarial loss: 1.224105, acc: 0.265625]\n",
      "4004: [discriminator loss: 0.465170, acc: 0.757812] [adversarial loss: 1.552054, acc: 0.093750]\n",
      "4005: [discriminator loss: 0.412333, acc: 0.789062] [adversarial loss: 1.309443, acc: 0.265625]\n",
      "4006: [discriminator loss: 0.525255, acc: 0.765625] [adversarial loss: 1.282172, acc: 0.187500]\n",
      "4007: [discriminator loss: 0.437722, acc: 0.789062] [adversarial loss: 1.519188, acc: 0.171875]\n",
      "4008: [discriminator loss: 0.510576, acc: 0.726562] [adversarial loss: 1.126038, acc: 0.296875]\n",
      "4009: [discriminator loss: 0.558144, acc: 0.656250] [adversarial loss: 2.050058, acc: 0.031250]\n",
      "4010: [discriminator loss: 0.466303, acc: 0.765625] [adversarial loss: 1.252490, acc: 0.281250]\n",
      "4011: [discriminator loss: 0.568919, acc: 0.710938] [adversarial loss: 1.666637, acc: 0.109375]\n",
      "4012: [discriminator loss: 0.546876, acc: 0.710938] [adversarial loss: 1.342643, acc: 0.171875]\n",
      "4013: [discriminator loss: 0.486724, acc: 0.765625] [adversarial loss: 1.346041, acc: 0.218750]\n",
      "4014: [discriminator loss: 0.446824, acc: 0.804688] [adversarial loss: 1.227067, acc: 0.171875]\n",
      "4015: [discriminator loss: 0.456507, acc: 0.789062] [adversarial loss: 1.498445, acc: 0.093750]\n",
      "4016: [discriminator loss: 0.526185, acc: 0.718750] [adversarial loss: 1.336781, acc: 0.203125]\n",
      "4017: [discriminator loss: 0.488264, acc: 0.765625] [adversarial loss: 1.420602, acc: 0.171875]\n",
      "4018: [discriminator loss: 0.502851, acc: 0.765625] [adversarial loss: 1.208760, acc: 0.343750]\n",
      "4019: [discriminator loss: 0.520494, acc: 0.742188] [adversarial loss: 1.845221, acc: 0.125000]\n",
      "4020: [discriminator loss: 0.534523, acc: 0.710938] [adversarial loss: 1.088049, acc: 0.296875]\n",
      "4021: [discriminator loss: 0.536049, acc: 0.703125] [adversarial loss: 1.606182, acc: 0.109375]\n",
      "4022: [discriminator loss: 0.470470, acc: 0.781250] [adversarial loss: 1.266399, acc: 0.281250]\n",
      "4023: [discriminator loss: 0.541065, acc: 0.757812] [adversarial loss: 1.310204, acc: 0.250000]\n",
      "4024: [discriminator loss: 0.511743, acc: 0.710938] [adversarial loss: 1.134803, acc: 0.312500]\n",
      "4025: [discriminator loss: 0.519659, acc: 0.718750] [adversarial loss: 1.300749, acc: 0.140625]\n",
      "4026: [discriminator loss: 0.529701, acc: 0.765625] [adversarial loss: 1.408519, acc: 0.109375]\n",
      "4027: [discriminator loss: 0.449425, acc: 0.765625] [adversarial loss: 1.267872, acc: 0.234375]\n",
      "4028: [discriminator loss: 0.564824, acc: 0.703125] [adversarial loss: 1.864644, acc: 0.109375]\n",
      "4029: [discriminator loss: 0.563848, acc: 0.703125] [adversarial loss: 0.893553, acc: 0.421875]\n",
      "4030: [discriminator loss: 0.546422, acc: 0.726562] [adversarial loss: 2.082214, acc: 0.031250]\n",
      "4031: [discriminator loss: 0.487494, acc: 0.726562] [adversarial loss: 1.063007, acc: 0.359375]\n",
      "4032: [discriminator loss: 0.480019, acc: 0.773438] [adversarial loss: 1.733639, acc: 0.093750]\n",
      "4033: [discriminator loss: 0.531512, acc: 0.765625] [adversarial loss: 1.242138, acc: 0.265625]\n",
      "4034: [discriminator loss: 0.475729, acc: 0.710938] [adversarial loss: 1.289929, acc: 0.187500]\n",
      "4035: [discriminator loss: 0.516897, acc: 0.734375] [adversarial loss: 1.244667, acc: 0.250000]\n",
      "4036: [discriminator loss: 0.493125, acc: 0.742188] [adversarial loss: 0.968732, acc: 0.437500]\n",
      "4037: [discriminator loss: 0.536026, acc: 0.710938] [adversarial loss: 1.820390, acc: 0.078125]\n",
      "4038: [discriminator loss: 0.494815, acc: 0.742188] [adversarial loss: 1.099329, acc: 0.328125]\n",
      "4039: [discriminator loss: 0.466595, acc: 0.781250] [adversarial loss: 1.150134, acc: 0.281250]\n",
      "4040: [discriminator loss: 0.499245, acc: 0.789062] [adversarial loss: 1.500284, acc: 0.187500]\n",
      "4041: [discriminator loss: 0.509919, acc: 0.734375] [adversarial loss: 1.149747, acc: 0.296875]\n",
      "4042: [discriminator loss: 0.545808, acc: 0.734375] [adversarial loss: 1.092779, acc: 0.296875]\n",
      "4043: [discriminator loss: 0.450751, acc: 0.781250] [adversarial loss: 1.234207, acc: 0.218750]\n",
      "4044: [discriminator loss: 0.505075, acc: 0.757812] [adversarial loss: 1.815312, acc: 0.062500]\n",
      "4045: [discriminator loss: 0.506182, acc: 0.757812] [adversarial loss: 1.097076, acc: 0.265625]\n",
      "4046: [discriminator loss: 0.537912, acc: 0.703125] [adversarial loss: 1.817771, acc: 0.093750]\n",
      "4047: [discriminator loss: 0.614628, acc: 0.695312] [adversarial loss: 0.750106, acc: 0.562500]\n",
      "4048: [discriminator loss: 0.541010, acc: 0.656250] [adversarial loss: 1.874802, acc: 0.078125]\n",
      "4049: [discriminator loss: 0.450246, acc: 0.773438] [adversarial loss: 1.183261, acc: 0.281250]\n",
      "4050: [discriminator loss: 0.574425, acc: 0.671875] [adversarial loss: 1.304027, acc: 0.187500]\n",
      "4051: [discriminator loss: 0.482225, acc: 0.757812] [adversarial loss: 1.223466, acc: 0.234375]\n",
      "4052: [discriminator loss: 0.567342, acc: 0.734375] [adversarial loss: 1.147876, acc: 0.328125]\n",
      "4053: [discriminator loss: 0.527973, acc: 0.734375] [adversarial loss: 1.513223, acc: 0.187500]\n",
      "4054: [discriminator loss: 0.528941, acc: 0.734375] [adversarial loss: 1.161129, acc: 0.234375]\n",
      "4055: [discriminator loss: 0.455034, acc: 0.804688] [adversarial loss: 1.430294, acc: 0.203125]\n",
      "4056: [discriminator loss: 0.465866, acc: 0.757812] [adversarial loss: 0.988290, acc: 0.375000]\n",
      "4057: [discriminator loss: 0.527905, acc: 0.726562] [adversarial loss: 1.932406, acc: 0.093750]\n",
      "4058: [discriminator loss: 0.457369, acc: 0.742188] [adversarial loss: 1.152810, acc: 0.250000]\n",
      "4059: [discriminator loss: 0.471845, acc: 0.796875] [adversarial loss: 1.715084, acc: 0.140625]\n",
      "4060: [discriminator loss: 0.532171, acc: 0.703125] [adversarial loss: 1.074893, acc: 0.296875]\n",
      "4061: [discriminator loss: 0.503135, acc: 0.742188] [adversarial loss: 1.657927, acc: 0.093750]\n",
      "4062: [discriminator loss: 0.562825, acc: 0.734375] [adversarial loss: 1.004799, acc: 0.375000]\n",
      "4063: [discriminator loss: 0.527823, acc: 0.703125] [adversarial loss: 1.728786, acc: 0.078125]\n",
      "4064: [discriminator loss: 0.475490, acc: 0.773438] [adversarial loss: 1.170916, acc: 0.296875]\n",
      "4065: [discriminator loss: 0.464975, acc: 0.789062] [adversarial loss: 1.547736, acc: 0.109375]\n",
      "4066: [discriminator loss: 0.535695, acc: 0.718750] [adversarial loss: 1.171086, acc: 0.281250]\n",
      "4067: [discriminator loss: 0.500358, acc: 0.734375] [adversarial loss: 1.866329, acc: 0.093750]\n",
      "4068: [discriminator loss: 0.436756, acc: 0.812500] [adversarial loss: 1.032277, acc: 0.312500]\n",
      "4069: [discriminator loss: 0.527505, acc: 0.703125] [adversarial loss: 1.580010, acc: 0.093750]\n",
      "4070: [discriminator loss: 0.500171, acc: 0.750000] [adversarial loss: 1.067463, acc: 0.281250]\n",
      "4071: [discriminator loss: 0.545696, acc: 0.773438] [adversarial loss: 1.575484, acc: 0.156250]\n",
      "4072: [discriminator loss: 0.477123, acc: 0.789062] [adversarial loss: 1.164641, acc: 0.343750]\n",
      "4073: [discriminator loss: 0.547436, acc: 0.726562] [adversarial loss: 1.468341, acc: 0.187500]\n",
      "4074: [discriminator loss: 0.525533, acc: 0.703125] [adversarial loss: 0.956321, acc: 0.390625]\n",
      "4075: [discriminator loss: 0.558762, acc: 0.695312] [adversarial loss: 1.785204, acc: 0.125000]\n",
      "4076: [discriminator loss: 0.562161, acc: 0.664062] [adversarial loss: 0.875049, acc: 0.500000]\n",
      "4077: [discriminator loss: 0.526131, acc: 0.742188] [adversarial loss: 1.734413, acc: 0.109375]\n",
      "4078: [discriminator loss: 0.527090, acc: 0.710938] [adversarial loss: 1.281807, acc: 0.203125]\n",
      "4079: [discriminator loss: 0.551774, acc: 0.718750] [adversarial loss: 1.817071, acc: 0.078125]\n",
      "4080: [discriminator loss: 0.468030, acc: 0.773438] [adversarial loss: 1.098450, acc: 0.312500]\n",
      "4081: [discriminator loss: 0.461741, acc: 0.796875] [adversarial loss: 1.999677, acc: 0.062500]\n",
      "4082: [discriminator loss: 0.505417, acc: 0.750000] [adversarial loss: 1.389799, acc: 0.203125]\n",
      "4083: [discriminator loss: 0.597232, acc: 0.734375] [adversarial loss: 1.375664, acc: 0.203125]\n",
      "4084: [discriminator loss: 0.461967, acc: 0.781250] [adversarial loss: 1.391039, acc: 0.187500]\n",
      "4085: [discriminator loss: 0.538881, acc: 0.703125] [adversarial loss: 1.520042, acc: 0.140625]\n",
      "4086: [discriminator loss: 0.569073, acc: 0.726562] [adversarial loss: 1.128191, acc: 0.312500]\n",
      "4087: [discriminator loss: 0.447554, acc: 0.757812] [adversarial loss: 1.429694, acc: 0.234375]\n",
      "4088: [discriminator loss: 0.546716, acc: 0.718750] [adversarial loss: 1.117726, acc: 0.312500]\n",
      "4089: [discriminator loss: 0.419578, acc: 0.828125] [adversarial loss: 1.580850, acc: 0.109375]\n",
      "4090: [discriminator loss: 0.486808, acc: 0.742188] [adversarial loss: 1.046107, acc: 0.296875]\n",
      "4091: [discriminator loss: 0.472903, acc: 0.773438] [adversarial loss: 1.432346, acc: 0.093750]\n",
      "4092: [discriminator loss: 0.457607, acc: 0.796875] [adversarial loss: 1.017764, acc: 0.328125]\n",
      "4093: [discriminator loss: 0.623454, acc: 0.703125] [adversarial loss: 1.794161, acc: 0.156250]\n",
      "4094: [discriminator loss: 0.501141, acc: 0.750000] [adversarial loss: 0.882571, acc: 0.390625]\n",
      "4095: [discriminator loss: 0.633892, acc: 0.648438] [adversarial loss: 1.911193, acc: 0.078125]\n",
      "4096: [discriminator loss: 0.530396, acc: 0.750000] [adversarial loss: 1.097348, acc: 0.281250]\n",
      "4097: [discriminator loss: 0.509460, acc: 0.789062] [adversarial loss: 1.478611, acc: 0.171875]\n",
      "4098: [discriminator loss: 0.497464, acc: 0.781250] [adversarial loss: 1.245848, acc: 0.296875]\n",
      "4099: [discriminator loss: 0.507632, acc: 0.757812] [adversarial loss: 0.820322, acc: 0.453125]\n",
      "4100: [discriminator loss: 0.507031, acc: 0.710938] [adversarial loss: 1.359682, acc: 0.250000]\n",
      "4101: [discriminator loss: 0.457027, acc: 0.773438] [adversarial loss: 1.145582, acc: 0.265625]\n",
      "4102: [discriminator loss: 0.565459, acc: 0.687500] [adversarial loss: 1.893684, acc: 0.109375]\n",
      "4103: [discriminator loss: 0.564090, acc: 0.687500] [adversarial loss: 0.913088, acc: 0.406250]\n",
      "4104: [discriminator loss: 0.506518, acc: 0.734375] [adversarial loss: 1.189943, acc: 0.265625]\n",
      "4105: [discriminator loss: 0.514248, acc: 0.726562] [adversarial loss: 1.262151, acc: 0.203125]\n",
      "4106: [discriminator loss: 0.486363, acc: 0.750000] [adversarial loss: 1.488762, acc: 0.171875]\n",
      "4107: [discriminator loss: 0.595913, acc: 0.687500] [adversarial loss: 1.186473, acc: 0.218750]\n",
      "4108: [discriminator loss: 0.457153, acc: 0.773438] [adversarial loss: 1.476579, acc: 0.171875]\n",
      "4109: [discriminator loss: 0.415488, acc: 0.851562] [adversarial loss: 1.292278, acc: 0.234375]\n",
      "4110: [discriminator loss: 0.470721, acc: 0.804688] [adversarial loss: 1.223241, acc: 0.250000]\n",
      "4111: [discriminator loss: 0.502098, acc: 0.765625] [adversarial loss: 1.235682, acc: 0.171875]\n",
      "4112: [discriminator loss: 0.372106, acc: 0.859375] [adversarial loss: 1.621951, acc: 0.156250]\n",
      "4113: [discriminator loss: 0.498813, acc: 0.750000] [adversarial loss: 1.190750, acc: 0.281250]\n",
      "4114: [discriminator loss: 0.504594, acc: 0.742188] [adversarial loss: 1.485946, acc: 0.171875]\n",
      "4115: [discriminator loss: 0.536963, acc: 0.710938] [adversarial loss: 1.146048, acc: 0.375000]\n",
      "4116: [discriminator loss: 0.513496, acc: 0.695312] [adversarial loss: 1.823604, acc: 0.078125]\n",
      "4117: [discriminator loss: 0.555642, acc: 0.742188] [adversarial loss: 0.832992, acc: 0.500000]\n",
      "4118: [discriminator loss: 0.537442, acc: 0.710938] [adversarial loss: 1.933416, acc: 0.031250]\n",
      "4119: [discriminator loss: 0.578519, acc: 0.687500] [adversarial loss: 0.932400, acc: 0.406250]\n",
      "4120: [discriminator loss: 0.600217, acc: 0.679688] [adversarial loss: 1.946660, acc: 0.062500]\n",
      "4121: [discriminator loss: 0.547905, acc: 0.726562] [adversarial loss: 1.021713, acc: 0.312500]\n",
      "4122: [discriminator loss: 0.490688, acc: 0.781250] [adversarial loss: 1.649179, acc: 0.171875]\n",
      "4123: [discriminator loss: 0.517468, acc: 0.757812] [adversarial loss: 1.284955, acc: 0.250000]\n",
      "4124: [discriminator loss: 0.471896, acc: 0.812500] [adversarial loss: 1.362419, acc: 0.156250]\n",
      "4125: [discriminator loss: 0.462920, acc: 0.757812] [adversarial loss: 0.953422, acc: 0.390625]\n",
      "4126: [discriminator loss: 0.449539, acc: 0.796875] [adversarial loss: 1.564742, acc: 0.125000]\n",
      "4127: [discriminator loss: 0.539517, acc: 0.726562] [adversarial loss: 1.446041, acc: 0.125000]\n",
      "4128: [discriminator loss: 0.517549, acc: 0.750000] [adversarial loss: 1.560967, acc: 0.140625]\n",
      "4129: [discriminator loss: 0.550888, acc: 0.734375] [adversarial loss: 0.932987, acc: 0.375000]\n",
      "4130: [discriminator loss: 0.483794, acc: 0.710938] [adversarial loss: 1.490268, acc: 0.140625]\n",
      "4131: [discriminator loss: 0.546203, acc: 0.695312] [adversarial loss: 1.045261, acc: 0.359375]\n",
      "4132: [discriminator loss: 0.544399, acc: 0.765625] [adversarial loss: 1.673002, acc: 0.078125]\n",
      "4133: [discriminator loss: 0.551049, acc: 0.710938] [adversarial loss: 1.233538, acc: 0.234375]\n",
      "4134: [discriminator loss: 0.450639, acc: 0.812500] [adversarial loss: 1.428642, acc: 0.203125]\n",
      "4135: [discriminator loss: 0.465255, acc: 0.734375] [adversarial loss: 1.229765, acc: 0.156250]\n",
      "4136: [discriminator loss: 0.509661, acc: 0.734375] [adversarial loss: 1.546708, acc: 0.046875]\n",
      "4137: [discriminator loss: 0.468621, acc: 0.820312] [adversarial loss: 1.302464, acc: 0.218750]\n",
      "4138: [discriminator loss: 0.417690, acc: 0.835938] [adversarial loss: 1.397687, acc: 0.234375]\n",
      "4139: [discriminator loss: 0.482431, acc: 0.742188] [adversarial loss: 0.971002, acc: 0.312500]\n",
      "4140: [discriminator loss: 0.514426, acc: 0.734375] [adversarial loss: 1.864483, acc: 0.062500]\n",
      "4141: [discriminator loss: 0.627103, acc: 0.671875] [adversarial loss: 0.842082, acc: 0.500000]\n",
      "4142: [discriminator loss: 0.577520, acc: 0.695312] [adversarial loss: 1.892099, acc: 0.062500]\n",
      "4143: [discriminator loss: 0.529889, acc: 0.750000] [adversarial loss: 1.237423, acc: 0.234375]\n",
      "4144: [discriminator loss: 0.538117, acc: 0.671875] [adversarial loss: 1.852913, acc: 0.031250]\n",
      "4145: [discriminator loss: 0.485628, acc: 0.750000] [adversarial loss: 1.131878, acc: 0.281250]\n",
      "4146: [discriminator loss: 0.449408, acc: 0.789062] [adversarial loss: 1.743892, acc: 0.078125]\n",
      "4147: [discriminator loss: 0.563841, acc: 0.695312] [adversarial loss: 1.203864, acc: 0.328125]\n",
      "4148: [discriminator loss: 0.455036, acc: 0.742188] [adversarial loss: 1.697520, acc: 0.062500]\n",
      "4149: [discriminator loss: 0.423776, acc: 0.789062] [adversarial loss: 1.258950, acc: 0.140625]\n",
      "4150: [discriminator loss: 0.469061, acc: 0.781250] [adversarial loss: 1.461679, acc: 0.156250]\n",
      "4151: [discriminator loss: 0.592141, acc: 0.703125] [adversarial loss: 1.104057, acc: 0.343750]\n",
      "4152: [discriminator loss: 0.508362, acc: 0.742188] [adversarial loss: 1.621537, acc: 0.078125]\n",
      "4153: [discriminator loss: 0.481238, acc: 0.750000] [adversarial loss: 1.042377, acc: 0.359375]\n",
      "4154: [discriminator loss: 0.521742, acc: 0.734375] [adversarial loss: 1.687553, acc: 0.093750]\n",
      "4155: [discriminator loss: 0.534580, acc: 0.742188] [adversarial loss: 1.033981, acc: 0.343750]\n",
      "4156: [discriminator loss: 0.474522, acc: 0.757812] [adversarial loss: 1.634819, acc: 0.156250]\n",
      "4157: [discriminator loss: 0.561251, acc: 0.703125] [adversarial loss: 1.087765, acc: 0.328125]\n",
      "4158: [discriminator loss: 0.556088, acc: 0.726562] [adversarial loss: 1.374208, acc: 0.218750]\n",
      "4159: [discriminator loss: 0.542682, acc: 0.734375] [adversarial loss: 1.563736, acc: 0.078125]\n",
      "4160: [discriminator loss: 0.517334, acc: 0.734375] [adversarial loss: 0.942146, acc: 0.375000]\n",
      "4161: [discriminator loss: 0.517846, acc: 0.750000] [adversarial loss: 1.312454, acc: 0.296875]\n",
      "4162: [discriminator loss: 0.520050, acc: 0.726562] [adversarial loss: 1.279572, acc: 0.218750]\n",
      "4163: [discriminator loss: 0.469507, acc: 0.796875] [adversarial loss: 1.518860, acc: 0.109375]\n",
      "4164: [discriminator loss: 0.424548, acc: 0.820312] [adversarial loss: 1.226265, acc: 0.234375]\n",
      "4165: [discriminator loss: 0.520427, acc: 0.742188] [adversarial loss: 1.380574, acc: 0.125000]\n",
      "4166: [discriminator loss: 0.544636, acc: 0.734375] [adversarial loss: 1.261883, acc: 0.234375]\n",
      "4167: [discriminator loss: 0.457342, acc: 0.781250] [adversarial loss: 1.677528, acc: 0.156250]\n",
      "4168: [discriminator loss: 0.453621, acc: 0.812500] [adversarial loss: 1.399945, acc: 0.281250]\n",
      "4169: [discriminator loss: 0.522292, acc: 0.726562] [adversarial loss: 1.000366, acc: 0.390625]\n",
      "4170: [discriminator loss: 0.591266, acc: 0.664062] [adversarial loss: 2.075577, acc: 0.015625]\n",
      "4171: [discriminator loss: 0.542308, acc: 0.703125] [adversarial loss: 0.969013, acc: 0.421875]\n",
      "4172: [discriminator loss: 0.547421, acc: 0.718750] [adversarial loss: 1.695942, acc: 0.046875]\n",
      "4173: [discriminator loss: 0.517605, acc: 0.789062] [adversarial loss: 0.912887, acc: 0.343750]\n",
      "4174: [discriminator loss: 0.585253, acc: 0.695312] [adversarial loss: 1.367954, acc: 0.109375]\n",
      "4175: [discriminator loss: 0.527161, acc: 0.718750] [adversarial loss: 1.332274, acc: 0.156250]\n",
      "4176: [discriminator loss: 0.544751, acc: 0.742188] [adversarial loss: 1.252846, acc: 0.250000]\n",
      "4177: [discriminator loss: 0.458276, acc: 0.796875] [adversarial loss: 1.339976, acc: 0.171875]\n",
      "4178: [discriminator loss: 0.516157, acc: 0.789062] [adversarial loss: 1.443387, acc: 0.140625]\n",
      "4179: [discriminator loss: 0.452533, acc: 0.804688] [adversarial loss: 1.271259, acc: 0.156250]\n",
      "4180: [discriminator loss: 0.483499, acc: 0.718750] [adversarial loss: 1.427017, acc: 0.203125]\n",
      "4181: [discriminator loss: 0.533418, acc: 0.742188] [adversarial loss: 0.973213, acc: 0.312500]\n",
      "4182: [discriminator loss: 0.602515, acc: 0.687500] [adversarial loss: 1.770339, acc: 0.109375]\n",
      "4183: [discriminator loss: 0.590523, acc: 0.710938] [adversarial loss: 1.300646, acc: 0.218750]\n",
      "4184: [discriminator loss: 0.522210, acc: 0.718750] [adversarial loss: 1.936050, acc: 0.015625]\n",
      "4185: [discriminator loss: 0.547084, acc: 0.710938] [adversarial loss: 0.896722, acc: 0.421875]\n",
      "4186: [discriminator loss: 0.549937, acc: 0.703125] [adversarial loss: 1.725554, acc: 0.125000]\n",
      "4187: [discriminator loss: 0.573235, acc: 0.703125] [adversarial loss: 1.034289, acc: 0.343750]\n",
      "4188: [discriminator loss: 0.518568, acc: 0.726562] [adversarial loss: 1.373076, acc: 0.093750]\n",
      "4189: [discriminator loss: 0.496950, acc: 0.742188] [adversarial loss: 1.242962, acc: 0.203125]\n",
      "4190: [discriminator loss: 0.386544, acc: 0.812500] [adversarial loss: 1.454779, acc: 0.187500]\n",
      "4191: [discriminator loss: 0.443661, acc: 0.812500] [adversarial loss: 1.101992, acc: 0.375000]\n",
      "4192: [discriminator loss: 0.497347, acc: 0.804688] [adversarial loss: 1.470261, acc: 0.125000]\n",
      "4193: [discriminator loss: 0.475697, acc: 0.750000] [adversarial loss: 1.180756, acc: 0.281250]\n",
      "4194: [discriminator loss: 0.526678, acc: 0.726562] [adversarial loss: 1.095186, acc: 0.328125]\n",
      "4195: [discriminator loss: 0.561132, acc: 0.710938] [adversarial loss: 1.352389, acc: 0.156250]\n",
      "4196: [discriminator loss: 0.527018, acc: 0.710938] [adversarial loss: 1.230164, acc: 0.359375]\n",
      "4197: [discriminator loss: 0.592703, acc: 0.671875] [adversarial loss: 1.380990, acc: 0.140625]\n",
      "4198: [discriminator loss: 0.475167, acc: 0.796875] [adversarial loss: 1.025999, acc: 0.328125]\n",
      "4199: [discriminator loss: 0.553238, acc: 0.726562] [adversarial loss: 1.448148, acc: 0.125000]\n",
      "4200: [discriminator loss: 0.569827, acc: 0.679688] [adversarial loss: 1.293904, acc: 0.218750]\n",
      "4201: [discriminator loss: 0.509878, acc: 0.765625] [adversarial loss: 1.204112, acc: 0.281250]\n",
      "4202: [discriminator loss: 0.424351, acc: 0.812500] [adversarial loss: 1.464403, acc: 0.156250]\n",
      "4203: [discriminator loss: 0.534368, acc: 0.718750] [adversarial loss: 0.987900, acc: 0.343750]\n",
      "4204: [discriminator loss: 0.527120, acc: 0.710938] [adversarial loss: 1.423593, acc: 0.171875]\n",
      "4205: [discriminator loss: 0.533480, acc: 0.750000] [adversarial loss: 1.371471, acc: 0.156250]\n",
      "4206: [discriminator loss: 0.545887, acc: 0.742188] [adversarial loss: 1.147135, acc: 0.312500]\n",
      "4207: [discriminator loss: 0.467641, acc: 0.781250] [adversarial loss: 1.455436, acc: 0.203125]\n",
      "4208: [discriminator loss: 0.515808, acc: 0.757812] [adversarial loss: 1.330517, acc: 0.187500]\n",
      "4209: [discriminator loss: 0.535444, acc: 0.750000] [adversarial loss: 1.192992, acc: 0.250000]\n",
      "4210: [discriminator loss: 0.531580, acc: 0.734375] [adversarial loss: 1.581849, acc: 0.156250]\n",
      "4211: [discriminator loss: 0.451568, acc: 0.781250] [adversarial loss: 1.071848, acc: 0.312500]\n",
      "4212: [discriminator loss: 0.556452, acc: 0.757812] [adversarial loss: 1.341952, acc: 0.281250]\n",
      "4213: [discriminator loss: 0.442180, acc: 0.789062] [adversarial loss: 1.515255, acc: 0.140625]\n",
      "4214: [discriminator loss: 0.546183, acc: 0.703125] [adversarial loss: 1.270378, acc: 0.312500]\n",
      "4215: [discriminator loss: 0.417691, acc: 0.812500] [adversarial loss: 1.561350, acc: 0.171875]\n",
      "4216: [discriminator loss: 0.487398, acc: 0.796875] [adversarial loss: 0.750341, acc: 0.531250]\n",
      "4217: [discriminator loss: 0.443386, acc: 0.765625] [adversarial loss: 1.996586, acc: 0.062500]\n",
      "4218: [discriminator loss: 0.520278, acc: 0.726562] [adversarial loss: 1.185407, acc: 0.312500]\n",
      "4219: [discriminator loss: 0.501875, acc: 0.742188] [adversarial loss: 1.844635, acc: 0.062500]\n",
      "4220: [discriminator loss: 0.489908, acc: 0.734375] [adversarial loss: 0.930649, acc: 0.406250]\n",
      "4221: [discriminator loss: 0.582806, acc: 0.664062] [adversarial loss: 1.989863, acc: 0.078125]\n",
      "4222: [discriminator loss: 0.539909, acc: 0.734375] [adversarial loss: 1.035668, acc: 0.359375]\n",
      "4223: [discriminator loss: 0.579603, acc: 0.710938] [adversarial loss: 1.573378, acc: 0.125000]\n",
      "4224: [discriminator loss: 0.483412, acc: 0.765625] [adversarial loss: 1.417697, acc: 0.218750]\n",
      "4225: [discriminator loss: 0.535452, acc: 0.734375] [adversarial loss: 1.206908, acc: 0.234375]\n",
      "4226: [discriminator loss: 0.531315, acc: 0.703125] [adversarial loss: 1.189802, acc: 0.328125]\n",
      "4227: [discriminator loss: 0.486091, acc: 0.757812] [adversarial loss: 1.192500, acc: 0.296875]\n",
      "4228: [discriminator loss: 0.433956, acc: 0.820312] [adversarial loss: 1.288886, acc: 0.234375]\n",
      "4229: [discriminator loss: 0.489042, acc: 0.742188] [adversarial loss: 1.407905, acc: 0.140625]\n",
      "4230: [discriminator loss: 0.543955, acc: 0.710938] [adversarial loss: 1.046220, acc: 0.265625]\n",
      "4231: [discriminator loss: 0.503055, acc: 0.789062] [adversarial loss: 1.856176, acc: 0.062500]\n",
      "4232: [discriminator loss: 0.474405, acc: 0.773438] [adversarial loss: 1.253428, acc: 0.250000]\n",
      "4233: [discriminator loss: 0.485416, acc: 0.757812] [adversarial loss: 1.810711, acc: 0.093750]\n",
      "4234: [discriminator loss: 0.536865, acc: 0.757812] [adversarial loss: 1.144180, acc: 0.234375]\n",
      "4235: [discriminator loss: 0.475341, acc: 0.750000] [adversarial loss: 1.581410, acc: 0.156250]\n",
      "4236: [discriminator loss: 0.456577, acc: 0.757812] [adversarial loss: 1.246170, acc: 0.296875]\n",
      "4237: [discriminator loss: 0.534169, acc: 0.726562] [adversarial loss: 1.718515, acc: 0.078125]\n",
      "4238: [discriminator loss: 0.517382, acc: 0.757812] [adversarial loss: 1.399141, acc: 0.203125]\n",
      "4239: [discriminator loss: 0.567639, acc: 0.718750] [adversarial loss: 1.166680, acc: 0.250000]\n",
      "4240: [discriminator loss: 0.445096, acc: 0.789062] [adversarial loss: 1.402337, acc: 0.187500]\n",
      "4241: [discriminator loss: 0.479762, acc: 0.750000] [adversarial loss: 0.993820, acc: 0.343750]\n",
      "4242: [discriminator loss: 0.493092, acc: 0.804688] [adversarial loss: 1.835466, acc: 0.046875]\n",
      "4243: [discriminator loss: 0.564375, acc: 0.726562] [adversarial loss: 0.987089, acc: 0.359375]\n",
      "4244: [discriminator loss: 0.501528, acc: 0.757812] [adversarial loss: 1.777253, acc: 0.109375]\n",
      "4245: [discriminator loss: 0.584487, acc: 0.695312] [adversarial loss: 1.042126, acc: 0.375000]\n",
      "4246: [discriminator loss: 0.573402, acc: 0.734375] [adversarial loss: 1.758661, acc: 0.046875]\n",
      "4247: [discriminator loss: 0.566577, acc: 0.718750] [adversarial loss: 0.910274, acc: 0.437500]\n",
      "4248: [discriminator loss: 0.507918, acc: 0.734375] [adversarial loss: 1.991786, acc: 0.093750]\n",
      "4249: [discriminator loss: 0.508490, acc: 0.750000] [adversarial loss: 1.111778, acc: 0.343750]\n",
      "4250: [discriminator loss: 0.456225, acc: 0.750000] [adversarial loss: 1.106284, acc: 0.312500]\n",
      "4251: [discriminator loss: 0.532210, acc: 0.765625] [adversarial loss: 2.159224, acc: 0.015625]\n",
      "4252: [discriminator loss: 0.638032, acc: 0.679688] [adversarial loss: 0.997361, acc: 0.390625]\n",
      "4253: [discriminator loss: 0.567261, acc: 0.687500] [adversarial loss: 1.776298, acc: 0.093750]\n",
      "4254: [discriminator loss: 0.549240, acc: 0.750000] [adversarial loss: 1.150038, acc: 0.375000]\n",
      "4255: [discriminator loss: 0.523518, acc: 0.781250] [adversarial loss: 1.323380, acc: 0.218750]\n",
      "4256: [discriminator loss: 0.491907, acc: 0.765625] [adversarial loss: 1.354968, acc: 0.156250]\n",
      "4257: [discriminator loss: 0.486324, acc: 0.726562] [adversarial loss: 1.180939, acc: 0.234375]\n",
      "4258: [discriminator loss: 0.443847, acc: 0.820312] [adversarial loss: 1.265795, acc: 0.296875]\n",
      "4259: [discriminator loss: 0.481909, acc: 0.765625] [adversarial loss: 1.071258, acc: 0.296875]\n",
      "4260: [discriminator loss: 0.468027, acc: 0.773438] [adversarial loss: 1.909984, acc: 0.062500]\n",
      "4261: [discriminator loss: 0.496170, acc: 0.757812] [adversarial loss: 1.112518, acc: 0.218750]\n",
      "4262: [discriminator loss: 0.545937, acc: 0.757812] [adversarial loss: 1.782578, acc: 0.125000]\n",
      "4263: [discriminator loss: 0.494244, acc: 0.757812] [adversarial loss: 0.971164, acc: 0.390625]\n",
      "4264: [discriminator loss: 0.493977, acc: 0.765625] [adversarial loss: 1.446179, acc: 0.125000]\n",
      "4265: [discriminator loss: 0.496255, acc: 0.750000] [adversarial loss: 1.156793, acc: 0.234375]\n",
      "4266: [discriminator loss: 0.548007, acc: 0.742188] [adversarial loss: 1.788652, acc: 0.062500]\n",
      "4267: [discriminator loss: 0.476730, acc: 0.750000] [adversarial loss: 1.255781, acc: 0.234375]\n",
      "4268: [discriminator loss: 0.438634, acc: 0.765625] [adversarial loss: 1.205506, acc: 0.296875]\n",
      "4269: [discriminator loss: 0.440729, acc: 0.781250] [adversarial loss: 1.396655, acc: 0.265625]\n",
      "4270: [discriminator loss: 0.549164, acc: 0.710938] [adversarial loss: 1.099896, acc: 0.375000]\n",
      "4271: [discriminator loss: 0.515391, acc: 0.757812] [adversarial loss: 1.381247, acc: 0.250000]\n",
      "4272: [discriminator loss: 0.457035, acc: 0.796875] [adversarial loss: 1.052178, acc: 0.312500]\n",
      "4273: [discriminator loss: 0.533006, acc: 0.703125] [adversarial loss: 1.533795, acc: 0.093750]\n",
      "4274: [discriminator loss: 0.499100, acc: 0.734375] [adversarial loss: 1.079154, acc: 0.328125]\n",
      "4275: [discriminator loss: 0.493851, acc: 0.757812] [adversarial loss: 1.557358, acc: 0.156250]\n",
      "4276: [discriminator loss: 0.528374, acc: 0.718750] [adversarial loss: 1.010952, acc: 0.390625]\n",
      "4277: [discriminator loss: 0.418565, acc: 0.789062] [adversarial loss: 2.029636, acc: 0.093750]\n",
      "4278: [discriminator loss: 0.573668, acc: 0.734375] [adversarial loss: 0.906042, acc: 0.437500]\n",
      "4279: [discriminator loss: 0.501495, acc: 0.726562] [adversarial loss: 1.661017, acc: 0.140625]\n",
      "4280: [discriminator loss: 0.435661, acc: 0.812500] [adversarial loss: 1.495584, acc: 0.187500]\n",
      "4281: [discriminator loss: 0.405372, acc: 0.820312] [adversarial loss: 1.258583, acc: 0.375000]\n",
      "4282: [discriminator loss: 0.543259, acc: 0.734375] [adversarial loss: 1.506016, acc: 0.171875]\n",
      "4283: [discriminator loss: 0.634942, acc: 0.679688] [adversarial loss: 1.285829, acc: 0.203125]\n",
      "4284: [discriminator loss: 0.496598, acc: 0.765625] [adversarial loss: 1.341802, acc: 0.218750]\n",
      "4285: [discriminator loss: 0.531263, acc: 0.726562] [adversarial loss: 1.323391, acc: 0.250000]\n",
      "4286: [discriminator loss: 0.501810, acc: 0.750000] [adversarial loss: 1.617348, acc: 0.171875]\n",
      "4287: [discriminator loss: 0.522417, acc: 0.703125] [adversarial loss: 1.008877, acc: 0.343750]\n",
      "4288: [discriminator loss: 0.520401, acc: 0.734375] [adversarial loss: 1.699625, acc: 0.093750]\n",
      "4289: [discriminator loss: 0.589196, acc: 0.742188] [adversarial loss: 1.157693, acc: 0.375000]\n",
      "4290: [discriminator loss: 0.563333, acc: 0.671875] [adversarial loss: 1.547992, acc: 0.140625]\n",
      "4291: [discriminator loss: 0.536807, acc: 0.718750] [adversarial loss: 1.440695, acc: 0.156250]\n",
      "4292: [discriminator loss: 0.508214, acc: 0.750000] [adversarial loss: 1.227340, acc: 0.265625]\n",
      "4293: [discriminator loss: 0.464769, acc: 0.812500] [adversarial loss: 1.900841, acc: 0.046875]\n",
      "4294: [discriminator loss: 0.544165, acc: 0.695312] [adversarial loss: 0.948946, acc: 0.453125]\n",
      "4295: [discriminator loss: 0.549347, acc: 0.734375] [adversarial loss: 1.555626, acc: 0.156250]\n",
      "4296: [discriminator loss: 0.487112, acc: 0.781250] [adversarial loss: 1.473436, acc: 0.156250]\n",
      "4297: [discriminator loss: 0.517499, acc: 0.750000] [adversarial loss: 1.254242, acc: 0.265625]\n",
      "4298: [discriminator loss: 0.423860, acc: 0.812500] [adversarial loss: 1.313018, acc: 0.171875]\n",
      "4299: [discriminator loss: 0.449788, acc: 0.789062] [adversarial loss: 1.518662, acc: 0.156250]\n",
      "4300: [discriminator loss: 0.562396, acc: 0.695312] [adversarial loss: 1.625270, acc: 0.125000]\n",
      "4301: [discriminator loss: 0.548752, acc: 0.734375] [adversarial loss: 1.210464, acc: 0.328125]\n",
      "4302: [discriminator loss: 0.527103, acc: 0.726562] [adversarial loss: 1.653126, acc: 0.093750]\n",
      "4303: [discriminator loss: 0.432342, acc: 0.804688] [adversarial loss: 1.086048, acc: 0.265625]\n",
      "4304: [discriminator loss: 0.526606, acc: 0.726562] [adversarial loss: 1.469221, acc: 0.156250]\n",
      "4305: [discriminator loss: 0.587963, acc: 0.710938] [adversarial loss: 1.028577, acc: 0.359375]\n",
      "4306: [discriminator loss: 0.513164, acc: 0.765625] [adversarial loss: 1.759313, acc: 0.078125]\n",
      "4307: [discriminator loss: 0.453797, acc: 0.765625] [adversarial loss: 1.303067, acc: 0.140625]\n",
      "4308: [discriminator loss: 0.517922, acc: 0.718750] [adversarial loss: 1.278817, acc: 0.218750]\n",
      "4309: [discriminator loss: 0.495531, acc: 0.750000] [adversarial loss: 1.028149, acc: 0.390625]\n",
      "4310: [discriminator loss: 0.558106, acc: 0.695312] [adversarial loss: 1.481209, acc: 0.171875]\n",
      "4311: [discriminator loss: 0.548061, acc: 0.703125] [adversarial loss: 0.874645, acc: 0.437500]\n",
      "4312: [discriminator loss: 0.595660, acc: 0.656250] [adversarial loss: 2.199897, acc: 0.062500]\n",
      "4313: [discriminator loss: 0.502480, acc: 0.718750] [adversarial loss: 1.040563, acc: 0.406250]\n",
      "4314: [discriminator loss: 0.532022, acc: 0.718750] [adversarial loss: 1.521625, acc: 0.109375]\n",
      "4315: [discriminator loss: 0.538108, acc: 0.710938] [adversarial loss: 1.197224, acc: 0.265625]\n",
      "4316: [discriminator loss: 0.457714, acc: 0.773438] [adversarial loss: 1.163125, acc: 0.312500]\n",
      "4317: [discriminator loss: 0.617739, acc: 0.664062] [adversarial loss: 1.428090, acc: 0.093750]\n",
      "4318: [discriminator loss: 0.563047, acc: 0.750000] [adversarial loss: 0.989942, acc: 0.312500]\n",
      "4319: [discriminator loss: 0.505521, acc: 0.718750] [adversarial loss: 1.782121, acc: 0.078125]\n",
      "4320: [discriminator loss: 0.449427, acc: 0.812500] [adversarial loss: 1.169906, acc: 0.234375]\n",
      "4321: [discriminator loss: 0.489844, acc: 0.796875] [adversarial loss: 1.196189, acc: 0.296875]\n",
      "4322: [discriminator loss: 0.548450, acc: 0.703125] [adversarial loss: 1.477172, acc: 0.125000]\n",
      "4323: [discriminator loss: 0.452002, acc: 0.812500] [adversarial loss: 1.188659, acc: 0.343750]\n",
      "4324: [discriminator loss: 0.512143, acc: 0.765625] [adversarial loss: 1.466542, acc: 0.187500]\n",
      "4325: [discriminator loss: 0.501267, acc: 0.773438] [adversarial loss: 1.049952, acc: 0.296875]\n",
      "4326: [discriminator loss: 0.593695, acc: 0.703125] [adversarial loss: 1.603708, acc: 0.171875]\n",
      "4327: [discriminator loss: 0.434837, acc: 0.796875] [adversarial loss: 1.220648, acc: 0.265625]\n",
      "4328: [discriminator loss: 0.509743, acc: 0.742188] [adversarial loss: 1.443597, acc: 0.187500]\n",
      "4329: [discriminator loss: 0.499820, acc: 0.757812] [adversarial loss: 1.133191, acc: 0.312500]\n",
      "4330: [discriminator loss: 0.486799, acc: 0.773438] [adversarial loss: 1.534387, acc: 0.171875]\n",
      "4331: [discriminator loss: 0.507211, acc: 0.757812] [adversarial loss: 1.007794, acc: 0.343750]\n",
      "4332: [discriminator loss: 0.461520, acc: 0.820312] [adversarial loss: 1.808006, acc: 0.078125]\n",
      "4333: [discriminator loss: 0.513363, acc: 0.726562] [adversarial loss: 0.994260, acc: 0.406250]\n",
      "4334: [discriminator loss: 0.514274, acc: 0.734375] [adversarial loss: 1.627554, acc: 0.140625]\n",
      "4335: [discriminator loss: 0.565934, acc: 0.703125] [adversarial loss: 1.003698, acc: 0.359375]\n",
      "4336: [discriminator loss: 0.534488, acc: 0.765625] [adversarial loss: 1.786365, acc: 0.140625]\n",
      "4337: [discriminator loss: 0.543815, acc: 0.695312] [adversarial loss: 1.173019, acc: 0.281250]\n",
      "4338: [discriminator loss: 0.521443, acc: 0.710938] [adversarial loss: 1.685159, acc: 0.046875]\n",
      "4339: [discriminator loss: 0.466034, acc: 0.773438] [adversarial loss: 1.079642, acc: 0.312500]\n",
      "4340: [discriminator loss: 0.465516, acc: 0.765625] [adversarial loss: 1.454138, acc: 0.203125]\n",
      "4341: [discriminator loss: 0.535398, acc: 0.726562] [adversarial loss: 1.305762, acc: 0.171875]\n",
      "4342: [discriminator loss: 0.475422, acc: 0.773438] [adversarial loss: 1.099995, acc: 0.312500]\n",
      "4343: [discriminator loss: 0.478944, acc: 0.781250] [adversarial loss: 1.516005, acc: 0.140625]\n",
      "4344: [discriminator loss: 0.438266, acc: 0.789062] [adversarial loss: 1.062095, acc: 0.218750]\n",
      "4345: [discriminator loss: 0.453826, acc: 0.796875] [adversarial loss: 1.745789, acc: 0.031250]\n",
      "4346: [discriminator loss: 0.624413, acc: 0.671875] [adversarial loss: 0.869887, acc: 0.437500]\n",
      "4347: [discriminator loss: 0.536989, acc: 0.718750] [adversarial loss: 1.709127, acc: 0.093750]\n",
      "4348: [discriminator loss: 0.519507, acc: 0.734375] [adversarial loss: 0.870863, acc: 0.546875]\n",
      "4349: [discriminator loss: 0.561045, acc: 0.687500] [adversarial loss: 2.191453, acc: 0.031250]\n",
      "4350: [discriminator loss: 0.568778, acc: 0.718750] [adversarial loss: 0.920434, acc: 0.468750]\n",
      "4351: [discriminator loss: 0.448808, acc: 0.773438] [adversarial loss: 1.409614, acc: 0.234375]\n",
      "4352: [discriminator loss: 0.546078, acc: 0.742188] [adversarial loss: 1.433459, acc: 0.187500]\n",
      "4353: [discriminator loss: 0.462471, acc: 0.765625] [adversarial loss: 1.105451, acc: 0.234375]\n",
      "4354: [discriminator loss: 0.529835, acc: 0.718750] [adversarial loss: 1.569283, acc: 0.187500]\n",
      "4355: [discriminator loss: 0.495554, acc: 0.789062] [adversarial loss: 1.212047, acc: 0.312500]\n",
      "4356: [discriminator loss: 0.529327, acc: 0.710938] [adversarial loss: 1.255689, acc: 0.218750]\n",
      "4357: [discriminator loss: 0.518001, acc: 0.718750] [adversarial loss: 1.372299, acc: 0.250000]\n",
      "4358: [discriminator loss: 0.517986, acc: 0.742188] [adversarial loss: 0.821835, acc: 0.500000]\n",
      "4359: [discriminator loss: 0.485772, acc: 0.710938] [adversarial loss: 1.669500, acc: 0.109375]\n",
      "4360: [discriminator loss: 0.478987, acc: 0.789062] [adversarial loss: 1.235252, acc: 0.328125]\n",
      "4361: [discriminator loss: 0.552865, acc: 0.710938] [adversarial loss: 1.600392, acc: 0.078125]\n",
      "4362: [discriminator loss: 0.491412, acc: 0.804688] [adversarial loss: 1.494565, acc: 0.140625]\n",
      "4363: [discriminator loss: 0.475671, acc: 0.765625] [adversarial loss: 1.387198, acc: 0.156250]\n",
      "4364: [discriminator loss: 0.473670, acc: 0.757812] [adversarial loss: 1.227647, acc: 0.250000]\n",
      "4365: [discriminator loss: 0.485304, acc: 0.781250] [adversarial loss: 1.678002, acc: 0.062500]\n",
      "4366: [discriminator loss: 0.449865, acc: 0.789062] [adversarial loss: 1.313655, acc: 0.218750]\n",
      "4367: [discriminator loss: 0.529646, acc: 0.703125] [adversarial loss: 1.691783, acc: 0.187500]\n",
      "4368: [discriminator loss: 0.563617, acc: 0.734375] [adversarial loss: 0.935204, acc: 0.421875]\n",
      "4369: [discriminator loss: 0.546381, acc: 0.718750] [adversarial loss: 1.964424, acc: 0.046875]\n",
      "4370: [discriminator loss: 0.592016, acc: 0.734375] [adversarial loss: 0.782482, acc: 0.531250]\n",
      "4371: [discriminator loss: 0.553066, acc: 0.710938] [adversarial loss: 1.565078, acc: 0.093750]\n",
      "4372: [discriminator loss: 0.466258, acc: 0.796875] [adversarial loss: 1.320944, acc: 0.156250]\n",
      "4373: [discriminator loss: 0.461349, acc: 0.835938] [adversarial loss: 1.335774, acc: 0.140625]\n",
      "4374: [discriminator loss: 0.466036, acc: 0.765625] [adversarial loss: 1.439620, acc: 0.156250]\n",
      "4375: [discriminator loss: 0.525401, acc: 0.726562] [adversarial loss: 1.419302, acc: 0.171875]\n",
      "4376: [discriminator loss: 0.525064, acc: 0.695312] [adversarial loss: 1.405903, acc: 0.140625]\n",
      "4377: [discriminator loss: 0.586870, acc: 0.687500] [adversarial loss: 1.638919, acc: 0.078125]\n",
      "4378: [discriminator loss: 0.490456, acc: 0.804688] [adversarial loss: 1.028970, acc: 0.375000]\n",
      "4379: [discriminator loss: 0.451487, acc: 0.835938] [adversarial loss: 1.563161, acc: 0.078125]\n",
      "4380: [discriminator loss: 0.495157, acc: 0.765625] [adversarial loss: 1.281979, acc: 0.328125]\n",
      "4381: [discriminator loss: 0.529989, acc: 0.687500] [adversarial loss: 1.832323, acc: 0.125000]\n",
      "4382: [discriminator loss: 0.505776, acc: 0.757812] [adversarial loss: 0.881260, acc: 0.375000]\n",
      "4383: [discriminator loss: 0.448610, acc: 0.804688] [adversarial loss: 1.772254, acc: 0.125000]\n",
      "4384: [discriminator loss: 0.536738, acc: 0.718750] [adversarial loss: 1.130229, acc: 0.281250]\n",
      "4385: [discriminator loss: 0.493100, acc: 0.781250] [adversarial loss: 1.621855, acc: 0.109375]\n",
      "4386: [discriminator loss: 0.503377, acc: 0.757812] [adversarial loss: 1.103115, acc: 0.265625]\n",
      "4387: [discriminator loss: 0.518253, acc: 0.765625] [adversarial loss: 1.434442, acc: 0.140625]\n",
      "4388: [discriminator loss: 0.476765, acc: 0.781250] [adversarial loss: 1.308783, acc: 0.218750]\n",
      "4389: [discriminator loss: 0.399611, acc: 0.859375] [adversarial loss: 1.440664, acc: 0.171875]\n",
      "4390: [discriminator loss: 0.579397, acc: 0.695312] [adversarial loss: 1.402500, acc: 0.171875]\n",
      "4391: [discriminator loss: 0.450623, acc: 0.804688] [adversarial loss: 1.174958, acc: 0.281250]\n",
      "4392: [discriminator loss: 0.547093, acc: 0.687500] [adversarial loss: 1.474090, acc: 0.171875]\n",
      "4393: [discriminator loss: 0.462950, acc: 0.781250] [adversarial loss: 1.397592, acc: 0.156250]\n",
      "4394: [discriminator loss: 0.499696, acc: 0.726562] [adversarial loss: 1.260845, acc: 0.265625]\n",
      "4395: [discriminator loss: 0.437889, acc: 0.828125] [adversarial loss: 1.433547, acc: 0.156250]\n",
      "4396: [discriminator loss: 0.575939, acc: 0.687500] [adversarial loss: 1.131212, acc: 0.296875]\n",
      "4397: [discriminator loss: 0.524950, acc: 0.726562] [adversarial loss: 1.258638, acc: 0.203125]\n",
      "4398: [discriminator loss: 0.619257, acc: 0.679688] [adversarial loss: 1.183342, acc: 0.234375]\n",
      "4399: [discriminator loss: 0.518234, acc: 0.726562] [adversarial loss: 1.235088, acc: 0.281250]\n",
      "4400: [discriminator loss: 0.577198, acc: 0.734375] [adversarial loss: 1.572839, acc: 0.093750]\n",
      "4401: [discriminator loss: 0.508212, acc: 0.742188] [adversarial loss: 1.080937, acc: 0.281250]\n",
      "4402: [discriminator loss: 0.571493, acc: 0.695312] [adversarial loss: 1.607496, acc: 0.093750]\n",
      "4403: [discriminator loss: 0.564215, acc: 0.695312] [adversarial loss: 1.078673, acc: 0.312500]\n",
      "4404: [discriminator loss: 0.511871, acc: 0.765625] [adversarial loss: 1.432961, acc: 0.187500]\n",
      "4405: [discriminator loss: 0.460884, acc: 0.781250] [adversarial loss: 1.376582, acc: 0.187500]\n",
      "4406: [discriminator loss: 0.481197, acc: 0.765625] [adversarial loss: 1.257545, acc: 0.187500]\n",
      "4407: [discriminator loss: 0.448984, acc: 0.843750] [adversarial loss: 1.337266, acc: 0.281250]\n",
      "4408: [discriminator loss: 0.512669, acc: 0.726562] [adversarial loss: 1.361518, acc: 0.250000]\n",
      "4409: [discriminator loss: 0.535082, acc: 0.703125] [adversarial loss: 0.983999, acc: 0.312500]\n",
      "4410: [discriminator loss: 0.520909, acc: 0.750000] [adversarial loss: 1.824259, acc: 0.140625]\n",
      "4411: [discriminator loss: 0.621608, acc: 0.664062] [adversarial loss: 0.973607, acc: 0.375000]\n",
      "4412: [discriminator loss: 0.543381, acc: 0.679688] [adversarial loss: 1.835994, acc: 0.156250]\n",
      "4413: [discriminator loss: 0.535461, acc: 0.734375] [adversarial loss: 1.091522, acc: 0.250000]\n",
      "4414: [discriminator loss: 0.503593, acc: 0.757812] [adversarial loss: 1.605177, acc: 0.203125]\n",
      "4415: [discriminator loss: 0.570678, acc: 0.679688] [adversarial loss: 1.224710, acc: 0.265625]\n",
      "4416: [discriminator loss: 0.503033, acc: 0.710938] [adversarial loss: 1.421058, acc: 0.109375]\n",
      "4417: [discriminator loss: 0.454616, acc: 0.781250] [adversarial loss: 1.098171, acc: 0.375000]\n",
      "4418: [discriminator loss: 0.579336, acc: 0.656250] [adversarial loss: 1.806157, acc: 0.031250]\n",
      "4419: [discriminator loss: 0.510282, acc: 0.773438] [adversarial loss: 1.114720, acc: 0.281250]\n",
      "4420: [discriminator loss: 0.575273, acc: 0.718750] [adversarial loss: 1.609337, acc: 0.062500]\n",
      "4421: [discriminator loss: 0.566105, acc: 0.703125] [adversarial loss: 1.092970, acc: 0.281250]\n",
      "4422: [discriminator loss: 0.444448, acc: 0.820312] [adversarial loss: 1.673729, acc: 0.093750]\n",
      "4423: [discriminator loss: 0.496424, acc: 0.757812] [adversarial loss: 1.226404, acc: 0.281250]\n",
      "4424: [discriminator loss: 0.546888, acc: 0.703125] [adversarial loss: 1.810369, acc: 0.140625]\n",
      "4425: [discriminator loss: 0.537228, acc: 0.703125] [adversarial loss: 1.192746, acc: 0.281250]\n",
      "4426: [discriminator loss: 0.509046, acc: 0.781250] [adversarial loss: 1.504717, acc: 0.125000]\n",
      "4427: [discriminator loss: 0.487973, acc: 0.734375] [adversarial loss: 1.126153, acc: 0.312500]\n",
      "4428: [discriminator loss: 0.552272, acc: 0.734375] [adversarial loss: 1.336459, acc: 0.171875]\n",
      "4429: [discriminator loss: 0.540715, acc: 0.765625] [adversarial loss: 1.314358, acc: 0.234375]\n",
      "4430: [discriminator loss: 0.488897, acc: 0.757812] [adversarial loss: 1.221604, acc: 0.265625]\n",
      "4431: [discriminator loss: 0.489006, acc: 0.781250] [adversarial loss: 1.438486, acc: 0.187500]\n",
      "4432: [discriminator loss: 0.491721, acc: 0.789062] [adversarial loss: 1.507922, acc: 0.140625]\n",
      "4433: [discriminator loss: 0.518369, acc: 0.742188] [adversarial loss: 1.123048, acc: 0.390625]\n",
      "4434: [discriminator loss: 0.562343, acc: 0.703125] [adversarial loss: 1.281528, acc: 0.234375]\n",
      "4435: [discriminator loss: 0.431762, acc: 0.796875] [adversarial loss: 1.421371, acc: 0.234375]\n",
      "4436: [discriminator loss: 0.560301, acc: 0.703125] [adversarial loss: 1.130960, acc: 0.265625]\n",
      "4437: [discriminator loss: 0.513894, acc: 0.765625] [adversarial loss: 1.825791, acc: 0.078125]\n",
      "4438: [discriminator loss: 0.514228, acc: 0.757812] [adversarial loss: 1.007903, acc: 0.281250]\n",
      "4439: [discriminator loss: 0.532291, acc: 0.750000] [adversarial loss: 1.866915, acc: 0.078125]\n",
      "4440: [discriminator loss: 0.511663, acc: 0.757812] [adversarial loss: 1.009362, acc: 0.312500]\n",
      "4441: [discriminator loss: 0.469459, acc: 0.781250] [adversarial loss: 1.686799, acc: 0.093750]\n",
      "4442: [discriminator loss: 0.440948, acc: 0.789062] [adversarial loss: 1.094087, acc: 0.390625]\n",
      "4443: [discriminator loss: 0.548550, acc: 0.742188] [adversarial loss: 1.623040, acc: 0.062500]\n",
      "4444: [discriminator loss: 0.464335, acc: 0.789062] [adversarial loss: 1.202797, acc: 0.296875]\n",
      "4445: [discriminator loss: 0.500402, acc: 0.750000] [adversarial loss: 1.869847, acc: 0.093750]\n",
      "4446: [discriminator loss: 0.559178, acc: 0.703125] [adversarial loss: 1.002917, acc: 0.390625]\n",
      "4447: [discriminator loss: 0.573985, acc: 0.734375] [adversarial loss: 1.318940, acc: 0.218750]\n",
      "4448: [discriminator loss: 0.537906, acc: 0.718750] [adversarial loss: 1.358695, acc: 0.171875]\n",
      "4449: [discriminator loss: 0.557877, acc: 0.703125] [adversarial loss: 1.290729, acc: 0.265625]\n",
      "4450: [discriminator loss: 0.489541, acc: 0.812500] [adversarial loss: 1.254932, acc: 0.250000]\n",
      "4451: [discriminator loss: 0.595939, acc: 0.664062] [adversarial loss: 1.533142, acc: 0.171875]\n",
      "4452: [discriminator loss: 0.467293, acc: 0.789062] [adversarial loss: 1.118352, acc: 0.328125]\n",
      "4453: [discriminator loss: 0.594723, acc: 0.625000] [adversarial loss: 1.546007, acc: 0.125000]\n",
      "4454: [discriminator loss: 0.459006, acc: 0.750000] [adversarial loss: 1.483265, acc: 0.171875]\n",
      "4455: [discriminator loss: 0.480127, acc: 0.781250] [adversarial loss: 1.125721, acc: 0.343750]\n",
      "4456: [discriminator loss: 0.555122, acc: 0.742188] [adversarial loss: 2.122393, acc: 0.046875]\n",
      "4457: [discriminator loss: 0.538678, acc: 0.710938] [adversarial loss: 1.105870, acc: 0.375000]\n",
      "4458: [discriminator loss: 0.587974, acc: 0.656250] [adversarial loss: 2.034296, acc: 0.046875]\n",
      "4459: [discriminator loss: 0.608414, acc: 0.679688] [adversarial loss: 0.832566, acc: 0.531250]\n",
      "4460: [discriminator loss: 0.507457, acc: 0.679688] [adversarial loss: 1.665759, acc: 0.078125]\n",
      "4461: [discriminator loss: 0.543506, acc: 0.757812] [adversarial loss: 0.972794, acc: 0.375000]\n",
      "4462: [discriminator loss: 0.573640, acc: 0.710938] [adversarial loss: 1.650801, acc: 0.109375]\n",
      "4463: [discriminator loss: 0.558381, acc: 0.726562] [adversarial loss: 1.207260, acc: 0.234375]\n",
      "4464: [discriminator loss: 0.560924, acc: 0.750000] [adversarial loss: 1.573619, acc: 0.062500]\n",
      "4465: [discriminator loss: 0.514575, acc: 0.726562] [adversarial loss: 1.176760, acc: 0.265625]\n",
      "4466: [discriminator loss: 0.482852, acc: 0.781250] [adversarial loss: 1.310273, acc: 0.281250]\n",
      "4467: [discriminator loss: 0.466728, acc: 0.820312] [adversarial loss: 1.136968, acc: 0.312500]\n",
      "4468: [discriminator loss: 0.512659, acc: 0.726562] [adversarial loss: 1.189809, acc: 0.265625]\n",
      "4469: [discriminator loss: 0.497805, acc: 0.781250] [adversarial loss: 1.697588, acc: 0.156250]\n",
      "4470: [discriminator loss: 0.514505, acc: 0.804688] [adversarial loss: 1.266980, acc: 0.234375]\n",
      "4471: [discriminator loss: 0.558021, acc: 0.734375] [adversarial loss: 1.250731, acc: 0.250000]\n",
      "4472: [discriminator loss: 0.547619, acc: 0.710938] [adversarial loss: 1.214402, acc: 0.234375]\n",
      "4473: [discriminator loss: 0.511084, acc: 0.726562] [adversarial loss: 1.123884, acc: 0.328125]\n",
      "4474: [discriminator loss: 0.492013, acc: 0.734375] [adversarial loss: 1.611319, acc: 0.125000]\n",
      "4475: [discriminator loss: 0.515144, acc: 0.750000] [adversarial loss: 1.122287, acc: 0.328125]\n",
      "4476: [discriminator loss: 0.525363, acc: 0.734375] [adversarial loss: 1.549180, acc: 0.078125]\n",
      "4477: [discriminator loss: 0.501506, acc: 0.726562] [adversarial loss: 1.229507, acc: 0.234375]\n",
      "4478: [discriminator loss: 0.509249, acc: 0.726562] [adversarial loss: 1.459972, acc: 0.093750]\n",
      "4479: [discriminator loss: 0.506111, acc: 0.789062] [adversarial loss: 1.035830, acc: 0.328125]\n",
      "4480: [discriminator loss: 0.543160, acc: 0.710938] [adversarial loss: 1.723224, acc: 0.140625]\n",
      "4481: [discriminator loss: 0.569207, acc: 0.726562] [adversarial loss: 0.961302, acc: 0.359375]\n",
      "4482: [discriminator loss: 0.509021, acc: 0.742188] [adversarial loss: 1.571209, acc: 0.062500]\n",
      "4483: [discriminator loss: 0.493368, acc: 0.765625] [adversarial loss: 0.939034, acc: 0.328125]\n",
      "4484: [discriminator loss: 0.491448, acc: 0.742188] [adversarial loss: 1.256193, acc: 0.250000]\n",
      "4485: [discriminator loss: 0.538785, acc: 0.703125] [adversarial loss: 0.877130, acc: 0.406250]\n",
      "4486: [discriminator loss: 0.538178, acc: 0.750000] [adversarial loss: 1.829760, acc: 0.078125]\n",
      "4487: [discriminator loss: 0.521424, acc: 0.718750] [adversarial loss: 0.913767, acc: 0.437500]\n",
      "4488: [discriminator loss: 0.557374, acc: 0.757812] [adversarial loss: 1.315300, acc: 0.171875]\n",
      "4489: [discriminator loss: 0.485859, acc: 0.765625] [adversarial loss: 1.239311, acc: 0.234375]\n",
      "4490: [discriminator loss: 0.572419, acc: 0.734375] [adversarial loss: 1.482033, acc: 0.187500]\n",
      "4491: [discriminator loss: 0.521866, acc: 0.742188] [adversarial loss: 1.441613, acc: 0.218750]\n",
      "4492: [discriminator loss: 0.522511, acc: 0.726562] [adversarial loss: 1.334940, acc: 0.250000]\n",
      "4493: [discriminator loss: 0.486818, acc: 0.820312] [adversarial loss: 1.275444, acc: 0.218750]\n",
      "4494: [discriminator loss: 0.508222, acc: 0.765625] [adversarial loss: 1.250426, acc: 0.250000]\n",
      "4495: [discriminator loss: 0.606137, acc: 0.710938] [adversarial loss: 1.331587, acc: 0.187500]\n",
      "4496: [discriminator loss: 0.472297, acc: 0.750000] [adversarial loss: 1.283061, acc: 0.281250]\n",
      "4497: [discriminator loss: 0.495110, acc: 0.757812] [adversarial loss: 1.622979, acc: 0.046875]\n",
      "4498: [discriminator loss: 0.460776, acc: 0.765625] [adversarial loss: 1.286885, acc: 0.265625]\n",
      "4499: [discriminator loss: 0.526154, acc: 0.757812] [adversarial loss: 1.504103, acc: 0.171875]\n",
      "4500: [discriminator loss: 0.510077, acc: 0.695312] [adversarial loss: 1.247437, acc: 0.250000]\n",
      "4501: [discriminator loss: 0.514149, acc: 0.742188] [adversarial loss: 1.296284, acc: 0.234375]\n",
      "4502: [discriminator loss: 0.484921, acc: 0.765625] [adversarial loss: 1.300055, acc: 0.312500]\n",
      "4503: [discriminator loss: 0.471815, acc: 0.765625] [adversarial loss: 1.369747, acc: 0.250000]\n",
      "4504: [discriminator loss: 0.524597, acc: 0.765625] [adversarial loss: 1.136506, acc: 0.312500]\n",
      "4505: [discriminator loss: 0.545983, acc: 0.718750] [adversarial loss: 1.269204, acc: 0.203125]\n",
      "4506: [discriminator loss: 0.463297, acc: 0.781250] [adversarial loss: 1.886240, acc: 0.093750]\n",
      "4507: [discriminator loss: 0.507560, acc: 0.718750] [adversarial loss: 0.969525, acc: 0.484375]\n",
      "4508: [discriminator loss: 0.577256, acc: 0.687500] [adversarial loss: 1.917392, acc: 0.062500]\n",
      "4509: [discriminator loss: 0.483971, acc: 0.750000] [adversarial loss: 1.003185, acc: 0.375000]\n",
      "4510: [discriminator loss: 0.543828, acc: 0.687500] [adversarial loss: 1.876649, acc: 0.156250]\n",
      "4511: [discriminator loss: 0.566463, acc: 0.718750] [adversarial loss: 1.051954, acc: 0.359375]\n",
      "4512: [discriminator loss: 0.532765, acc: 0.718750] [adversarial loss: 1.949386, acc: 0.015625]\n",
      "4513: [discriminator loss: 0.563859, acc: 0.734375] [adversarial loss: 1.099637, acc: 0.312500]\n",
      "4514: [discriminator loss: 0.491239, acc: 0.796875] [adversarial loss: 1.428203, acc: 0.187500]\n",
      "4515: [discriminator loss: 0.523466, acc: 0.750000] [adversarial loss: 1.060173, acc: 0.296875]\n",
      "4516: [discriminator loss: 0.480627, acc: 0.726562] [adversarial loss: 1.327588, acc: 0.234375]\n",
      "4517: [discriminator loss: 0.507299, acc: 0.757812] [adversarial loss: 1.602212, acc: 0.093750]\n",
      "4518: [discriminator loss: 0.530561, acc: 0.742188] [adversarial loss: 1.063023, acc: 0.343750]\n",
      "4519: [discriminator loss: 0.493461, acc: 0.773438] [adversarial loss: 1.179284, acc: 0.265625]\n",
      "4520: [discriminator loss: 0.543657, acc: 0.742188] [adversarial loss: 1.241347, acc: 0.109375]\n",
      "4521: [discriminator loss: 0.515058, acc: 0.757812] [adversarial loss: 1.332171, acc: 0.046875]\n",
      "4522: [discriminator loss: 0.450445, acc: 0.757812] [adversarial loss: 1.473707, acc: 0.218750]\n",
      "4523: [discriminator loss: 0.490723, acc: 0.750000] [adversarial loss: 1.148525, acc: 0.203125]\n",
      "4524: [discriminator loss: 0.446735, acc: 0.812500] [adversarial loss: 1.906090, acc: 0.109375]\n",
      "4525: [discriminator loss: 0.589706, acc: 0.718750] [adversarial loss: 1.108735, acc: 0.296875]\n",
      "4526: [discriminator loss: 0.648187, acc: 0.679688] [adversarial loss: 1.556754, acc: 0.109375]\n",
      "4527: [discriminator loss: 0.527020, acc: 0.765625] [adversarial loss: 0.875498, acc: 0.531250]\n",
      "4528: [discriminator loss: 0.522619, acc: 0.718750] [adversarial loss: 1.723136, acc: 0.093750]\n",
      "4529: [discriminator loss: 0.538612, acc: 0.703125] [adversarial loss: 0.926064, acc: 0.406250]\n",
      "4530: [discriminator loss: 0.509081, acc: 0.750000] [adversarial loss: 1.564881, acc: 0.171875]\n",
      "4531: [discriminator loss: 0.479263, acc: 0.742188] [adversarial loss: 1.235811, acc: 0.234375]\n",
      "4532: [discriminator loss: 0.490705, acc: 0.757812] [adversarial loss: 1.440603, acc: 0.125000]\n",
      "4533: [discriminator loss: 0.443350, acc: 0.789062] [adversarial loss: 1.508933, acc: 0.187500]\n",
      "4534: [discriminator loss: 0.458686, acc: 0.781250] [adversarial loss: 1.506137, acc: 0.140625]\n",
      "4535: [discriminator loss: 0.490728, acc: 0.789062] [adversarial loss: 1.292281, acc: 0.140625]\n",
      "4536: [discriminator loss: 0.481541, acc: 0.781250] [adversarial loss: 1.391504, acc: 0.171875]\n",
      "4537: [discriminator loss: 0.522135, acc: 0.781250] [adversarial loss: 1.731272, acc: 0.062500]\n",
      "4538: [discriminator loss: 0.595970, acc: 0.679688] [adversarial loss: 0.881006, acc: 0.484375]\n",
      "4539: [discriminator loss: 0.597856, acc: 0.679688] [adversarial loss: 1.837610, acc: 0.109375]\n",
      "4540: [discriminator loss: 0.510026, acc: 0.757812] [adversarial loss: 1.049387, acc: 0.421875]\n",
      "4541: [discriminator loss: 0.481293, acc: 0.781250] [adversarial loss: 1.691302, acc: 0.156250]\n",
      "4542: [discriminator loss: 0.564954, acc: 0.726562] [adversarial loss: 0.991516, acc: 0.406250]\n",
      "4543: [discriminator loss: 0.535586, acc: 0.757812] [adversarial loss: 1.290397, acc: 0.250000]\n",
      "4544: [discriminator loss: 0.526729, acc: 0.726562] [adversarial loss: 1.131540, acc: 0.312500]\n",
      "4545: [discriminator loss: 0.580651, acc: 0.718750] [adversarial loss: 1.412403, acc: 0.093750]\n",
      "4546: [discriminator loss: 0.558389, acc: 0.718750] [adversarial loss: 1.203120, acc: 0.281250]\n",
      "4547: [discriminator loss: 0.517798, acc: 0.726562] [adversarial loss: 1.331100, acc: 0.140625]\n",
      "4548: [discriminator loss: 0.573732, acc: 0.679688] [adversarial loss: 0.972003, acc: 0.375000]\n",
      "4549: [discriminator loss: 0.521117, acc: 0.742188] [adversarial loss: 1.909497, acc: 0.109375]\n",
      "4550: [discriminator loss: 0.703191, acc: 0.625000] [adversarial loss: 0.829120, acc: 0.500000]\n",
      "4551: [discriminator loss: 0.546696, acc: 0.703125] [adversarial loss: 1.441057, acc: 0.140625]\n",
      "4552: [discriminator loss: 0.577732, acc: 0.703125] [adversarial loss: 1.016276, acc: 0.406250]\n",
      "4553: [discriminator loss: 0.475455, acc: 0.773438] [adversarial loss: 1.427135, acc: 0.187500]\n",
      "4554: [discriminator loss: 0.552833, acc: 0.687500] [adversarial loss: 1.258827, acc: 0.218750]\n",
      "4555: [discriminator loss: 0.441609, acc: 0.765625] [adversarial loss: 1.162785, acc: 0.343750]\n",
      "4556: [discriminator loss: 0.552748, acc: 0.710938] [adversarial loss: 1.583159, acc: 0.125000]\n",
      "4557: [discriminator loss: 0.454938, acc: 0.804688] [adversarial loss: 1.315179, acc: 0.156250]\n",
      "4558: [discriminator loss: 0.545440, acc: 0.734375] [adversarial loss: 1.539381, acc: 0.093750]\n",
      "4559: [discriminator loss: 0.447387, acc: 0.742188] [adversarial loss: 1.045892, acc: 0.312500]\n",
      "4560: [discriminator loss: 0.471624, acc: 0.804688] [adversarial loss: 1.288373, acc: 0.156250]\n",
      "4561: [discriminator loss: 0.545546, acc: 0.757812] [adversarial loss: 1.280035, acc: 0.156250]\n",
      "4562: [discriminator loss: 0.533402, acc: 0.703125] [adversarial loss: 1.166722, acc: 0.265625]\n",
      "4563: [discriminator loss: 0.499214, acc: 0.796875] [adversarial loss: 1.411531, acc: 0.140625]\n",
      "4564: [discriminator loss: 0.532523, acc: 0.703125] [adversarial loss: 1.131615, acc: 0.265625]\n",
      "4565: [discriminator loss: 0.483845, acc: 0.765625] [adversarial loss: 1.505166, acc: 0.125000]\n",
      "4566: [discriminator loss: 0.522368, acc: 0.726562] [adversarial loss: 0.909948, acc: 0.406250]\n",
      "4567: [discriminator loss: 0.493416, acc: 0.750000] [adversarial loss: 1.278598, acc: 0.234375]\n",
      "4568: [discriminator loss: 0.544103, acc: 0.687500] [adversarial loss: 1.139375, acc: 0.265625]\n",
      "4569: [discriminator loss: 0.549822, acc: 0.703125] [adversarial loss: 1.610164, acc: 0.140625]\n",
      "4570: [discriminator loss: 0.527164, acc: 0.742188] [adversarial loss: 0.910957, acc: 0.421875]\n",
      "4571: [discriminator loss: 0.573636, acc: 0.710938] [adversarial loss: 1.742294, acc: 0.109375]\n",
      "4572: [discriminator loss: 0.566825, acc: 0.718750] [adversarial loss: 1.097419, acc: 0.343750]\n",
      "4573: [discriminator loss: 0.510099, acc: 0.734375] [adversarial loss: 1.353674, acc: 0.125000]\n",
      "4574: [discriminator loss: 0.454987, acc: 0.789062] [adversarial loss: 1.162619, acc: 0.250000]\n",
      "4575: [discriminator loss: 0.511855, acc: 0.742188] [adversarial loss: 1.301567, acc: 0.140625]\n",
      "4576: [discriminator loss: 0.539212, acc: 0.750000] [adversarial loss: 1.128941, acc: 0.312500]\n",
      "4577: [discriminator loss: 0.510366, acc: 0.726562] [adversarial loss: 1.583853, acc: 0.140625]\n",
      "4578: [discriminator loss: 0.500625, acc: 0.781250] [adversarial loss: 1.237617, acc: 0.265625]\n",
      "4579: [discriminator loss: 0.447023, acc: 0.789062] [adversarial loss: 1.365834, acc: 0.187500]\n",
      "4580: [discriminator loss: 0.590058, acc: 0.710938] [adversarial loss: 0.828496, acc: 0.406250]\n",
      "4581: [discriminator loss: 0.499863, acc: 0.671875] [adversarial loss: 1.635861, acc: 0.125000]\n",
      "4582: [discriminator loss: 0.548979, acc: 0.703125] [adversarial loss: 1.089506, acc: 0.296875]\n",
      "4583: [discriminator loss: 0.442989, acc: 0.812500] [adversarial loss: 1.661052, acc: 0.171875]\n",
      "4584: [discriminator loss: 0.532453, acc: 0.734375] [adversarial loss: 0.827412, acc: 0.437500]\n",
      "4585: [discriminator loss: 0.535898, acc: 0.710938] [adversarial loss: 1.476780, acc: 0.203125]\n",
      "4586: [discriminator loss: 0.543783, acc: 0.757812] [adversarial loss: 0.885344, acc: 0.406250]\n",
      "4587: [discriminator loss: 0.457487, acc: 0.828125] [adversarial loss: 1.655764, acc: 0.125000]\n",
      "4588: [discriminator loss: 0.545944, acc: 0.734375] [adversarial loss: 0.949649, acc: 0.390625]\n",
      "4589: [discriminator loss: 0.473814, acc: 0.781250] [adversarial loss: 1.521242, acc: 0.109375]\n",
      "4590: [discriminator loss: 0.501797, acc: 0.804688] [adversarial loss: 1.189404, acc: 0.250000]\n",
      "4591: [discriminator loss: 0.556898, acc: 0.726562] [adversarial loss: 1.309270, acc: 0.203125]\n",
      "4592: [discriminator loss: 0.538566, acc: 0.750000] [adversarial loss: 0.853292, acc: 0.484375]\n",
      "4593: [discriminator loss: 0.501711, acc: 0.773438] [adversarial loss: 1.656736, acc: 0.062500]\n",
      "4594: [discriminator loss: 0.533529, acc: 0.734375] [adversarial loss: 1.237755, acc: 0.203125]\n",
      "4595: [discriminator loss: 0.608808, acc: 0.695312] [adversarial loss: 1.390898, acc: 0.171875]\n",
      "4596: [discriminator loss: 0.543031, acc: 0.726562] [adversarial loss: 0.907843, acc: 0.390625]\n",
      "4597: [discriminator loss: 0.590887, acc: 0.710938] [adversarial loss: 1.692259, acc: 0.093750]\n",
      "4598: [discriminator loss: 0.495720, acc: 0.726562] [adversarial loss: 1.287489, acc: 0.156250]\n",
      "4599: [discriminator loss: 0.473724, acc: 0.789062] [adversarial loss: 1.555427, acc: 0.187500]\n",
      "4600: [discriminator loss: 0.521074, acc: 0.742188] [adversarial loss: 1.096168, acc: 0.296875]\n",
      "4601: [discriminator loss: 0.596117, acc: 0.710938] [adversarial loss: 1.504152, acc: 0.156250]\n",
      "4602: [discriminator loss: 0.479180, acc: 0.781250] [adversarial loss: 0.944531, acc: 0.359375]\n",
      "4603: [discriminator loss: 0.649870, acc: 0.656250] [adversarial loss: 1.869338, acc: 0.078125]\n",
      "4604: [discriminator loss: 0.528698, acc: 0.718750] [adversarial loss: 0.979947, acc: 0.359375]\n",
      "4605: [discriminator loss: 0.461215, acc: 0.781250] [adversarial loss: 1.343891, acc: 0.187500]\n",
      "4606: [discriminator loss: 0.460931, acc: 0.812500] [adversarial loss: 1.149413, acc: 0.281250]\n",
      "4607: [discriminator loss: 0.497748, acc: 0.773438] [adversarial loss: 1.407331, acc: 0.156250]\n",
      "4608: [discriminator loss: 0.454970, acc: 0.789062] [adversarial loss: 1.276903, acc: 0.187500]\n",
      "4609: [discriminator loss: 0.501065, acc: 0.718750] [adversarial loss: 1.578816, acc: 0.156250]\n",
      "4610: [discriminator loss: 0.553857, acc: 0.710938] [adversarial loss: 1.269360, acc: 0.250000]\n",
      "4611: [discriminator loss: 0.549779, acc: 0.757812] [adversarial loss: 1.215411, acc: 0.203125]\n",
      "4612: [discriminator loss: 0.490535, acc: 0.734375] [adversarial loss: 1.488235, acc: 0.156250]\n",
      "4613: [discriminator loss: 0.492399, acc: 0.773438] [adversarial loss: 1.198686, acc: 0.234375]\n",
      "4614: [discriminator loss: 0.528985, acc: 0.734375] [adversarial loss: 1.846578, acc: 0.093750]\n",
      "4615: [discriminator loss: 0.613096, acc: 0.695312] [adversarial loss: 0.930278, acc: 0.390625]\n",
      "4616: [discriminator loss: 0.520579, acc: 0.734375] [adversarial loss: 2.106352, acc: 0.078125]\n",
      "4617: [discriminator loss: 0.549290, acc: 0.726562] [adversarial loss: 0.910913, acc: 0.406250]\n",
      "4618: [discriminator loss: 0.486696, acc: 0.765625] [adversarial loss: 1.710247, acc: 0.109375]\n",
      "4619: [discriminator loss: 0.601432, acc: 0.687500] [adversarial loss: 0.910446, acc: 0.375000]\n",
      "4620: [discriminator loss: 0.544819, acc: 0.664062] [adversarial loss: 1.681695, acc: 0.156250]\n",
      "4621: [discriminator loss: 0.498211, acc: 0.765625] [adversarial loss: 1.020859, acc: 0.328125]\n",
      "4622: [discriminator loss: 0.493241, acc: 0.765625] [adversarial loss: 1.627328, acc: 0.078125]\n",
      "4623: [discriminator loss: 0.606394, acc: 0.664062] [adversarial loss: 1.015850, acc: 0.312500]\n",
      "4624: [discriminator loss: 0.562889, acc: 0.710938] [adversarial loss: 1.496808, acc: 0.171875]\n",
      "4625: [discriminator loss: 0.454658, acc: 0.765625] [adversarial loss: 1.442895, acc: 0.140625]\n",
      "4626: [discriminator loss: 0.484396, acc: 0.781250] [adversarial loss: 1.144667, acc: 0.234375]\n",
      "4627: [discriminator loss: 0.504493, acc: 0.757812] [adversarial loss: 1.693242, acc: 0.078125]\n",
      "4628: [discriminator loss: 0.488734, acc: 0.742188] [adversarial loss: 1.222909, acc: 0.250000]\n",
      "4629: [discriminator loss: 0.448654, acc: 0.765625] [adversarial loss: 1.162467, acc: 0.265625]\n",
      "4630: [discriminator loss: 0.491286, acc: 0.750000] [adversarial loss: 1.418249, acc: 0.328125]\n",
      "4631: [discriminator loss: 0.624047, acc: 0.632812] [adversarial loss: 1.383225, acc: 0.171875]\n",
      "4632: [discriminator loss: 0.465679, acc: 0.781250] [adversarial loss: 1.155257, acc: 0.265625]\n",
      "4633: [discriminator loss: 0.490392, acc: 0.734375] [adversarial loss: 1.159489, acc: 0.328125]\n",
      "4634: [discriminator loss: 0.509279, acc: 0.710938] [adversarial loss: 1.645128, acc: 0.046875]\n",
      "4635: [discriminator loss: 0.490407, acc: 0.765625] [adversarial loss: 1.117395, acc: 0.359375]\n",
      "4636: [discriminator loss: 0.474774, acc: 0.796875] [adversarial loss: 1.644791, acc: 0.156250]\n",
      "4637: [discriminator loss: 0.531051, acc: 0.781250] [adversarial loss: 1.186832, acc: 0.281250]\n",
      "4638: [discriminator loss: 0.563991, acc: 0.718750] [adversarial loss: 1.652979, acc: 0.109375]\n",
      "4639: [discriminator loss: 0.499881, acc: 0.773438] [adversarial loss: 1.082709, acc: 0.265625]\n",
      "4640: [discriminator loss: 0.540665, acc: 0.734375] [adversarial loss: 1.646194, acc: 0.109375]\n",
      "4641: [discriminator loss: 0.547041, acc: 0.726562] [adversarial loss: 0.905270, acc: 0.406250]\n",
      "4642: [discriminator loss: 0.611939, acc: 0.656250] [adversarial loss: 1.914894, acc: 0.093750]\n",
      "4643: [discriminator loss: 0.584664, acc: 0.703125] [adversarial loss: 1.000553, acc: 0.484375]\n",
      "4644: [discriminator loss: 0.496598, acc: 0.757812] [adversarial loss: 1.156614, acc: 0.265625]\n",
      "4645: [discriminator loss: 0.488787, acc: 0.742188] [adversarial loss: 1.600211, acc: 0.140625]\n",
      "4646: [discriminator loss: 0.512905, acc: 0.757812] [adversarial loss: 1.477084, acc: 0.218750]\n",
      "4647: [discriminator loss: 0.483815, acc: 0.773438] [adversarial loss: 1.643419, acc: 0.125000]\n",
      "4648: [discriminator loss: 0.588324, acc: 0.695312] [adversarial loss: 0.971672, acc: 0.390625]\n",
      "4649: [discriminator loss: 0.576428, acc: 0.718750] [adversarial loss: 1.816263, acc: 0.078125]\n",
      "4650: [discriminator loss: 0.529761, acc: 0.734375] [adversarial loss: 0.856626, acc: 0.484375]\n",
      "4651: [discriminator loss: 0.563746, acc: 0.687500] [adversarial loss: 1.588393, acc: 0.109375]\n",
      "4652: [discriminator loss: 0.556387, acc: 0.687500] [adversarial loss: 1.057668, acc: 0.343750]\n",
      "4653: [discriminator loss: 0.508495, acc: 0.781250] [adversarial loss: 1.726919, acc: 0.125000]\n",
      "4654: [discriminator loss: 0.516114, acc: 0.765625] [adversarial loss: 0.900784, acc: 0.453125]\n",
      "4655: [discriminator loss: 0.516200, acc: 0.773438] [adversarial loss: 1.525576, acc: 0.171875]\n",
      "4656: [discriminator loss: 0.455942, acc: 0.781250] [adversarial loss: 1.105839, acc: 0.203125]\n",
      "4657: [discriminator loss: 0.517518, acc: 0.734375] [adversarial loss: 1.371048, acc: 0.140625]\n",
      "4658: [discriminator loss: 0.528678, acc: 0.773438] [adversarial loss: 1.288007, acc: 0.187500]\n",
      "4659: [discriminator loss: 0.540061, acc: 0.750000] [adversarial loss: 1.271579, acc: 0.250000]\n",
      "4660: [discriminator loss: 0.504058, acc: 0.757812] [adversarial loss: 1.746163, acc: 0.125000]\n",
      "4661: [discriminator loss: 0.606613, acc: 0.742188] [adversarial loss: 0.760164, acc: 0.500000]\n",
      "4662: [discriminator loss: 0.609751, acc: 0.718750] [adversarial loss: 1.597579, acc: 0.062500]\n",
      "4663: [discriminator loss: 0.492156, acc: 0.773438] [adversarial loss: 1.016568, acc: 0.375000]\n",
      "4664: [discriminator loss: 0.563704, acc: 0.710938] [adversarial loss: 1.242438, acc: 0.250000]\n",
      "4665: [discriminator loss: 0.435365, acc: 0.804688] [adversarial loss: 1.242286, acc: 0.203125]\n",
      "4666: [discriminator loss: 0.480632, acc: 0.789062] [adversarial loss: 1.407343, acc: 0.218750]\n",
      "4667: [discriminator loss: 0.580033, acc: 0.664062] [adversarial loss: 1.322493, acc: 0.171875]\n",
      "4668: [discriminator loss: 0.600351, acc: 0.656250] [adversarial loss: 1.099889, acc: 0.265625]\n",
      "4669: [discriminator loss: 0.534531, acc: 0.773438] [adversarial loss: 1.237953, acc: 0.234375]\n",
      "4670: [discriminator loss: 0.462619, acc: 0.765625] [adversarial loss: 1.800064, acc: 0.109375]\n",
      "4671: [discriminator loss: 0.553480, acc: 0.718750] [adversarial loss: 1.131758, acc: 0.250000]\n",
      "4672: [discriminator loss: 0.481629, acc: 0.781250] [adversarial loss: 1.557427, acc: 0.171875]\n",
      "4673: [discriminator loss: 0.534105, acc: 0.695312] [adversarial loss: 1.075411, acc: 0.359375]\n",
      "4674: [discriminator loss: 0.496406, acc: 0.835938] [adversarial loss: 1.171443, acc: 0.281250]\n",
      "4675: [discriminator loss: 0.590352, acc: 0.726562] [adversarial loss: 1.455375, acc: 0.156250]\n",
      "4676: [discriminator loss: 0.491350, acc: 0.773438] [adversarial loss: 0.998738, acc: 0.406250]\n",
      "4677: [discriminator loss: 0.499680, acc: 0.757812] [adversarial loss: 1.478943, acc: 0.156250]\n",
      "4678: [discriminator loss: 0.538390, acc: 0.742188] [adversarial loss: 0.960142, acc: 0.343750]\n",
      "4679: [discriminator loss: 0.484440, acc: 0.781250] [adversarial loss: 1.756485, acc: 0.109375]\n",
      "4680: [discriminator loss: 0.594134, acc: 0.695312] [adversarial loss: 0.949958, acc: 0.406250]\n",
      "4681: [discriminator loss: 0.442437, acc: 0.828125] [adversarial loss: 1.636182, acc: 0.109375]\n",
      "4682: [discriminator loss: 0.558134, acc: 0.718750] [adversarial loss: 1.110281, acc: 0.375000]\n",
      "4683: [discriminator loss: 0.472167, acc: 0.726562] [adversarial loss: 1.494627, acc: 0.218750]\n",
      "4684: [discriminator loss: 0.543413, acc: 0.710938] [adversarial loss: 0.831770, acc: 0.390625]\n",
      "4685: [discriminator loss: 0.570500, acc: 0.734375] [adversarial loss: 1.599692, acc: 0.109375]\n",
      "4686: [discriminator loss: 0.528499, acc: 0.726562] [adversarial loss: 0.896252, acc: 0.390625]\n",
      "4687: [discriminator loss: 0.588123, acc: 0.664062] [adversarial loss: 1.463436, acc: 0.187500]\n",
      "4688: [discriminator loss: 0.534608, acc: 0.742188] [adversarial loss: 0.935781, acc: 0.421875]\n",
      "4689: [discriminator loss: 0.537769, acc: 0.718750] [adversarial loss: 1.617615, acc: 0.125000]\n",
      "4690: [discriminator loss: 0.482080, acc: 0.765625] [adversarial loss: 1.219683, acc: 0.250000]\n",
      "4691: [discriminator loss: 0.527767, acc: 0.742188] [adversarial loss: 1.331998, acc: 0.171875]\n",
      "4692: [discriminator loss: 0.506184, acc: 0.734375] [adversarial loss: 1.226247, acc: 0.328125]\n",
      "4693: [discriminator loss: 0.512623, acc: 0.812500] [adversarial loss: 1.769036, acc: 0.062500]\n",
      "4694: [discriminator loss: 0.534514, acc: 0.687500] [adversarial loss: 0.919628, acc: 0.296875]\n",
      "4695: [discriminator loss: 0.544622, acc: 0.742188] [adversarial loss: 1.621963, acc: 0.046875]\n",
      "4696: [discriminator loss: 0.507952, acc: 0.789062] [adversarial loss: 1.063984, acc: 0.328125]\n",
      "4697: [discriminator loss: 0.522892, acc: 0.750000] [adversarial loss: 1.865496, acc: 0.062500]\n",
      "4698: [discriminator loss: 0.480953, acc: 0.789062] [adversarial loss: 1.038901, acc: 0.359375]\n",
      "4699: [discriminator loss: 0.565492, acc: 0.710938] [adversarial loss: 1.271425, acc: 0.156250]\n",
      "4700: [discriminator loss: 0.463368, acc: 0.781250] [adversarial loss: 1.235974, acc: 0.281250]\n",
      "4701: [discriminator loss: 0.539523, acc: 0.695312] [adversarial loss: 1.099425, acc: 0.328125]\n",
      "4702: [discriminator loss: 0.527756, acc: 0.742188] [adversarial loss: 1.363580, acc: 0.156250]\n",
      "4703: [discriminator loss: 0.463306, acc: 0.757812] [adversarial loss: 1.121999, acc: 0.234375]\n",
      "4704: [discriminator loss: 0.411063, acc: 0.820312] [adversarial loss: 1.708198, acc: 0.093750]\n",
      "4705: [discriminator loss: 0.500356, acc: 0.734375] [adversarial loss: 0.933050, acc: 0.343750]\n",
      "4706: [discriminator loss: 0.601059, acc: 0.664062] [adversarial loss: 1.450263, acc: 0.187500]\n",
      "4707: [discriminator loss: 0.531532, acc: 0.750000] [adversarial loss: 1.202946, acc: 0.250000]\n",
      "4708: [discriminator loss: 0.560314, acc: 0.656250] [adversarial loss: 1.235510, acc: 0.218750]\n",
      "4709: [discriminator loss: 0.495757, acc: 0.750000] [adversarial loss: 0.974108, acc: 0.359375]\n",
      "4710: [discriminator loss: 0.488587, acc: 0.789062] [adversarial loss: 1.797515, acc: 0.093750]\n",
      "4711: [discriminator loss: 0.558640, acc: 0.710938] [adversarial loss: 0.995357, acc: 0.296875]\n",
      "4712: [discriminator loss: 0.506661, acc: 0.789062] [adversarial loss: 1.791352, acc: 0.046875]\n",
      "4713: [discriminator loss: 0.575342, acc: 0.710938] [adversarial loss: 0.852456, acc: 0.437500]\n",
      "4714: [discriminator loss: 0.599171, acc: 0.640625] [adversarial loss: 1.580081, acc: 0.125000]\n",
      "4715: [discriminator loss: 0.553851, acc: 0.742188] [adversarial loss: 1.049931, acc: 0.296875]\n",
      "4716: [discriminator loss: 0.495744, acc: 0.750000] [adversarial loss: 1.361010, acc: 0.187500]\n",
      "4717: [discriminator loss: 0.553529, acc: 0.695312] [adversarial loss: 1.032229, acc: 0.343750]\n",
      "4718: [discriminator loss: 0.520231, acc: 0.750000] [adversarial loss: 1.532724, acc: 0.125000]\n",
      "4719: [discriminator loss: 0.484313, acc: 0.742188] [adversarial loss: 0.921578, acc: 0.390625]\n",
      "4720: [discriminator loss: 0.504106, acc: 0.718750] [adversarial loss: 1.425262, acc: 0.187500]\n",
      "4721: [discriminator loss: 0.466845, acc: 0.789062] [adversarial loss: 1.215091, acc: 0.296875]\n",
      "4722: [discriminator loss: 0.573735, acc: 0.687500] [adversarial loss: 1.488354, acc: 0.171875]\n",
      "4723: [discriminator loss: 0.462105, acc: 0.796875] [adversarial loss: 1.392537, acc: 0.109375]\n",
      "4724: [discriminator loss: 0.544204, acc: 0.726562] [adversarial loss: 1.276900, acc: 0.218750]\n",
      "4725: [discriminator loss: 0.469949, acc: 0.773438] [adversarial loss: 1.081830, acc: 0.281250]\n",
      "4726: [discriminator loss: 0.488854, acc: 0.726562] [adversarial loss: 1.652268, acc: 0.078125]\n",
      "4727: [discriminator loss: 0.497798, acc: 0.757812] [adversarial loss: 1.020813, acc: 0.328125]\n",
      "4728: [discriminator loss: 0.505818, acc: 0.781250] [adversarial loss: 1.634784, acc: 0.140625]\n",
      "4729: [discriminator loss: 0.578223, acc: 0.679688] [adversarial loss: 1.092300, acc: 0.312500]\n",
      "4730: [discriminator loss: 0.531522, acc: 0.710938] [adversarial loss: 0.927351, acc: 0.375000]\n",
      "4731: [discriminator loss: 0.534282, acc: 0.750000] [adversarial loss: 1.709832, acc: 0.078125]\n",
      "4732: [discriminator loss: 0.484312, acc: 0.757812] [adversarial loss: 1.115729, acc: 0.296875]\n",
      "4733: [discriminator loss: 0.439850, acc: 0.765625] [adversarial loss: 1.485623, acc: 0.187500]\n",
      "4734: [discriminator loss: 0.554380, acc: 0.726562] [adversarial loss: 1.168388, acc: 0.218750]\n",
      "4735: [discriminator loss: 0.513536, acc: 0.703125] [adversarial loss: 1.576887, acc: 0.125000]\n",
      "4736: [discriminator loss: 0.470207, acc: 0.796875] [adversarial loss: 1.325942, acc: 0.171875]\n",
      "4737: [discriminator loss: 0.574216, acc: 0.679688] [adversarial loss: 1.263185, acc: 0.156250]\n",
      "4738: [discriminator loss: 0.542758, acc: 0.734375] [adversarial loss: 1.013437, acc: 0.328125]\n",
      "4739: [discriminator loss: 0.512359, acc: 0.750000] [adversarial loss: 1.521821, acc: 0.078125]\n",
      "4740: [discriminator loss: 0.582945, acc: 0.695312] [adversarial loss: 0.856078, acc: 0.468750]\n",
      "4741: [discriminator loss: 0.589868, acc: 0.695312] [adversarial loss: 1.778264, acc: 0.078125]\n",
      "4742: [discriminator loss: 0.571180, acc: 0.703125] [adversarial loss: 0.828494, acc: 0.421875]\n",
      "4743: [discriminator loss: 0.460723, acc: 0.812500] [adversarial loss: 1.717959, acc: 0.093750]\n",
      "4744: [discriminator loss: 0.555075, acc: 0.765625] [adversarial loss: 1.076066, acc: 0.281250]\n",
      "4745: [discriminator loss: 0.529925, acc: 0.726562] [adversarial loss: 1.334664, acc: 0.203125]\n",
      "4746: [discriminator loss: 0.570437, acc: 0.695312] [adversarial loss: 1.149195, acc: 0.187500]\n",
      "4747: [discriminator loss: 0.496940, acc: 0.781250] [adversarial loss: 1.613871, acc: 0.109375]\n",
      "4748: [discriminator loss: 0.521283, acc: 0.695312] [adversarial loss: 1.099845, acc: 0.250000]\n",
      "4749: [discriminator loss: 0.578014, acc: 0.687500] [adversarial loss: 1.776204, acc: 0.046875]\n",
      "4750: [discriminator loss: 0.553010, acc: 0.718750] [adversarial loss: 0.815092, acc: 0.500000]\n",
      "4751: [discriminator loss: 0.618603, acc: 0.648438] [adversarial loss: 1.371886, acc: 0.125000]\n",
      "4752: [discriminator loss: 0.515872, acc: 0.718750] [adversarial loss: 0.899976, acc: 0.437500]\n",
      "4753: [discriminator loss: 0.513955, acc: 0.734375] [adversarial loss: 1.185622, acc: 0.187500]\n",
      "4754: [discriminator loss: 0.516924, acc: 0.734375] [adversarial loss: 1.104147, acc: 0.312500]\n",
      "4755: [discriminator loss: 0.546356, acc: 0.703125] [adversarial loss: 1.543024, acc: 0.109375]\n",
      "4756: [discriminator loss: 0.509836, acc: 0.718750] [adversarial loss: 0.974459, acc: 0.406250]\n",
      "4757: [discriminator loss: 0.531017, acc: 0.742188] [adversarial loss: 1.347797, acc: 0.218750]\n",
      "4758: [discriminator loss: 0.532737, acc: 0.734375] [adversarial loss: 1.167481, acc: 0.359375]\n",
      "4759: [discriminator loss: 0.418358, acc: 0.820312] [adversarial loss: 1.276298, acc: 0.218750]\n",
      "4760: [discriminator loss: 0.476796, acc: 0.804688] [adversarial loss: 1.563154, acc: 0.093750]\n",
      "4761: [discriminator loss: 0.649605, acc: 0.648438] [adversarial loss: 1.009272, acc: 0.328125]\n",
      "4762: [discriminator loss: 0.543019, acc: 0.718750] [adversarial loss: 1.952994, acc: 0.109375]\n",
      "4763: [discriminator loss: 0.536712, acc: 0.664062] [adversarial loss: 0.923175, acc: 0.375000]\n",
      "4764: [discriminator loss: 0.463789, acc: 0.828125] [adversarial loss: 1.427659, acc: 0.125000]\n",
      "4765: [discriminator loss: 0.548860, acc: 0.734375] [adversarial loss: 1.242271, acc: 0.250000]\n",
      "4766: [discriminator loss: 0.536529, acc: 0.765625] [adversarial loss: 1.165331, acc: 0.343750]\n",
      "4767: [discriminator loss: 0.412001, acc: 0.812500] [adversarial loss: 1.287571, acc: 0.218750]\n",
      "4768: [discriminator loss: 0.485681, acc: 0.796875] [adversarial loss: 1.031601, acc: 0.328125]\n",
      "4769: [discriminator loss: 0.562418, acc: 0.695312] [adversarial loss: 1.349566, acc: 0.265625]\n",
      "4770: [discriminator loss: 0.561424, acc: 0.726562] [adversarial loss: 1.053343, acc: 0.406250]\n",
      "4771: [discriminator loss: 0.433949, acc: 0.828125] [adversarial loss: 1.636752, acc: 0.078125]\n",
      "4772: [discriminator loss: 0.480236, acc: 0.750000] [adversarial loss: 1.379148, acc: 0.171875]\n",
      "4773: [discriminator loss: 0.510384, acc: 0.750000] [adversarial loss: 1.100535, acc: 0.265625]\n",
      "4774: [discriminator loss: 0.467377, acc: 0.750000] [adversarial loss: 1.518938, acc: 0.093750]\n",
      "4775: [discriminator loss: 0.611762, acc: 0.632812] [adversarial loss: 1.244906, acc: 0.203125]\n",
      "4776: [discriminator loss: 0.509628, acc: 0.710938] [adversarial loss: 1.444676, acc: 0.109375]\n",
      "4777: [discriminator loss: 0.539198, acc: 0.734375] [adversarial loss: 0.736521, acc: 0.531250]\n",
      "4778: [discriminator loss: 0.493580, acc: 0.726562] [adversarial loss: 1.608020, acc: 0.125000]\n",
      "4779: [discriminator loss: 0.504450, acc: 0.687500] [adversarial loss: 1.197745, acc: 0.296875]\n",
      "4780: [discriminator loss: 0.556327, acc: 0.718750] [adversarial loss: 1.190691, acc: 0.265625]\n",
      "4781: [discriminator loss: 0.493504, acc: 0.757812] [adversarial loss: 1.490317, acc: 0.093750]\n",
      "4782: [discriminator loss: 0.452064, acc: 0.781250] [adversarial loss: 1.176973, acc: 0.218750]\n",
      "4783: [discriminator loss: 0.481759, acc: 0.734375] [adversarial loss: 1.309241, acc: 0.218750]\n",
      "4784: [discriminator loss: 0.476215, acc: 0.750000] [adversarial loss: 1.269968, acc: 0.250000]\n",
      "4785: [discriminator loss: 0.576803, acc: 0.687500] [adversarial loss: 1.458356, acc: 0.156250]\n",
      "4786: [discriminator loss: 0.558432, acc: 0.687500] [adversarial loss: 1.016303, acc: 0.328125]\n",
      "4787: [discriminator loss: 0.512021, acc: 0.703125] [adversarial loss: 1.587949, acc: 0.187500]\n",
      "4788: [discriminator loss: 0.546686, acc: 0.750000] [adversarial loss: 1.133890, acc: 0.265625]\n",
      "4789: [discriminator loss: 0.486552, acc: 0.750000] [adversarial loss: 1.145290, acc: 0.250000]\n",
      "4790: [discriminator loss: 0.458132, acc: 0.765625] [adversarial loss: 1.784440, acc: 0.078125]\n",
      "4791: [discriminator loss: 0.545691, acc: 0.710938] [adversarial loss: 0.884578, acc: 0.453125]\n",
      "4792: [discriminator loss: 0.541492, acc: 0.703125] [adversarial loss: 1.873774, acc: 0.078125]\n",
      "4793: [discriminator loss: 0.558487, acc: 0.679688] [adversarial loss: 0.801004, acc: 0.484375]\n",
      "4794: [discriminator loss: 0.522051, acc: 0.726562] [adversarial loss: 1.812991, acc: 0.078125]\n",
      "4795: [discriminator loss: 0.641142, acc: 0.718750] [adversarial loss: 0.978304, acc: 0.343750]\n",
      "4796: [discriminator loss: 0.573190, acc: 0.671875] [adversarial loss: 1.541396, acc: 0.156250]\n",
      "4797: [discriminator loss: 0.527972, acc: 0.750000] [adversarial loss: 1.218263, acc: 0.203125]\n",
      "4798: [discriminator loss: 0.519088, acc: 0.718750] [adversarial loss: 1.237505, acc: 0.234375]\n",
      "4799: [discriminator loss: 0.533210, acc: 0.765625] [adversarial loss: 1.060107, acc: 0.328125]\n",
      "4800: [discriminator loss: 0.524541, acc: 0.773438] [adversarial loss: 1.516858, acc: 0.125000]\n",
      "4801: [discriminator loss: 0.523285, acc: 0.742188] [adversarial loss: 1.203298, acc: 0.234375]\n",
      "4802: [discriminator loss: 0.462660, acc: 0.796875] [adversarial loss: 1.417673, acc: 0.156250]\n",
      "4803: [discriminator loss: 0.523977, acc: 0.750000] [adversarial loss: 1.263842, acc: 0.203125]\n",
      "4804: [discriminator loss: 0.482661, acc: 0.726562] [adversarial loss: 1.227355, acc: 0.250000]\n",
      "4805: [discriminator loss: 0.556327, acc: 0.710938] [adversarial loss: 1.615064, acc: 0.093750]\n",
      "4806: [discriminator loss: 0.539729, acc: 0.710938] [adversarial loss: 1.046923, acc: 0.328125]\n",
      "4807: [discriminator loss: 0.535809, acc: 0.710938] [adversarial loss: 1.704916, acc: 0.109375]\n",
      "4808: [discriminator loss: 0.537614, acc: 0.703125] [adversarial loss: 1.050132, acc: 0.312500]\n",
      "4809: [discriminator loss: 0.593781, acc: 0.656250] [adversarial loss: 1.571166, acc: 0.156250]\n",
      "4810: [discriminator loss: 0.519367, acc: 0.718750] [adversarial loss: 1.326361, acc: 0.156250]\n",
      "4811: [discriminator loss: 0.525503, acc: 0.710938] [adversarial loss: 1.254996, acc: 0.171875]\n",
      "4812: [discriminator loss: 0.453151, acc: 0.781250] [adversarial loss: 1.126262, acc: 0.187500]\n",
      "4813: [discriminator loss: 0.489786, acc: 0.734375] [adversarial loss: 1.504298, acc: 0.171875]\n",
      "4814: [discriminator loss: 0.451342, acc: 0.773438] [adversarial loss: 1.012594, acc: 0.281250]\n",
      "4815: [discriminator loss: 0.464601, acc: 0.781250] [adversarial loss: 1.358941, acc: 0.171875]\n",
      "4816: [discriminator loss: 0.572119, acc: 0.718750] [adversarial loss: 1.193849, acc: 0.265625]\n",
      "4817: [discriminator loss: 0.494403, acc: 0.812500] [adversarial loss: 1.542736, acc: 0.125000]\n",
      "4818: [discriminator loss: 0.498814, acc: 0.757812] [adversarial loss: 1.198149, acc: 0.250000]\n",
      "4819: [discriminator loss: 0.518851, acc: 0.734375] [adversarial loss: 1.493094, acc: 0.109375]\n",
      "4820: [discriminator loss: 0.578909, acc: 0.710938] [adversarial loss: 1.345246, acc: 0.171875]\n",
      "4821: [discriminator loss: 0.445804, acc: 0.757812] [adversarial loss: 1.186374, acc: 0.312500]\n",
      "4822: [discriminator loss: 0.490737, acc: 0.742188] [adversarial loss: 1.653573, acc: 0.156250]\n",
      "4823: [discriminator loss: 0.549822, acc: 0.726562] [adversarial loss: 1.087653, acc: 0.296875]\n",
      "4824: [discriminator loss: 0.551592, acc: 0.726562] [adversarial loss: 1.578408, acc: 0.125000]\n",
      "4825: [discriminator loss: 0.597668, acc: 0.703125] [adversarial loss: 0.722778, acc: 0.562500]\n",
      "4826: [discriminator loss: 0.631745, acc: 0.664062] [adversarial loss: 1.791043, acc: 0.093750]\n",
      "4827: [discriminator loss: 0.536511, acc: 0.726562] [adversarial loss: 0.923280, acc: 0.453125]\n",
      "4828: [discriminator loss: 0.644396, acc: 0.593750] [adversarial loss: 1.545835, acc: 0.140625]\n",
      "4829: [discriminator loss: 0.503165, acc: 0.765625] [adversarial loss: 1.117974, acc: 0.296875]\n",
      "4830: [discriminator loss: 0.582544, acc: 0.718750] [adversarial loss: 1.343881, acc: 0.218750]\n",
      "4831: [discriminator loss: 0.547813, acc: 0.679688] [adversarial loss: 1.052168, acc: 0.343750]\n",
      "4832: [discriminator loss: 0.541297, acc: 0.710938] [adversarial loss: 1.431334, acc: 0.109375]\n",
      "4833: [discriminator loss: 0.472029, acc: 0.765625] [adversarial loss: 1.091911, acc: 0.343750]\n",
      "4834: [discriminator loss: 0.577632, acc: 0.656250] [adversarial loss: 1.542055, acc: 0.109375]\n",
      "4835: [discriminator loss: 0.500712, acc: 0.726562] [adversarial loss: 1.025476, acc: 0.328125]\n",
      "4836: [discriminator loss: 0.489215, acc: 0.765625] [adversarial loss: 1.411422, acc: 0.187500]\n",
      "4837: [discriminator loss: 0.530866, acc: 0.765625] [adversarial loss: 0.981552, acc: 0.328125]\n",
      "4838: [discriminator loss: 0.547904, acc: 0.695312] [adversarial loss: 1.632200, acc: 0.109375]\n",
      "4839: [discriminator loss: 0.489802, acc: 0.757812] [adversarial loss: 1.046341, acc: 0.250000]\n",
      "4840: [discriminator loss: 0.521875, acc: 0.726562] [adversarial loss: 1.477721, acc: 0.171875]\n",
      "4841: [discriminator loss: 0.495849, acc: 0.757812] [adversarial loss: 1.183724, acc: 0.171875]\n",
      "4842: [discriminator loss: 0.489412, acc: 0.789062] [adversarial loss: 1.365101, acc: 0.171875]\n",
      "4843: [discriminator loss: 0.545624, acc: 0.757812] [adversarial loss: 1.832998, acc: 0.125000]\n",
      "4844: [discriminator loss: 0.572265, acc: 0.710938] [adversarial loss: 0.829891, acc: 0.531250]\n",
      "4845: [discriminator loss: 0.517714, acc: 0.773438] [adversarial loss: 1.489491, acc: 0.156250]\n",
      "4846: [discriminator loss: 0.537318, acc: 0.734375] [adversarial loss: 0.922726, acc: 0.437500]\n",
      "4847: [discriminator loss: 0.538046, acc: 0.718750] [adversarial loss: 1.614197, acc: 0.125000]\n",
      "4848: [discriminator loss: 0.542962, acc: 0.726562] [adversarial loss: 1.239743, acc: 0.171875]\n",
      "4849: [discriminator loss: 0.489449, acc: 0.773438] [adversarial loss: 1.507797, acc: 0.140625]\n",
      "4850: [discriminator loss: 0.543531, acc: 0.734375] [adversarial loss: 1.096193, acc: 0.296875]\n",
      "4851: [discriminator loss: 0.499385, acc: 0.750000] [adversarial loss: 1.391752, acc: 0.156250]\n",
      "4852: [discriminator loss: 0.609171, acc: 0.664062] [adversarial loss: 1.000047, acc: 0.390625]\n",
      "4853: [discriminator loss: 0.549554, acc: 0.718750] [adversarial loss: 1.262774, acc: 0.250000]\n",
      "4854: [discriminator loss: 0.595175, acc: 0.671875] [adversarial loss: 1.414878, acc: 0.140625]\n",
      "4855: [discriminator loss: 0.524766, acc: 0.750000] [adversarial loss: 1.139921, acc: 0.265625]\n",
      "4856: [discriminator loss: 0.446187, acc: 0.796875] [adversarial loss: 1.295986, acc: 0.156250]\n",
      "4857: [discriminator loss: 0.463851, acc: 0.804688] [adversarial loss: 1.469481, acc: 0.156250]\n",
      "4858: [discriminator loss: 0.525878, acc: 0.742188] [adversarial loss: 1.193070, acc: 0.250000]\n",
      "4859: [discriminator loss: 0.470967, acc: 0.757812] [adversarial loss: 1.269192, acc: 0.218750]\n",
      "4860: [discriminator loss: 0.528408, acc: 0.726562] [adversarial loss: 1.158494, acc: 0.312500]\n",
      "4861: [discriminator loss: 0.386667, acc: 0.875000] [adversarial loss: 1.196041, acc: 0.328125]\n",
      "4862: [discriminator loss: 0.514246, acc: 0.734375] [adversarial loss: 1.189067, acc: 0.203125]\n",
      "4863: [discriminator loss: 0.509159, acc: 0.757812] [adversarial loss: 1.119942, acc: 0.343750]\n",
      "4864: [discriminator loss: 0.437444, acc: 0.789062] [adversarial loss: 1.430273, acc: 0.156250]\n",
      "4865: [discriminator loss: 0.484710, acc: 0.757812] [adversarial loss: 1.046227, acc: 0.296875]\n",
      "4866: [discriminator loss: 0.600293, acc: 0.671875] [adversarial loss: 2.102728, acc: 0.062500]\n",
      "4867: [discriminator loss: 0.619988, acc: 0.664062] [adversarial loss: 0.902045, acc: 0.375000]\n",
      "4868: [discriminator loss: 0.567182, acc: 0.710938] [adversarial loss: 1.654631, acc: 0.062500]\n",
      "4869: [discriminator loss: 0.604932, acc: 0.695312] [adversarial loss: 0.998122, acc: 0.421875]\n",
      "4870: [discriminator loss: 0.512948, acc: 0.710938] [adversarial loss: 1.812279, acc: 0.078125]\n",
      "4871: [discriminator loss: 0.482790, acc: 0.765625] [adversarial loss: 1.119689, acc: 0.250000]\n",
      "4872: [discriminator loss: 0.523354, acc: 0.742188] [adversarial loss: 1.475163, acc: 0.125000]\n",
      "4873: [discriminator loss: 0.481525, acc: 0.750000] [adversarial loss: 1.150864, acc: 0.218750]\n",
      "4874: [discriminator loss: 0.509435, acc: 0.695312] [adversarial loss: 1.499072, acc: 0.125000]\n",
      "4875: [discriminator loss: 0.485067, acc: 0.820312] [adversarial loss: 1.389732, acc: 0.156250]\n",
      "4876: [discriminator loss: 0.493895, acc: 0.765625] [adversarial loss: 1.032681, acc: 0.312500]\n",
      "4877: [discriminator loss: 0.553277, acc: 0.695312] [adversarial loss: 1.314510, acc: 0.203125]\n",
      "4878: [discriminator loss: 0.516670, acc: 0.773438] [adversarial loss: 1.293400, acc: 0.171875]\n",
      "4879: [discriminator loss: 0.438852, acc: 0.812500] [adversarial loss: 1.310113, acc: 0.203125]\n",
      "4880: [discriminator loss: 0.541929, acc: 0.703125] [adversarial loss: 0.989789, acc: 0.296875]\n",
      "4881: [discriminator loss: 0.457809, acc: 0.773438] [adversarial loss: 1.514028, acc: 0.093750]\n",
      "4882: [discriminator loss: 0.495037, acc: 0.750000] [adversarial loss: 0.840557, acc: 0.468750]\n",
      "4883: [discriminator loss: 0.531036, acc: 0.765625] [adversarial loss: 1.567710, acc: 0.187500]\n",
      "4884: [discriminator loss: 0.667318, acc: 0.625000] [adversarial loss: 0.703844, acc: 0.546875]\n",
      "4885: [discriminator loss: 0.536391, acc: 0.734375] [adversarial loss: 1.587494, acc: 0.125000]\n",
      "4886: [discriminator loss: 0.495227, acc: 0.742188] [adversarial loss: 1.113023, acc: 0.375000]\n",
      "4887: [discriminator loss: 0.489713, acc: 0.734375] [adversarial loss: 1.347572, acc: 0.187500]\n",
      "4888: [discriminator loss: 0.542345, acc: 0.710938] [adversarial loss: 1.271003, acc: 0.203125]\n",
      "4889: [discriminator loss: 0.496004, acc: 0.734375] [adversarial loss: 1.188991, acc: 0.203125]\n",
      "4890: [discriminator loss: 0.522089, acc: 0.718750] [adversarial loss: 1.508904, acc: 0.015625]\n",
      "4891: [discriminator loss: 0.541355, acc: 0.726562] [adversarial loss: 0.858425, acc: 0.390625]\n",
      "4892: [discriminator loss: 0.526049, acc: 0.742188] [adversarial loss: 1.183693, acc: 0.218750]\n",
      "4893: [discriminator loss: 0.498347, acc: 0.750000] [adversarial loss: 1.529048, acc: 0.062500]\n",
      "4894: [discriminator loss: 0.447986, acc: 0.781250] [adversarial loss: 0.988576, acc: 0.343750]\n",
      "4895: [discriminator loss: 0.520140, acc: 0.687500] [adversarial loss: 1.673683, acc: 0.109375]\n",
      "4896: [discriminator loss: 0.475214, acc: 0.781250] [adversarial loss: 1.191824, acc: 0.281250]\n",
      "4897: [discriminator loss: 0.548885, acc: 0.718750] [adversarial loss: 1.352099, acc: 0.203125]\n",
      "4898: [discriminator loss: 0.418869, acc: 0.828125] [adversarial loss: 1.040199, acc: 0.390625]\n",
      "4899: [discriminator loss: 0.516135, acc: 0.726562] [adversarial loss: 1.402984, acc: 0.234375]\n",
      "4900: [discriminator loss: 0.520573, acc: 0.750000] [adversarial loss: 1.407409, acc: 0.187500]\n",
      "4901: [discriminator loss: 0.625732, acc: 0.640625] [adversarial loss: 1.636418, acc: 0.109375]\n",
      "4902: [discriminator loss: 0.557783, acc: 0.695312] [adversarial loss: 1.080256, acc: 0.390625]\n",
      "4903: [discriminator loss: 0.590631, acc: 0.710938] [adversarial loss: 1.512795, acc: 0.218750]\n",
      "4904: [discriminator loss: 0.633640, acc: 0.703125] [adversarial loss: 0.817608, acc: 0.484375]\n",
      "4905: [discriminator loss: 0.573104, acc: 0.687500] [adversarial loss: 1.592003, acc: 0.046875]\n",
      "4906: [discriminator loss: 0.571432, acc: 0.710938] [adversarial loss: 0.882530, acc: 0.406250]\n",
      "4907: [discriminator loss: 0.493216, acc: 0.765625] [adversarial loss: 1.394304, acc: 0.125000]\n",
      "4908: [discriminator loss: 0.506936, acc: 0.773438] [adversarial loss: 1.160652, acc: 0.312500]\n",
      "4909: [discriminator loss: 0.533661, acc: 0.703125] [adversarial loss: 1.420231, acc: 0.156250]\n",
      "4910: [discriminator loss: 0.445409, acc: 0.796875] [adversarial loss: 0.978847, acc: 0.328125]\n",
      "4911: [discriminator loss: 0.522591, acc: 0.750000] [adversarial loss: 1.563720, acc: 0.062500]\n",
      "4912: [discriminator loss: 0.567756, acc: 0.687500] [adversarial loss: 0.980570, acc: 0.484375]\n",
      "4913: [discriminator loss: 0.469956, acc: 0.726562] [adversarial loss: 1.423188, acc: 0.140625]\n",
      "4914: [discriminator loss: 0.621485, acc: 0.687500] [adversarial loss: 1.145659, acc: 0.296875]\n",
      "4915: [discriminator loss: 0.485904, acc: 0.734375] [adversarial loss: 1.472597, acc: 0.109375]\n",
      "4916: [discriminator loss: 0.518089, acc: 0.742188] [adversarial loss: 1.055650, acc: 0.328125]\n",
      "4917: [discriminator loss: 0.506386, acc: 0.750000] [adversarial loss: 1.066622, acc: 0.296875]\n",
      "4918: [discriminator loss: 0.438742, acc: 0.812500] [adversarial loss: 1.418788, acc: 0.140625]\n",
      "4919: [discriminator loss: 0.517136, acc: 0.757812] [adversarial loss: 1.294621, acc: 0.234375]\n",
      "4920: [discriminator loss: 0.545803, acc: 0.687500] [adversarial loss: 1.575267, acc: 0.140625]\n",
      "4921: [discriminator loss: 0.559656, acc: 0.703125] [adversarial loss: 1.049799, acc: 0.421875]\n",
      "4922: [discriminator loss: 0.512880, acc: 0.750000] [adversarial loss: 1.763926, acc: 0.062500]\n",
      "4923: [discriminator loss: 0.585401, acc: 0.632812] [adversarial loss: 0.842041, acc: 0.468750]\n",
      "4924: [discriminator loss: 0.530478, acc: 0.687500] [adversarial loss: 1.798068, acc: 0.046875]\n",
      "4925: [discriminator loss: 0.547526, acc: 0.726562] [adversarial loss: 1.240021, acc: 0.218750]\n",
      "4926: [discriminator loss: 0.451626, acc: 0.828125] [adversarial loss: 1.457280, acc: 0.109375]\n",
      "4927: [discriminator loss: 0.468596, acc: 0.765625] [adversarial loss: 1.137543, acc: 0.296875]\n",
      "4928: [discriminator loss: 0.395539, acc: 0.867188] [adversarial loss: 1.319737, acc: 0.140625]\n",
      "4929: [discriminator loss: 0.593105, acc: 0.648438] [adversarial loss: 1.248921, acc: 0.171875]\n",
      "4930: [discriminator loss: 0.561746, acc: 0.703125] [adversarial loss: 1.300414, acc: 0.234375]\n",
      "4931: [discriminator loss: 0.517129, acc: 0.718750] [adversarial loss: 1.300431, acc: 0.203125]\n",
      "4932: [discriminator loss: 0.516100, acc: 0.750000] [adversarial loss: 1.328895, acc: 0.203125]\n",
      "4933: [discriminator loss: 0.523483, acc: 0.734375] [adversarial loss: 1.207590, acc: 0.218750]\n",
      "4934: [discriminator loss: 0.450599, acc: 0.789062] [adversarial loss: 1.122230, acc: 0.359375]\n",
      "4935: [discriminator loss: 0.542593, acc: 0.742188] [adversarial loss: 1.346902, acc: 0.203125]\n",
      "4936: [discriminator loss: 0.527858, acc: 0.710938] [adversarial loss: 1.364839, acc: 0.218750]\n",
      "4937: [discriminator loss: 0.539118, acc: 0.734375] [adversarial loss: 1.409193, acc: 0.171875]\n",
      "4938: [discriminator loss: 0.510449, acc: 0.757812] [adversarial loss: 1.396668, acc: 0.187500]\n",
      "4939: [discriminator loss: 0.500683, acc: 0.742188] [adversarial loss: 1.422341, acc: 0.125000]\n",
      "4940: [discriminator loss: 0.548355, acc: 0.710938] [adversarial loss: 1.006100, acc: 0.296875]\n",
      "4941: [discriminator loss: 0.486214, acc: 0.726562] [adversarial loss: 1.708055, acc: 0.093750]\n",
      "4942: [discriminator loss: 0.478639, acc: 0.750000] [adversarial loss: 0.976357, acc: 0.375000]\n",
      "4943: [discriminator loss: 0.576285, acc: 0.734375] [adversarial loss: 1.750190, acc: 0.046875]\n",
      "4944: [discriminator loss: 0.495159, acc: 0.781250] [adversarial loss: 1.111996, acc: 0.390625]\n",
      "4945: [discriminator loss: 0.508702, acc: 0.726562] [adversarial loss: 1.680147, acc: 0.140625]\n",
      "4946: [discriminator loss: 0.561085, acc: 0.695312] [adversarial loss: 1.226123, acc: 0.203125]\n",
      "4947: [discriminator loss: 0.537034, acc: 0.718750] [adversarial loss: 1.041003, acc: 0.328125]\n",
      "4948: [discriminator loss: 0.514613, acc: 0.710938] [adversarial loss: 1.640895, acc: 0.093750]\n",
      "4949: [discriminator loss: 0.445641, acc: 0.812500] [adversarial loss: 1.092419, acc: 0.343750]\n",
      "4950: [discriminator loss: 0.569658, acc: 0.679688] [adversarial loss: 1.259964, acc: 0.203125]\n",
      "4951: [discriminator loss: 0.474600, acc: 0.789062] [adversarial loss: 1.019983, acc: 0.328125]\n",
      "4952: [discriminator loss: 0.487867, acc: 0.742188] [adversarial loss: 1.698537, acc: 0.140625]\n",
      "4953: [discriminator loss: 0.614200, acc: 0.671875] [adversarial loss: 1.041989, acc: 0.375000]\n",
      "4954: [discriminator loss: 0.572452, acc: 0.718750] [adversarial loss: 1.376423, acc: 0.187500]\n",
      "4955: [discriminator loss: 0.584482, acc: 0.664062] [adversarial loss: 1.134281, acc: 0.203125]\n",
      "4956: [discriminator loss: 0.480680, acc: 0.757812] [adversarial loss: 1.555535, acc: 0.062500]\n",
      "4957: [discriminator loss: 0.517292, acc: 0.757812] [adversarial loss: 1.097993, acc: 0.375000]\n",
      "4958: [discriminator loss: 0.476705, acc: 0.757812] [adversarial loss: 1.626469, acc: 0.078125]\n",
      "4959: [discriminator loss: 0.441287, acc: 0.765625] [adversarial loss: 1.189755, acc: 0.218750]\n",
      "4960: [discriminator loss: 0.511557, acc: 0.773438] [adversarial loss: 1.694752, acc: 0.062500]\n",
      "4961: [discriminator loss: 0.508809, acc: 0.757812] [adversarial loss: 1.430060, acc: 0.140625]\n",
      "4962: [discriminator loss: 0.476543, acc: 0.796875] [adversarial loss: 1.147885, acc: 0.281250]\n",
      "4963: [discriminator loss: 0.509836, acc: 0.734375] [adversarial loss: 1.410742, acc: 0.140625]\n",
      "4964: [discriminator loss: 0.583024, acc: 0.695312] [adversarial loss: 1.562508, acc: 0.109375]\n",
      "4965: [discriminator loss: 0.479085, acc: 0.765625] [adversarial loss: 1.321252, acc: 0.203125]\n",
      "4966: [discriminator loss: 0.482368, acc: 0.765625] [adversarial loss: 1.566807, acc: 0.156250]\n",
      "4967: [discriminator loss: 0.491274, acc: 0.781250] [adversarial loss: 0.800491, acc: 0.546875]\n",
      "4968: [discriminator loss: 0.550041, acc: 0.718750] [adversarial loss: 2.009368, acc: 0.093750]\n",
      "4969: [discriminator loss: 0.628916, acc: 0.679688] [adversarial loss: 0.857779, acc: 0.453125]\n",
      "4970: [discriminator loss: 0.572242, acc: 0.671875] [adversarial loss: 1.570695, acc: 0.093750]\n",
      "4971: [discriminator loss: 0.498309, acc: 0.781250] [adversarial loss: 1.186034, acc: 0.250000]\n",
      "4972: [discriminator loss: 0.508918, acc: 0.773438] [adversarial loss: 1.116372, acc: 0.328125]\n",
      "4973: [discriminator loss: 0.494301, acc: 0.773438] [adversarial loss: 1.670906, acc: 0.109375]\n",
      "4974: [discriminator loss: 0.510406, acc: 0.781250] [adversarial loss: 1.069161, acc: 0.265625]\n",
      "4975: [discriminator loss: 0.435070, acc: 0.765625] [adversarial loss: 1.279119, acc: 0.218750]\n",
      "4976: [discriminator loss: 0.488601, acc: 0.796875] [adversarial loss: 1.407465, acc: 0.171875]\n",
      "4977: [discriminator loss: 0.476264, acc: 0.789062] [adversarial loss: 1.102212, acc: 0.250000]\n",
      "4978: [discriminator loss: 0.518989, acc: 0.734375] [adversarial loss: 1.120589, acc: 0.265625]\n",
      "4979: [discriminator loss: 0.550233, acc: 0.679688] [adversarial loss: 1.214717, acc: 0.250000]\n",
      "4980: [discriminator loss: 0.574401, acc: 0.726562] [adversarial loss: 1.459813, acc: 0.234375]\n",
      "4981: [discriminator loss: 0.515799, acc: 0.757812] [adversarial loss: 1.325642, acc: 0.265625]\n",
      "4982: [discriminator loss: 0.484519, acc: 0.734375] [adversarial loss: 0.953311, acc: 0.406250]\n",
      "4983: [discriminator loss: 0.541708, acc: 0.742188] [adversarial loss: 1.700456, acc: 0.062500]\n",
      "4984: [discriminator loss: 0.605804, acc: 0.703125] [adversarial loss: 0.957534, acc: 0.406250]\n",
      "4985: [discriminator loss: 0.580101, acc: 0.695312] [adversarial loss: 1.579558, acc: 0.140625]\n",
      "4986: [discriminator loss: 0.469583, acc: 0.765625] [adversarial loss: 1.075349, acc: 0.281250]\n",
      "4987: [discriminator loss: 0.500251, acc: 0.804688] [adversarial loss: 1.800887, acc: 0.093750]\n",
      "4988: [discriminator loss: 0.531046, acc: 0.773438] [adversarial loss: 1.170535, acc: 0.281250]\n",
      "4989: [discriminator loss: 0.523694, acc: 0.757812] [adversarial loss: 1.181456, acc: 0.328125]\n",
      "4990: [discriminator loss: 0.495935, acc: 0.781250] [adversarial loss: 1.116539, acc: 0.359375]\n",
      "4991: [discriminator loss: 0.613482, acc: 0.664062] [adversarial loss: 1.052612, acc: 0.328125]\n",
      "4992: [discriminator loss: 0.452296, acc: 0.773438] [adversarial loss: 1.533163, acc: 0.125000]\n",
      "4993: [discriminator loss: 0.546293, acc: 0.718750] [adversarial loss: 1.158502, acc: 0.203125]\n",
      "4994: [discriminator loss: 0.553909, acc: 0.703125] [adversarial loss: 1.227908, acc: 0.156250]\n",
      "4995: [discriminator loss: 0.464071, acc: 0.796875] [adversarial loss: 1.314239, acc: 0.250000]\n",
      "4996: [discriminator loss: 0.515687, acc: 0.750000] [adversarial loss: 1.103748, acc: 0.281250]\n",
      "4997: [discriminator loss: 0.514028, acc: 0.703125] [adversarial loss: 1.562613, acc: 0.046875]\n",
      "4998: [discriminator loss: 0.643689, acc: 0.656250] [adversarial loss: 0.762265, acc: 0.562500]\n",
      "4999: [discriminator loss: 0.581952, acc: 0.671875] [adversarial loss: 1.663954, acc: 0.078125]\n",
      "5000: [discriminator loss: 0.533594, acc: 0.695312] [adversarial loss: 1.065576, acc: 0.359375]\n",
      "5001: [discriminator loss: 0.560399, acc: 0.742188] [adversarial loss: 1.683489, acc: 0.125000]\n",
      "5002: [discriminator loss: 0.573810, acc: 0.687500] [adversarial loss: 0.863520, acc: 0.468750]\n",
      "5003: [discriminator loss: 0.576128, acc: 0.632812] [adversarial loss: 1.416032, acc: 0.187500]\n",
      "5004: [discriminator loss: 0.603466, acc: 0.718750] [adversarial loss: 0.930941, acc: 0.375000]\n",
      "5005: [discriminator loss: 0.526974, acc: 0.734375] [adversarial loss: 1.487337, acc: 0.234375]\n",
      "5006: [discriminator loss: 0.619165, acc: 0.679688] [adversarial loss: 0.836073, acc: 0.546875]\n",
      "5007: [discriminator loss: 0.496993, acc: 0.750000] [adversarial loss: 1.529062, acc: 0.187500]\n",
      "5008: [discriminator loss: 0.497322, acc: 0.742188] [adversarial loss: 1.306166, acc: 0.156250]\n",
      "5009: [discriminator loss: 0.477708, acc: 0.750000] [adversarial loss: 1.147951, acc: 0.203125]\n",
      "5010: [discriminator loss: 0.531284, acc: 0.765625] [adversarial loss: 1.538084, acc: 0.140625]\n",
      "5011: [discriminator loss: 0.443987, acc: 0.789062] [adversarial loss: 1.255745, acc: 0.218750]\n",
      "5012: [discriminator loss: 0.426825, acc: 0.757812] [adversarial loss: 1.399881, acc: 0.250000]\n",
      "5013: [discriminator loss: 0.490511, acc: 0.742188] [adversarial loss: 1.195387, acc: 0.203125]\n",
      "5014: [discriminator loss: 0.480464, acc: 0.765625] [adversarial loss: 1.362574, acc: 0.250000]\n",
      "5015: [discriminator loss: 0.524706, acc: 0.734375] [adversarial loss: 0.947056, acc: 0.468750]\n",
      "5016: [discriminator loss: 0.562532, acc: 0.718750] [adversarial loss: 1.581718, acc: 0.171875]\n",
      "5017: [discriminator loss: 0.596805, acc: 0.757812] [adversarial loss: 1.278675, acc: 0.140625]\n",
      "5018: [discriminator loss: 0.480057, acc: 0.773438] [adversarial loss: 1.424007, acc: 0.140625]\n",
      "5019: [discriminator loss: 0.619402, acc: 0.671875] [adversarial loss: 1.194914, acc: 0.296875]\n",
      "5020: [discriminator loss: 0.509939, acc: 0.750000] [adversarial loss: 0.949598, acc: 0.359375]\n",
      "5021: [discriminator loss: 0.535154, acc: 0.757812] [adversarial loss: 1.410326, acc: 0.125000]\n",
      "5022: [discriminator loss: 0.543715, acc: 0.687500] [adversarial loss: 0.965387, acc: 0.296875]\n",
      "5023: [discriminator loss: 0.410199, acc: 0.835938] [adversarial loss: 1.403923, acc: 0.171875]\n",
      "5024: [discriminator loss: 0.494009, acc: 0.742188] [adversarial loss: 1.273324, acc: 0.203125]\n",
      "5025: [discriminator loss: 0.478647, acc: 0.718750] [adversarial loss: 1.309729, acc: 0.203125]\n",
      "5026: [discriminator loss: 0.553612, acc: 0.710938] [adversarial loss: 1.270704, acc: 0.218750]\n",
      "5027: [discriminator loss: 0.513737, acc: 0.765625] [adversarial loss: 1.328009, acc: 0.265625]\n",
      "5028: [discriminator loss: 0.523203, acc: 0.757812] [adversarial loss: 1.159591, acc: 0.234375]\n",
      "5029: [discriminator loss: 0.527154, acc: 0.750000] [adversarial loss: 1.766110, acc: 0.093750]\n",
      "5030: [discriminator loss: 0.519628, acc: 0.734375] [adversarial loss: 1.116917, acc: 0.312500]\n",
      "5031: [discriminator loss: 0.497958, acc: 0.773438] [adversarial loss: 1.125636, acc: 0.250000]\n",
      "5032: [discriminator loss: 0.542167, acc: 0.695312] [adversarial loss: 1.499496, acc: 0.078125]\n",
      "5033: [discriminator loss: 0.398120, acc: 0.820312] [adversarial loss: 1.041072, acc: 0.343750]\n",
      "5034: [discriminator loss: 0.508181, acc: 0.742188] [adversarial loss: 1.632107, acc: 0.109375]\n",
      "5035: [discriminator loss: 0.540260, acc: 0.742188] [adversarial loss: 0.972859, acc: 0.390625]\n",
      "5036: [discriminator loss: 0.587378, acc: 0.742188] [adversarial loss: 1.801850, acc: 0.031250]\n",
      "5037: [discriminator loss: 0.601262, acc: 0.671875] [adversarial loss: 0.946415, acc: 0.406250]\n",
      "5038: [discriminator loss: 0.566000, acc: 0.687500] [adversarial loss: 1.888395, acc: 0.046875]\n",
      "5039: [discriminator loss: 0.498000, acc: 0.757812] [adversarial loss: 0.956930, acc: 0.406250]\n",
      "5040: [discriminator loss: 0.555686, acc: 0.726562] [adversarial loss: 1.330548, acc: 0.265625]\n",
      "5041: [discriminator loss: 0.457908, acc: 0.765625] [adversarial loss: 0.936589, acc: 0.343750]\n",
      "5042: [discriminator loss: 0.477117, acc: 0.781250] [adversarial loss: 1.261525, acc: 0.359375]\n",
      "5043: [discriminator loss: 0.584518, acc: 0.695312] [adversarial loss: 1.222540, acc: 0.250000]\n",
      "5044: [discriminator loss: 0.557380, acc: 0.718750] [adversarial loss: 1.419471, acc: 0.171875]\n",
      "5045: [discriminator loss: 0.530561, acc: 0.734375] [adversarial loss: 1.299685, acc: 0.187500]\n",
      "5046: [discriminator loss: 0.427463, acc: 0.812500] [adversarial loss: 1.129570, acc: 0.218750]\n",
      "5047: [discriminator loss: 0.457227, acc: 0.789062] [adversarial loss: 1.178344, acc: 0.156250]\n",
      "5048: [discriminator loss: 0.440170, acc: 0.820312] [adversarial loss: 1.187882, acc: 0.312500]\n",
      "5049: [discriminator loss: 0.491452, acc: 0.742188] [adversarial loss: 1.258016, acc: 0.234375]\n",
      "5050: [discriminator loss: 0.475196, acc: 0.765625] [adversarial loss: 1.518898, acc: 0.203125]\n",
      "5051: [discriminator loss: 0.550860, acc: 0.718750] [adversarial loss: 1.278279, acc: 0.328125]\n",
      "5052: [discriminator loss: 0.587028, acc: 0.703125] [adversarial loss: 1.643867, acc: 0.109375]\n",
      "5053: [discriminator loss: 0.492681, acc: 0.750000] [adversarial loss: 0.960893, acc: 0.453125]\n",
      "5054: [discriminator loss: 0.506984, acc: 0.765625] [adversarial loss: 1.510801, acc: 0.093750]\n",
      "5055: [discriminator loss: 0.591758, acc: 0.687500] [adversarial loss: 1.111404, acc: 0.312500]\n",
      "5056: [discriminator loss: 0.546822, acc: 0.695312] [adversarial loss: 1.402447, acc: 0.125000]\n",
      "5057: [discriminator loss: 0.437790, acc: 0.804688] [adversarial loss: 1.269119, acc: 0.250000]\n",
      "5058: [discriminator loss: 0.563264, acc: 0.726562] [adversarial loss: 1.352723, acc: 0.234375]\n",
      "5059: [discriminator loss: 0.467243, acc: 0.765625] [adversarial loss: 1.191552, acc: 0.296875]\n",
      "5060: [discriminator loss: 0.567367, acc: 0.687500] [adversarial loss: 1.331509, acc: 0.234375]\n",
      "5061: [discriminator loss: 0.524131, acc: 0.796875] [adversarial loss: 1.185865, acc: 0.265625]\n",
      "5062: [discriminator loss: 0.494115, acc: 0.757812] [adversarial loss: 1.348313, acc: 0.203125]\n",
      "5063: [discriminator loss: 0.545085, acc: 0.687500] [adversarial loss: 1.114794, acc: 0.312500]\n",
      "5064: [discriminator loss: 0.511567, acc: 0.734375] [adversarial loss: 1.733250, acc: 0.156250]\n",
      "5065: [discriminator loss: 0.574580, acc: 0.671875] [adversarial loss: 1.130275, acc: 0.296875]\n",
      "5066: [discriminator loss: 0.550735, acc: 0.757812] [adversarial loss: 1.432711, acc: 0.203125]\n",
      "5067: [discriminator loss: 0.494581, acc: 0.773438] [adversarial loss: 1.188655, acc: 0.218750]\n",
      "5068: [discriminator loss: 0.585605, acc: 0.671875] [adversarial loss: 1.356836, acc: 0.203125]\n",
      "5069: [discriminator loss: 0.541925, acc: 0.718750] [adversarial loss: 0.968394, acc: 0.406250]\n",
      "5070: [discriminator loss: 0.559329, acc: 0.765625] [adversarial loss: 1.367748, acc: 0.156250]\n",
      "5071: [discriminator loss: 0.582869, acc: 0.734375] [adversarial loss: 0.944915, acc: 0.421875]\n",
      "5072: [discriminator loss: 0.589698, acc: 0.703125] [adversarial loss: 1.403872, acc: 0.156250]\n",
      "5073: [discriminator loss: 0.620427, acc: 0.671875] [adversarial loss: 1.198743, acc: 0.187500]\n",
      "5074: [discriminator loss: 0.494586, acc: 0.742188] [adversarial loss: 1.224620, acc: 0.203125]\n",
      "5075: [discriminator loss: 0.600609, acc: 0.625000] [adversarial loss: 1.316168, acc: 0.203125]\n",
      "5076: [discriminator loss: 0.590565, acc: 0.656250] [adversarial loss: 1.182262, acc: 0.203125]\n",
      "5077: [discriminator loss: 0.442212, acc: 0.835938] [adversarial loss: 1.283911, acc: 0.218750]\n",
      "5078: [discriminator loss: 0.521003, acc: 0.726562] [adversarial loss: 1.246508, acc: 0.140625]\n",
      "5079: [discriminator loss: 0.528670, acc: 0.710938] [adversarial loss: 1.388975, acc: 0.125000]\n",
      "5080: [discriminator loss: 0.481731, acc: 0.742188] [adversarial loss: 1.495865, acc: 0.125000]\n",
      "5081: [discriminator loss: 0.458944, acc: 0.781250] [adversarial loss: 1.309753, acc: 0.203125]\n",
      "5082: [discriminator loss: 0.550625, acc: 0.710938] [adversarial loss: 1.501787, acc: 0.109375]\n",
      "5083: [discriminator loss: 0.514230, acc: 0.742188] [adversarial loss: 1.015152, acc: 0.468750]\n",
      "5084: [discriminator loss: 0.539987, acc: 0.703125] [adversarial loss: 1.384470, acc: 0.187500]\n",
      "5085: [discriminator loss: 0.542129, acc: 0.710938] [adversarial loss: 1.144722, acc: 0.250000]\n",
      "5086: [discriminator loss: 0.499020, acc: 0.773438] [adversarial loss: 1.291795, acc: 0.171875]\n",
      "5087: [discriminator loss: 0.420434, acc: 0.820312] [adversarial loss: 1.234377, acc: 0.312500]\n",
      "5088: [discriminator loss: 0.623526, acc: 0.648438] [adversarial loss: 1.475846, acc: 0.218750]\n",
      "5089: [discriminator loss: 0.513155, acc: 0.710938] [adversarial loss: 0.974752, acc: 0.343750]\n",
      "5090: [discriminator loss: 0.485626, acc: 0.781250] [adversarial loss: 1.878169, acc: 0.156250]\n",
      "5091: [discriminator loss: 0.614026, acc: 0.656250] [adversarial loss: 0.735753, acc: 0.562500]\n",
      "5092: [discriminator loss: 0.545310, acc: 0.710938] [adversarial loss: 1.892737, acc: 0.078125]\n",
      "5093: [discriminator loss: 0.455663, acc: 0.796875] [adversarial loss: 1.372308, acc: 0.234375]\n",
      "5094: [discriminator loss: 0.619594, acc: 0.664062] [adversarial loss: 1.403357, acc: 0.125000]\n",
      "5095: [discriminator loss: 0.477372, acc: 0.750000] [adversarial loss: 1.091020, acc: 0.312500]\n",
      "5096: [discriminator loss: 0.472443, acc: 0.812500] [adversarial loss: 1.335429, acc: 0.171875]\n",
      "5097: [discriminator loss: 0.543538, acc: 0.695312] [adversarial loss: 1.047394, acc: 0.281250]\n",
      "5098: [discriminator loss: 0.527656, acc: 0.734375] [adversarial loss: 1.385593, acc: 0.171875]\n",
      "5099: [discriminator loss: 0.532006, acc: 0.734375] [adversarial loss: 1.128134, acc: 0.281250]\n",
      "5100: [discriminator loss: 0.532937, acc: 0.750000] [adversarial loss: 1.335953, acc: 0.140625]\n",
      "5101: [discriminator loss: 0.521943, acc: 0.671875] [adversarial loss: 1.164932, acc: 0.296875]\n",
      "5102: [discriminator loss: 0.531100, acc: 0.718750] [adversarial loss: 1.251740, acc: 0.234375]\n",
      "5103: [discriminator loss: 0.473734, acc: 0.765625] [adversarial loss: 1.666698, acc: 0.093750]\n",
      "5104: [discriminator loss: 0.547010, acc: 0.734375] [adversarial loss: 1.042990, acc: 0.265625]\n",
      "5105: [discriminator loss: 0.524086, acc: 0.718750] [adversarial loss: 1.465362, acc: 0.140625]\n",
      "5106: [discriminator loss: 0.490982, acc: 0.757812] [adversarial loss: 0.869185, acc: 0.421875]\n",
      "5107: [discriminator loss: 0.532592, acc: 0.742188] [adversarial loss: 1.834182, acc: 0.046875]\n",
      "5108: [discriminator loss: 0.591894, acc: 0.625000] [adversarial loss: 1.225035, acc: 0.296875]\n",
      "5109: [discriminator loss: 0.547042, acc: 0.695312] [adversarial loss: 1.580024, acc: 0.109375]\n",
      "5110: [discriminator loss: 0.521473, acc: 0.734375] [adversarial loss: 1.005819, acc: 0.312500]\n",
      "5111: [discriminator loss: 0.530692, acc: 0.742188] [adversarial loss: 1.409057, acc: 0.171875]\n",
      "5112: [discriminator loss: 0.615340, acc: 0.648438] [adversarial loss: 0.992062, acc: 0.343750]\n",
      "5113: [discriminator loss: 0.493526, acc: 0.765625] [adversarial loss: 1.526752, acc: 0.125000]\n",
      "5114: [discriminator loss: 0.555419, acc: 0.703125] [adversarial loss: 0.982850, acc: 0.406250]\n",
      "5115: [discriminator loss: 0.496115, acc: 0.734375] [adversarial loss: 1.455821, acc: 0.187500]\n",
      "5116: [discriminator loss: 0.541780, acc: 0.750000] [adversarial loss: 1.055437, acc: 0.312500]\n",
      "5117: [discriminator loss: 0.524324, acc: 0.765625] [adversarial loss: 1.343006, acc: 0.171875]\n",
      "5118: [discriminator loss: 0.496149, acc: 0.812500] [adversarial loss: 1.045115, acc: 0.296875]\n",
      "5119: [discriminator loss: 0.547010, acc: 0.734375] [adversarial loss: 1.116775, acc: 0.281250]\n",
      "5120: [discriminator loss: 0.472325, acc: 0.757812] [adversarial loss: 1.154985, acc: 0.328125]\n",
      "5121: [discriminator loss: 0.568894, acc: 0.695312] [adversarial loss: 1.294027, acc: 0.203125]\n",
      "5122: [discriminator loss: 0.533121, acc: 0.742188] [adversarial loss: 1.147014, acc: 0.328125]\n",
      "5123: [discriminator loss: 0.522770, acc: 0.734375] [adversarial loss: 1.509643, acc: 0.171875]\n",
      "5124: [discriminator loss: 0.560256, acc: 0.671875] [adversarial loss: 1.000426, acc: 0.281250]\n",
      "5125: [discriminator loss: 0.542502, acc: 0.757812] [adversarial loss: 1.257861, acc: 0.234375]\n",
      "5126: [discriminator loss: 0.453615, acc: 0.820312] [adversarial loss: 1.350271, acc: 0.171875]\n",
      "5127: [discriminator loss: 0.523449, acc: 0.757812] [adversarial loss: 1.344848, acc: 0.156250]\n",
      "5128: [discriminator loss: 0.506380, acc: 0.734375] [adversarial loss: 1.022966, acc: 0.375000]\n",
      "5129: [discriminator loss: 0.491861, acc: 0.734375] [adversarial loss: 1.637682, acc: 0.093750]\n",
      "5130: [discriminator loss: 0.581879, acc: 0.710938] [adversarial loss: 0.991904, acc: 0.437500]\n",
      "5131: [discriminator loss: 0.577197, acc: 0.695312] [adversarial loss: 2.150260, acc: 0.031250]\n",
      "5132: [discriminator loss: 0.679128, acc: 0.617188] [adversarial loss: 0.758446, acc: 0.515625]\n",
      "5133: [discriminator loss: 0.518525, acc: 0.757812] [adversarial loss: 1.223392, acc: 0.250000]\n",
      "5134: [discriminator loss: 0.547610, acc: 0.726562] [adversarial loss: 0.892296, acc: 0.390625]\n",
      "5135: [discriminator loss: 0.580786, acc: 0.695312] [adversarial loss: 1.367505, acc: 0.187500]\n",
      "5136: [discriminator loss: 0.527549, acc: 0.757812] [adversarial loss: 1.183632, acc: 0.312500]\n",
      "5137: [discriminator loss: 0.505880, acc: 0.781250] [adversarial loss: 1.334910, acc: 0.171875]\n",
      "5138: [discriminator loss: 0.514816, acc: 0.734375] [adversarial loss: 1.189089, acc: 0.234375]\n",
      "5139: [discriminator loss: 0.496159, acc: 0.750000] [adversarial loss: 1.275529, acc: 0.265625]\n",
      "5140: [discriminator loss: 0.537095, acc: 0.718750] [adversarial loss: 1.317318, acc: 0.171875]\n",
      "5141: [discriminator loss: 0.419047, acc: 0.835938] [adversarial loss: 1.431855, acc: 0.203125]\n",
      "5142: [discriminator loss: 0.583534, acc: 0.679688] [adversarial loss: 0.998681, acc: 0.406250]\n",
      "5143: [discriminator loss: 0.582902, acc: 0.679688] [adversarial loss: 2.025345, acc: 0.000000]\n",
      "5144: [discriminator loss: 0.536825, acc: 0.703125] [adversarial loss: 0.797204, acc: 0.531250]\n",
      "5145: [discriminator loss: 0.625290, acc: 0.648438] [adversarial loss: 1.628876, acc: 0.062500]\n",
      "5146: [discriminator loss: 0.531832, acc: 0.742188] [adversarial loss: 1.078368, acc: 0.312500]\n",
      "5147: [discriminator loss: 0.471766, acc: 0.757812] [adversarial loss: 1.273196, acc: 0.218750]\n",
      "5148: [discriminator loss: 0.510246, acc: 0.718750] [adversarial loss: 1.167288, acc: 0.203125]\n",
      "5149: [discriminator loss: 0.526074, acc: 0.757812] [adversarial loss: 1.377522, acc: 0.187500]\n",
      "5150: [discriminator loss: 0.450253, acc: 0.820312] [adversarial loss: 1.477926, acc: 0.062500]\n",
      "5151: [discriminator loss: 0.537295, acc: 0.726562] [adversarial loss: 1.177109, acc: 0.281250]\n",
      "5152: [discriminator loss: 0.473738, acc: 0.757812] [adversarial loss: 1.376094, acc: 0.156250]\n",
      "5153: [discriminator loss: 0.501686, acc: 0.765625] [adversarial loss: 1.519988, acc: 0.140625]\n",
      "5154: [discriminator loss: 0.550331, acc: 0.726562] [adversarial loss: 1.144006, acc: 0.234375]\n",
      "5155: [discriminator loss: 0.508075, acc: 0.765625] [adversarial loss: 1.541075, acc: 0.125000]\n",
      "5156: [discriminator loss: 0.520034, acc: 0.781250] [adversarial loss: 1.011937, acc: 0.375000]\n",
      "5157: [discriminator loss: 0.508469, acc: 0.750000] [adversarial loss: 1.727167, acc: 0.156250]\n",
      "5158: [discriminator loss: 0.548473, acc: 0.726562] [adversarial loss: 0.964624, acc: 0.328125]\n",
      "5159: [discriminator loss: 0.462671, acc: 0.789062] [adversarial loss: 1.353726, acc: 0.203125]\n",
      "5160: [discriminator loss: 0.528096, acc: 0.726562] [adversarial loss: 0.970993, acc: 0.390625]\n",
      "5161: [discriminator loss: 0.503943, acc: 0.718750] [adversarial loss: 1.290744, acc: 0.203125]\n",
      "5162: [discriminator loss: 0.594249, acc: 0.648438] [adversarial loss: 1.103216, acc: 0.265625]\n",
      "5163: [discriminator loss: 0.506237, acc: 0.750000] [adversarial loss: 1.362706, acc: 0.265625]\n",
      "5164: [discriminator loss: 0.497526, acc: 0.734375] [adversarial loss: 1.205344, acc: 0.187500]\n",
      "5165: [discriminator loss: 0.531687, acc: 0.710938] [adversarial loss: 1.406424, acc: 0.156250]\n",
      "5166: [discriminator loss: 0.549444, acc: 0.742188] [adversarial loss: 0.923572, acc: 0.390625]\n",
      "5167: [discriminator loss: 0.502851, acc: 0.765625] [adversarial loss: 1.693520, acc: 0.093750]\n",
      "5168: [discriminator loss: 0.557153, acc: 0.687500] [adversarial loss: 0.940634, acc: 0.437500]\n",
      "5169: [discriminator loss: 0.572810, acc: 0.679688] [adversarial loss: 1.369912, acc: 0.171875]\n",
      "5170: [discriminator loss: 0.522813, acc: 0.734375] [adversarial loss: 1.120445, acc: 0.281250]\n",
      "5171: [discriminator loss: 0.516703, acc: 0.710938] [adversarial loss: 1.470869, acc: 0.093750]\n",
      "5172: [discriminator loss: 0.500317, acc: 0.757812] [adversarial loss: 1.195791, acc: 0.234375]\n",
      "5173: [discriminator loss: 0.495865, acc: 0.750000] [adversarial loss: 1.428086, acc: 0.156250]\n",
      "5174: [discriminator loss: 0.493648, acc: 0.789062] [adversarial loss: 1.247303, acc: 0.218750]\n",
      "5175: [discriminator loss: 0.514230, acc: 0.710938] [adversarial loss: 1.240155, acc: 0.234375]\n",
      "5176: [discriminator loss: 0.463642, acc: 0.789062] [adversarial loss: 1.259118, acc: 0.265625]\n",
      "5177: [discriminator loss: 0.531548, acc: 0.718750] [adversarial loss: 1.366702, acc: 0.156250]\n",
      "5178: [discriminator loss: 0.501010, acc: 0.796875] [adversarial loss: 1.066719, acc: 0.359375]\n",
      "5179: [discriminator loss: 0.595166, acc: 0.679688] [adversarial loss: 1.290465, acc: 0.218750]\n",
      "5180: [discriminator loss: 0.489427, acc: 0.773438] [adversarial loss: 1.460993, acc: 0.125000]\n",
      "5181: [discriminator loss: 0.534250, acc: 0.718750] [adversarial loss: 1.104411, acc: 0.328125]\n",
      "5182: [discriminator loss: 0.552679, acc: 0.726562] [adversarial loss: 1.680432, acc: 0.062500]\n",
      "5183: [discriminator loss: 0.509101, acc: 0.750000] [adversarial loss: 1.069209, acc: 0.234375]\n",
      "5184: [discriminator loss: 0.545703, acc: 0.703125] [adversarial loss: 1.509840, acc: 0.140625]\n",
      "5185: [discriminator loss: 0.541125, acc: 0.710938] [adversarial loss: 0.971977, acc: 0.390625]\n",
      "5186: [discriminator loss: 0.540628, acc: 0.750000] [adversarial loss: 1.609676, acc: 0.109375]\n",
      "5187: [discriminator loss: 0.562116, acc: 0.710938] [adversarial loss: 0.995098, acc: 0.343750]\n",
      "5188: [discriminator loss: 0.567013, acc: 0.679688] [adversarial loss: 1.789113, acc: 0.125000]\n",
      "5189: [discriminator loss: 0.511380, acc: 0.734375] [adversarial loss: 0.899034, acc: 0.421875]\n",
      "5190: [discriminator loss: 0.521369, acc: 0.710938] [adversarial loss: 1.668484, acc: 0.078125]\n",
      "5191: [discriminator loss: 0.605726, acc: 0.679688] [adversarial loss: 1.021848, acc: 0.328125]\n",
      "5192: [discriminator loss: 0.565226, acc: 0.656250] [adversarial loss: 1.465055, acc: 0.125000]\n",
      "5193: [discriminator loss: 0.476687, acc: 0.789062] [adversarial loss: 1.183527, acc: 0.265625]\n",
      "5194: [discriminator loss: 0.576486, acc: 0.671875] [adversarial loss: 1.253182, acc: 0.234375]\n",
      "5195: [discriminator loss: 0.466273, acc: 0.750000] [adversarial loss: 1.309347, acc: 0.140625]\n",
      "5196: [discriminator loss: 0.450692, acc: 0.820312] [adversarial loss: 1.262332, acc: 0.125000]\n",
      "5197: [discriminator loss: 0.532690, acc: 0.726562] [adversarial loss: 1.130006, acc: 0.296875]\n",
      "5198: [discriminator loss: 0.463949, acc: 0.765625] [adversarial loss: 1.375859, acc: 0.125000]\n",
      "5199: [discriminator loss: 0.525114, acc: 0.781250] [adversarial loss: 1.046025, acc: 0.343750]\n",
      "5200: [discriminator loss: 0.542193, acc: 0.718750] [adversarial loss: 1.476804, acc: 0.093750]\n",
      "5201: [discriminator loss: 0.546752, acc: 0.710938] [adversarial loss: 1.007359, acc: 0.375000]\n",
      "5202: [discriminator loss: 0.563272, acc: 0.710938] [adversarial loss: 1.651351, acc: 0.109375]\n",
      "5203: [discriminator loss: 0.552250, acc: 0.718750] [adversarial loss: 0.996935, acc: 0.421875]\n",
      "5204: [discriminator loss: 0.493283, acc: 0.734375] [adversarial loss: 1.291522, acc: 0.234375]\n",
      "5205: [discriminator loss: 0.500089, acc: 0.757812] [adversarial loss: 1.360185, acc: 0.140625]\n",
      "5206: [discriminator loss: 0.554707, acc: 0.695312] [adversarial loss: 1.343849, acc: 0.203125]\n",
      "5207: [discriminator loss: 0.610178, acc: 0.687500] [adversarial loss: 0.967401, acc: 0.359375]\n",
      "5208: [discriminator loss: 0.444363, acc: 0.796875] [adversarial loss: 1.294360, acc: 0.218750]\n",
      "5209: [discriminator loss: 0.547054, acc: 0.726562] [adversarial loss: 1.020496, acc: 0.375000]\n",
      "5210: [discriminator loss: 0.586676, acc: 0.695312] [adversarial loss: 1.412262, acc: 0.125000]\n",
      "5211: [discriminator loss: 0.473194, acc: 0.757812] [adversarial loss: 1.491810, acc: 0.156250]\n",
      "5212: [discriminator loss: 0.507433, acc: 0.734375] [adversarial loss: 1.123071, acc: 0.250000]\n",
      "5213: [discriminator loss: 0.533232, acc: 0.726562] [adversarial loss: 1.517312, acc: 0.109375]\n",
      "5214: [discriminator loss: 0.516778, acc: 0.773438] [adversarial loss: 0.953211, acc: 0.375000]\n",
      "5215: [discriminator loss: 0.612449, acc: 0.687500] [adversarial loss: 1.837226, acc: 0.046875]\n",
      "5216: [discriminator loss: 0.550187, acc: 0.703125] [adversarial loss: 0.968286, acc: 0.328125]\n",
      "5217: [discriminator loss: 0.495975, acc: 0.781250] [adversarial loss: 1.516323, acc: 0.093750]\n",
      "5218: [discriminator loss: 0.597975, acc: 0.679688] [adversarial loss: 1.066293, acc: 0.359375]\n",
      "5219: [discriminator loss: 0.525769, acc: 0.757812] [adversarial loss: 1.156131, acc: 0.265625]\n",
      "5220: [discriminator loss: 0.562987, acc: 0.687500] [adversarial loss: 1.331857, acc: 0.187500]\n",
      "5221: [discriminator loss: 0.595460, acc: 0.640625] [adversarial loss: 1.448833, acc: 0.125000]\n",
      "5222: [discriminator loss: 0.501963, acc: 0.804688] [adversarial loss: 1.342968, acc: 0.156250]\n",
      "5223: [discriminator loss: 0.585753, acc: 0.703125] [adversarial loss: 1.090851, acc: 0.234375]\n",
      "5224: [discriminator loss: 0.526255, acc: 0.742188] [adversarial loss: 1.512162, acc: 0.140625]\n",
      "5225: [discriminator loss: 0.501937, acc: 0.773438] [adversarial loss: 1.075224, acc: 0.203125]\n",
      "5226: [discriminator loss: 0.475400, acc: 0.750000] [adversarial loss: 1.445769, acc: 0.171875]\n",
      "5227: [discriminator loss: 0.510811, acc: 0.757812] [adversarial loss: 1.192248, acc: 0.171875]\n",
      "5228: [discriminator loss: 0.484819, acc: 0.734375] [adversarial loss: 1.166985, acc: 0.250000]\n",
      "5229: [discriminator loss: 0.481095, acc: 0.773438] [adversarial loss: 1.085081, acc: 0.265625]\n",
      "5230: [discriminator loss: 0.570020, acc: 0.718750] [adversarial loss: 1.408103, acc: 0.156250]\n",
      "5231: [discriminator loss: 0.505661, acc: 0.726562] [adversarial loss: 0.961778, acc: 0.343750]\n",
      "5232: [discriminator loss: 0.593122, acc: 0.687500] [adversarial loss: 1.725870, acc: 0.078125]\n",
      "5233: [discriminator loss: 0.502180, acc: 0.710938] [adversarial loss: 1.157764, acc: 0.250000]\n",
      "5234: [discriminator loss: 0.472778, acc: 0.773438] [adversarial loss: 0.882771, acc: 0.406250]\n",
      "5235: [discriminator loss: 0.515490, acc: 0.750000] [adversarial loss: 1.249292, acc: 0.218750]\n",
      "5236: [discriminator loss: 0.516137, acc: 0.742188] [adversarial loss: 1.528648, acc: 0.156250]\n",
      "5237: [discriminator loss: 0.506116, acc: 0.710938] [adversarial loss: 0.761943, acc: 0.546875]\n",
      "5238: [discriminator loss: 0.481273, acc: 0.812500] [adversarial loss: 1.360997, acc: 0.140625]\n",
      "5239: [discriminator loss: 0.516167, acc: 0.695312] [adversarial loss: 1.213581, acc: 0.203125]\n",
      "5240: [discriminator loss: 0.555466, acc: 0.750000] [adversarial loss: 1.312947, acc: 0.140625]\n",
      "5241: [discriminator loss: 0.536192, acc: 0.734375] [adversarial loss: 1.086288, acc: 0.312500]\n",
      "5242: [discriminator loss: 0.482133, acc: 0.773438] [adversarial loss: 1.073053, acc: 0.281250]\n",
      "5243: [discriminator loss: 0.489750, acc: 0.726562] [adversarial loss: 1.613765, acc: 0.125000]\n",
      "5244: [discriminator loss: 0.532786, acc: 0.726562] [adversarial loss: 0.767470, acc: 0.531250]\n",
      "5245: [discriminator loss: 0.577361, acc: 0.710938] [adversarial loss: 1.697077, acc: 0.046875]\n",
      "5246: [discriminator loss: 0.575906, acc: 0.710938] [adversarial loss: 0.813591, acc: 0.468750]\n",
      "5247: [discriminator loss: 0.519469, acc: 0.726562] [adversarial loss: 1.599485, acc: 0.125000]\n",
      "5248: [discriminator loss: 0.507035, acc: 0.773438] [adversarial loss: 1.174682, acc: 0.250000]\n",
      "5249: [discriminator loss: 0.472427, acc: 0.781250] [adversarial loss: 1.405777, acc: 0.171875]\n",
      "5250: [discriminator loss: 0.556778, acc: 0.726562] [adversarial loss: 1.151325, acc: 0.265625]\n",
      "5251: [discriminator loss: 0.550560, acc: 0.695312] [adversarial loss: 1.513382, acc: 0.125000]\n",
      "5252: [discriminator loss: 0.505763, acc: 0.679688] [adversarial loss: 1.276114, acc: 0.203125]\n",
      "5253: [discriminator loss: 0.498907, acc: 0.765625] [adversarial loss: 1.394603, acc: 0.109375]\n",
      "5254: [discriminator loss: 0.575753, acc: 0.671875] [adversarial loss: 1.315233, acc: 0.187500]\n",
      "5255: [discriminator loss: 0.478788, acc: 0.773438] [adversarial loss: 1.251050, acc: 0.265625]\n",
      "5256: [discriminator loss: 0.544822, acc: 0.695312] [adversarial loss: 1.429033, acc: 0.218750]\n",
      "5257: [discriminator loss: 0.629078, acc: 0.656250] [adversarial loss: 1.074188, acc: 0.296875]\n",
      "5258: [discriminator loss: 0.508782, acc: 0.734375] [adversarial loss: 1.402444, acc: 0.140625]\n",
      "5259: [discriminator loss: 0.521142, acc: 0.710938] [adversarial loss: 1.048887, acc: 0.359375]\n",
      "5260: [discriminator loss: 0.596905, acc: 0.710938] [adversarial loss: 1.287605, acc: 0.250000]\n",
      "5261: [discriminator loss: 0.544557, acc: 0.726562] [adversarial loss: 1.245769, acc: 0.203125]\n",
      "5262: [discriminator loss: 0.575129, acc: 0.679688] [adversarial loss: 1.414370, acc: 0.234375]\n",
      "5263: [discriminator loss: 0.605361, acc: 0.703125] [adversarial loss: 0.894190, acc: 0.437500]\n",
      "5264: [discriminator loss: 0.532486, acc: 0.726562] [adversarial loss: 1.465064, acc: 0.109375]\n",
      "5265: [discriminator loss: 0.517380, acc: 0.765625] [adversarial loss: 1.016882, acc: 0.453125]\n",
      "5266: [discriminator loss: 0.463740, acc: 0.781250] [adversarial loss: 1.440114, acc: 0.125000]\n",
      "5267: [discriminator loss: 0.552863, acc: 0.703125] [adversarial loss: 0.971779, acc: 0.343750]\n",
      "5268: [discriminator loss: 0.609621, acc: 0.695312] [adversarial loss: 2.001347, acc: 0.078125]\n",
      "5269: [discriminator loss: 0.541349, acc: 0.726562] [adversarial loss: 1.043532, acc: 0.296875]\n",
      "5270: [discriminator loss: 0.526913, acc: 0.671875] [adversarial loss: 1.407880, acc: 0.156250]\n",
      "5271: [discriminator loss: 0.513602, acc: 0.734375] [adversarial loss: 1.096807, acc: 0.281250]\n",
      "5272: [discriminator loss: 0.535490, acc: 0.718750] [adversarial loss: 0.957952, acc: 0.328125]\n",
      "5273: [discriminator loss: 0.495682, acc: 0.773438] [adversarial loss: 1.437034, acc: 0.125000]\n",
      "5274: [discriminator loss: 0.483530, acc: 0.765625] [adversarial loss: 1.189013, acc: 0.296875]\n",
      "5275: [discriminator loss: 0.468891, acc: 0.796875] [adversarial loss: 1.309803, acc: 0.234375]\n",
      "5276: [discriminator loss: 0.549653, acc: 0.734375] [adversarial loss: 1.057364, acc: 0.265625]\n",
      "5277: [discriminator loss: 0.514925, acc: 0.750000] [adversarial loss: 1.130545, acc: 0.218750]\n",
      "5278: [discriminator loss: 0.494262, acc: 0.789062] [adversarial loss: 1.014369, acc: 0.328125]\n",
      "5279: [discriminator loss: 0.550778, acc: 0.679688] [adversarial loss: 1.722324, acc: 0.062500]\n",
      "5280: [discriminator loss: 0.571930, acc: 0.734375] [adversarial loss: 0.858077, acc: 0.453125]\n",
      "5281: [discriminator loss: 0.663143, acc: 0.625000] [adversarial loss: 1.348768, acc: 0.156250]\n",
      "5282: [discriminator loss: 0.549497, acc: 0.726562] [adversarial loss: 1.315694, acc: 0.156250]\n",
      "5283: [discriminator loss: 0.496708, acc: 0.742188] [adversarial loss: 1.283545, acc: 0.281250]\n",
      "5284: [discriminator loss: 0.530941, acc: 0.703125] [adversarial loss: 0.878313, acc: 0.390625]\n",
      "5285: [discriminator loss: 0.466497, acc: 0.765625] [adversarial loss: 1.429097, acc: 0.171875]\n",
      "5286: [discriminator loss: 0.505990, acc: 0.750000] [adversarial loss: 1.314653, acc: 0.171875]\n",
      "5287: [discriminator loss: 0.453552, acc: 0.765625] [adversarial loss: 1.439770, acc: 0.187500]\n",
      "5288: [discriminator loss: 0.517583, acc: 0.750000] [adversarial loss: 1.346691, acc: 0.156250]\n",
      "5289: [discriminator loss: 0.540347, acc: 0.726562] [adversarial loss: 1.415339, acc: 0.250000]\n",
      "5290: [discriminator loss: 0.497064, acc: 0.765625] [adversarial loss: 0.971884, acc: 0.312500]\n",
      "5291: [discriminator loss: 0.558717, acc: 0.687500] [adversarial loss: 1.232809, acc: 0.250000]\n",
      "5292: [discriminator loss: 0.553255, acc: 0.765625] [adversarial loss: 1.305132, acc: 0.203125]\n",
      "5293: [discriminator loss: 0.523953, acc: 0.718750] [adversarial loss: 0.934460, acc: 0.343750]\n",
      "5294: [discriminator loss: 0.523896, acc: 0.726562] [adversarial loss: 1.730393, acc: 0.109375]\n",
      "5295: [discriminator loss: 0.548219, acc: 0.718750] [adversarial loss: 0.648533, acc: 0.609375]\n",
      "5296: [discriminator loss: 0.583108, acc: 0.695312] [adversarial loss: 1.915460, acc: 0.015625]\n",
      "5297: [discriminator loss: 0.549553, acc: 0.734375] [adversarial loss: 0.950628, acc: 0.343750]\n",
      "5298: [discriminator loss: 0.522348, acc: 0.726562] [adversarial loss: 1.677832, acc: 0.078125]\n",
      "5299: [discriminator loss: 0.600538, acc: 0.664062] [adversarial loss: 0.858193, acc: 0.390625]\n",
      "5300: [discriminator loss: 0.460168, acc: 0.726562] [adversarial loss: 1.350738, acc: 0.140625]\n",
      "5301: [discriminator loss: 0.563997, acc: 0.695312] [adversarial loss: 1.125822, acc: 0.218750]\n",
      "5302: [discriminator loss: 0.513823, acc: 0.710938] [adversarial loss: 1.196992, acc: 0.265625]\n",
      "5303: [discriminator loss: 0.529034, acc: 0.750000] [adversarial loss: 1.342645, acc: 0.234375]\n",
      "5304: [discriminator loss: 0.610274, acc: 0.718750] [adversarial loss: 1.057953, acc: 0.296875]\n",
      "5305: [discriminator loss: 0.479537, acc: 0.742188] [adversarial loss: 1.309489, acc: 0.140625]\n",
      "5306: [discriminator loss: 0.468397, acc: 0.773438] [adversarial loss: 1.112774, acc: 0.234375]\n",
      "5307: [discriminator loss: 0.487581, acc: 0.765625] [adversarial loss: 1.510033, acc: 0.125000]\n",
      "5308: [discriminator loss: 0.477337, acc: 0.750000] [adversarial loss: 1.081020, acc: 0.328125]\n",
      "5309: [discriminator loss: 0.557763, acc: 0.710938] [adversarial loss: 1.299592, acc: 0.234375]\n",
      "5310: [discriminator loss: 0.467912, acc: 0.773438] [adversarial loss: 0.960093, acc: 0.359375]\n",
      "5311: [discriminator loss: 0.478974, acc: 0.781250] [adversarial loss: 1.247119, acc: 0.265625]\n",
      "5312: [discriminator loss: 0.531697, acc: 0.757812] [adversarial loss: 1.132623, acc: 0.343750]\n",
      "5313: [discriminator loss: 0.506626, acc: 0.796875] [adversarial loss: 1.279540, acc: 0.250000]\n",
      "5314: [discriminator loss: 0.536071, acc: 0.750000] [adversarial loss: 1.358305, acc: 0.203125]\n",
      "5315: [discriminator loss: 0.468555, acc: 0.781250] [adversarial loss: 1.358066, acc: 0.187500]\n",
      "5316: [discriminator loss: 0.489805, acc: 0.765625] [adversarial loss: 0.987376, acc: 0.312500]\n",
      "5317: [discriminator loss: 0.464252, acc: 0.796875] [adversarial loss: 1.467893, acc: 0.140625]\n",
      "5318: [discriminator loss: 0.490246, acc: 0.773438] [adversarial loss: 0.888196, acc: 0.390625]\n",
      "5319: [discriminator loss: 0.542037, acc: 0.718750] [adversarial loss: 1.475385, acc: 0.187500]\n",
      "5320: [discriminator loss: 0.536347, acc: 0.726562] [adversarial loss: 1.004470, acc: 0.343750]\n",
      "5321: [discriminator loss: 0.515664, acc: 0.750000] [adversarial loss: 1.384560, acc: 0.140625]\n",
      "5322: [discriminator loss: 0.525841, acc: 0.718750] [adversarial loss: 0.997829, acc: 0.343750]\n",
      "5323: [discriminator loss: 0.518592, acc: 0.695312] [adversarial loss: 1.142817, acc: 0.250000]\n",
      "5324: [discriminator loss: 0.519639, acc: 0.742188] [adversarial loss: 1.488470, acc: 0.187500]\n",
      "5325: [discriminator loss: 0.588704, acc: 0.695312] [adversarial loss: 1.182523, acc: 0.296875]\n",
      "5326: [discriminator loss: 0.564387, acc: 0.679688] [adversarial loss: 1.601623, acc: 0.171875]\n",
      "5327: [discriminator loss: 0.551443, acc: 0.734375] [adversarial loss: 1.135746, acc: 0.187500]\n",
      "5328: [discriminator loss: 0.566530, acc: 0.710938] [adversarial loss: 1.473837, acc: 0.156250]\n",
      "5329: [discriminator loss: 0.564008, acc: 0.703125] [adversarial loss: 0.922838, acc: 0.437500]\n",
      "5330: [discriminator loss: 0.515862, acc: 0.726562] [adversarial loss: 1.694325, acc: 0.093750]\n",
      "5331: [discriminator loss: 0.536729, acc: 0.726562] [adversarial loss: 0.909831, acc: 0.421875]\n",
      "5332: [discriminator loss: 0.558527, acc: 0.656250] [adversarial loss: 2.057136, acc: 0.093750]\n",
      "5333: [discriminator loss: 0.489891, acc: 0.781250] [adversarial loss: 1.331458, acc: 0.203125]\n",
      "5334: [discriminator loss: 0.545140, acc: 0.734375] [adversarial loss: 1.149373, acc: 0.281250]\n",
      "5335: [discriminator loss: 0.530821, acc: 0.726562] [adversarial loss: 1.357565, acc: 0.156250]\n",
      "5336: [discriminator loss: 0.499916, acc: 0.757812] [adversarial loss: 1.208349, acc: 0.234375]\n",
      "5337: [discriminator loss: 0.459987, acc: 0.796875] [adversarial loss: 1.315239, acc: 0.140625]\n",
      "5338: [discriminator loss: 0.493071, acc: 0.757812] [adversarial loss: 1.219902, acc: 0.203125]\n",
      "5339: [discriminator loss: 0.532199, acc: 0.726562] [adversarial loss: 1.194381, acc: 0.281250]\n",
      "5340: [discriminator loss: 0.478948, acc: 0.789062] [adversarial loss: 1.190001, acc: 0.187500]\n",
      "5341: [discriminator loss: 0.505441, acc: 0.750000] [adversarial loss: 1.327499, acc: 0.203125]\n",
      "5342: [discriminator loss: 0.448362, acc: 0.773438] [adversarial loss: 1.093750, acc: 0.218750]\n",
      "5343: [discriminator loss: 0.519381, acc: 0.742188] [adversarial loss: 1.241705, acc: 0.203125]\n",
      "5344: [discriminator loss: 0.516080, acc: 0.742188] [adversarial loss: 1.534462, acc: 0.140625]\n",
      "5345: [discriminator loss: 0.642647, acc: 0.703125] [adversarial loss: 0.815436, acc: 0.484375]\n",
      "5346: [discriminator loss: 0.447863, acc: 0.812500] [adversarial loss: 1.682345, acc: 0.093750]\n",
      "5347: [discriminator loss: 0.484657, acc: 0.757812] [adversarial loss: 1.196731, acc: 0.203125]\n",
      "5348: [discriminator loss: 0.522276, acc: 0.726562] [adversarial loss: 1.432895, acc: 0.171875]\n",
      "5349: [discriminator loss: 0.529267, acc: 0.734375] [adversarial loss: 1.092523, acc: 0.328125]\n",
      "5350: [discriminator loss: 0.518995, acc: 0.773438] [adversarial loss: 1.527101, acc: 0.078125]\n",
      "5351: [discriminator loss: 0.591485, acc: 0.687500] [adversarial loss: 0.944496, acc: 0.453125]\n",
      "5352: [discriminator loss: 0.550605, acc: 0.734375] [adversarial loss: 1.636166, acc: 0.093750]\n",
      "5353: [discriminator loss: 0.559334, acc: 0.687500] [adversarial loss: 1.570293, acc: 0.093750]\n",
      "5354: [discriminator loss: 0.566900, acc: 0.703125] [adversarial loss: 0.881746, acc: 0.343750]\n",
      "5355: [discriminator loss: 0.522664, acc: 0.710938] [adversarial loss: 1.350072, acc: 0.125000]\n",
      "5356: [discriminator loss: 0.502724, acc: 0.757812] [adversarial loss: 0.969932, acc: 0.468750]\n",
      "5357: [discriminator loss: 0.555760, acc: 0.734375] [adversarial loss: 1.783046, acc: 0.109375]\n",
      "5358: [discriminator loss: 0.478947, acc: 0.726562] [adversarial loss: 1.188033, acc: 0.218750]\n",
      "5359: [discriminator loss: 0.543941, acc: 0.703125] [adversarial loss: 1.250192, acc: 0.265625]\n",
      "5360: [discriminator loss: 0.513558, acc: 0.757812] [adversarial loss: 1.322435, acc: 0.109375]\n",
      "5361: [discriminator loss: 0.528146, acc: 0.781250] [adversarial loss: 1.247281, acc: 0.187500]\n",
      "5362: [discriminator loss: 0.510404, acc: 0.750000] [adversarial loss: 1.261541, acc: 0.171875]\n",
      "5363: [discriminator loss: 0.557800, acc: 0.710938] [adversarial loss: 1.250468, acc: 0.250000]\n",
      "5364: [discriminator loss: 0.500027, acc: 0.726562] [adversarial loss: 0.999867, acc: 0.234375]\n",
      "5365: [discriminator loss: 0.431575, acc: 0.781250] [adversarial loss: 1.234771, acc: 0.234375]\n",
      "5366: [discriminator loss: 0.470613, acc: 0.773438] [adversarial loss: 1.453291, acc: 0.125000]\n",
      "5367: [discriminator loss: 0.492478, acc: 0.750000] [adversarial loss: 0.913787, acc: 0.421875]\n",
      "5368: [discriminator loss: 0.569989, acc: 0.695312] [adversarial loss: 1.602874, acc: 0.109375]\n",
      "5369: [discriminator loss: 0.530526, acc: 0.710938] [adversarial loss: 0.919874, acc: 0.359375]\n",
      "5370: [discriminator loss: 0.526556, acc: 0.718750] [adversarial loss: 2.006976, acc: 0.062500]\n",
      "5371: [discriminator loss: 0.679688, acc: 0.648438] [adversarial loss: 0.817134, acc: 0.484375]\n",
      "5372: [discriminator loss: 0.536072, acc: 0.742188] [adversarial loss: 1.606610, acc: 0.093750]\n",
      "5373: [discriminator loss: 0.607414, acc: 0.695312] [adversarial loss: 1.040536, acc: 0.296875]\n",
      "5374: [discriminator loss: 0.525620, acc: 0.742188] [adversarial loss: 1.317802, acc: 0.125000]\n",
      "5375: [discriminator loss: 0.552211, acc: 0.718750] [adversarial loss: 1.049303, acc: 0.250000]\n",
      "5376: [discriminator loss: 0.543640, acc: 0.734375] [adversarial loss: 1.314655, acc: 0.171875]\n",
      "5377: [discriminator loss: 0.489097, acc: 0.742188] [adversarial loss: 1.047125, acc: 0.312500]\n",
      "5378: [discriminator loss: 0.566236, acc: 0.718750] [adversarial loss: 1.504131, acc: 0.140625]\n",
      "5379: [discriminator loss: 0.541252, acc: 0.726562] [adversarial loss: 0.961027, acc: 0.406250]\n",
      "5380: [discriminator loss: 0.565110, acc: 0.734375] [adversarial loss: 1.541628, acc: 0.140625]\n",
      "5381: [discriminator loss: 0.491895, acc: 0.742188] [adversarial loss: 1.010046, acc: 0.281250]\n",
      "5382: [discriminator loss: 0.486219, acc: 0.789062] [adversarial loss: 1.295269, acc: 0.250000]\n",
      "5383: [discriminator loss: 0.605428, acc: 0.664062] [adversarial loss: 1.097934, acc: 0.359375]\n",
      "5384: [discriminator loss: 0.527123, acc: 0.726562] [adversarial loss: 1.192707, acc: 0.265625]\n",
      "5385: [discriminator loss: 0.449155, acc: 0.781250] [adversarial loss: 1.092237, acc: 0.296875]\n",
      "5386: [discriminator loss: 0.561448, acc: 0.742188] [adversarial loss: 1.435919, acc: 0.187500]\n",
      "5387: [discriminator loss: 0.572935, acc: 0.695312] [adversarial loss: 0.892261, acc: 0.375000]\n",
      "5388: [discriminator loss: 0.503784, acc: 0.765625] [adversarial loss: 1.493229, acc: 0.109375]\n",
      "5389: [discriminator loss: 0.565438, acc: 0.710938] [adversarial loss: 1.069879, acc: 0.312500]\n",
      "5390: [discriminator loss: 0.501431, acc: 0.734375] [adversarial loss: 1.150718, acc: 0.296875]\n",
      "5391: [discriminator loss: 0.472664, acc: 0.828125] [adversarial loss: 1.371896, acc: 0.078125]\n",
      "5392: [discriminator loss: 0.613375, acc: 0.695312] [adversarial loss: 1.014672, acc: 0.312500]\n",
      "5393: [discriminator loss: 0.625888, acc: 0.671875] [adversarial loss: 1.801391, acc: 0.078125]\n",
      "5394: [discriminator loss: 0.632329, acc: 0.664062] [adversarial loss: 0.983606, acc: 0.359375]\n",
      "5395: [discriminator loss: 0.456927, acc: 0.750000] [adversarial loss: 1.677752, acc: 0.140625]\n",
      "5396: [discriminator loss: 0.576233, acc: 0.703125] [adversarial loss: 0.913346, acc: 0.421875]\n",
      "5397: [discriminator loss: 0.584425, acc: 0.695312] [adversarial loss: 1.674553, acc: 0.078125]\n",
      "5398: [discriminator loss: 0.509741, acc: 0.703125] [adversarial loss: 0.979786, acc: 0.265625]\n",
      "5399: [discriminator loss: 0.572302, acc: 0.695312] [adversarial loss: 1.358084, acc: 0.187500]\n",
      "5400: [discriminator loss: 0.565188, acc: 0.695312] [adversarial loss: 1.097098, acc: 0.234375]\n",
      "5401: [discriminator loss: 0.530631, acc: 0.742188] [adversarial loss: 1.106457, acc: 0.265625]\n",
      "5402: [discriminator loss: 0.532211, acc: 0.734375] [adversarial loss: 0.975840, acc: 0.406250]\n",
      "5403: [discriminator loss: 0.539187, acc: 0.726562] [adversarial loss: 1.300454, acc: 0.187500]\n",
      "5404: [discriminator loss: 0.508030, acc: 0.750000] [adversarial loss: 1.248654, acc: 0.218750]\n",
      "5405: [discriminator loss: 0.445043, acc: 0.781250] [adversarial loss: 1.651099, acc: 0.093750]\n",
      "5406: [discriminator loss: 0.585924, acc: 0.664062] [adversarial loss: 0.855965, acc: 0.453125]\n",
      "5407: [discriminator loss: 0.589568, acc: 0.656250] [adversarial loss: 1.638473, acc: 0.093750]\n",
      "5408: [discriminator loss: 0.480512, acc: 0.773438] [adversarial loss: 1.237770, acc: 0.187500]\n",
      "5409: [discriminator loss: 0.547819, acc: 0.718750] [adversarial loss: 1.159766, acc: 0.265625]\n",
      "5410: [discriminator loss: 0.530605, acc: 0.734375] [adversarial loss: 1.079465, acc: 0.328125]\n",
      "5411: [discriminator loss: 0.617347, acc: 0.585938] [adversarial loss: 1.360095, acc: 0.140625]\n",
      "5412: [discriminator loss: 0.473903, acc: 0.812500] [adversarial loss: 1.261950, acc: 0.156250]\n",
      "5413: [discriminator loss: 0.533527, acc: 0.757812] [adversarial loss: 0.861498, acc: 0.437500]\n",
      "5414: [discriminator loss: 0.549534, acc: 0.695312] [adversarial loss: 1.229707, acc: 0.140625]\n",
      "5415: [discriminator loss: 0.513563, acc: 0.742188] [adversarial loss: 0.943889, acc: 0.390625]\n",
      "5416: [discriminator loss: 0.577569, acc: 0.695312] [adversarial loss: 1.434328, acc: 0.093750]\n",
      "5417: [discriminator loss: 0.510280, acc: 0.726562] [adversarial loss: 1.092177, acc: 0.281250]\n",
      "5418: [discriminator loss: 0.514720, acc: 0.773438] [adversarial loss: 1.139691, acc: 0.296875]\n",
      "5419: [discriminator loss: 0.546905, acc: 0.718750] [adversarial loss: 1.492637, acc: 0.125000]\n",
      "5420: [discriminator loss: 0.569173, acc: 0.710938] [adversarial loss: 0.737341, acc: 0.531250]\n",
      "5421: [discriminator loss: 0.516066, acc: 0.757812] [adversarial loss: 1.691011, acc: 0.031250]\n",
      "5422: [discriminator loss: 0.498427, acc: 0.750000] [adversarial loss: 1.160280, acc: 0.281250]\n",
      "5423: [discriminator loss: 0.468428, acc: 0.757812] [adversarial loss: 1.478597, acc: 0.203125]\n",
      "5424: [discriminator loss: 0.463158, acc: 0.789062] [adversarial loss: 1.120384, acc: 0.218750]\n",
      "5425: [discriminator loss: 0.508599, acc: 0.773438] [adversarial loss: 1.274636, acc: 0.234375]\n",
      "5426: [discriminator loss: 0.538905, acc: 0.718750] [adversarial loss: 1.710964, acc: 0.062500]\n",
      "5427: [discriminator loss: 0.500048, acc: 0.796875] [adversarial loss: 0.981985, acc: 0.390625]\n",
      "5428: [discriminator loss: 0.514005, acc: 0.742188] [adversarial loss: 1.274165, acc: 0.218750]\n",
      "5429: [discriminator loss: 0.527184, acc: 0.765625] [adversarial loss: 1.373585, acc: 0.203125]\n",
      "5430: [discriminator loss: 0.512189, acc: 0.781250] [adversarial loss: 1.334401, acc: 0.218750]\n",
      "5431: [discriminator loss: 0.485639, acc: 0.781250] [adversarial loss: 1.181162, acc: 0.218750]\n",
      "5432: [discriminator loss: 0.522763, acc: 0.750000] [adversarial loss: 1.401165, acc: 0.218750]\n",
      "5433: [discriminator loss: 0.525000, acc: 0.718750] [adversarial loss: 1.163499, acc: 0.250000]\n",
      "5434: [discriminator loss: 0.568888, acc: 0.695312] [adversarial loss: 1.147026, acc: 0.281250]\n",
      "5435: [discriminator loss: 0.549970, acc: 0.726562] [adversarial loss: 1.044410, acc: 0.421875]\n",
      "5436: [discriminator loss: 0.604094, acc: 0.703125] [adversarial loss: 1.705792, acc: 0.156250]\n",
      "5437: [discriminator loss: 0.637395, acc: 0.656250] [adversarial loss: 1.142584, acc: 0.250000]\n",
      "5438: [discriminator loss: 0.533722, acc: 0.718750] [adversarial loss: 1.437359, acc: 0.140625]\n",
      "5439: [discriminator loss: 0.558397, acc: 0.726562] [adversarial loss: 1.257288, acc: 0.296875]\n",
      "5440: [discriminator loss: 0.553908, acc: 0.734375] [adversarial loss: 1.264273, acc: 0.203125]\n",
      "5441: [discriminator loss: 0.573604, acc: 0.695312] [adversarial loss: 1.076679, acc: 0.250000]\n",
      "5442: [discriminator loss: 0.581352, acc: 0.695312] [adversarial loss: 1.646992, acc: 0.046875]\n",
      "5443: [discriminator loss: 0.564823, acc: 0.703125] [adversarial loss: 0.964817, acc: 0.296875]\n",
      "5444: [discriminator loss: 0.537703, acc: 0.710938] [adversarial loss: 1.626375, acc: 0.078125]\n",
      "5445: [discriminator loss: 0.536584, acc: 0.750000] [adversarial loss: 1.322230, acc: 0.281250]\n",
      "5446: [discriminator loss: 0.593231, acc: 0.664062] [adversarial loss: 1.314716, acc: 0.156250]\n",
      "5447: [discriminator loss: 0.517104, acc: 0.726562] [adversarial loss: 1.625654, acc: 0.078125]\n",
      "5448: [discriminator loss: 0.466631, acc: 0.742188] [adversarial loss: 1.199332, acc: 0.218750]\n",
      "5449: [discriminator loss: 0.456903, acc: 0.781250] [adversarial loss: 1.423894, acc: 0.125000]\n",
      "5450: [discriminator loss: 0.514577, acc: 0.734375] [adversarial loss: 1.095103, acc: 0.234375]\n",
      "5451: [discriminator loss: 0.575925, acc: 0.710938] [adversarial loss: 1.576991, acc: 0.125000]\n",
      "5452: [discriminator loss: 0.592596, acc: 0.695312] [adversarial loss: 0.934298, acc: 0.390625]\n",
      "5453: [discriminator loss: 0.533695, acc: 0.734375] [adversarial loss: 1.510827, acc: 0.140625]\n",
      "5454: [discriminator loss: 0.580857, acc: 0.687500] [adversarial loss: 1.101259, acc: 0.375000]\n",
      "5455: [discriminator loss: 0.547875, acc: 0.734375] [adversarial loss: 1.480575, acc: 0.140625]\n",
      "5456: [discriminator loss: 0.533271, acc: 0.710938] [adversarial loss: 1.099598, acc: 0.328125]\n",
      "5457: [discriminator loss: 0.453005, acc: 0.843750] [adversarial loss: 1.548224, acc: 0.109375]\n",
      "5458: [discriminator loss: 0.613996, acc: 0.632812] [adversarial loss: 0.900504, acc: 0.406250]\n",
      "5459: [discriminator loss: 0.638099, acc: 0.632812] [adversarial loss: 1.391235, acc: 0.125000]\n",
      "5460: [discriminator loss: 0.480926, acc: 0.781250] [adversarial loss: 0.962584, acc: 0.375000]\n",
      "5461: [discriminator loss: 0.496175, acc: 0.757812] [adversarial loss: 1.589098, acc: 0.125000]\n",
      "5462: [discriminator loss: 0.563351, acc: 0.687500] [adversarial loss: 1.083617, acc: 0.265625]\n",
      "5463: [discriminator loss: 0.515152, acc: 0.726562] [adversarial loss: 1.303057, acc: 0.125000]\n",
      "5464: [discriminator loss: 0.577264, acc: 0.710938] [adversarial loss: 0.751381, acc: 0.562500]\n",
      "5465: [discriminator loss: 0.492695, acc: 0.742188] [adversarial loss: 1.790345, acc: 0.015625]\n",
      "5466: [discriminator loss: 0.558465, acc: 0.710938] [adversarial loss: 0.921208, acc: 0.375000]\n",
      "5467: [discriminator loss: 0.488165, acc: 0.765625] [adversarial loss: 1.401156, acc: 0.109375]\n",
      "5468: [discriminator loss: 0.512284, acc: 0.750000] [adversarial loss: 1.325754, acc: 0.156250]\n",
      "5469: [discriminator loss: 0.559148, acc: 0.734375] [adversarial loss: 1.109519, acc: 0.281250]\n",
      "5470: [discriminator loss: 0.565432, acc: 0.750000] [adversarial loss: 0.955910, acc: 0.421875]\n",
      "5471: [discriminator loss: 0.513760, acc: 0.734375] [adversarial loss: 1.116045, acc: 0.250000]\n",
      "5472: [discriminator loss: 0.542364, acc: 0.718750] [adversarial loss: 1.206292, acc: 0.218750]\n",
      "5473: [discriminator loss: 0.577803, acc: 0.679688] [adversarial loss: 1.073735, acc: 0.312500]\n",
      "5474: [discriminator loss: 0.535783, acc: 0.718750] [adversarial loss: 1.370809, acc: 0.203125]\n",
      "5475: [discriminator loss: 0.590860, acc: 0.718750] [adversarial loss: 1.175584, acc: 0.265625]\n",
      "5476: [discriminator loss: 0.511633, acc: 0.726562] [adversarial loss: 1.414759, acc: 0.140625]\n",
      "5477: [discriminator loss: 0.615515, acc: 0.695312] [adversarial loss: 0.788288, acc: 0.484375]\n",
      "5478: [discriminator loss: 0.527870, acc: 0.710938] [adversarial loss: 1.734078, acc: 0.093750]\n",
      "5479: [discriminator loss: 0.654329, acc: 0.632812] [adversarial loss: 0.840169, acc: 0.453125]\n",
      "5480: [discriminator loss: 0.585544, acc: 0.687500] [adversarial loss: 1.627270, acc: 0.093750]\n",
      "5481: [discriminator loss: 0.545807, acc: 0.718750] [adversarial loss: 1.089170, acc: 0.312500]\n",
      "5482: [discriminator loss: 0.500686, acc: 0.781250] [adversarial loss: 1.431923, acc: 0.125000]\n",
      "5483: [discriminator loss: 0.423128, acc: 0.804688] [adversarial loss: 1.175030, acc: 0.250000]\n",
      "5484: [discriminator loss: 0.536131, acc: 0.742188] [adversarial loss: 1.102572, acc: 0.281250]\n",
      "5485: [discriminator loss: 0.476921, acc: 0.781250] [adversarial loss: 1.012834, acc: 0.296875]\n",
      "5486: [discriminator loss: 0.477667, acc: 0.765625] [adversarial loss: 1.306835, acc: 0.187500]\n",
      "5487: [discriminator loss: 0.540603, acc: 0.710938] [adversarial loss: 1.193880, acc: 0.203125]\n",
      "5488: [discriminator loss: 0.533093, acc: 0.726562] [adversarial loss: 1.269069, acc: 0.343750]\n",
      "5489: [discriminator loss: 0.571252, acc: 0.718750] [adversarial loss: 1.668367, acc: 0.109375]\n",
      "5490: [discriminator loss: 0.482191, acc: 0.726562] [adversarial loss: 1.069786, acc: 0.359375]\n",
      "5491: [discriminator loss: 0.588423, acc: 0.625000] [adversarial loss: 1.567548, acc: 0.078125]\n",
      "5492: [discriminator loss: 0.492763, acc: 0.671875] [adversarial loss: 1.069566, acc: 0.281250]\n",
      "5493: [discriminator loss: 0.485869, acc: 0.781250] [adversarial loss: 1.531531, acc: 0.171875]\n",
      "5494: [discriminator loss: 0.501935, acc: 0.757812] [adversarial loss: 0.924450, acc: 0.500000]\n",
      "5495: [discriminator loss: 0.487161, acc: 0.765625] [adversarial loss: 1.621492, acc: 0.140625]\n",
      "5496: [discriminator loss: 0.729644, acc: 0.617188] [adversarial loss: 1.048813, acc: 0.296875]\n",
      "5497: [discriminator loss: 0.514042, acc: 0.718750] [adversarial loss: 1.333695, acc: 0.140625]\n",
      "5498: [discriminator loss: 0.482511, acc: 0.781250] [adversarial loss: 0.984214, acc: 0.343750]\n",
      "5499: [discriminator loss: 0.543271, acc: 0.750000] [adversarial loss: 1.630058, acc: 0.046875]\n",
      "5500: [discriminator loss: 0.515259, acc: 0.703125] [adversarial loss: 1.194204, acc: 0.187500]\n",
      "5501: [discriminator loss: 0.576357, acc: 0.695312] [adversarial loss: 1.389062, acc: 0.156250]\n",
      "5502: [discriminator loss: 0.586960, acc: 0.734375] [adversarial loss: 1.002427, acc: 0.296875]\n",
      "5503: [discriminator loss: 0.555702, acc: 0.710938] [adversarial loss: 1.454575, acc: 0.171875]\n",
      "5504: [discriminator loss: 0.542679, acc: 0.726562] [adversarial loss: 0.954169, acc: 0.375000]\n",
      "5505: [discriminator loss: 0.527584, acc: 0.742188] [adversarial loss: 1.183650, acc: 0.218750]\n",
      "5506: [discriminator loss: 0.522678, acc: 0.726562] [adversarial loss: 1.148965, acc: 0.281250]\n",
      "5507: [discriminator loss: 0.528021, acc: 0.765625] [adversarial loss: 1.373172, acc: 0.125000]\n",
      "5508: [discriminator loss: 0.554658, acc: 0.750000] [adversarial loss: 0.881950, acc: 0.437500]\n",
      "5509: [discriminator loss: 0.530036, acc: 0.710938] [adversarial loss: 1.488371, acc: 0.109375]\n",
      "5510: [discriminator loss: 0.526802, acc: 0.703125] [adversarial loss: 0.855356, acc: 0.453125]\n",
      "5511: [discriminator loss: 0.469550, acc: 0.773438] [adversarial loss: 1.337735, acc: 0.156250]\n",
      "5512: [discriminator loss: 0.557588, acc: 0.710938] [adversarial loss: 1.179839, acc: 0.312500]\n",
      "5513: [discriminator loss: 0.559641, acc: 0.710938] [adversarial loss: 1.634761, acc: 0.109375]\n",
      "5514: [discriminator loss: 0.527986, acc: 0.734375] [adversarial loss: 0.988808, acc: 0.390625]\n",
      "5515: [discriminator loss: 0.533788, acc: 0.726562] [adversarial loss: 1.537100, acc: 0.093750]\n",
      "5516: [discriminator loss: 0.532576, acc: 0.726562] [adversarial loss: 0.891059, acc: 0.421875]\n",
      "5517: [discriminator loss: 0.502057, acc: 0.781250] [adversarial loss: 1.256596, acc: 0.281250]\n",
      "5518: [discriminator loss: 0.450934, acc: 0.812500] [adversarial loss: 1.061750, acc: 0.312500]\n",
      "5519: [discriminator loss: 0.610036, acc: 0.648438] [adversarial loss: 1.169278, acc: 0.250000]\n",
      "5520: [discriminator loss: 0.551264, acc: 0.718750] [adversarial loss: 1.304953, acc: 0.203125]\n",
      "5521: [discriminator loss: 0.464277, acc: 0.789062] [adversarial loss: 1.420868, acc: 0.140625]\n",
      "5522: [discriminator loss: 0.540554, acc: 0.671875] [adversarial loss: 1.287583, acc: 0.265625]\n",
      "5523: [discriminator loss: 0.579278, acc: 0.726562] [adversarial loss: 1.823928, acc: 0.125000]\n",
      "5524: [discriminator loss: 0.489647, acc: 0.734375] [adversarial loss: 1.092659, acc: 0.359375]\n",
      "5525: [discriminator loss: 0.548108, acc: 0.742188] [adversarial loss: 1.338970, acc: 0.187500]\n",
      "5526: [discriminator loss: 0.606797, acc: 0.679688] [adversarial loss: 1.142140, acc: 0.343750]\n",
      "5527: [discriminator loss: 0.576879, acc: 0.726562] [adversarial loss: 1.295298, acc: 0.203125]\n",
      "5528: [discriminator loss: 0.563301, acc: 0.695312] [adversarial loss: 1.039748, acc: 0.375000]\n",
      "5529: [discriminator loss: 0.504721, acc: 0.757812] [adversarial loss: 1.253554, acc: 0.140625]\n",
      "5530: [discriminator loss: 0.551378, acc: 0.703125] [adversarial loss: 0.949290, acc: 0.390625]\n",
      "5531: [discriminator loss: 0.541502, acc: 0.742188] [adversarial loss: 1.638009, acc: 0.062500]\n",
      "5532: [discriminator loss: 0.499833, acc: 0.757812] [adversarial loss: 0.912582, acc: 0.421875]\n",
      "5533: [discriminator loss: 0.621061, acc: 0.664062] [adversarial loss: 1.636826, acc: 0.062500]\n",
      "5534: [discriminator loss: 0.531051, acc: 0.757812] [adversarial loss: 0.945787, acc: 0.406250]\n",
      "5535: [discriminator loss: 0.545922, acc: 0.718750] [adversarial loss: 1.307177, acc: 0.203125]\n",
      "5536: [discriminator loss: 0.513412, acc: 0.718750] [adversarial loss: 1.045642, acc: 0.281250]\n",
      "5537: [discriminator loss: 0.531619, acc: 0.773438] [adversarial loss: 1.280578, acc: 0.125000]\n",
      "5538: [discriminator loss: 0.543941, acc: 0.726562] [adversarial loss: 0.822281, acc: 0.500000]\n",
      "5539: [discriminator loss: 0.611578, acc: 0.671875] [adversarial loss: 1.761644, acc: 0.031250]\n",
      "5540: [discriminator loss: 0.608083, acc: 0.695312] [adversarial loss: 0.950887, acc: 0.390625]\n",
      "5541: [discriminator loss: 0.601861, acc: 0.664062] [adversarial loss: 1.440370, acc: 0.109375]\n",
      "5542: [discriminator loss: 0.543300, acc: 0.726562] [adversarial loss: 1.255657, acc: 0.125000]\n",
      "5543: [discriminator loss: 0.494592, acc: 0.773438] [adversarial loss: 0.889678, acc: 0.421875]\n",
      "5544: [discriminator loss: 0.470309, acc: 0.750000] [adversarial loss: 1.419076, acc: 0.125000]\n",
      "5545: [discriminator loss: 0.537444, acc: 0.726562] [adversarial loss: 1.146117, acc: 0.203125]\n",
      "5546: [discriminator loss: 0.576424, acc: 0.718750] [adversarial loss: 1.466705, acc: 0.109375]\n",
      "5547: [discriminator loss: 0.508391, acc: 0.726562] [adversarial loss: 0.905925, acc: 0.359375]\n",
      "5548: [discriminator loss: 0.649383, acc: 0.578125] [adversarial loss: 1.284044, acc: 0.203125]\n",
      "5549: [discriminator loss: 0.535810, acc: 0.695312] [adversarial loss: 1.178030, acc: 0.203125]\n",
      "5550: [discriminator loss: 0.594977, acc: 0.656250] [adversarial loss: 1.360493, acc: 0.203125]\n",
      "5551: [discriminator loss: 0.529436, acc: 0.695312] [adversarial loss: 1.479970, acc: 0.078125]\n",
      "5552: [discriminator loss: 0.504106, acc: 0.734375] [adversarial loss: 0.913456, acc: 0.375000]\n",
      "5553: [discriminator loss: 0.654211, acc: 0.726562] [adversarial loss: 1.402780, acc: 0.171875]\n",
      "5554: [discriminator loss: 0.588495, acc: 0.687500] [adversarial loss: 1.379903, acc: 0.109375]\n",
      "5555: [discriminator loss: 0.573129, acc: 0.695312] [adversarial loss: 0.948585, acc: 0.343750]\n",
      "5556: [discriminator loss: 0.550869, acc: 0.703125] [adversarial loss: 1.529207, acc: 0.140625]\n",
      "5557: [discriminator loss: 0.508752, acc: 0.695312] [adversarial loss: 0.984946, acc: 0.328125]\n",
      "5558: [discriminator loss: 0.496016, acc: 0.742188] [adversarial loss: 1.489943, acc: 0.125000]\n",
      "5559: [discriminator loss: 0.561103, acc: 0.703125] [adversarial loss: 0.908518, acc: 0.406250]\n",
      "5560: [discriminator loss: 0.561557, acc: 0.695312] [adversarial loss: 1.418220, acc: 0.109375]\n",
      "5561: [discriminator loss: 0.577482, acc: 0.687500] [adversarial loss: 0.960394, acc: 0.328125]\n",
      "5562: [discriminator loss: 0.586745, acc: 0.695312] [adversarial loss: 1.474207, acc: 0.093750]\n",
      "5563: [discriminator loss: 0.575540, acc: 0.664062] [adversarial loss: 0.921590, acc: 0.375000]\n",
      "5564: [discriminator loss: 0.501081, acc: 0.710938] [adversarial loss: 1.356570, acc: 0.218750]\n",
      "5565: [discriminator loss: 0.521144, acc: 0.742188] [adversarial loss: 1.032658, acc: 0.250000]\n",
      "5566: [discriminator loss: 0.551787, acc: 0.718750] [adversarial loss: 1.125350, acc: 0.265625]\n",
      "5567: [discriminator loss: 0.531484, acc: 0.718750] [adversarial loss: 0.994089, acc: 0.250000]\n",
      "5568: [discriminator loss: 0.511141, acc: 0.695312] [adversarial loss: 1.264708, acc: 0.187500]\n",
      "5569: [discriminator loss: 0.533471, acc: 0.742188] [adversarial loss: 1.309900, acc: 0.187500]\n",
      "5570: [discriminator loss: 0.510551, acc: 0.726562] [adversarial loss: 0.998846, acc: 0.343750]\n",
      "5571: [discriminator loss: 0.436224, acc: 0.812500] [adversarial loss: 1.190912, acc: 0.234375]\n",
      "5572: [discriminator loss: 0.519772, acc: 0.750000] [adversarial loss: 1.473497, acc: 0.109375]\n",
      "5573: [discriminator loss: 0.572016, acc: 0.726562] [adversarial loss: 0.938405, acc: 0.343750]\n",
      "5574: [discriminator loss: 0.471769, acc: 0.804688] [adversarial loss: 1.308047, acc: 0.156250]\n",
      "5575: [discriminator loss: 0.517518, acc: 0.742188] [adversarial loss: 1.149205, acc: 0.265625]\n",
      "5576: [discriminator loss: 0.564630, acc: 0.687500] [adversarial loss: 1.274340, acc: 0.171875]\n",
      "5577: [discriminator loss: 0.514823, acc: 0.726562] [adversarial loss: 1.261367, acc: 0.171875]\n",
      "5578: [discriminator loss: 0.560765, acc: 0.679688] [adversarial loss: 1.213355, acc: 0.234375]\n",
      "5579: [discriminator loss: 0.571038, acc: 0.726562] [adversarial loss: 1.538978, acc: 0.093750]\n",
      "5580: [discriminator loss: 0.519424, acc: 0.726562] [adversarial loss: 1.070231, acc: 0.187500]\n",
      "5581: [discriminator loss: 0.548599, acc: 0.718750] [adversarial loss: 1.333876, acc: 0.187500]\n",
      "5582: [discriminator loss: 0.457448, acc: 0.781250] [adversarial loss: 1.200652, acc: 0.250000]\n",
      "5583: [discriminator loss: 0.513557, acc: 0.773438] [adversarial loss: 1.243042, acc: 0.156250]\n",
      "5584: [discriminator loss: 0.436088, acc: 0.804688] [adversarial loss: 1.279450, acc: 0.156250]\n",
      "5585: [discriminator loss: 0.456932, acc: 0.812500] [adversarial loss: 1.368767, acc: 0.187500]\n",
      "5586: [discriminator loss: 0.575237, acc: 0.703125] [adversarial loss: 1.493009, acc: 0.078125]\n",
      "5587: [discriminator loss: 0.463913, acc: 0.796875] [adversarial loss: 0.987430, acc: 0.296875]\n",
      "5588: [discriminator loss: 0.595716, acc: 0.734375] [adversarial loss: 1.260276, acc: 0.234375]\n",
      "5589: [discriminator loss: 0.487642, acc: 0.781250] [adversarial loss: 1.003137, acc: 0.250000]\n",
      "5590: [discriminator loss: 0.531638, acc: 0.726562] [adversarial loss: 1.614330, acc: 0.140625]\n",
      "5591: [discriminator loss: 0.592943, acc: 0.695312] [adversarial loss: 0.940396, acc: 0.406250]\n",
      "5592: [discriminator loss: 0.585470, acc: 0.750000] [adversarial loss: 1.471442, acc: 0.140625]\n",
      "5593: [discriminator loss: 0.536762, acc: 0.750000] [adversarial loss: 0.967259, acc: 0.359375]\n",
      "5594: [discriminator loss: 0.418895, acc: 0.789062] [adversarial loss: 1.129032, acc: 0.265625]\n",
      "5595: [discriminator loss: 0.477480, acc: 0.757812] [adversarial loss: 1.063426, acc: 0.281250]\n",
      "5596: [discriminator loss: 0.510382, acc: 0.742188] [adversarial loss: 1.177882, acc: 0.250000]\n",
      "5597: [discriminator loss: 0.499745, acc: 0.781250] [adversarial loss: 1.099507, acc: 0.265625]\n",
      "5598: [discriminator loss: 0.556502, acc: 0.687500] [adversarial loss: 1.415547, acc: 0.156250]\n",
      "5599: [discriminator loss: 0.559577, acc: 0.687500] [adversarial loss: 1.044681, acc: 0.359375]\n",
      "5600: [discriminator loss: 0.560847, acc: 0.718750] [adversarial loss: 1.518323, acc: 0.156250]\n",
      "5601: [discriminator loss: 0.535886, acc: 0.734375] [adversarial loss: 0.937551, acc: 0.437500]\n",
      "5602: [discriminator loss: 0.611663, acc: 0.703125] [adversarial loss: 1.429241, acc: 0.140625]\n",
      "5603: [discriminator loss: 0.510587, acc: 0.734375] [adversarial loss: 0.869709, acc: 0.437500]\n",
      "5604: [discriminator loss: 0.649683, acc: 0.648438] [adversarial loss: 1.657095, acc: 0.046875]\n",
      "5605: [discriminator loss: 0.597690, acc: 0.679688] [adversarial loss: 0.977343, acc: 0.328125]\n",
      "5606: [discriminator loss: 0.523795, acc: 0.750000] [adversarial loss: 1.565313, acc: 0.125000]\n",
      "5607: [discriminator loss: 0.570805, acc: 0.695312] [adversarial loss: 0.957848, acc: 0.359375]\n",
      "5608: [discriminator loss: 0.506566, acc: 0.750000] [adversarial loss: 1.462610, acc: 0.187500]\n",
      "5609: [discriminator loss: 0.497276, acc: 0.757812] [adversarial loss: 1.027626, acc: 0.250000]\n",
      "5610: [discriminator loss: 0.572020, acc: 0.687500] [adversarial loss: 1.533969, acc: 0.031250]\n",
      "5611: [discriminator loss: 0.526987, acc: 0.710938] [adversarial loss: 1.027334, acc: 0.296875]\n",
      "5612: [discriminator loss: 0.485651, acc: 0.734375] [adversarial loss: 1.525638, acc: 0.156250]\n",
      "5613: [discriminator loss: 0.564519, acc: 0.664062] [adversarial loss: 1.049367, acc: 0.328125]\n",
      "5614: [discriminator loss: 0.486887, acc: 0.765625] [adversarial loss: 1.345559, acc: 0.156250]\n",
      "5615: [discriminator loss: 0.491165, acc: 0.765625] [adversarial loss: 1.100078, acc: 0.250000]\n",
      "5616: [discriminator loss: 0.508667, acc: 0.734375] [adversarial loss: 1.633943, acc: 0.125000]\n",
      "5617: [discriminator loss: 0.547692, acc: 0.695312] [adversarial loss: 1.088036, acc: 0.265625]\n",
      "5618: [discriminator loss: 0.492323, acc: 0.750000] [adversarial loss: 1.233860, acc: 0.296875]\n",
      "5619: [discriminator loss: 0.601110, acc: 0.679688] [adversarial loss: 1.407963, acc: 0.109375]\n",
      "5620: [discriminator loss: 0.479305, acc: 0.742188] [adversarial loss: 1.299998, acc: 0.171875]\n",
      "5621: [discriminator loss: 0.543477, acc: 0.742188] [adversarial loss: 1.358038, acc: 0.140625]\n",
      "5622: [discriminator loss: 0.499466, acc: 0.765625] [adversarial loss: 0.861039, acc: 0.437500]\n",
      "5623: [discriminator loss: 0.502437, acc: 0.726562] [adversarial loss: 1.212766, acc: 0.171875]\n",
      "5624: [discriminator loss: 0.481063, acc: 0.765625] [adversarial loss: 1.266079, acc: 0.234375]\n",
      "5625: [discriminator loss: 0.496817, acc: 0.742188] [adversarial loss: 1.234425, acc: 0.250000]\n",
      "5626: [discriminator loss: 0.456727, acc: 0.765625] [adversarial loss: 1.234735, acc: 0.265625]\n",
      "5627: [discriminator loss: 0.613981, acc: 0.656250] [adversarial loss: 1.484560, acc: 0.062500]\n",
      "5628: [discriminator loss: 0.557560, acc: 0.734375] [adversarial loss: 0.912279, acc: 0.375000]\n",
      "5629: [discriminator loss: 0.581589, acc: 0.671875] [adversarial loss: 1.400079, acc: 0.234375]\n",
      "5630: [discriminator loss: 0.586327, acc: 0.703125] [adversarial loss: 0.989708, acc: 0.343750]\n",
      "5631: [discriminator loss: 0.537585, acc: 0.703125] [adversarial loss: 1.347058, acc: 0.156250]\n",
      "5632: [discriminator loss: 0.532334, acc: 0.726562] [adversarial loss: 1.178445, acc: 0.250000]\n",
      "5633: [discriminator loss: 0.539353, acc: 0.726562] [adversarial loss: 1.087993, acc: 0.359375]\n",
      "5634: [discriminator loss: 0.519865, acc: 0.726562] [adversarial loss: 1.332222, acc: 0.140625]\n",
      "5635: [discriminator loss: 0.509084, acc: 0.710938] [adversarial loss: 1.142333, acc: 0.265625]\n",
      "5636: [discriminator loss: 0.522994, acc: 0.742188] [adversarial loss: 1.589205, acc: 0.078125]\n",
      "5637: [discriminator loss: 0.490847, acc: 0.773438] [adversarial loss: 0.791085, acc: 0.546875]\n",
      "5638: [discriminator loss: 0.537995, acc: 0.710938] [adversarial loss: 1.563609, acc: 0.171875]\n",
      "5639: [discriminator loss: 0.465582, acc: 0.781250] [adversarial loss: 1.235585, acc: 0.203125]\n",
      "5640: [discriminator loss: 0.538193, acc: 0.687500] [adversarial loss: 1.335584, acc: 0.125000]\n",
      "5641: [discriminator loss: 0.445381, acc: 0.820312] [adversarial loss: 1.236639, acc: 0.281250]\n",
      "5642: [discriminator loss: 0.560792, acc: 0.703125] [adversarial loss: 1.260047, acc: 0.156250]\n",
      "5643: [discriminator loss: 0.545849, acc: 0.726562] [adversarial loss: 1.207167, acc: 0.250000]\n",
      "5644: [discriminator loss: 0.499950, acc: 0.773438] [adversarial loss: 1.406405, acc: 0.171875]\n",
      "5645: [discriminator loss: 0.516321, acc: 0.750000] [adversarial loss: 1.159879, acc: 0.218750]\n",
      "5646: [discriminator loss: 0.529527, acc: 0.742188] [adversarial loss: 1.493715, acc: 0.203125]\n",
      "5647: [discriminator loss: 0.532300, acc: 0.710938] [adversarial loss: 1.542245, acc: 0.156250]\n",
      "5648: [discriminator loss: 0.545414, acc: 0.710938] [adversarial loss: 1.265111, acc: 0.265625]\n",
      "5649: [discriminator loss: 0.526732, acc: 0.695312] [adversarial loss: 1.353120, acc: 0.203125]\n",
      "5650: [discriminator loss: 0.590772, acc: 0.703125] [adversarial loss: 1.070178, acc: 0.218750]\n",
      "5651: [discriminator loss: 0.535947, acc: 0.718750] [adversarial loss: 1.471719, acc: 0.156250]\n",
      "5652: [discriminator loss: 0.527322, acc: 0.695312] [adversarial loss: 1.192803, acc: 0.203125]\n",
      "5653: [discriminator loss: 0.602783, acc: 0.664062] [adversarial loss: 1.136325, acc: 0.312500]\n",
      "5654: [discriminator loss: 0.527562, acc: 0.726562] [adversarial loss: 1.206037, acc: 0.218750]\n",
      "5655: [discriminator loss: 0.552577, acc: 0.726562] [adversarial loss: 1.008677, acc: 0.296875]\n",
      "5656: [discriminator loss: 0.516388, acc: 0.734375] [adversarial loss: 1.475287, acc: 0.140625]\n",
      "5657: [discriminator loss: 0.553734, acc: 0.710938] [adversarial loss: 1.026192, acc: 0.328125]\n",
      "5658: [discriminator loss: 0.534726, acc: 0.750000] [adversarial loss: 1.507147, acc: 0.156250]\n",
      "5659: [discriminator loss: 0.535224, acc: 0.710938] [adversarial loss: 1.042780, acc: 0.281250]\n",
      "5660: [discriminator loss: 0.626773, acc: 0.703125] [adversarial loss: 1.358525, acc: 0.250000]\n",
      "5661: [discriminator loss: 0.534292, acc: 0.718750] [adversarial loss: 1.282005, acc: 0.203125]\n",
      "5662: [discriminator loss: 0.583171, acc: 0.679688] [adversarial loss: 1.537762, acc: 0.125000]\n",
      "5663: [discriminator loss: 0.461591, acc: 0.812500] [adversarial loss: 0.936415, acc: 0.359375]\n",
      "5664: [discriminator loss: 0.599827, acc: 0.687500] [adversarial loss: 1.402088, acc: 0.140625]\n",
      "5665: [discriminator loss: 0.510782, acc: 0.757812] [adversarial loss: 1.038166, acc: 0.281250]\n",
      "5666: [discriminator loss: 0.587327, acc: 0.687500] [adversarial loss: 1.503950, acc: 0.125000]\n",
      "5667: [discriminator loss: 0.561813, acc: 0.734375] [adversarial loss: 1.404254, acc: 0.125000]\n",
      "5668: [discriminator loss: 0.569149, acc: 0.671875] [adversarial loss: 1.198685, acc: 0.203125]\n",
      "5669: [discriminator loss: 0.531862, acc: 0.710938] [adversarial loss: 1.153908, acc: 0.359375]\n",
      "5670: [discriminator loss: 0.538687, acc: 0.695312] [adversarial loss: 1.458677, acc: 0.140625]\n",
      "5671: [discriminator loss: 0.466047, acc: 0.765625] [adversarial loss: 1.149396, acc: 0.375000]\n",
      "5672: [discriminator loss: 0.495283, acc: 0.757812] [adversarial loss: 1.402949, acc: 0.203125]\n",
      "5673: [discriminator loss: 0.491896, acc: 0.750000] [adversarial loss: 1.098857, acc: 0.312500]\n",
      "5674: [discriminator loss: 0.489796, acc: 0.773438] [adversarial loss: 1.468897, acc: 0.125000]\n",
      "5675: [discriminator loss: 0.581680, acc: 0.671875] [adversarial loss: 1.214521, acc: 0.312500]\n",
      "5676: [discriminator loss: 0.449674, acc: 0.781250] [adversarial loss: 1.123341, acc: 0.343750]\n",
      "5677: [discriminator loss: 0.575682, acc: 0.695312] [adversarial loss: 1.291670, acc: 0.171875]\n",
      "5678: [discriminator loss: 0.462885, acc: 0.796875] [adversarial loss: 1.585561, acc: 0.125000]\n",
      "5679: [discriminator loss: 0.530447, acc: 0.750000] [adversarial loss: 1.143847, acc: 0.187500]\n",
      "5680: [discriminator loss: 0.520031, acc: 0.757812] [adversarial loss: 1.244943, acc: 0.218750]\n",
      "5681: [discriminator loss: 0.497238, acc: 0.734375] [adversarial loss: 1.211606, acc: 0.265625]\n",
      "5682: [discriminator loss: 0.560033, acc: 0.695312] [adversarial loss: 1.703326, acc: 0.140625]\n",
      "5683: [discriminator loss: 0.557129, acc: 0.718750] [adversarial loss: 0.781006, acc: 0.453125]\n",
      "5684: [discriminator loss: 0.547860, acc: 0.679688] [adversarial loss: 1.807232, acc: 0.046875]\n",
      "5685: [discriminator loss: 0.487564, acc: 0.734375] [adversarial loss: 1.034932, acc: 0.281250]\n",
      "5686: [discriminator loss: 0.610250, acc: 0.687500] [adversarial loss: 1.720412, acc: 0.093750]\n",
      "5687: [discriminator loss: 0.561908, acc: 0.679688] [adversarial loss: 1.080406, acc: 0.218750]\n",
      "5688: [discriminator loss: 0.511100, acc: 0.757812] [adversarial loss: 1.411999, acc: 0.125000]\n",
      "5689: [discriminator loss: 0.464269, acc: 0.796875] [adversarial loss: 1.136737, acc: 0.265625]\n",
      "5690: [discriminator loss: 0.601035, acc: 0.679688] [adversarial loss: 1.214538, acc: 0.281250]\n",
      "5691: [discriminator loss: 0.530546, acc: 0.726562] [adversarial loss: 1.292550, acc: 0.156250]\n",
      "5692: [discriminator loss: 0.521117, acc: 0.703125] [adversarial loss: 1.265790, acc: 0.203125]\n",
      "5693: [discriminator loss: 0.537988, acc: 0.695312] [adversarial loss: 1.076967, acc: 0.328125]\n",
      "5694: [discriminator loss: 0.546918, acc: 0.718750] [adversarial loss: 1.172538, acc: 0.250000]\n",
      "5695: [discriminator loss: 0.554626, acc: 0.742188] [adversarial loss: 1.199032, acc: 0.250000]\n",
      "5696: [discriminator loss: 0.482306, acc: 0.828125] [adversarial loss: 1.178385, acc: 0.203125]\n",
      "5697: [discriminator loss: 0.536430, acc: 0.742188] [adversarial loss: 1.268092, acc: 0.203125]\n",
      "5698: [discriminator loss: 0.502419, acc: 0.773438] [adversarial loss: 1.584376, acc: 0.046875]\n",
      "5699: [discriminator loss: 0.538021, acc: 0.726562] [adversarial loss: 1.251612, acc: 0.250000]\n",
      "5700: [discriminator loss: 0.529391, acc: 0.734375] [adversarial loss: 1.385917, acc: 0.296875]\n",
      "5701: [discriminator loss: 0.500703, acc: 0.750000] [adversarial loss: 1.160627, acc: 0.296875]\n",
      "5702: [discriminator loss: 0.463723, acc: 0.757812] [adversarial loss: 1.532420, acc: 0.078125]\n",
      "5703: [discriminator loss: 0.496786, acc: 0.750000] [adversarial loss: 1.191411, acc: 0.312500]\n",
      "5704: [discriminator loss: 0.543139, acc: 0.703125] [adversarial loss: 1.499453, acc: 0.140625]\n",
      "5705: [discriminator loss: 0.538552, acc: 0.695312] [adversarial loss: 1.151562, acc: 0.312500]\n",
      "5706: [discriminator loss: 0.442280, acc: 0.773438] [adversarial loss: 1.501397, acc: 0.078125]\n",
      "5707: [discriminator loss: 0.568577, acc: 0.703125] [adversarial loss: 0.964792, acc: 0.343750]\n",
      "5708: [discriminator loss: 0.618600, acc: 0.664062] [adversarial loss: 1.776189, acc: 0.125000]\n",
      "5709: [discriminator loss: 0.683079, acc: 0.601562] [adversarial loss: 0.930388, acc: 0.343750]\n",
      "5710: [discriminator loss: 0.540229, acc: 0.765625] [adversarial loss: 1.368753, acc: 0.218750]\n",
      "5711: [discriminator loss: 0.521273, acc: 0.742188] [adversarial loss: 0.885105, acc: 0.343750]\n",
      "5712: [discriminator loss: 0.629723, acc: 0.695312] [adversarial loss: 1.570279, acc: 0.062500]\n",
      "5713: [discriminator loss: 0.557592, acc: 0.757812] [adversarial loss: 1.101828, acc: 0.265625]\n",
      "5714: [discriminator loss: 0.498555, acc: 0.757812] [adversarial loss: 1.354979, acc: 0.109375]\n",
      "5715: [discriminator loss: 0.490126, acc: 0.757812] [adversarial loss: 1.382043, acc: 0.078125]\n",
      "5716: [discriminator loss: 0.576682, acc: 0.679688] [adversarial loss: 1.113034, acc: 0.281250]\n",
      "5717: [discriminator loss: 0.571449, acc: 0.695312] [adversarial loss: 1.836251, acc: 0.015625]\n",
      "5718: [discriminator loss: 0.524895, acc: 0.710938] [adversarial loss: 0.992654, acc: 0.265625]\n",
      "5719: [discriminator loss: 0.491441, acc: 0.765625] [adversarial loss: 1.351895, acc: 0.203125]\n",
      "5720: [discriminator loss: 0.565028, acc: 0.742188] [adversarial loss: 1.119931, acc: 0.328125]\n",
      "5721: [discriminator loss: 0.546254, acc: 0.757812] [adversarial loss: 1.456094, acc: 0.140625]\n",
      "5722: [discriminator loss: 0.458193, acc: 0.789062] [adversarial loss: 1.130297, acc: 0.203125]\n",
      "5723: [discriminator loss: 0.500260, acc: 0.750000] [adversarial loss: 1.012056, acc: 0.265625]\n",
      "5724: [discriminator loss: 0.549810, acc: 0.695312] [adversarial loss: 1.085719, acc: 0.296875]\n",
      "5725: [discriminator loss: 0.530871, acc: 0.742188] [adversarial loss: 1.371051, acc: 0.171875]\n",
      "5726: [discriminator loss: 0.598824, acc: 0.679688] [adversarial loss: 1.248047, acc: 0.171875]\n",
      "5727: [discriminator loss: 0.547366, acc: 0.710938] [adversarial loss: 1.275042, acc: 0.218750]\n",
      "5728: [discriminator loss: 0.552595, acc: 0.710938] [adversarial loss: 1.383170, acc: 0.218750]\n",
      "5729: [discriminator loss: 0.491328, acc: 0.750000] [adversarial loss: 1.285068, acc: 0.125000]\n",
      "5730: [discriminator loss: 0.545855, acc: 0.703125] [adversarial loss: 1.028967, acc: 0.343750]\n",
      "5731: [discriminator loss: 0.565014, acc: 0.742188] [adversarial loss: 1.028016, acc: 0.265625]\n",
      "5732: [discriminator loss: 0.545936, acc: 0.679688] [adversarial loss: 1.030897, acc: 0.296875]\n",
      "5733: [discriminator loss: 0.520616, acc: 0.765625] [adversarial loss: 1.266606, acc: 0.187500]\n",
      "5734: [discriminator loss: 0.525263, acc: 0.679688] [adversarial loss: 0.862457, acc: 0.421875]\n",
      "5735: [discriminator loss: 0.599288, acc: 0.648438] [adversarial loss: 1.366694, acc: 0.140625]\n",
      "5736: [discriminator loss: 0.520191, acc: 0.710938] [adversarial loss: 1.183895, acc: 0.218750]\n",
      "5737: [discriminator loss: 0.558506, acc: 0.687500] [adversarial loss: 1.266944, acc: 0.218750]\n",
      "5738: [discriminator loss: 0.602321, acc: 0.671875] [adversarial loss: 1.134883, acc: 0.265625]\n",
      "5739: [discriminator loss: 0.464144, acc: 0.757812] [adversarial loss: 1.417781, acc: 0.140625]\n",
      "5740: [discriminator loss: 0.486595, acc: 0.750000] [adversarial loss: 0.940374, acc: 0.375000]\n",
      "5741: [discriminator loss: 0.536384, acc: 0.710938] [adversarial loss: 1.337397, acc: 0.171875]\n",
      "5742: [discriminator loss: 0.572381, acc: 0.671875] [adversarial loss: 1.099868, acc: 0.328125]\n",
      "5743: [discriminator loss: 0.544353, acc: 0.734375] [adversarial loss: 1.116547, acc: 0.250000]\n",
      "5744: [discriminator loss: 0.515790, acc: 0.750000] [adversarial loss: 1.240317, acc: 0.203125]\n",
      "5745: [discriminator loss: 0.531758, acc: 0.695312] [adversarial loss: 1.401738, acc: 0.187500]\n",
      "5746: [discriminator loss: 0.482360, acc: 0.773438] [adversarial loss: 1.005390, acc: 0.281250]\n",
      "5747: [discriminator loss: 0.572351, acc: 0.718750] [adversarial loss: 1.205806, acc: 0.265625]\n",
      "5748: [discriminator loss: 0.569660, acc: 0.718750] [adversarial loss: 1.172058, acc: 0.265625]\n",
      "5749: [discriminator loss: 0.498625, acc: 0.750000] [adversarial loss: 1.175559, acc: 0.218750]\n",
      "5750: [discriminator loss: 0.581576, acc: 0.710938] [adversarial loss: 1.093345, acc: 0.265625]\n",
      "5751: [discriminator loss: 0.452825, acc: 0.789062] [adversarial loss: 1.423328, acc: 0.171875]\n",
      "5752: [discriminator loss: 0.533626, acc: 0.757812] [adversarial loss: 1.170501, acc: 0.250000]\n",
      "5753: [discriminator loss: 0.507228, acc: 0.750000] [adversarial loss: 1.117128, acc: 0.265625]\n",
      "5754: [discriminator loss: 0.494946, acc: 0.773438] [adversarial loss: 1.070911, acc: 0.296875]\n",
      "5755: [discriminator loss: 0.483760, acc: 0.742188] [adversarial loss: 1.595238, acc: 0.187500]\n",
      "5756: [discriminator loss: 0.592666, acc: 0.671875] [adversarial loss: 0.911448, acc: 0.406250]\n",
      "5757: [discriminator loss: 0.555648, acc: 0.671875] [adversarial loss: 1.621217, acc: 0.109375]\n",
      "5758: [discriminator loss: 0.515769, acc: 0.726562] [adversarial loss: 0.846482, acc: 0.437500]\n",
      "5759: [discriminator loss: 0.532946, acc: 0.703125] [adversarial loss: 1.411459, acc: 0.218750]\n",
      "5760: [discriminator loss: 0.548360, acc: 0.718750] [adversarial loss: 1.144126, acc: 0.234375]\n",
      "5761: [discriminator loss: 0.623554, acc: 0.687500] [adversarial loss: 1.113959, acc: 0.265625]\n",
      "5762: [discriminator loss: 0.553017, acc: 0.710938] [adversarial loss: 1.129858, acc: 0.281250]\n",
      "5763: [discriminator loss: 0.494549, acc: 0.765625] [adversarial loss: 1.542891, acc: 0.109375]\n",
      "5764: [discriminator loss: 0.523483, acc: 0.726562] [adversarial loss: 1.104773, acc: 0.265625]\n",
      "5765: [discriminator loss: 0.571938, acc: 0.695312] [adversarial loss: 1.667259, acc: 0.078125]\n",
      "5766: [discriminator loss: 0.574371, acc: 0.734375] [adversarial loss: 1.559370, acc: 0.109375]\n",
      "5767: [discriminator loss: 0.506824, acc: 0.750000] [adversarial loss: 0.967572, acc: 0.390625]\n",
      "5768: [discriminator loss: 0.503046, acc: 0.773438] [adversarial loss: 1.295411, acc: 0.203125]\n",
      "5769: [discriminator loss: 0.515702, acc: 0.718750] [adversarial loss: 1.427309, acc: 0.171875]\n",
      "5770: [discriminator loss: 0.504380, acc: 0.726562] [adversarial loss: 1.163785, acc: 0.250000]\n",
      "5771: [discriminator loss: 0.544886, acc: 0.718750] [adversarial loss: 1.577372, acc: 0.031250]\n",
      "5772: [discriminator loss: 0.590352, acc: 0.656250] [adversarial loss: 0.902974, acc: 0.375000]\n",
      "5773: [discriminator loss: 0.529163, acc: 0.742188] [adversarial loss: 1.325397, acc: 0.156250]\n",
      "5774: [discriminator loss: 0.578895, acc: 0.656250] [adversarial loss: 1.157079, acc: 0.203125]\n",
      "5775: [discriminator loss: 0.535605, acc: 0.710938] [adversarial loss: 1.551479, acc: 0.140625]\n",
      "5776: [discriminator loss: 0.540942, acc: 0.726562] [adversarial loss: 1.027866, acc: 0.375000]\n",
      "5777: [discriminator loss: 0.505553, acc: 0.765625] [adversarial loss: 1.474757, acc: 0.171875]\n",
      "5778: [discriminator loss: 0.611328, acc: 0.695312] [adversarial loss: 0.756180, acc: 0.625000]\n",
      "5779: [discriminator loss: 0.493562, acc: 0.765625] [adversarial loss: 1.604804, acc: 0.093750]\n",
      "5780: [discriminator loss: 0.574850, acc: 0.679688] [adversarial loss: 0.734439, acc: 0.562500]\n",
      "5781: [discriminator loss: 0.528345, acc: 0.742188] [adversarial loss: 1.509964, acc: 0.125000]\n",
      "5782: [discriminator loss: 0.514673, acc: 0.765625] [adversarial loss: 1.228301, acc: 0.218750]\n",
      "5783: [discriminator loss: 0.558811, acc: 0.703125] [adversarial loss: 1.357841, acc: 0.234375]\n",
      "5784: [discriminator loss: 0.606818, acc: 0.687500] [adversarial loss: 0.998651, acc: 0.281250]\n",
      "5785: [discriminator loss: 0.539201, acc: 0.718750] [adversarial loss: 1.280454, acc: 0.203125]\n",
      "5786: [discriminator loss: 0.532119, acc: 0.742188] [adversarial loss: 0.973570, acc: 0.312500]\n",
      "5787: [discriminator loss: 0.530138, acc: 0.710938] [adversarial loss: 1.340937, acc: 0.125000]\n",
      "5788: [discriminator loss: 0.447709, acc: 0.789062] [adversarial loss: 1.359067, acc: 0.125000]\n",
      "5789: [discriminator loss: 0.500853, acc: 0.765625] [adversarial loss: 1.048943, acc: 0.312500]\n",
      "5790: [discriminator loss: 0.497186, acc: 0.742188] [adversarial loss: 1.280759, acc: 0.265625]\n",
      "5791: [discriminator loss: 0.565135, acc: 0.671875] [adversarial loss: 1.568002, acc: 0.125000]\n",
      "5792: [discriminator loss: 0.546054, acc: 0.742188] [adversarial loss: 0.974936, acc: 0.375000]\n",
      "5793: [discriminator loss: 0.518812, acc: 0.765625] [adversarial loss: 1.324802, acc: 0.156250]\n",
      "5794: [discriminator loss: 0.490979, acc: 0.765625] [adversarial loss: 1.321882, acc: 0.156250]\n",
      "5795: [discriminator loss: 0.542790, acc: 0.742188] [adversarial loss: 1.160612, acc: 0.296875]\n",
      "5796: [discriminator loss: 0.545183, acc: 0.695312] [adversarial loss: 1.478719, acc: 0.125000]\n",
      "5797: [discriminator loss: 0.501294, acc: 0.726562] [adversarial loss: 1.043659, acc: 0.343750]\n",
      "5798: [discriminator loss: 0.565721, acc: 0.695312] [adversarial loss: 1.679860, acc: 0.140625]\n",
      "5799: [discriminator loss: 0.579730, acc: 0.695312] [adversarial loss: 1.435700, acc: 0.218750]\n",
      "5800: [discriminator loss: 0.582724, acc: 0.734375] [adversarial loss: 1.260308, acc: 0.203125]\n",
      "5801: [discriminator loss: 0.538699, acc: 0.687500] [adversarial loss: 1.172912, acc: 0.250000]\n",
      "5802: [discriminator loss: 0.558158, acc: 0.671875] [adversarial loss: 1.667251, acc: 0.031250]\n",
      "5803: [discriminator loss: 0.513018, acc: 0.703125] [adversarial loss: 1.014927, acc: 0.312500]\n",
      "5804: [discriminator loss: 0.518254, acc: 0.765625] [adversarial loss: 1.679430, acc: 0.187500]\n",
      "5805: [discriminator loss: 0.467138, acc: 0.804688] [adversarial loss: 1.357759, acc: 0.140625]\n",
      "5806: [discriminator loss: 0.502507, acc: 0.765625] [adversarial loss: 1.196086, acc: 0.265625]\n",
      "5807: [discriminator loss: 0.494679, acc: 0.742188] [adversarial loss: 1.111276, acc: 0.312500]\n",
      "5808: [discriminator loss: 0.525931, acc: 0.710938] [adversarial loss: 1.380323, acc: 0.203125]\n",
      "5809: [discriminator loss: 0.495060, acc: 0.750000] [adversarial loss: 1.159416, acc: 0.250000]\n",
      "5810: [discriminator loss: 0.542520, acc: 0.695312] [adversarial loss: 1.325200, acc: 0.140625]\n",
      "5811: [discriminator loss: 0.615027, acc: 0.679688] [adversarial loss: 1.052726, acc: 0.312500]\n",
      "5812: [discriminator loss: 0.575310, acc: 0.687500] [adversarial loss: 1.204281, acc: 0.234375]\n",
      "5813: [discriminator loss: 0.460551, acc: 0.781250] [adversarial loss: 1.047047, acc: 0.234375]\n",
      "5814: [discriminator loss: 0.543635, acc: 0.687500] [adversarial loss: 1.572680, acc: 0.078125]\n",
      "5815: [discriminator loss: 0.473397, acc: 0.796875] [adversarial loss: 0.943701, acc: 0.406250]\n",
      "5816: [discriminator loss: 0.593872, acc: 0.718750] [adversarial loss: 1.553756, acc: 0.093750]\n",
      "5817: [discriminator loss: 0.490364, acc: 0.710938] [adversarial loss: 1.045634, acc: 0.281250]\n",
      "5818: [discriminator loss: 0.530790, acc: 0.710938] [adversarial loss: 1.600756, acc: 0.156250]\n",
      "5819: [discriminator loss: 0.496454, acc: 0.726562] [adversarial loss: 1.050305, acc: 0.343750]\n",
      "5820: [discriminator loss: 0.577167, acc: 0.703125] [adversarial loss: 1.556501, acc: 0.109375]\n",
      "5821: [discriminator loss: 0.583847, acc: 0.726562] [adversarial loss: 1.008662, acc: 0.343750]\n",
      "5822: [discriminator loss: 0.529646, acc: 0.734375] [adversarial loss: 1.459337, acc: 0.156250]\n",
      "5823: [discriminator loss: 0.603329, acc: 0.695312] [adversarial loss: 1.056731, acc: 0.281250]\n",
      "5824: [discriminator loss: 0.555775, acc: 0.734375] [adversarial loss: 1.336768, acc: 0.125000]\n",
      "5825: [discriminator loss: 0.479473, acc: 0.796875] [adversarial loss: 1.275873, acc: 0.250000]\n",
      "5826: [discriminator loss: 0.531330, acc: 0.710938] [adversarial loss: 1.132066, acc: 0.312500]\n",
      "5827: [discriminator loss: 0.538889, acc: 0.718750] [adversarial loss: 1.318563, acc: 0.187500]\n",
      "5828: [discriminator loss: 0.489568, acc: 0.796875] [adversarial loss: 1.408861, acc: 0.125000]\n",
      "5829: [discriminator loss: 0.500794, acc: 0.726562] [adversarial loss: 1.271174, acc: 0.203125]\n",
      "5830: [discriminator loss: 0.574237, acc: 0.734375] [adversarial loss: 1.422553, acc: 0.156250]\n",
      "5831: [discriminator loss: 0.634303, acc: 0.640625] [adversarial loss: 1.043940, acc: 0.359375]\n",
      "5832: [discriminator loss: 0.593794, acc: 0.656250] [adversarial loss: 1.730955, acc: 0.078125]\n",
      "5833: [discriminator loss: 0.635948, acc: 0.664062] [adversarial loss: 1.080923, acc: 0.265625]\n",
      "5834: [discriminator loss: 0.520421, acc: 0.742188] [adversarial loss: 1.461992, acc: 0.109375]\n",
      "5835: [discriminator loss: 0.449567, acc: 0.781250] [adversarial loss: 1.126406, acc: 0.265625]\n",
      "5836: [discriminator loss: 0.527803, acc: 0.687500] [adversarial loss: 1.312450, acc: 0.218750]\n",
      "5837: [discriminator loss: 0.539818, acc: 0.695312] [adversarial loss: 1.114761, acc: 0.265625]\n",
      "5838: [discriminator loss: 0.530692, acc: 0.750000] [adversarial loss: 1.348433, acc: 0.203125]\n",
      "5839: [discriminator loss: 0.549864, acc: 0.687500] [adversarial loss: 1.358436, acc: 0.156250]\n",
      "5840: [discriminator loss: 0.442495, acc: 0.796875] [adversarial loss: 1.460011, acc: 0.156250]\n",
      "5841: [discriminator loss: 0.503444, acc: 0.718750] [adversarial loss: 1.418838, acc: 0.218750]\n",
      "5842: [discriminator loss: 0.596907, acc: 0.679688] [adversarial loss: 0.974259, acc: 0.359375]\n",
      "5843: [discriminator loss: 0.474356, acc: 0.765625] [adversarial loss: 1.781111, acc: 0.062500]\n",
      "5844: [discriminator loss: 0.548117, acc: 0.703125] [adversarial loss: 0.929564, acc: 0.421875]\n",
      "5845: [discriminator loss: 0.579731, acc: 0.679688] [adversarial loss: 1.522949, acc: 0.078125]\n",
      "5846: [discriminator loss: 0.531227, acc: 0.757812] [adversarial loss: 1.051117, acc: 0.250000]\n",
      "5847: [discriminator loss: 0.545738, acc: 0.695312] [adversarial loss: 1.543751, acc: 0.109375]\n",
      "5848: [discriminator loss: 0.536271, acc: 0.726562] [adversarial loss: 0.893449, acc: 0.437500]\n",
      "5849: [discriminator loss: 0.559415, acc: 0.773438] [adversarial loss: 1.436512, acc: 0.171875]\n",
      "5850: [discriminator loss: 0.560008, acc: 0.726562] [adversarial loss: 1.320306, acc: 0.218750]\n",
      "5851: [discriminator loss: 0.502485, acc: 0.726562] [adversarial loss: 1.195142, acc: 0.234375]\n",
      "5852: [discriminator loss: 0.510892, acc: 0.757812] [adversarial loss: 1.306288, acc: 0.203125]\n",
      "5853: [discriminator loss: 0.462778, acc: 0.812500] [adversarial loss: 1.301096, acc: 0.250000]\n",
      "5854: [discriminator loss: 0.569922, acc: 0.718750] [adversarial loss: 1.416295, acc: 0.156250]\n",
      "5855: [discriminator loss: 0.486376, acc: 0.718750] [adversarial loss: 1.267801, acc: 0.171875]\n",
      "5856: [discriminator loss: 0.451668, acc: 0.804688] [adversarial loss: 1.029995, acc: 0.343750]\n",
      "5857: [discriminator loss: 0.635471, acc: 0.648438] [adversarial loss: 1.613301, acc: 0.156250]\n",
      "5858: [discriminator loss: 0.534052, acc: 0.718750] [adversarial loss: 0.871748, acc: 0.343750]\n",
      "5859: [discriminator loss: 0.538066, acc: 0.695312] [adversarial loss: 1.134644, acc: 0.265625]\n",
      "5860: [discriminator loss: 0.521515, acc: 0.750000] [adversarial loss: 1.128277, acc: 0.296875]\n",
      "5861: [discriminator loss: 0.518991, acc: 0.726562] [adversarial loss: 1.228901, acc: 0.218750]\n",
      "5862: [discriminator loss: 0.511487, acc: 0.742188] [adversarial loss: 0.924893, acc: 0.390625]\n",
      "5863: [discriminator loss: 0.545259, acc: 0.773438] [adversarial loss: 1.615380, acc: 0.125000]\n",
      "5864: [discriminator loss: 0.570420, acc: 0.679688] [adversarial loss: 0.934369, acc: 0.406250]\n",
      "5865: [discriminator loss: 0.466278, acc: 0.765625] [adversarial loss: 1.547256, acc: 0.140625]\n",
      "5866: [discriminator loss: 0.542108, acc: 0.710938] [adversarial loss: 1.083309, acc: 0.296875]\n",
      "5867: [discriminator loss: 0.516289, acc: 0.750000] [adversarial loss: 1.182601, acc: 0.203125]\n",
      "5868: [discriminator loss: 0.506038, acc: 0.773438] [adversarial loss: 1.266611, acc: 0.234375]\n",
      "5869: [discriminator loss: 0.583711, acc: 0.664062] [adversarial loss: 1.120170, acc: 0.328125]\n",
      "5870: [discriminator loss: 0.536650, acc: 0.726562] [adversarial loss: 1.394625, acc: 0.171875]\n",
      "5871: [discriminator loss: 0.561976, acc: 0.718750] [adversarial loss: 1.017575, acc: 0.312500]\n",
      "5872: [discriminator loss: 0.587036, acc: 0.687500] [adversarial loss: 1.613409, acc: 0.171875]\n",
      "5873: [discriminator loss: 0.556711, acc: 0.671875] [adversarial loss: 0.943559, acc: 0.390625]\n",
      "5874: [discriminator loss: 0.534229, acc: 0.695312] [adversarial loss: 1.422534, acc: 0.281250]\n",
      "5875: [discriminator loss: 0.552007, acc: 0.726562] [adversarial loss: 1.026467, acc: 0.359375]\n",
      "5876: [discriminator loss: 0.526548, acc: 0.734375] [adversarial loss: 1.327016, acc: 0.187500]\n",
      "5877: [discriminator loss: 0.573756, acc: 0.679688] [adversarial loss: 1.535810, acc: 0.093750]\n",
      "5878: [discriminator loss: 0.560798, acc: 0.679688] [adversarial loss: 1.097620, acc: 0.265625]\n",
      "5879: [discriminator loss: 0.546527, acc: 0.679688] [adversarial loss: 1.255768, acc: 0.218750]\n",
      "5880: [discriminator loss: 0.576614, acc: 0.695312] [adversarial loss: 1.165800, acc: 0.265625]\n",
      "5881: [discriminator loss: 0.473376, acc: 0.781250] [adversarial loss: 1.090210, acc: 0.312500]\n",
      "5882: [discriminator loss: 0.535979, acc: 0.695312] [adversarial loss: 1.279092, acc: 0.203125]\n",
      "5883: [discriminator loss: 0.498744, acc: 0.765625] [adversarial loss: 1.007035, acc: 0.312500]\n",
      "5884: [discriminator loss: 0.497593, acc: 0.750000] [adversarial loss: 1.377458, acc: 0.171875]\n",
      "5885: [discriminator loss: 0.588669, acc: 0.695312] [adversarial loss: 1.012618, acc: 0.359375]\n",
      "5886: [discriminator loss: 0.535947, acc: 0.710938] [adversarial loss: 1.646488, acc: 0.140625]\n",
      "5887: [discriminator loss: 0.558691, acc: 0.640625] [adversarial loss: 0.957739, acc: 0.328125]\n",
      "5888: [discriminator loss: 0.530395, acc: 0.742188] [adversarial loss: 1.292062, acc: 0.218750]\n",
      "5889: [discriminator loss: 0.577387, acc: 0.679688] [adversarial loss: 1.218357, acc: 0.171875]\n",
      "5890: [discriminator loss: 0.481075, acc: 0.796875] [adversarial loss: 1.457267, acc: 0.093750]\n",
      "5891: [discriminator loss: 0.551082, acc: 0.710938] [adversarial loss: 1.239486, acc: 0.187500]\n",
      "5892: [discriminator loss: 0.528913, acc: 0.710938] [adversarial loss: 1.340439, acc: 0.156250]\n",
      "5893: [discriminator loss: 0.470010, acc: 0.781250] [adversarial loss: 0.999566, acc: 0.265625]\n",
      "5894: [discriminator loss: 0.584884, acc: 0.671875] [adversarial loss: 1.422710, acc: 0.171875]\n",
      "5895: [discriminator loss: 0.500436, acc: 0.757812] [adversarial loss: 0.971302, acc: 0.343750]\n",
      "5896: [discriminator loss: 0.484112, acc: 0.765625] [adversarial loss: 1.430755, acc: 0.140625]\n",
      "5897: [discriminator loss: 0.517406, acc: 0.703125] [adversarial loss: 0.942477, acc: 0.328125]\n",
      "5898: [discriminator loss: 0.524029, acc: 0.734375] [adversarial loss: 1.791094, acc: 0.109375]\n",
      "5899: [discriminator loss: 0.565289, acc: 0.734375] [adversarial loss: 0.905629, acc: 0.343750]\n",
      "5900: [discriminator loss: 0.517077, acc: 0.750000] [adversarial loss: 1.432451, acc: 0.109375]\n",
      "5901: [discriminator loss: 0.547376, acc: 0.734375] [adversarial loss: 0.876783, acc: 0.375000]\n",
      "5902: [discriminator loss: 0.524499, acc: 0.718750] [adversarial loss: 1.267668, acc: 0.140625]\n",
      "5903: [discriminator loss: 0.532370, acc: 0.710938] [adversarial loss: 0.973442, acc: 0.328125]\n",
      "5904: [discriminator loss: 0.494146, acc: 0.710938] [adversarial loss: 1.560705, acc: 0.140625]\n",
      "5905: [discriminator loss: 0.505433, acc: 0.750000] [adversarial loss: 1.162072, acc: 0.187500]\n",
      "5906: [discriminator loss: 0.490333, acc: 0.750000] [adversarial loss: 1.256359, acc: 0.203125]\n",
      "5907: [discriminator loss: 0.549877, acc: 0.726562] [adversarial loss: 1.270742, acc: 0.125000]\n",
      "5908: [discriminator loss: 0.538826, acc: 0.726562] [adversarial loss: 1.405384, acc: 0.125000]\n",
      "5909: [discriminator loss: 0.491436, acc: 0.781250] [adversarial loss: 1.277890, acc: 0.203125]\n",
      "5910: [discriminator loss: 0.489331, acc: 0.757812] [adversarial loss: 1.136603, acc: 0.250000]\n",
      "5911: [discriminator loss: 0.511522, acc: 0.718750] [adversarial loss: 1.577295, acc: 0.093750]\n",
      "5912: [discriminator loss: 0.532341, acc: 0.742188] [adversarial loss: 1.130012, acc: 0.265625]\n",
      "5913: [discriminator loss: 0.449612, acc: 0.796875] [adversarial loss: 1.528883, acc: 0.140625]\n",
      "5914: [discriminator loss: 0.507720, acc: 0.750000] [adversarial loss: 1.126900, acc: 0.250000]\n",
      "5915: [discriminator loss: 0.630830, acc: 0.671875] [adversarial loss: 1.105208, acc: 0.296875]\n",
      "5916: [discriminator loss: 0.582181, acc: 0.671875] [adversarial loss: 1.444902, acc: 0.125000]\n",
      "5917: [discriminator loss: 0.546726, acc: 0.687500] [adversarial loss: 1.209781, acc: 0.234375]\n",
      "5918: [discriminator loss: 0.475187, acc: 0.804688] [adversarial loss: 1.491193, acc: 0.156250]\n",
      "5919: [discriminator loss: 0.441351, acc: 0.820312] [adversarial loss: 0.937271, acc: 0.312500]\n",
      "5920: [discriminator loss: 0.560363, acc: 0.695312] [adversarial loss: 1.325505, acc: 0.156250]\n",
      "5921: [discriminator loss: 0.517610, acc: 0.718750] [adversarial loss: 0.918519, acc: 0.437500]\n",
      "5922: [discriminator loss: 0.633267, acc: 0.664062] [adversarial loss: 1.639573, acc: 0.046875]\n",
      "5923: [discriminator loss: 0.501731, acc: 0.710938] [adversarial loss: 1.270576, acc: 0.140625]\n",
      "5924: [discriminator loss: 0.545052, acc: 0.757812] [adversarial loss: 1.293670, acc: 0.234375]\n",
      "5925: [discriminator loss: 0.539694, acc: 0.750000] [adversarial loss: 1.448889, acc: 0.171875]\n",
      "5926: [discriminator loss: 0.530806, acc: 0.718750] [adversarial loss: 0.975248, acc: 0.359375]\n",
      "5927: [discriminator loss: 0.553877, acc: 0.718750] [adversarial loss: 1.636644, acc: 0.093750]\n",
      "5928: [discriminator loss: 0.556453, acc: 0.679688] [adversarial loss: 0.924671, acc: 0.437500]\n",
      "5929: [discriminator loss: 0.588808, acc: 0.593750] [adversarial loss: 1.671091, acc: 0.109375]\n",
      "5930: [discriminator loss: 0.556717, acc: 0.703125] [adversarial loss: 0.909420, acc: 0.406250]\n",
      "5931: [discriminator loss: 0.543381, acc: 0.757812] [adversarial loss: 1.472586, acc: 0.109375]\n",
      "5932: [discriminator loss: 0.444922, acc: 0.773438] [adversarial loss: 1.223055, acc: 0.203125]\n",
      "5933: [discriminator loss: 0.538206, acc: 0.750000] [adversarial loss: 1.282800, acc: 0.187500]\n",
      "5934: [discriminator loss: 0.476833, acc: 0.812500] [adversarial loss: 1.433570, acc: 0.093750]\n",
      "5935: [discriminator loss: 0.561424, acc: 0.734375] [adversarial loss: 0.956411, acc: 0.343750]\n",
      "5936: [discriminator loss: 0.597629, acc: 0.703125] [adversarial loss: 1.673399, acc: 0.125000]\n",
      "5937: [discriminator loss: 0.603369, acc: 0.656250] [adversarial loss: 1.068327, acc: 0.328125]\n",
      "5938: [discriminator loss: 0.525507, acc: 0.710938] [adversarial loss: 1.376815, acc: 0.109375]\n",
      "5939: [discriminator loss: 0.584774, acc: 0.664062] [adversarial loss: 1.206885, acc: 0.250000]\n",
      "5940: [discriminator loss: 0.573500, acc: 0.718750] [adversarial loss: 1.207547, acc: 0.218750]\n",
      "5941: [discriminator loss: 0.547180, acc: 0.726562] [adversarial loss: 1.154377, acc: 0.281250]\n",
      "5942: [discriminator loss: 0.513128, acc: 0.726562] [adversarial loss: 1.139649, acc: 0.218750]\n",
      "5943: [discriminator loss: 0.515287, acc: 0.765625] [adversarial loss: 1.209012, acc: 0.234375]\n",
      "5944: [discriminator loss: 0.562646, acc: 0.726562] [adversarial loss: 1.120045, acc: 0.296875]\n",
      "5945: [discriminator loss: 0.534465, acc: 0.703125] [adversarial loss: 1.112743, acc: 0.296875]\n",
      "5946: [discriminator loss: 0.491100, acc: 0.773438] [adversarial loss: 1.367129, acc: 0.203125]\n",
      "5947: [discriminator loss: 0.527554, acc: 0.742188] [adversarial loss: 1.184594, acc: 0.234375]\n",
      "5948: [discriminator loss: 0.645667, acc: 0.648438] [adversarial loss: 1.004256, acc: 0.406250]\n",
      "5949: [discriminator loss: 0.498187, acc: 0.765625] [adversarial loss: 1.683942, acc: 0.062500]\n",
      "5950: [discriminator loss: 0.560658, acc: 0.679688] [adversarial loss: 0.805512, acc: 0.421875]\n",
      "5951: [discriminator loss: 0.499276, acc: 0.757812] [adversarial loss: 1.455684, acc: 0.109375]\n",
      "5952: [discriminator loss: 0.526672, acc: 0.718750] [adversarial loss: 0.840475, acc: 0.500000]\n",
      "5953: [discriminator loss: 0.466679, acc: 0.742188] [adversarial loss: 1.385087, acc: 0.203125]\n",
      "5954: [discriminator loss: 0.431193, acc: 0.812500] [adversarial loss: 1.139084, acc: 0.203125]\n",
      "5955: [discriminator loss: 0.483458, acc: 0.804688] [adversarial loss: 1.184991, acc: 0.203125]\n",
      "5956: [discriminator loss: 0.479357, acc: 0.757812] [adversarial loss: 1.288156, acc: 0.218750]\n",
      "5957: [discriminator loss: 0.524436, acc: 0.734375] [adversarial loss: 1.120667, acc: 0.250000]\n",
      "5958: [discriminator loss: 0.500016, acc: 0.757812] [adversarial loss: 1.330777, acc: 0.140625]\n",
      "5959: [discriminator loss: 0.602361, acc: 0.671875] [adversarial loss: 0.751355, acc: 0.578125]\n",
      "5960: [discriminator loss: 0.636995, acc: 0.656250] [adversarial loss: 1.985967, acc: 0.062500]\n",
      "5961: [discriminator loss: 0.594617, acc: 0.695312] [adversarial loss: 0.833908, acc: 0.500000]\n",
      "5962: [discriminator loss: 0.544361, acc: 0.695312] [adversarial loss: 1.452366, acc: 0.125000]\n",
      "5963: [discriminator loss: 0.544313, acc: 0.734375] [adversarial loss: 0.851374, acc: 0.421875]\n",
      "5964: [discriminator loss: 0.549157, acc: 0.710938] [adversarial loss: 1.429470, acc: 0.109375]\n",
      "5965: [discriminator loss: 0.495198, acc: 0.718750] [adversarial loss: 1.112642, acc: 0.296875]\n",
      "5966: [discriminator loss: 0.582696, acc: 0.726562] [adversarial loss: 1.073059, acc: 0.328125]\n",
      "5967: [discriminator loss: 0.519060, acc: 0.734375] [adversarial loss: 1.137411, acc: 0.250000]\n",
      "5968: [discriminator loss: 0.539673, acc: 0.679688] [adversarial loss: 1.289235, acc: 0.109375]\n",
      "5969: [discriminator loss: 0.530482, acc: 0.679688] [adversarial loss: 1.315424, acc: 0.171875]\n",
      "5970: [discriminator loss: 0.475694, acc: 0.812500] [adversarial loss: 1.192685, acc: 0.250000]\n",
      "5971: [discriminator loss: 0.509617, acc: 0.734375] [adversarial loss: 1.078075, acc: 0.312500]\n",
      "5972: [discriminator loss: 0.579844, acc: 0.710938] [adversarial loss: 1.265374, acc: 0.187500]\n",
      "5973: [discriminator loss: 0.480696, acc: 0.820312] [adversarial loss: 1.097050, acc: 0.265625]\n",
      "5974: [discriminator loss: 0.473533, acc: 0.789062] [adversarial loss: 1.033327, acc: 0.312500]\n",
      "5975: [discriminator loss: 0.522615, acc: 0.750000] [adversarial loss: 1.613258, acc: 0.093750]\n",
      "5976: [discriminator loss: 0.544680, acc: 0.687500] [adversarial loss: 0.797518, acc: 0.562500]\n",
      "5977: [discriminator loss: 0.616263, acc: 0.726562] [adversarial loss: 1.679355, acc: 0.093750]\n",
      "5978: [discriminator loss: 0.540557, acc: 0.695312] [adversarial loss: 1.071200, acc: 0.421875]\n",
      "5979: [discriminator loss: 0.642475, acc: 0.648438] [adversarial loss: 1.517127, acc: 0.171875]\n",
      "5980: [discriminator loss: 0.605429, acc: 0.679688] [adversarial loss: 1.122443, acc: 0.265625]\n",
      "5981: [discriminator loss: 0.513176, acc: 0.718750] [adversarial loss: 1.322162, acc: 0.125000]\n",
      "5982: [discriminator loss: 0.502421, acc: 0.750000] [adversarial loss: 1.010209, acc: 0.328125]\n",
      "5983: [discriminator loss: 0.525416, acc: 0.742188] [adversarial loss: 1.545852, acc: 0.109375]\n",
      "5984: [discriminator loss: 0.483513, acc: 0.750000] [adversarial loss: 0.973463, acc: 0.375000]\n",
      "5985: [discriminator loss: 0.539602, acc: 0.710938] [adversarial loss: 1.280115, acc: 0.250000]\n",
      "5986: [discriminator loss: 0.494503, acc: 0.750000] [adversarial loss: 1.128397, acc: 0.203125]\n",
      "5987: [discriminator loss: 0.529735, acc: 0.710938] [adversarial loss: 1.308054, acc: 0.171875]\n",
      "5988: [discriminator loss: 0.546060, acc: 0.757812] [adversarial loss: 1.414086, acc: 0.156250]\n",
      "5989: [discriminator loss: 0.522159, acc: 0.742188] [adversarial loss: 0.930761, acc: 0.296875]\n",
      "5990: [discriminator loss: 0.511373, acc: 0.742188] [adversarial loss: 1.442328, acc: 0.140625]\n",
      "5991: [discriminator loss: 0.591112, acc: 0.671875] [adversarial loss: 0.961978, acc: 0.343750]\n",
      "5992: [discriminator loss: 0.591586, acc: 0.710938] [adversarial loss: 1.286929, acc: 0.234375]\n",
      "5993: [discriminator loss: 0.528541, acc: 0.773438] [adversarial loss: 1.232632, acc: 0.218750]\n",
      "5994: [discriminator loss: 0.543511, acc: 0.734375] [adversarial loss: 1.156698, acc: 0.234375]\n",
      "5995: [discriminator loss: 0.556799, acc: 0.734375] [adversarial loss: 1.628882, acc: 0.078125]\n",
      "5996: [discriminator loss: 0.573900, acc: 0.742188] [adversarial loss: 0.980938, acc: 0.328125]\n",
      "5997: [discriminator loss: 0.497936, acc: 0.757812] [adversarial loss: 1.317448, acc: 0.156250]\n",
      "5998: [discriminator loss: 0.453440, acc: 0.804688] [adversarial loss: 0.935374, acc: 0.375000]\n",
      "5999: [discriminator loss: 0.530951, acc: 0.726562] [adversarial loss: 1.424481, acc: 0.140625]\n",
      "6000: [discriminator loss: 0.531276, acc: 0.687500] [adversarial loss: 0.859580, acc: 0.437500]\n",
      "6001: [discriminator loss: 0.540008, acc: 0.734375] [adversarial loss: 1.446433, acc: 0.187500]\n",
      "6002: [discriminator loss: 0.573729, acc: 0.710938] [adversarial loss: 1.253983, acc: 0.187500]\n",
      "6003: [discriminator loss: 0.578045, acc: 0.695312] [adversarial loss: 1.651363, acc: 0.093750]\n",
      "6004: [discriminator loss: 0.580583, acc: 0.695312] [adversarial loss: 1.104991, acc: 0.281250]\n",
      "6005: [discriminator loss: 0.529853, acc: 0.734375] [adversarial loss: 1.432773, acc: 0.218750]\n",
      "6006: [discriminator loss: 0.587073, acc: 0.679688] [adversarial loss: 1.095110, acc: 0.343750]\n",
      "6007: [discriminator loss: 0.528666, acc: 0.742188] [adversarial loss: 0.923639, acc: 0.343750]\n",
      "6008: [discriminator loss: 0.486423, acc: 0.781250] [adversarial loss: 1.345106, acc: 0.203125]\n",
      "6009: [discriminator loss: 0.503225, acc: 0.750000] [adversarial loss: 0.983677, acc: 0.359375]\n",
      "6010: [discriminator loss: 0.523888, acc: 0.734375] [adversarial loss: 1.232199, acc: 0.218750]\n",
      "6011: [discriminator loss: 0.552592, acc: 0.679688] [adversarial loss: 1.147694, acc: 0.250000]\n",
      "6012: [discriminator loss: 0.464240, acc: 0.796875] [adversarial loss: 1.267974, acc: 0.250000]\n",
      "6013: [discriminator loss: 0.511083, acc: 0.750000] [adversarial loss: 1.442965, acc: 0.171875]\n",
      "6014: [discriminator loss: 0.538136, acc: 0.718750] [adversarial loss: 1.192835, acc: 0.218750]\n",
      "6015: [discriminator loss: 0.454328, acc: 0.789062] [adversarial loss: 1.489435, acc: 0.218750]\n",
      "6016: [discriminator loss: 0.562668, acc: 0.679688] [adversarial loss: 0.995538, acc: 0.312500]\n",
      "6017: [discriminator loss: 0.521060, acc: 0.671875] [adversarial loss: 1.528268, acc: 0.093750]\n",
      "6018: [discriminator loss: 0.548306, acc: 0.703125] [adversarial loss: 0.953513, acc: 0.375000]\n",
      "6019: [discriminator loss: 0.467239, acc: 0.757812] [adversarial loss: 1.644146, acc: 0.093750]\n",
      "6020: [discriminator loss: 0.573449, acc: 0.703125] [adversarial loss: 0.805870, acc: 0.484375]\n",
      "6021: [discriminator loss: 0.528846, acc: 0.664062] [adversarial loss: 1.764975, acc: 0.078125]\n",
      "6022: [discriminator loss: 0.590544, acc: 0.687500] [adversarial loss: 0.852019, acc: 0.437500]\n",
      "6023: [discriminator loss: 0.560154, acc: 0.750000] [adversarial loss: 1.359629, acc: 0.187500]\n",
      "6024: [discriminator loss: 0.543392, acc: 0.695312] [adversarial loss: 1.221403, acc: 0.187500]\n",
      "6025: [discriminator loss: 0.502822, acc: 0.757812] [adversarial loss: 1.294669, acc: 0.234375]\n",
      "6026: [discriminator loss: 0.479585, acc: 0.742188] [adversarial loss: 1.176687, acc: 0.328125]\n",
      "6027: [discriminator loss: 0.522392, acc: 0.703125] [adversarial loss: 1.202088, acc: 0.250000]\n",
      "6028: [discriminator loss: 0.569412, acc: 0.726562] [adversarial loss: 1.237104, acc: 0.218750]\n",
      "6029: [discriminator loss: 0.501207, acc: 0.812500] [adversarial loss: 1.286182, acc: 0.187500]\n",
      "6030: [discriminator loss: 0.480612, acc: 0.734375] [adversarial loss: 1.127240, acc: 0.265625]\n",
      "6031: [discriminator loss: 0.571815, acc: 0.664062] [adversarial loss: 1.396015, acc: 0.203125]\n",
      "6032: [discriminator loss: 0.500958, acc: 0.734375] [adversarial loss: 1.157255, acc: 0.265625]\n",
      "6033: [discriminator loss: 0.572495, acc: 0.718750] [adversarial loss: 1.172467, acc: 0.203125]\n",
      "6034: [discriminator loss: 0.527253, acc: 0.703125] [adversarial loss: 1.389944, acc: 0.218750]\n",
      "6035: [discriminator loss: 0.590514, acc: 0.687500] [adversarial loss: 0.934638, acc: 0.390625]\n",
      "6036: [discriminator loss: 0.553093, acc: 0.703125] [adversarial loss: 1.606173, acc: 0.062500]\n",
      "6037: [discriminator loss: 0.524703, acc: 0.742188] [adversarial loss: 0.869106, acc: 0.390625]\n",
      "6038: [discriminator loss: 0.560742, acc: 0.710938] [adversarial loss: 1.556670, acc: 0.125000]\n",
      "6039: [discriminator loss: 0.551153, acc: 0.718750] [adversarial loss: 0.850632, acc: 0.437500]\n",
      "6040: [discriminator loss: 0.590086, acc: 0.664062] [adversarial loss: 1.596366, acc: 0.109375]\n",
      "6041: [discriminator loss: 0.500821, acc: 0.726562] [adversarial loss: 1.169036, acc: 0.312500]\n",
      "6042: [discriminator loss: 0.526729, acc: 0.757812] [adversarial loss: 1.463061, acc: 0.171875]\n",
      "6043: [discriminator loss: 0.521588, acc: 0.757812] [adversarial loss: 1.327536, acc: 0.156250]\n",
      "6044: [discriminator loss: 0.517733, acc: 0.757812] [adversarial loss: 1.196268, acc: 0.250000]\n",
      "6045: [discriminator loss: 0.558247, acc: 0.750000] [adversarial loss: 1.154900, acc: 0.328125]\n",
      "6046: [discriminator loss: 0.511614, acc: 0.734375] [adversarial loss: 1.349642, acc: 0.171875]\n",
      "6047: [discriminator loss: 0.496197, acc: 0.742188] [adversarial loss: 0.748922, acc: 0.500000]\n",
      "6048: [discriminator loss: 0.540184, acc: 0.710938] [adversarial loss: 1.510978, acc: 0.062500]\n",
      "6049: [discriminator loss: 0.497855, acc: 0.750000] [adversarial loss: 1.020012, acc: 0.343750]\n",
      "6050: [discriminator loss: 0.652592, acc: 0.617188] [adversarial loss: 1.339017, acc: 0.140625]\n",
      "6051: [discriminator loss: 0.510514, acc: 0.765625] [adversarial loss: 1.295550, acc: 0.156250]\n",
      "6052: [discriminator loss: 0.629584, acc: 0.664062] [adversarial loss: 1.082581, acc: 0.281250]\n",
      "6053: [discriminator loss: 0.434615, acc: 0.796875] [adversarial loss: 1.013116, acc: 0.250000]\n",
      "6054: [discriminator loss: 0.490458, acc: 0.679688] [adversarial loss: 0.977681, acc: 0.312500]\n",
      "6055: [discriminator loss: 0.512211, acc: 0.757812] [adversarial loss: 1.513971, acc: 0.140625]\n",
      "6056: [discriminator loss: 0.519444, acc: 0.703125] [adversarial loss: 1.029067, acc: 0.312500]\n",
      "6057: [discriminator loss: 0.488423, acc: 0.796875] [adversarial loss: 1.405652, acc: 0.187500]\n",
      "6058: [discriminator loss: 0.394536, acc: 0.812500] [adversarial loss: 1.359693, acc: 0.171875]\n",
      "6059: [discriminator loss: 0.580137, acc: 0.718750] [adversarial loss: 1.209753, acc: 0.218750]\n",
      "6060: [discriminator loss: 0.512495, acc: 0.742188] [adversarial loss: 1.234832, acc: 0.218750]\n",
      "6061: [discriminator loss: 0.529893, acc: 0.726562] [adversarial loss: 1.325776, acc: 0.218750]\n",
      "6062: [discriminator loss: 0.498537, acc: 0.710938] [adversarial loss: 1.167416, acc: 0.250000]\n",
      "6063: [discriminator loss: 0.496906, acc: 0.750000] [adversarial loss: 1.429227, acc: 0.171875]\n",
      "6064: [discriminator loss: 0.493062, acc: 0.781250] [adversarial loss: 1.038040, acc: 0.281250]\n",
      "6065: [discriminator loss: 0.540767, acc: 0.671875] [adversarial loss: 1.168632, acc: 0.203125]\n",
      "6066: [discriminator loss: 0.494284, acc: 0.750000] [adversarial loss: 1.414811, acc: 0.171875]\n",
      "6067: [discriminator loss: 0.476984, acc: 0.757812] [adversarial loss: 1.053917, acc: 0.328125]\n",
      "6068: [discriminator loss: 0.636844, acc: 0.625000] [adversarial loss: 1.422202, acc: 0.109375]\n",
      "6069: [discriminator loss: 0.502015, acc: 0.765625] [adversarial loss: 0.971813, acc: 0.390625]\n",
      "6070: [discriminator loss: 0.546720, acc: 0.703125] [adversarial loss: 1.648519, acc: 0.156250]\n",
      "6071: [discriminator loss: 0.469768, acc: 0.773438] [adversarial loss: 1.096582, acc: 0.281250]\n",
      "6072: [discriminator loss: 0.446059, acc: 0.843750] [adversarial loss: 1.566386, acc: 0.156250]\n",
      "6073: [discriminator loss: 0.602117, acc: 0.656250] [adversarial loss: 1.029974, acc: 0.312500]\n",
      "6074: [discriminator loss: 0.547538, acc: 0.710938] [adversarial loss: 1.429966, acc: 0.109375]\n",
      "6075: [discriminator loss: 0.529417, acc: 0.742188] [adversarial loss: 1.374161, acc: 0.093750]\n",
      "6076: [discriminator loss: 0.548973, acc: 0.750000] [adversarial loss: 1.198238, acc: 0.250000]\n",
      "6077: [discriminator loss: 0.563496, acc: 0.695312] [adversarial loss: 0.936821, acc: 0.468750]\n",
      "6078: [discriminator loss: 0.547133, acc: 0.750000] [adversarial loss: 1.428847, acc: 0.109375]\n",
      "6079: [discriminator loss: 0.586557, acc: 0.671875] [adversarial loss: 1.228225, acc: 0.218750]\n",
      "6080: [discriminator loss: 0.562191, acc: 0.695312] [adversarial loss: 1.105552, acc: 0.265625]\n",
      "6081: [discriminator loss: 0.552229, acc: 0.734375] [adversarial loss: 1.500580, acc: 0.218750]\n",
      "6082: [discriminator loss: 0.500106, acc: 0.765625] [adversarial loss: 0.925561, acc: 0.390625]\n",
      "6083: [discriminator loss: 0.608841, acc: 0.648438] [adversarial loss: 1.351714, acc: 0.187500]\n",
      "6084: [discriminator loss: 0.541146, acc: 0.734375] [adversarial loss: 0.873016, acc: 0.500000]\n",
      "6085: [discriminator loss: 0.524660, acc: 0.742188] [adversarial loss: 1.749651, acc: 0.093750]\n",
      "6086: [discriminator loss: 0.528980, acc: 0.726562] [adversarial loss: 1.068309, acc: 0.265625]\n",
      "6087: [discriminator loss: 0.538396, acc: 0.750000] [adversarial loss: 1.310115, acc: 0.125000]\n",
      "6088: [discriminator loss: 0.481715, acc: 0.734375] [adversarial loss: 0.971061, acc: 0.328125]\n",
      "6089: [discriminator loss: 0.521371, acc: 0.734375] [adversarial loss: 1.357011, acc: 0.125000]\n",
      "6090: [discriminator loss: 0.576142, acc: 0.687500] [adversarial loss: 1.153897, acc: 0.265625]\n",
      "6091: [discriminator loss: 0.602546, acc: 0.695312] [adversarial loss: 1.718113, acc: 0.031250]\n",
      "6092: [discriminator loss: 0.484323, acc: 0.750000] [adversarial loss: 1.065802, acc: 0.343750]\n",
      "6093: [discriminator loss: 0.673214, acc: 0.632812] [adversarial loss: 1.501569, acc: 0.109375]\n",
      "6094: [discriminator loss: 0.546724, acc: 0.671875] [adversarial loss: 0.823056, acc: 0.484375]\n",
      "6095: [discriminator loss: 0.563650, acc: 0.695312] [adversarial loss: 1.441592, acc: 0.171875]\n",
      "6096: [discriminator loss: 0.518149, acc: 0.726562] [adversarial loss: 0.976193, acc: 0.343750]\n",
      "6097: [discriminator loss: 0.565231, acc: 0.742188] [adversarial loss: 1.675842, acc: 0.093750]\n",
      "6098: [discriminator loss: 0.541402, acc: 0.742188] [adversarial loss: 1.005944, acc: 0.375000]\n",
      "6099: [discriminator loss: 0.570819, acc: 0.695312] [adversarial loss: 1.443544, acc: 0.140625]\n",
      "6100: [discriminator loss: 0.556017, acc: 0.695312] [adversarial loss: 1.178257, acc: 0.187500]\n",
      "6101: [discriminator loss: 0.510201, acc: 0.750000] [adversarial loss: 1.179875, acc: 0.296875]\n",
      "6102: [discriminator loss: 0.530962, acc: 0.695312] [adversarial loss: 1.393102, acc: 0.187500]\n",
      "6103: [discriminator loss: 0.554080, acc: 0.687500] [adversarial loss: 1.275739, acc: 0.203125]\n",
      "6104: [discriminator loss: 0.535549, acc: 0.703125] [adversarial loss: 1.022588, acc: 0.343750]\n",
      "6105: [discriminator loss: 0.440671, acc: 0.804688] [adversarial loss: 1.534663, acc: 0.140625]\n",
      "6106: [discriminator loss: 0.537561, acc: 0.734375] [adversarial loss: 0.953808, acc: 0.406250]\n",
      "6107: [discriminator loss: 0.561576, acc: 0.718750] [adversarial loss: 1.476061, acc: 0.156250]\n",
      "6108: [discriminator loss: 0.495402, acc: 0.773438] [adversarial loss: 1.001904, acc: 0.390625]\n",
      "6109: [discriminator loss: 0.571280, acc: 0.726562] [adversarial loss: 1.275054, acc: 0.218750]\n",
      "6110: [discriminator loss: 0.518286, acc: 0.734375] [adversarial loss: 1.048418, acc: 0.343750]\n",
      "6111: [discriminator loss: 0.595504, acc: 0.703125] [adversarial loss: 1.513534, acc: 0.156250]\n",
      "6112: [discriminator loss: 0.558113, acc: 0.726562] [adversarial loss: 1.085920, acc: 0.312500]\n",
      "6113: [discriminator loss: 0.561162, acc: 0.687500] [adversarial loss: 1.210207, acc: 0.296875]\n",
      "6114: [discriminator loss: 0.552276, acc: 0.742188] [adversarial loss: 1.187238, acc: 0.265625]\n",
      "6115: [discriminator loss: 0.526463, acc: 0.781250] [adversarial loss: 1.350044, acc: 0.203125]\n",
      "6116: [discriminator loss: 0.604109, acc: 0.718750] [adversarial loss: 1.042621, acc: 0.343750]\n",
      "6117: [discriminator loss: 0.540454, acc: 0.734375] [adversarial loss: 1.311062, acc: 0.218750]\n",
      "6118: [discriminator loss: 0.517356, acc: 0.757812] [adversarial loss: 1.055836, acc: 0.296875]\n",
      "6119: [discriminator loss: 0.507091, acc: 0.726562] [adversarial loss: 1.420207, acc: 0.250000]\n",
      "6120: [discriminator loss: 0.466230, acc: 0.781250] [adversarial loss: 1.090379, acc: 0.296875]\n",
      "6121: [discriminator loss: 0.536419, acc: 0.734375] [adversarial loss: 1.444202, acc: 0.218750]\n",
      "6122: [discriminator loss: 0.617218, acc: 0.695312] [adversarial loss: 0.839345, acc: 0.531250]\n",
      "6123: [discriminator loss: 0.555131, acc: 0.742188] [adversarial loss: 1.487204, acc: 0.125000]\n",
      "6124: [discriminator loss: 0.520167, acc: 0.718750] [adversarial loss: 1.265923, acc: 0.187500]\n",
      "6125: [discriminator loss: 0.525072, acc: 0.765625] [adversarial loss: 1.391457, acc: 0.109375]\n",
      "6126: [discriminator loss: 0.515688, acc: 0.703125] [adversarial loss: 1.091076, acc: 0.296875]\n",
      "6127: [discriminator loss: 0.489454, acc: 0.710938] [adversarial loss: 1.485137, acc: 0.109375]\n",
      "6128: [discriminator loss: 0.552184, acc: 0.718750] [adversarial loss: 0.696729, acc: 0.593750]\n",
      "6129: [discriminator loss: 0.516173, acc: 0.726562] [adversarial loss: 1.351876, acc: 0.156250]\n",
      "6130: [discriminator loss: 0.481040, acc: 0.773438] [adversarial loss: 1.322717, acc: 0.187500]\n",
      "6131: [discriminator loss: 0.482803, acc: 0.773438] [adversarial loss: 1.315644, acc: 0.203125]\n",
      "6132: [discriminator loss: 0.526708, acc: 0.726562] [adversarial loss: 1.346064, acc: 0.187500]\n",
      "6133: [discriminator loss: 0.573089, acc: 0.703125] [adversarial loss: 1.238589, acc: 0.218750]\n",
      "6134: [discriminator loss: 0.464520, acc: 0.789062] [adversarial loss: 1.230558, acc: 0.125000]\n",
      "6135: [discriminator loss: 0.479064, acc: 0.742188] [adversarial loss: 1.188047, acc: 0.218750]\n",
      "6136: [discriminator loss: 0.549270, acc: 0.710938] [adversarial loss: 1.626303, acc: 0.125000]\n",
      "6137: [discriminator loss: 0.512894, acc: 0.695312] [adversarial loss: 1.039385, acc: 0.281250]\n",
      "6138: [discriminator loss: 0.536369, acc: 0.726562] [adversarial loss: 1.270398, acc: 0.281250]\n",
      "6139: [discriminator loss: 0.526115, acc: 0.710938] [adversarial loss: 1.488980, acc: 0.156250]\n",
      "6140: [discriminator loss: 0.519809, acc: 0.750000] [adversarial loss: 1.237048, acc: 0.296875]\n",
      "6141: [discriminator loss: 0.552511, acc: 0.710938] [adversarial loss: 1.427337, acc: 0.156250]\n",
      "6142: [discriminator loss: 0.559135, acc: 0.726562] [adversarial loss: 1.064192, acc: 0.296875]\n",
      "6143: [discriminator loss: 0.556857, acc: 0.726562] [adversarial loss: 1.486603, acc: 0.093750]\n",
      "6144: [discriminator loss: 0.517357, acc: 0.781250] [adversarial loss: 1.155025, acc: 0.109375]\n",
      "6145: [discriminator loss: 0.462335, acc: 0.804688] [adversarial loss: 1.443585, acc: 0.140625]\n",
      "6146: [discriminator loss: 0.523353, acc: 0.742188] [adversarial loss: 0.956885, acc: 0.390625]\n",
      "6147: [discriminator loss: 0.524656, acc: 0.742188] [adversarial loss: 1.443421, acc: 0.140625]\n",
      "6148: [discriminator loss: 0.589905, acc: 0.703125] [adversarial loss: 1.054936, acc: 0.296875]\n",
      "6149: [discriminator loss: 0.564324, acc: 0.703125] [adversarial loss: 1.293869, acc: 0.156250]\n",
      "6150: [discriminator loss: 0.507568, acc: 0.781250] [adversarial loss: 1.730408, acc: 0.125000]\n",
      "6151: [discriminator loss: 0.559224, acc: 0.710938] [adversarial loss: 0.951276, acc: 0.375000]\n",
      "6152: [discriminator loss: 0.568568, acc: 0.656250] [adversarial loss: 1.605323, acc: 0.078125]\n",
      "6153: [discriminator loss: 0.542014, acc: 0.687500] [adversarial loss: 0.828012, acc: 0.453125]\n",
      "6154: [discriminator loss: 0.538815, acc: 0.703125] [adversarial loss: 1.277392, acc: 0.234375]\n",
      "6155: [discriminator loss: 0.482726, acc: 0.757812] [adversarial loss: 1.055405, acc: 0.343750]\n",
      "6156: [discriminator loss: 0.554662, acc: 0.718750] [adversarial loss: 1.268015, acc: 0.171875]\n",
      "6157: [discriminator loss: 0.457380, acc: 0.789062] [adversarial loss: 1.237476, acc: 0.281250]\n",
      "6158: [discriminator loss: 0.572391, acc: 0.734375] [adversarial loss: 1.303973, acc: 0.187500]\n",
      "6159: [discriminator loss: 0.484483, acc: 0.765625] [adversarial loss: 1.032112, acc: 0.328125]\n",
      "6160: [discriminator loss: 0.465850, acc: 0.773438] [adversarial loss: 1.469118, acc: 0.156250]\n",
      "6161: [discriminator loss: 0.636317, acc: 0.656250] [adversarial loss: 1.272009, acc: 0.171875]\n",
      "6162: [discriminator loss: 0.501653, acc: 0.750000] [adversarial loss: 1.055080, acc: 0.359375]\n",
      "6163: [discriminator loss: 0.583795, acc: 0.718750] [adversarial loss: 1.578572, acc: 0.187500]\n",
      "6164: [discriminator loss: 0.581165, acc: 0.710938] [adversarial loss: 0.892619, acc: 0.390625]\n",
      "6165: [discriminator loss: 0.515692, acc: 0.726562] [adversarial loss: 1.406025, acc: 0.171875]\n",
      "6166: [discriminator loss: 0.590891, acc: 0.687500] [adversarial loss: 0.995164, acc: 0.359375]\n",
      "6167: [discriminator loss: 0.599618, acc: 0.687500] [adversarial loss: 1.732641, acc: 0.062500]\n",
      "6168: [discriminator loss: 0.471046, acc: 0.757812] [adversarial loss: 1.189981, acc: 0.218750]\n",
      "6169: [discriminator loss: 0.538296, acc: 0.710938] [adversarial loss: 1.254122, acc: 0.218750]\n",
      "6170: [discriminator loss: 0.564098, acc: 0.695312] [adversarial loss: 1.416441, acc: 0.156250]\n",
      "6171: [discriminator loss: 0.560953, acc: 0.734375] [adversarial loss: 0.973172, acc: 0.343750]\n",
      "6172: [discriminator loss: 0.575218, acc: 0.718750] [adversarial loss: 1.585678, acc: 0.093750]\n",
      "6173: [discriminator loss: 0.563973, acc: 0.726562] [adversarial loss: 1.168784, acc: 0.234375]\n",
      "6174: [discriminator loss: 0.507040, acc: 0.750000] [adversarial loss: 1.388124, acc: 0.187500]\n",
      "6175: [discriminator loss: 0.545588, acc: 0.742188] [adversarial loss: 0.961729, acc: 0.328125]\n",
      "6176: [discriminator loss: 0.588775, acc: 0.656250] [adversarial loss: 1.658684, acc: 0.125000]\n",
      "6177: [discriminator loss: 0.536055, acc: 0.656250] [adversarial loss: 1.091942, acc: 0.312500]\n",
      "6178: [discriminator loss: 0.563922, acc: 0.703125] [adversarial loss: 1.162099, acc: 0.187500]\n",
      "6179: [discriminator loss: 0.467900, acc: 0.750000] [adversarial loss: 1.293483, acc: 0.218750]\n",
      "6180: [discriminator loss: 0.542861, acc: 0.695312] [adversarial loss: 1.495023, acc: 0.109375]\n",
      "6181: [discriminator loss: 0.556245, acc: 0.710938] [adversarial loss: 1.091620, acc: 0.281250]\n",
      "6182: [discriminator loss: 0.526730, acc: 0.750000] [adversarial loss: 1.367100, acc: 0.203125]\n",
      "6183: [discriminator loss: 0.491469, acc: 0.742188] [adversarial loss: 1.147874, acc: 0.250000]\n",
      "6184: [discriminator loss: 0.559411, acc: 0.687500] [adversarial loss: 1.352021, acc: 0.203125]\n",
      "6185: [discriminator loss: 0.517003, acc: 0.750000] [adversarial loss: 0.885717, acc: 0.484375]\n",
      "6186: [discriminator loss: 0.573983, acc: 0.664062] [adversarial loss: 1.156222, acc: 0.265625]\n",
      "6187: [discriminator loss: 0.519245, acc: 0.750000] [adversarial loss: 1.140011, acc: 0.281250]\n",
      "6188: [discriminator loss: 0.489343, acc: 0.765625] [adversarial loss: 1.059411, acc: 0.250000]\n",
      "6189: [discriminator loss: 0.535535, acc: 0.734375] [adversarial loss: 1.180823, acc: 0.250000]\n",
      "6190: [discriminator loss: 0.548275, acc: 0.750000] [adversarial loss: 1.177605, acc: 0.109375]\n",
      "6191: [discriminator loss: 0.519621, acc: 0.726562] [adversarial loss: 0.900529, acc: 0.359375]\n",
      "6192: [discriminator loss: 0.504582, acc: 0.757812] [adversarial loss: 1.664945, acc: 0.125000]\n",
      "6193: [discriminator loss: 0.500197, acc: 0.734375] [adversarial loss: 0.835637, acc: 0.500000]\n",
      "6194: [discriminator loss: 0.631564, acc: 0.640625] [adversarial loss: 1.904280, acc: 0.062500]\n",
      "6195: [discriminator loss: 0.527740, acc: 0.710938] [adversarial loss: 0.970089, acc: 0.375000]\n",
      "6196: [discriminator loss: 0.559159, acc: 0.703125] [adversarial loss: 1.509365, acc: 0.078125]\n",
      "6197: [discriminator loss: 0.592615, acc: 0.671875] [adversarial loss: 0.878238, acc: 0.468750]\n",
      "6198: [discriminator loss: 0.561403, acc: 0.703125] [adversarial loss: 1.377351, acc: 0.109375]\n",
      "6199: [discriminator loss: 0.529016, acc: 0.726562] [adversarial loss: 1.235642, acc: 0.156250]\n",
      "6200: [discriminator loss: 0.505158, acc: 0.789062] [adversarial loss: 1.051506, acc: 0.359375]\n",
      "6201: [discriminator loss: 0.528114, acc: 0.757812] [adversarial loss: 1.340775, acc: 0.156250]\n",
      "6202: [discriminator loss: 0.540828, acc: 0.695312] [adversarial loss: 1.112638, acc: 0.281250]\n",
      "6203: [discriminator loss: 0.507200, acc: 0.734375] [adversarial loss: 1.414448, acc: 0.156250]\n",
      "6204: [discriminator loss: 0.556798, acc: 0.734375] [adversarial loss: 1.033236, acc: 0.343750]\n",
      "6205: [discriminator loss: 0.527318, acc: 0.742188] [adversarial loss: 1.218495, acc: 0.250000]\n",
      "6206: [discriminator loss: 0.500409, acc: 0.750000] [adversarial loss: 1.192638, acc: 0.250000]\n",
      "6207: [discriminator loss: 0.527344, acc: 0.726562] [adversarial loss: 1.535888, acc: 0.046875]\n",
      "6208: [discriminator loss: 0.595908, acc: 0.640625] [adversarial loss: 0.965856, acc: 0.296875]\n",
      "6209: [discriminator loss: 0.551375, acc: 0.703125] [adversarial loss: 1.298720, acc: 0.234375]\n",
      "6210: [discriminator loss: 0.602969, acc: 0.679688] [adversarial loss: 1.223667, acc: 0.203125]\n",
      "6211: [discriminator loss: 0.495725, acc: 0.781250] [adversarial loss: 1.218401, acc: 0.187500]\n",
      "6212: [discriminator loss: 0.540682, acc: 0.695312] [adversarial loss: 1.157974, acc: 0.328125]\n",
      "6213: [discriminator loss: 0.468358, acc: 0.765625] [adversarial loss: 1.208213, acc: 0.203125]\n",
      "6214: [discriminator loss: 0.603587, acc: 0.640625] [adversarial loss: 1.329409, acc: 0.093750]\n",
      "6215: [discriminator loss: 0.479037, acc: 0.781250] [adversarial loss: 0.953364, acc: 0.359375]\n",
      "6216: [discriminator loss: 0.435582, acc: 0.804688] [adversarial loss: 1.352558, acc: 0.125000]\n",
      "6217: [discriminator loss: 0.567812, acc: 0.695312] [adversarial loss: 1.284378, acc: 0.187500]\n",
      "6218: [discriminator loss: 0.553979, acc: 0.679688] [adversarial loss: 0.902949, acc: 0.437500]\n",
      "6219: [discriminator loss: 0.602788, acc: 0.671875] [adversarial loss: 1.713447, acc: 0.140625]\n",
      "6220: [discriminator loss: 0.654521, acc: 0.656250] [adversarial loss: 0.854267, acc: 0.468750]\n",
      "6221: [discriminator loss: 0.562260, acc: 0.718750] [adversarial loss: 1.566717, acc: 0.125000]\n",
      "6222: [discriminator loss: 0.581832, acc: 0.679688] [adversarial loss: 1.142582, acc: 0.265625]\n",
      "6223: [discriminator loss: 0.596966, acc: 0.695312] [adversarial loss: 1.786373, acc: 0.093750]\n",
      "6224: [discriminator loss: 0.509185, acc: 0.710938] [adversarial loss: 0.910656, acc: 0.328125]\n",
      "6225: [discriminator loss: 0.571578, acc: 0.695312] [adversarial loss: 1.243442, acc: 0.187500]\n",
      "6226: [discriminator loss: 0.643601, acc: 0.640625] [adversarial loss: 1.025825, acc: 0.281250]\n",
      "6227: [discriminator loss: 0.563216, acc: 0.710938] [adversarial loss: 1.406924, acc: 0.078125]\n",
      "6228: [discriminator loss: 0.493255, acc: 0.750000] [adversarial loss: 0.984082, acc: 0.296875]\n",
      "6229: [discriminator loss: 0.496262, acc: 0.742188] [adversarial loss: 1.436766, acc: 0.109375]\n",
      "6230: [discriminator loss: 0.489159, acc: 0.726562] [adversarial loss: 1.028477, acc: 0.265625]\n",
      "6231: [discriminator loss: 0.500050, acc: 0.773438] [adversarial loss: 1.274726, acc: 0.156250]\n",
      "6232: [discriminator loss: 0.580220, acc: 0.687500] [adversarial loss: 0.931804, acc: 0.359375]\n",
      "6233: [discriminator loss: 0.534628, acc: 0.695312] [adversarial loss: 1.379699, acc: 0.156250]\n",
      "6234: [discriminator loss: 0.486753, acc: 0.781250] [adversarial loss: 1.125445, acc: 0.234375]\n",
      "6235: [discriminator loss: 0.434199, acc: 0.828125] [adversarial loss: 1.343021, acc: 0.125000]\n",
      "6236: [discriminator loss: 0.523460, acc: 0.718750] [adversarial loss: 1.095301, acc: 0.234375]\n",
      "6237: [discriminator loss: 0.436116, acc: 0.835938] [adversarial loss: 1.367165, acc: 0.109375]\n",
      "6238: [discriminator loss: 0.500569, acc: 0.781250] [adversarial loss: 1.133533, acc: 0.312500]\n",
      "6239: [discriminator loss: 0.544737, acc: 0.679688] [adversarial loss: 1.121083, acc: 0.281250]\n",
      "6240: [discriminator loss: 0.537484, acc: 0.718750] [adversarial loss: 0.979933, acc: 0.406250]\n",
      "6241: [discriminator loss: 0.487547, acc: 0.812500] [adversarial loss: 1.183283, acc: 0.187500]\n",
      "6242: [discriminator loss: 0.570349, acc: 0.664062] [adversarial loss: 1.024525, acc: 0.375000]\n",
      "6243: [discriminator loss: 0.463247, acc: 0.789062] [adversarial loss: 1.379044, acc: 0.156250]\n",
      "6244: [discriminator loss: 0.564549, acc: 0.710938] [adversarial loss: 0.985426, acc: 0.468750]\n",
      "6245: [discriminator loss: 0.594340, acc: 0.703125] [adversarial loss: 1.904918, acc: 0.015625]\n",
      "6246: [discriminator loss: 0.548228, acc: 0.695312] [adversarial loss: 0.901001, acc: 0.390625]\n",
      "6247: [discriminator loss: 0.528196, acc: 0.726562] [adversarial loss: 1.614569, acc: 0.093750]\n",
      "6248: [discriminator loss: 0.598143, acc: 0.710938] [adversarial loss: 1.327845, acc: 0.171875]\n",
      "6249: [discriminator loss: 0.599141, acc: 0.671875] [adversarial loss: 1.504971, acc: 0.062500]\n",
      "6250: [discriminator loss: 0.539297, acc: 0.742188] [adversarial loss: 1.124080, acc: 0.281250]\n",
      "6251: [discriminator loss: 0.502071, acc: 0.718750] [adversarial loss: 1.363909, acc: 0.140625]\n",
      "6252: [discriminator loss: 0.483682, acc: 0.742188] [adversarial loss: 1.314948, acc: 0.187500]\n",
      "6253: [discriminator loss: 0.516045, acc: 0.742188] [adversarial loss: 1.074413, acc: 0.281250]\n",
      "6254: [discriminator loss: 0.507295, acc: 0.726562] [adversarial loss: 1.203237, acc: 0.156250]\n",
      "6255: [discriminator loss: 0.527014, acc: 0.781250] [adversarial loss: 1.313130, acc: 0.171875]\n",
      "6256: [discriminator loss: 0.418429, acc: 0.804688] [adversarial loss: 1.190222, acc: 0.234375]\n",
      "6257: [discriminator loss: 0.493073, acc: 0.781250] [adversarial loss: 1.135778, acc: 0.250000]\n",
      "6258: [discriminator loss: 0.543544, acc: 0.726562] [adversarial loss: 1.534853, acc: 0.093750]\n",
      "6259: [discriminator loss: 0.497745, acc: 0.742188] [adversarial loss: 0.739871, acc: 0.531250]\n",
      "6260: [discriminator loss: 0.598655, acc: 0.687500] [adversarial loss: 1.771116, acc: 0.062500]\n",
      "6261: [discriminator loss: 0.596789, acc: 0.703125] [adversarial loss: 0.915585, acc: 0.312500]\n",
      "6262: [discriminator loss: 0.493967, acc: 0.789062] [adversarial loss: 1.462068, acc: 0.062500]\n",
      "6263: [discriminator loss: 0.540111, acc: 0.726562] [adversarial loss: 1.079846, acc: 0.265625]\n",
      "6264: [discriminator loss: 0.477987, acc: 0.710938] [adversarial loss: 1.211855, acc: 0.234375]\n",
      "6265: [discriminator loss: 0.560204, acc: 0.695312] [adversarial loss: 1.153286, acc: 0.234375]\n",
      "6266: [discriminator loss: 0.559079, acc: 0.742188] [adversarial loss: 1.371770, acc: 0.140625]\n",
      "6267: [discriminator loss: 0.548285, acc: 0.718750] [adversarial loss: 1.194603, acc: 0.234375]\n",
      "6268: [discriminator loss: 0.600720, acc: 0.671875] [adversarial loss: 1.471927, acc: 0.140625]\n",
      "6269: [discriminator loss: 0.519860, acc: 0.750000] [adversarial loss: 1.235299, acc: 0.250000]\n",
      "6270: [discriminator loss: 0.533999, acc: 0.687500] [adversarial loss: 1.390264, acc: 0.171875]\n",
      "6271: [discriminator loss: 0.595565, acc: 0.703125] [adversarial loss: 0.883986, acc: 0.375000]\n",
      "6272: [discriminator loss: 0.622190, acc: 0.679688] [adversarial loss: 1.038623, acc: 0.296875]\n",
      "6273: [discriminator loss: 0.485728, acc: 0.781250] [adversarial loss: 1.136773, acc: 0.265625]\n",
      "6274: [discriminator loss: 0.474190, acc: 0.796875] [adversarial loss: 1.029032, acc: 0.359375]\n",
      "6275: [discriminator loss: 0.521878, acc: 0.703125] [adversarial loss: 1.212795, acc: 0.203125]\n",
      "6276: [discriminator loss: 0.500065, acc: 0.773438] [adversarial loss: 1.248962, acc: 0.156250]\n",
      "6277: [discriminator loss: 0.539018, acc: 0.710938] [adversarial loss: 1.205811, acc: 0.265625]\n",
      "6278: [discriminator loss: 0.515651, acc: 0.742188] [adversarial loss: 1.142454, acc: 0.281250]\n",
      "6279: [discriminator loss: 0.584237, acc: 0.687500] [adversarial loss: 1.249309, acc: 0.281250]\n",
      "6280: [discriminator loss: 0.500726, acc: 0.742188] [adversarial loss: 1.102277, acc: 0.265625]\n",
      "6281: [discriminator loss: 0.548821, acc: 0.757812] [adversarial loss: 1.338852, acc: 0.218750]\n",
      "6282: [discriminator loss: 0.492077, acc: 0.750000] [adversarial loss: 1.058401, acc: 0.359375]\n",
      "6283: [discriminator loss: 0.536468, acc: 0.718750] [adversarial loss: 1.447871, acc: 0.171875]\n",
      "6284: [discriminator loss: 0.444401, acc: 0.781250] [adversarial loss: 1.047117, acc: 0.328125]\n",
      "6285: [discriminator loss: 0.444412, acc: 0.812500] [adversarial loss: 1.587297, acc: 0.109375]\n",
      "6286: [discriminator loss: 0.530904, acc: 0.710938] [adversarial loss: 1.114116, acc: 0.187500]\n",
      "6287: [discriminator loss: 0.465495, acc: 0.757812] [adversarial loss: 1.414019, acc: 0.203125]\n",
      "6288: [discriminator loss: 0.458945, acc: 0.773438] [adversarial loss: 0.871864, acc: 0.375000]\n",
      "6289: [discriminator loss: 0.685615, acc: 0.648438] [adversarial loss: 1.579799, acc: 0.078125]\n",
      "6290: [discriminator loss: 0.576180, acc: 0.695312] [adversarial loss: 0.947303, acc: 0.390625]\n",
      "6291: [discriminator loss: 0.630770, acc: 0.656250] [adversarial loss: 1.669511, acc: 0.078125]\n",
      "6292: [discriminator loss: 0.630898, acc: 0.625000] [adversarial loss: 0.752342, acc: 0.500000]\n",
      "6293: [discriminator loss: 0.476686, acc: 0.757812] [adversarial loss: 1.619266, acc: 0.109375]\n",
      "6294: [discriminator loss: 0.587457, acc: 0.710938] [adversarial loss: 1.037396, acc: 0.390625]\n",
      "6295: [discriminator loss: 0.524492, acc: 0.710938] [adversarial loss: 0.956620, acc: 0.359375]\n",
      "6296: [discriminator loss: 0.496220, acc: 0.773438] [adversarial loss: 1.185391, acc: 0.234375]\n",
      "6297: [discriminator loss: 0.565081, acc: 0.687500] [adversarial loss: 1.403475, acc: 0.109375]\n",
      "6298: [discriminator loss: 0.521813, acc: 0.726562] [adversarial loss: 1.131192, acc: 0.250000]\n",
      "6299: [discriminator loss: 0.530431, acc: 0.757812] [adversarial loss: 1.215375, acc: 0.203125]\n",
      "6300: [discriminator loss: 0.488319, acc: 0.750000] [adversarial loss: 1.359003, acc: 0.093750]\n",
      "6301: [discriminator loss: 0.509597, acc: 0.742188] [adversarial loss: 1.083911, acc: 0.390625]\n",
      "6302: [discriminator loss: 0.623318, acc: 0.656250] [adversarial loss: 0.889691, acc: 0.453125]\n",
      "6303: [discriminator loss: 0.540455, acc: 0.695312] [adversarial loss: 1.645739, acc: 0.109375]\n",
      "6304: [discriminator loss: 0.538606, acc: 0.687500] [adversarial loss: 0.816891, acc: 0.453125]\n",
      "6305: [discriminator loss: 0.618475, acc: 0.687500] [adversarial loss: 1.840769, acc: 0.093750]\n",
      "6306: [discriminator loss: 0.489252, acc: 0.726562] [adversarial loss: 1.200592, acc: 0.296875]\n",
      "6307: [discriminator loss: 0.566612, acc: 0.710938] [adversarial loss: 1.280090, acc: 0.156250]\n",
      "6308: [discriminator loss: 0.578503, acc: 0.664062] [adversarial loss: 1.149949, acc: 0.250000]\n",
      "6309: [discriminator loss: 0.547408, acc: 0.726562] [adversarial loss: 1.269033, acc: 0.125000]\n",
      "6310: [discriminator loss: 0.517297, acc: 0.789062] [adversarial loss: 1.087741, acc: 0.250000]\n",
      "6311: [discriminator loss: 0.448320, acc: 0.804688] [adversarial loss: 1.155926, acc: 0.203125]\n",
      "6312: [discriminator loss: 0.555539, acc: 0.726562] [adversarial loss: 1.152890, acc: 0.187500]\n",
      "6313: [discriminator loss: 0.562714, acc: 0.671875] [adversarial loss: 1.335802, acc: 0.140625]\n",
      "6314: [discriminator loss: 0.481555, acc: 0.757812] [adversarial loss: 0.892269, acc: 0.484375]\n",
      "6315: [discriminator loss: 0.612998, acc: 0.671875] [adversarial loss: 1.825898, acc: 0.046875]\n",
      "6316: [discriminator loss: 0.591221, acc: 0.656250] [adversarial loss: 0.716734, acc: 0.515625]\n",
      "6317: [discriminator loss: 0.561277, acc: 0.734375] [adversarial loss: 1.449678, acc: 0.203125]\n",
      "6318: [discriminator loss: 0.543145, acc: 0.718750] [adversarial loss: 1.152535, acc: 0.203125]\n",
      "6319: [discriminator loss: 0.559525, acc: 0.718750] [adversarial loss: 1.426314, acc: 0.187500]\n",
      "6320: [discriminator loss: 0.486165, acc: 0.734375] [adversarial loss: 1.169328, acc: 0.281250]\n",
      "6321: [discriminator loss: 0.535403, acc: 0.710938] [adversarial loss: 1.295469, acc: 0.203125]\n",
      "6322: [discriminator loss: 0.491820, acc: 0.750000] [adversarial loss: 1.246955, acc: 0.156250]\n",
      "6323: [discriminator loss: 0.592561, acc: 0.687500] [adversarial loss: 1.441077, acc: 0.140625]\n",
      "6324: [discriminator loss: 0.563401, acc: 0.726562] [adversarial loss: 1.198733, acc: 0.234375]\n",
      "6325: [discriminator loss: 0.452037, acc: 0.820312] [adversarial loss: 1.198373, acc: 0.218750]\n",
      "6326: [discriminator loss: 0.488718, acc: 0.750000] [adversarial loss: 1.240359, acc: 0.218750]\n",
      "6327: [discriminator loss: 0.486162, acc: 0.765625] [adversarial loss: 1.206065, acc: 0.281250]\n",
      "6328: [discriminator loss: 0.514248, acc: 0.710938] [adversarial loss: 1.281955, acc: 0.218750]\n",
      "6329: [discriminator loss: 0.520306, acc: 0.734375] [adversarial loss: 0.983623, acc: 0.328125]\n",
      "6330: [discriminator loss: 0.503203, acc: 0.773438] [adversarial loss: 1.686222, acc: 0.125000]\n",
      "6331: [discriminator loss: 0.580596, acc: 0.671875] [adversarial loss: 1.114597, acc: 0.203125]\n",
      "6332: [discriminator loss: 0.540585, acc: 0.726562] [adversarial loss: 1.104676, acc: 0.312500]\n",
      "6333: [discriminator loss: 0.484062, acc: 0.796875] [adversarial loss: 1.204684, acc: 0.218750]\n",
      "6334: [discriminator loss: 0.508247, acc: 0.726562] [adversarial loss: 1.021238, acc: 0.265625]\n",
      "6335: [discriminator loss: 0.599781, acc: 0.687500] [adversarial loss: 1.732414, acc: 0.046875]\n",
      "6336: [discriminator loss: 0.494117, acc: 0.742188] [adversarial loss: 1.099062, acc: 0.218750]\n",
      "6337: [discriminator loss: 0.452960, acc: 0.796875] [adversarial loss: 1.217609, acc: 0.140625]\n",
      "6338: [discriminator loss: 0.551919, acc: 0.710938] [adversarial loss: 1.063593, acc: 0.343750]\n",
      "6339: [discriminator loss: 0.651634, acc: 0.656250] [adversarial loss: 1.710494, acc: 0.093750]\n",
      "6340: [discriminator loss: 0.579866, acc: 0.742188] [adversarial loss: 1.107951, acc: 0.312500]\n",
      "6341: [discriminator loss: 0.551785, acc: 0.718750] [adversarial loss: 1.386617, acc: 0.187500]\n",
      "6342: [discriminator loss: 0.482817, acc: 0.726562] [adversarial loss: 0.981510, acc: 0.312500]\n",
      "6343: [discriminator loss: 0.619714, acc: 0.656250] [adversarial loss: 1.361762, acc: 0.203125]\n",
      "6344: [discriminator loss: 0.533920, acc: 0.726562] [adversarial loss: 1.125610, acc: 0.281250]\n",
      "6345: [discriminator loss: 0.539713, acc: 0.726562] [adversarial loss: 1.357417, acc: 0.203125]\n",
      "6346: [discriminator loss: 0.520338, acc: 0.781250] [adversarial loss: 0.976181, acc: 0.328125]\n",
      "6347: [discriminator loss: 0.480751, acc: 0.765625] [adversarial loss: 1.364721, acc: 0.156250]\n",
      "6348: [discriminator loss: 0.576356, acc: 0.695312] [adversarial loss: 0.966216, acc: 0.375000]\n",
      "6349: [discriminator loss: 0.557103, acc: 0.656250] [adversarial loss: 1.359761, acc: 0.171875]\n",
      "6350: [discriminator loss: 0.614787, acc: 0.640625] [adversarial loss: 1.098965, acc: 0.218750]\n",
      "6351: [discriminator loss: 0.570414, acc: 0.687500] [adversarial loss: 1.225487, acc: 0.234375]\n",
      "6352: [discriminator loss: 0.541926, acc: 0.781250] [adversarial loss: 1.451199, acc: 0.187500]\n",
      "6353: [discriminator loss: 0.579985, acc: 0.687500] [adversarial loss: 0.878610, acc: 0.468750]\n",
      "6354: [discriminator loss: 0.494862, acc: 0.820312] [adversarial loss: 1.556481, acc: 0.109375]\n",
      "6355: [discriminator loss: 0.477132, acc: 0.765625] [adversarial loss: 0.927570, acc: 0.390625]\n",
      "6356: [discriminator loss: 0.505877, acc: 0.773438] [adversarial loss: 1.325957, acc: 0.156250]\n",
      "6357: [discriminator loss: 0.493981, acc: 0.726562] [adversarial loss: 0.990681, acc: 0.359375]\n",
      "6358: [discriminator loss: 0.532851, acc: 0.734375] [adversarial loss: 1.257856, acc: 0.171875]\n",
      "6359: [discriminator loss: 0.483087, acc: 0.781250] [adversarial loss: 1.356467, acc: 0.125000]\n",
      "6360: [discriminator loss: 0.547799, acc: 0.742188] [adversarial loss: 1.227228, acc: 0.203125]\n",
      "6361: [discriminator loss: 0.518262, acc: 0.734375] [adversarial loss: 1.637978, acc: 0.125000]\n",
      "6362: [discriminator loss: 0.623339, acc: 0.679688] [adversarial loss: 1.159119, acc: 0.281250]\n",
      "6363: [discriminator loss: 0.502356, acc: 0.734375] [adversarial loss: 1.415731, acc: 0.140625]\n",
      "6364: [discriminator loss: 0.545182, acc: 0.703125] [adversarial loss: 1.206297, acc: 0.187500]\n",
      "6365: [discriminator loss: 0.528806, acc: 0.765625] [adversarial loss: 0.903369, acc: 0.406250]\n",
      "6366: [discriminator loss: 0.538345, acc: 0.718750] [adversarial loss: 1.754791, acc: 0.125000]\n",
      "6367: [discriminator loss: 0.604672, acc: 0.695312] [adversarial loss: 0.802507, acc: 0.468750]\n",
      "6368: [discriminator loss: 0.537139, acc: 0.750000] [adversarial loss: 1.503801, acc: 0.156250]\n",
      "6369: [discriminator loss: 0.518315, acc: 0.734375] [adversarial loss: 1.096587, acc: 0.296875]\n",
      "6370: [discriminator loss: 0.586739, acc: 0.726562] [adversarial loss: 1.383589, acc: 0.140625]\n",
      "6371: [discriminator loss: 0.487664, acc: 0.726562] [adversarial loss: 1.412879, acc: 0.171875]\n",
      "6372: [discriminator loss: 0.547796, acc: 0.750000] [adversarial loss: 1.068014, acc: 0.250000]\n",
      "6373: [discriminator loss: 0.562228, acc: 0.710938] [adversarial loss: 1.310971, acc: 0.234375]\n",
      "6374: [discriminator loss: 0.509284, acc: 0.757812] [adversarial loss: 1.290457, acc: 0.203125]\n",
      "6375: [discriminator loss: 0.479462, acc: 0.796875] [adversarial loss: 1.339955, acc: 0.234375]\n",
      "6376: [discriminator loss: 0.548021, acc: 0.710938] [adversarial loss: 1.274364, acc: 0.203125]\n",
      "6377: [discriminator loss: 0.623422, acc: 0.640625] [adversarial loss: 1.235146, acc: 0.187500]\n",
      "6378: [discriminator loss: 0.563879, acc: 0.703125] [adversarial loss: 1.324869, acc: 0.187500]\n",
      "6379: [discriminator loss: 0.538532, acc: 0.718750] [adversarial loss: 1.169329, acc: 0.187500]\n",
      "6380: [discriminator loss: 0.528490, acc: 0.734375] [adversarial loss: 1.236588, acc: 0.171875]\n",
      "6381: [discriminator loss: 0.537552, acc: 0.718750] [adversarial loss: 1.203881, acc: 0.250000]\n",
      "6382: [discriminator loss: 0.519816, acc: 0.734375] [adversarial loss: 1.350164, acc: 0.187500]\n",
      "6383: [discriminator loss: 0.510197, acc: 0.687500] [adversarial loss: 1.214823, acc: 0.312500]\n",
      "6384: [discriminator loss: 0.500769, acc: 0.742188] [adversarial loss: 1.112053, acc: 0.328125]\n",
      "6385: [discriminator loss: 0.548534, acc: 0.718750] [adversarial loss: 1.413892, acc: 0.093750]\n",
      "6386: [discriminator loss: 0.564447, acc: 0.695312] [adversarial loss: 1.179981, acc: 0.203125]\n",
      "6387: [discriminator loss: 0.544454, acc: 0.718750] [adversarial loss: 1.367982, acc: 0.203125]\n",
      "6388: [discriminator loss: 0.592086, acc: 0.687500] [adversarial loss: 1.257986, acc: 0.156250]\n",
      "6389: [discriminator loss: 0.482613, acc: 0.773438] [adversarial loss: 1.462366, acc: 0.109375]\n",
      "6390: [discriminator loss: 0.584419, acc: 0.695312] [adversarial loss: 0.998539, acc: 0.328125]\n",
      "6391: [discriminator loss: 0.598489, acc: 0.695312] [adversarial loss: 1.504792, acc: 0.156250]\n",
      "6392: [discriminator loss: 0.528904, acc: 0.750000] [adversarial loss: 0.894034, acc: 0.406250]\n",
      "6393: [discriminator loss: 0.547977, acc: 0.710938] [adversarial loss: 1.585803, acc: 0.156250]\n",
      "6394: [discriminator loss: 0.556506, acc: 0.718750] [adversarial loss: 1.230014, acc: 0.281250]\n",
      "6395: [discriminator loss: 0.569021, acc: 0.679688] [adversarial loss: 1.085832, acc: 0.250000]\n",
      "6396: [discriminator loss: 0.475657, acc: 0.734375] [adversarial loss: 1.251147, acc: 0.187500]\n",
      "6397: [discriminator loss: 0.509757, acc: 0.734375] [adversarial loss: 1.034712, acc: 0.359375]\n",
      "6398: [discriminator loss: 0.505258, acc: 0.742188] [adversarial loss: 1.322768, acc: 0.203125]\n",
      "6399: [discriminator loss: 0.602174, acc: 0.703125] [adversarial loss: 1.137057, acc: 0.218750]\n",
      "6400: [discriminator loss: 0.534026, acc: 0.742188] [adversarial loss: 1.273833, acc: 0.203125]\n",
      "6401: [discriminator loss: 0.533152, acc: 0.718750] [adversarial loss: 0.827819, acc: 0.421875]\n",
      "6402: [discriminator loss: 0.497235, acc: 0.765625] [adversarial loss: 1.437807, acc: 0.171875]\n",
      "6403: [discriminator loss: 0.529694, acc: 0.742188] [adversarial loss: 0.788079, acc: 0.453125]\n",
      "6404: [discriminator loss: 0.546062, acc: 0.718750] [adversarial loss: 1.449665, acc: 0.125000]\n",
      "6405: [discriminator loss: 0.557358, acc: 0.679688] [adversarial loss: 0.949945, acc: 0.421875]\n",
      "6406: [discriminator loss: 0.476696, acc: 0.765625] [adversarial loss: 1.647164, acc: 0.156250]\n",
      "6407: [discriminator loss: 0.595204, acc: 0.695312] [adversarial loss: 0.917722, acc: 0.421875]\n",
      "6408: [discriminator loss: 0.566439, acc: 0.703125] [adversarial loss: 1.163088, acc: 0.312500]\n",
      "6409: [discriminator loss: 0.502778, acc: 0.765625] [adversarial loss: 1.329078, acc: 0.187500]\n",
      "6410: [discriminator loss: 0.556446, acc: 0.718750] [adversarial loss: 1.051121, acc: 0.265625]\n",
      "6411: [discriminator loss: 0.539711, acc: 0.718750] [adversarial loss: 1.454106, acc: 0.109375]\n",
      "6412: [discriminator loss: 0.498880, acc: 0.710938] [adversarial loss: 1.159247, acc: 0.234375]\n",
      "6413: [discriminator loss: 0.543640, acc: 0.718750] [adversarial loss: 1.185502, acc: 0.234375]\n",
      "6414: [discriminator loss: 0.526179, acc: 0.742188] [adversarial loss: 0.993345, acc: 0.390625]\n",
      "6415: [discriminator loss: 0.495190, acc: 0.726562] [adversarial loss: 1.537341, acc: 0.093750]\n",
      "6416: [discriminator loss: 0.571771, acc: 0.703125] [adversarial loss: 0.934435, acc: 0.343750]\n",
      "6417: [discriminator loss: 0.594831, acc: 0.703125] [adversarial loss: 1.146174, acc: 0.281250]\n",
      "6418: [discriminator loss: 0.510266, acc: 0.734375] [adversarial loss: 1.438722, acc: 0.125000]\n",
      "6419: [discriminator loss: 0.566350, acc: 0.703125] [adversarial loss: 0.917410, acc: 0.343750]\n",
      "6420: [discriminator loss: 0.528284, acc: 0.765625] [adversarial loss: 1.302413, acc: 0.093750]\n",
      "6421: [discriminator loss: 0.436886, acc: 0.781250] [adversarial loss: 1.091657, acc: 0.281250]\n",
      "6422: [discriminator loss: 0.560360, acc: 0.718750] [adversarial loss: 1.087998, acc: 0.281250]\n",
      "6423: [discriminator loss: 0.530270, acc: 0.781250] [adversarial loss: 1.239508, acc: 0.171875]\n",
      "6424: [discriminator loss: 0.515754, acc: 0.765625] [adversarial loss: 1.155375, acc: 0.281250]\n",
      "6425: [discriminator loss: 0.529911, acc: 0.742188] [adversarial loss: 1.042137, acc: 0.296875]\n",
      "6426: [discriminator loss: 0.527595, acc: 0.757812] [adversarial loss: 1.441894, acc: 0.125000]\n",
      "6427: [discriminator loss: 0.508683, acc: 0.734375] [adversarial loss: 0.856490, acc: 0.468750]\n",
      "6428: [discriminator loss: 0.581726, acc: 0.656250] [adversarial loss: 1.851452, acc: 0.015625]\n",
      "6429: [discriminator loss: 0.498109, acc: 0.726562] [adversarial loss: 1.019128, acc: 0.250000]\n",
      "6430: [discriminator loss: 0.520394, acc: 0.726562] [adversarial loss: 1.424423, acc: 0.078125]\n",
      "6431: [discriminator loss: 0.553930, acc: 0.671875] [adversarial loss: 1.090451, acc: 0.296875]\n",
      "6432: [discriminator loss: 0.560657, acc: 0.703125] [adversarial loss: 1.293480, acc: 0.187500]\n",
      "6433: [discriminator loss: 0.517104, acc: 0.718750] [adversarial loss: 0.883738, acc: 0.421875]\n",
      "6434: [discriminator loss: 0.495495, acc: 0.757812] [adversarial loss: 1.287098, acc: 0.203125]\n",
      "6435: [discriminator loss: 0.525679, acc: 0.695312] [adversarial loss: 1.140745, acc: 0.203125]\n",
      "6436: [discriminator loss: 0.585253, acc: 0.710938] [adversarial loss: 1.394273, acc: 0.218750]\n",
      "6437: [discriminator loss: 0.544941, acc: 0.765625] [adversarial loss: 0.985898, acc: 0.375000]\n",
      "6438: [discriminator loss: 0.523721, acc: 0.718750] [adversarial loss: 1.383329, acc: 0.218750]\n",
      "6439: [discriminator loss: 0.511068, acc: 0.750000] [adversarial loss: 1.219650, acc: 0.296875]\n",
      "6440: [discriminator loss: 0.536133, acc: 0.734375] [adversarial loss: 0.942928, acc: 0.359375]\n",
      "6441: [discriminator loss: 0.536606, acc: 0.710938] [adversarial loss: 1.514094, acc: 0.171875]\n",
      "6442: [discriminator loss: 0.515153, acc: 0.734375] [adversarial loss: 1.035799, acc: 0.250000]\n",
      "6443: [discriminator loss: 0.563692, acc: 0.679688] [adversarial loss: 1.660009, acc: 0.140625]\n",
      "6444: [discriminator loss: 0.611379, acc: 0.625000] [adversarial loss: 1.149342, acc: 0.281250]\n",
      "6445: [discriminator loss: 0.549826, acc: 0.734375] [adversarial loss: 1.362395, acc: 0.109375]\n",
      "6446: [discriminator loss: 0.483078, acc: 0.710938] [adversarial loss: 1.402455, acc: 0.171875]\n",
      "6447: [discriminator loss: 0.549998, acc: 0.726562] [adversarial loss: 0.921485, acc: 0.375000]\n",
      "6448: [discriminator loss: 0.585336, acc: 0.679688] [adversarial loss: 1.363127, acc: 0.140625]\n",
      "6449: [discriminator loss: 0.463625, acc: 0.796875] [adversarial loss: 1.032894, acc: 0.265625]\n",
      "6450: [discriminator loss: 0.552915, acc: 0.742188] [adversarial loss: 1.439595, acc: 0.109375]\n",
      "6451: [discriminator loss: 0.576885, acc: 0.679688] [adversarial loss: 0.768932, acc: 0.515625]\n",
      "6452: [discriminator loss: 0.560353, acc: 0.718750] [adversarial loss: 1.734148, acc: 0.031250]\n",
      "6453: [discriminator loss: 0.607442, acc: 0.710938] [adversarial loss: 0.901614, acc: 0.500000]\n",
      "6454: [discriminator loss: 0.625944, acc: 0.601562] [adversarial loss: 1.396260, acc: 0.125000]\n",
      "6455: [discriminator loss: 0.502327, acc: 0.765625] [adversarial loss: 1.009515, acc: 0.312500]\n",
      "6456: [discriminator loss: 0.500128, acc: 0.789062] [adversarial loss: 1.509656, acc: 0.109375]\n",
      "6457: [discriminator loss: 0.523197, acc: 0.718750] [adversarial loss: 1.084690, acc: 0.187500]\n",
      "6458: [discriminator loss: 0.598548, acc: 0.664062] [adversarial loss: 1.421629, acc: 0.109375]\n",
      "6459: [discriminator loss: 0.548119, acc: 0.734375] [adversarial loss: 1.096492, acc: 0.265625]\n",
      "6460: [discriminator loss: 0.519266, acc: 0.687500] [adversarial loss: 1.490843, acc: 0.109375]\n",
      "6461: [discriminator loss: 0.563988, acc: 0.703125] [adversarial loss: 0.986271, acc: 0.390625]\n",
      "6462: [discriminator loss: 0.540093, acc: 0.718750] [adversarial loss: 1.449298, acc: 0.125000]\n",
      "6463: [discriminator loss: 0.645404, acc: 0.632812] [adversarial loss: 0.885509, acc: 0.406250]\n",
      "6464: [discriminator loss: 0.522964, acc: 0.773438] [adversarial loss: 1.247938, acc: 0.203125]\n",
      "6465: [discriminator loss: 0.490821, acc: 0.781250] [adversarial loss: 1.232300, acc: 0.234375]\n",
      "6466: [discriminator loss: 0.500514, acc: 0.734375] [adversarial loss: 1.120604, acc: 0.250000]\n",
      "6467: [discriminator loss: 0.542596, acc: 0.687500] [adversarial loss: 1.258203, acc: 0.171875]\n",
      "6468: [discriminator loss: 0.546317, acc: 0.664062] [adversarial loss: 1.173850, acc: 0.203125]\n",
      "6469: [discriminator loss: 0.450418, acc: 0.781250] [adversarial loss: 1.075351, acc: 0.390625]\n",
      "6470: [discriminator loss: 0.553155, acc: 0.718750] [adversarial loss: 1.427027, acc: 0.078125]\n",
      "6471: [discriminator loss: 0.501648, acc: 0.757812] [adversarial loss: 1.255280, acc: 0.218750]\n",
      "6472: [discriminator loss: 0.508927, acc: 0.734375] [adversarial loss: 1.493909, acc: 0.203125]\n",
      "6473: [discriminator loss: 0.544107, acc: 0.742188] [adversarial loss: 1.204053, acc: 0.265625]\n",
      "6474: [discriminator loss: 0.534886, acc: 0.710938] [adversarial loss: 1.193482, acc: 0.281250]\n",
      "6475: [discriminator loss: 0.539319, acc: 0.695312] [adversarial loss: 1.117049, acc: 0.265625]\n",
      "6476: [discriminator loss: 0.577068, acc: 0.687500] [adversarial loss: 0.984651, acc: 0.265625]\n",
      "6477: [discriminator loss: 0.556208, acc: 0.734375] [adversarial loss: 1.393768, acc: 0.125000]\n",
      "6478: [discriminator loss: 0.517565, acc: 0.710938] [adversarial loss: 0.999132, acc: 0.328125]\n",
      "6479: [discriminator loss: 0.520278, acc: 0.703125] [adversarial loss: 1.813221, acc: 0.093750]\n",
      "6480: [discriminator loss: 0.480439, acc: 0.781250] [adversarial loss: 0.957844, acc: 0.437500]\n",
      "6481: [discriminator loss: 0.596027, acc: 0.664062] [adversarial loss: 1.323071, acc: 0.203125]\n",
      "6482: [discriminator loss: 0.553004, acc: 0.703125] [adversarial loss: 1.148295, acc: 0.265625]\n",
      "6483: [discriminator loss: 0.523164, acc: 0.726562] [adversarial loss: 1.525678, acc: 0.187500]\n",
      "6484: [discriminator loss: 0.538544, acc: 0.742188] [adversarial loss: 0.935172, acc: 0.312500]\n",
      "6485: [discriminator loss: 0.572587, acc: 0.671875] [adversarial loss: 1.606312, acc: 0.031250]\n",
      "6486: [discriminator loss: 0.513295, acc: 0.765625] [adversarial loss: 1.111530, acc: 0.250000]\n",
      "6487: [discriminator loss: 0.526002, acc: 0.734375] [adversarial loss: 1.544823, acc: 0.125000]\n",
      "6488: [discriminator loss: 0.585618, acc: 0.703125] [adversarial loss: 0.957594, acc: 0.296875]\n",
      "6489: [discriminator loss: 0.523926, acc: 0.718750] [adversarial loss: 1.470054, acc: 0.140625]\n",
      "6490: [discriminator loss: 0.552515, acc: 0.679688] [adversarial loss: 0.972401, acc: 0.359375]\n",
      "6491: [discriminator loss: 0.612655, acc: 0.632812] [adversarial loss: 1.073330, acc: 0.281250]\n",
      "6492: [discriminator loss: 0.509660, acc: 0.710938] [adversarial loss: 1.094794, acc: 0.250000]\n",
      "6493: [discriminator loss: 0.565209, acc: 0.726562] [adversarial loss: 1.289884, acc: 0.203125]\n",
      "6494: [discriminator loss: 0.560251, acc: 0.726562] [adversarial loss: 1.014000, acc: 0.343750]\n",
      "6495: [discriminator loss: 0.568966, acc: 0.695312] [adversarial loss: 1.371384, acc: 0.125000]\n",
      "6496: [discriminator loss: 0.544885, acc: 0.671875] [adversarial loss: 1.119386, acc: 0.312500]\n",
      "6497: [discriminator loss: 0.611709, acc: 0.632812] [adversarial loss: 1.477881, acc: 0.109375]\n",
      "6498: [discriminator loss: 0.482172, acc: 0.742188] [adversarial loss: 1.258148, acc: 0.250000]\n",
      "6499: [discriminator loss: 0.551089, acc: 0.671875] [adversarial loss: 1.012689, acc: 0.312500]\n",
      "6500: [discriminator loss: 0.598676, acc: 0.718750] [adversarial loss: 1.407493, acc: 0.125000]\n",
      "6501: [discriminator loss: 0.554435, acc: 0.742188] [adversarial loss: 0.957155, acc: 0.390625]\n",
      "6502: [discriminator loss: 0.503262, acc: 0.757812] [adversarial loss: 1.259033, acc: 0.125000]\n",
      "6503: [discriminator loss: 0.510754, acc: 0.742188] [adversarial loss: 1.094861, acc: 0.250000]\n",
      "6504: [discriminator loss: 0.484357, acc: 0.789062] [adversarial loss: 1.323384, acc: 0.203125]\n",
      "6505: [discriminator loss: 0.545942, acc: 0.718750] [adversarial loss: 1.113434, acc: 0.328125]\n",
      "6506: [discriminator loss: 0.556923, acc: 0.726562] [adversarial loss: 1.236782, acc: 0.171875]\n",
      "6507: [discriminator loss: 0.497441, acc: 0.757812] [adversarial loss: 1.389507, acc: 0.140625]\n",
      "6508: [discriminator loss: 0.557886, acc: 0.687500] [adversarial loss: 1.123118, acc: 0.234375]\n",
      "6509: [discriminator loss: 0.492103, acc: 0.757812] [adversarial loss: 1.112207, acc: 0.218750]\n",
      "6510: [discriminator loss: 0.577449, acc: 0.679688] [adversarial loss: 1.395641, acc: 0.171875]\n",
      "6511: [discriminator loss: 0.425745, acc: 0.804688] [adversarial loss: 1.071151, acc: 0.359375]\n",
      "6512: [discriminator loss: 0.498194, acc: 0.773438] [adversarial loss: 1.297914, acc: 0.140625]\n",
      "6513: [discriminator loss: 0.508438, acc: 0.710938] [adversarial loss: 1.278139, acc: 0.171875]\n",
      "6514: [discriminator loss: 0.524436, acc: 0.710938] [adversarial loss: 1.090800, acc: 0.296875]\n",
      "6515: [discriminator loss: 0.586795, acc: 0.710938] [adversarial loss: 1.360597, acc: 0.171875]\n",
      "6516: [discriminator loss: 0.506451, acc: 0.742188] [adversarial loss: 1.190317, acc: 0.218750]\n",
      "6517: [discriminator loss: 0.621750, acc: 0.648438] [adversarial loss: 1.465511, acc: 0.140625]\n",
      "6518: [discriminator loss: 0.594227, acc: 0.695312] [adversarial loss: 0.993262, acc: 0.328125]\n",
      "6519: [discriminator loss: 0.487064, acc: 0.718750] [adversarial loss: 1.227091, acc: 0.171875]\n",
      "6520: [discriminator loss: 0.515989, acc: 0.726562] [adversarial loss: 0.818752, acc: 0.500000]\n",
      "6521: [discriminator loss: 0.573854, acc: 0.703125] [adversarial loss: 1.472927, acc: 0.093750]\n",
      "6522: [discriminator loss: 0.503556, acc: 0.781250] [adversarial loss: 0.979401, acc: 0.343750]\n",
      "6523: [discriminator loss: 0.565662, acc: 0.734375] [adversarial loss: 1.396063, acc: 0.109375]\n",
      "6524: [discriminator loss: 0.641879, acc: 0.632812] [adversarial loss: 0.946933, acc: 0.421875]\n",
      "6525: [discriminator loss: 0.591099, acc: 0.664062] [adversarial loss: 1.582273, acc: 0.187500]\n",
      "6526: [discriminator loss: 0.486699, acc: 0.765625] [adversarial loss: 0.929435, acc: 0.390625]\n",
      "6527: [discriminator loss: 0.461478, acc: 0.765625] [adversarial loss: 1.392854, acc: 0.156250]\n",
      "6528: [discriminator loss: 0.614801, acc: 0.648438] [adversarial loss: 1.021135, acc: 0.359375]\n",
      "6529: [discriminator loss: 0.538427, acc: 0.710938] [adversarial loss: 1.600891, acc: 0.109375]\n",
      "6530: [discriminator loss: 0.562357, acc: 0.687500] [adversarial loss: 0.995174, acc: 0.359375]\n",
      "6531: [discriminator loss: 0.538751, acc: 0.710938] [adversarial loss: 1.123706, acc: 0.281250]\n",
      "6532: [discriminator loss: 0.575787, acc: 0.734375] [adversarial loss: 1.125146, acc: 0.296875]\n",
      "6533: [discriminator loss: 0.521514, acc: 0.726562] [adversarial loss: 1.637474, acc: 0.109375]\n",
      "6534: [discriminator loss: 0.611337, acc: 0.679688] [adversarial loss: 1.110891, acc: 0.203125]\n",
      "6535: [discriminator loss: 0.505747, acc: 0.773438] [adversarial loss: 1.192002, acc: 0.187500]\n",
      "6536: [discriminator loss: 0.551208, acc: 0.703125] [adversarial loss: 1.229748, acc: 0.187500]\n",
      "6537: [discriminator loss: 0.470647, acc: 0.750000] [adversarial loss: 0.917871, acc: 0.453125]\n",
      "6538: [discriminator loss: 0.588415, acc: 0.687500] [adversarial loss: 1.138299, acc: 0.250000]\n",
      "6539: [discriminator loss: 0.529852, acc: 0.734375] [adversarial loss: 1.309877, acc: 0.093750]\n",
      "6540: [discriminator loss: 0.577723, acc: 0.695312] [adversarial loss: 1.209323, acc: 0.281250]\n",
      "6541: [discriminator loss: 0.554206, acc: 0.695312] [adversarial loss: 1.227531, acc: 0.187500]\n",
      "6542: [discriminator loss: 0.634791, acc: 0.640625] [adversarial loss: 1.231270, acc: 0.140625]\n",
      "6543: [discriminator loss: 0.487100, acc: 0.757812] [adversarial loss: 0.870090, acc: 0.468750]\n",
      "6544: [discriminator loss: 0.548664, acc: 0.726562] [adversarial loss: 1.539007, acc: 0.125000]\n",
      "6545: [discriminator loss: 0.531818, acc: 0.710938] [adversarial loss: 1.406634, acc: 0.156250]\n",
      "6546: [discriminator loss: 0.506036, acc: 0.765625] [adversarial loss: 0.901116, acc: 0.406250]\n",
      "6547: [discriminator loss: 0.567396, acc: 0.679688] [adversarial loss: 1.362241, acc: 0.187500]\n",
      "6548: [discriminator loss: 0.468805, acc: 0.765625] [adversarial loss: 1.004637, acc: 0.312500]\n",
      "6549: [discriminator loss: 0.499826, acc: 0.750000] [adversarial loss: 1.477366, acc: 0.093750]\n",
      "6550: [discriminator loss: 0.549065, acc: 0.718750] [adversarial loss: 0.982202, acc: 0.484375]\n",
      "6551: [discriminator loss: 0.598463, acc: 0.640625] [adversarial loss: 1.665142, acc: 0.109375]\n",
      "6552: [discriminator loss: 0.622755, acc: 0.679688] [adversarial loss: 1.018490, acc: 0.359375]\n",
      "6553: [discriminator loss: 0.614278, acc: 0.687500] [adversarial loss: 1.631066, acc: 0.078125]\n",
      "6554: [discriminator loss: 0.474130, acc: 0.742188] [adversarial loss: 0.893945, acc: 0.421875]\n",
      "6555: [discriminator loss: 0.495631, acc: 0.742188] [adversarial loss: 1.299885, acc: 0.234375]\n",
      "6556: [discriminator loss: 0.481026, acc: 0.750000] [adversarial loss: 1.044327, acc: 0.265625]\n",
      "6557: [discriminator loss: 0.601094, acc: 0.679688] [adversarial loss: 1.130059, acc: 0.281250]\n",
      "6558: [discriminator loss: 0.555867, acc: 0.718750] [adversarial loss: 0.864418, acc: 0.390625]\n",
      "6559: [discriminator loss: 0.511520, acc: 0.742188] [adversarial loss: 1.462732, acc: 0.078125]\n",
      "6560: [discriminator loss: 0.548060, acc: 0.742188] [adversarial loss: 0.971458, acc: 0.312500]\n",
      "6561: [discriminator loss: 0.554370, acc: 0.671875] [adversarial loss: 1.271654, acc: 0.250000]\n",
      "6562: [discriminator loss: 0.489842, acc: 0.765625] [adversarial loss: 1.021547, acc: 0.343750]\n",
      "6563: [discriminator loss: 0.465654, acc: 0.789062] [adversarial loss: 0.983647, acc: 0.375000]\n",
      "6564: [discriminator loss: 0.605798, acc: 0.632812] [adversarial loss: 1.255743, acc: 0.218750]\n",
      "6565: [discriminator loss: 0.507413, acc: 0.742188] [adversarial loss: 1.240033, acc: 0.156250]\n",
      "6566: [discriminator loss: 0.496387, acc: 0.742188] [adversarial loss: 1.456748, acc: 0.234375]\n",
      "6567: [discriminator loss: 0.582462, acc: 0.625000] [adversarial loss: 1.111778, acc: 0.250000]\n",
      "6568: [discriminator loss: 0.526356, acc: 0.703125] [adversarial loss: 1.504090, acc: 0.140625]\n",
      "6569: [discriminator loss: 0.494803, acc: 0.757812] [adversarial loss: 0.845754, acc: 0.453125]\n",
      "6570: [discriminator loss: 0.519240, acc: 0.734375] [adversarial loss: 1.234729, acc: 0.218750]\n",
      "6571: [discriminator loss: 0.552898, acc: 0.703125] [adversarial loss: 1.296889, acc: 0.218750]\n",
      "6572: [discriminator loss: 0.523338, acc: 0.734375] [adversarial loss: 1.044400, acc: 0.296875]\n",
      "6573: [discriminator loss: 0.521320, acc: 0.742188] [adversarial loss: 1.027998, acc: 0.265625]\n",
      "6574: [discriminator loss: 0.557156, acc: 0.695312] [adversarial loss: 1.772790, acc: 0.125000]\n",
      "6575: [discriminator loss: 0.605125, acc: 0.671875] [adversarial loss: 0.812768, acc: 0.484375]\n",
      "6576: [discriminator loss: 0.619620, acc: 0.703125] [adversarial loss: 1.547072, acc: 0.109375]\n",
      "6577: [discriminator loss: 0.598281, acc: 0.656250] [adversarial loss: 0.888217, acc: 0.468750]\n",
      "6578: [discriminator loss: 0.573309, acc: 0.671875] [adversarial loss: 1.601781, acc: 0.093750]\n",
      "6579: [discriminator loss: 0.548240, acc: 0.679688] [adversarial loss: 1.093716, acc: 0.265625]\n",
      "6580: [discriminator loss: 0.491913, acc: 0.773438] [adversarial loss: 1.211046, acc: 0.250000]\n",
      "6581: [discriminator loss: 0.616333, acc: 0.640625] [adversarial loss: 1.127584, acc: 0.187500]\n",
      "6582: [discriminator loss: 0.530576, acc: 0.695312] [adversarial loss: 1.245512, acc: 0.218750]\n",
      "6583: [discriminator loss: 0.589888, acc: 0.687500] [adversarial loss: 1.323737, acc: 0.187500]\n",
      "6584: [discriminator loss: 0.522237, acc: 0.734375] [adversarial loss: 0.920116, acc: 0.406250]\n",
      "6585: [discriminator loss: 0.595643, acc: 0.687500] [adversarial loss: 1.093461, acc: 0.296875]\n",
      "6586: [discriminator loss: 0.544433, acc: 0.703125] [adversarial loss: 1.033677, acc: 0.359375]\n",
      "6587: [discriminator loss: 0.567803, acc: 0.648438] [adversarial loss: 1.180718, acc: 0.234375]\n",
      "6588: [discriminator loss: 0.564231, acc: 0.687500] [adversarial loss: 0.975527, acc: 0.328125]\n",
      "6589: [discriminator loss: 0.509994, acc: 0.750000] [adversarial loss: 1.293564, acc: 0.125000]\n",
      "6590: [discriminator loss: 0.555781, acc: 0.742188] [adversarial loss: 1.013808, acc: 0.343750]\n",
      "6591: [discriminator loss: 0.548202, acc: 0.726562] [adversarial loss: 1.608302, acc: 0.046875]\n",
      "6592: [discriminator loss: 0.372286, acc: 0.867188] [adversarial loss: 1.359050, acc: 0.218750]\n",
      "6593: [discriminator loss: 0.679042, acc: 0.664062] [adversarial loss: 1.139356, acc: 0.312500]\n",
      "6594: [discriminator loss: 0.538849, acc: 0.687500] [adversarial loss: 1.096386, acc: 0.296875]\n",
      "6595: [discriminator loss: 0.600659, acc: 0.695312] [adversarial loss: 1.345406, acc: 0.234375]\n",
      "6596: [discriminator loss: 0.533535, acc: 0.750000] [adversarial loss: 1.307904, acc: 0.203125]\n",
      "6597: [discriminator loss: 0.530421, acc: 0.726562] [adversarial loss: 1.148446, acc: 0.250000]\n",
      "6598: [discriminator loss: 0.526815, acc: 0.726562] [adversarial loss: 1.173318, acc: 0.234375]\n",
      "6599: [discriminator loss: 0.561137, acc: 0.679688] [adversarial loss: 0.852198, acc: 0.437500]\n",
      "6600: [discriminator loss: 0.523255, acc: 0.742188] [adversarial loss: 1.550585, acc: 0.093750]\n",
      "6601: [discriminator loss: 0.438776, acc: 0.812500] [adversarial loss: 1.039020, acc: 0.359375]\n",
      "6602: [discriminator loss: 0.480693, acc: 0.781250] [adversarial loss: 1.258225, acc: 0.234375]\n",
      "6603: [discriminator loss: 0.602596, acc: 0.648438] [adversarial loss: 0.830891, acc: 0.484375]\n",
      "6604: [discriminator loss: 0.512415, acc: 0.734375] [adversarial loss: 1.715535, acc: 0.125000]\n",
      "6605: [discriminator loss: 0.585423, acc: 0.710938] [adversarial loss: 0.890224, acc: 0.375000]\n",
      "6606: [discriminator loss: 0.521301, acc: 0.742188] [adversarial loss: 1.346290, acc: 0.125000]\n",
      "6607: [discriminator loss: 0.541344, acc: 0.718750] [adversarial loss: 1.024521, acc: 0.328125]\n",
      "6608: [discriminator loss: 0.558382, acc: 0.671875] [adversarial loss: 1.392384, acc: 0.187500]\n",
      "6609: [discriminator loss: 0.553999, acc: 0.710938] [adversarial loss: 0.868264, acc: 0.343750]\n",
      "6610: [discriminator loss: 0.492830, acc: 0.742188] [adversarial loss: 1.317052, acc: 0.171875]\n",
      "6611: [discriminator loss: 0.505109, acc: 0.742188] [adversarial loss: 1.041625, acc: 0.218750]\n",
      "6612: [discriminator loss: 0.566040, acc: 0.687500] [adversarial loss: 1.205436, acc: 0.296875]\n",
      "6613: [discriminator loss: 0.541504, acc: 0.671875] [adversarial loss: 1.202475, acc: 0.265625]\n",
      "6614: [discriminator loss: 0.532343, acc: 0.710938] [adversarial loss: 1.728616, acc: 0.062500]\n",
      "6615: [discriminator loss: 0.540539, acc: 0.687500] [adversarial loss: 0.997768, acc: 0.296875]\n",
      "6616: [discriminator loss: 0.536197, acc: 0.695312] [adversarial loss: 1.238523, acc: 0.171875]\n",
      "6617: [discriminator loss: 0.527983, acc: 0.726562] [adversarial loss: 1.157782, acc: 0.250000]\n",
      "6618: [discriminator loss: 0.523896, acc: 0.781250] [adversarial loss: 1.484043, acc: 0.078125]\n",
      "6619: [discriminator loss: 0.570148, acc: 0.695312] [adversarial loss: 1.185541, acc: 0.218750]\n",
      "6620: [discriminator loss: 0.431075, acc: 0.851562] [adversarial loss: 0.955763, acc: 0.390625]\n",
      "6621: [discriminator loss: 0.488102, acc: 0.757812] [adversarial loss: 1.490484, acc: 0.093750]\n",
      "6622: [discriminator loss: 0.527713, acc: 0.703125] [adversarial loss: 1.108968, acc: 0.328125]\n",
      "6623: [discriminator loss: 0.525724, acc: 0.734375] [adversarial loss: 1.294538, acc: 0.109375]\n",
      "6624: [discriminator loss: 0.508008, acc: 0.757812] [adversarial loss: 0.971788, acc: 0.359375]\n",
      "6625: [discriminator loss: 0.565861, acc: 0.687500] [adversarial loss: 1.807075, acc: 0.140625]\n",
      "6626: [discriminator loss: 0.584664, acc: 0.664062] [adversarial loss: 0.801957, acc: 0.515625]\n",
      "6627: [discriminator loss: 0.483203, acc: 0.765625] [adversarial loss: 1.598430, acc: 0.062500]\n",
      "6628: [discriminator loss: 0.589220, acc: 0.617188] [adversarial loss: 0.794271, acc: 0.437500]\n",
      "6629: [discriminator loss: 0.557557, acc: 0.656250] [adversarial loss: 1.226097, acc: 0.250000]\n",
      "6630: [discriminator loss: 0.585826, acc: 0.679688] [adversarial loss: 1.196910, acc: 0.250000]\n",
      "6631: [discriminator loss: 0.509892, acc: 0.718750] [adversarial loss: 1.142903, acc: 0.265625]\n",
      "6632: [discriminator loss: 0.617368, acc: 0.640625] [adversarial loss: 1.045404, acc: 0.250000]\n",
      "6633: [discriminator loss: 0.555751, acc: 0.695312] [adversarial loss: 1.147928, acc: 0.140625]\n",
      "6634: [discriminator loss: 0.507561, acc: 0.710938] [adversarial loss: 0.728972, acc: 0.562500]\n",
      "6635: [discriminator loss: 0.568573, acc: 0.664062] [adversarial loss: 1.577539, acc: 0.078125]\n",
      "6636: [discriminator loss: 0.526620, acc: 0.710938] [adversarial loss: 1.238268, acc: 0.234375]\n",
      "6637: [discriminator loss: 0.474360, acc: 0.757812] [adversarial loss: 1.279399, acc: 0.234375]\n",
      "6638: [discriminator loss: 0.530332, acc: 0.750000] [adversarial loss: 0.979395, acc: 0.375000]\n",
      "6639: [discriminator loss: 0.522288, acc: 0.726562] [adversarial loss: 1.418080, acc: 0.250000]\n",
      "6640: [discriminator loss: 0.518847, acc: 0.734375] [adversarial loss: 1.190995, acc: 0.234375]\n",
      "6641: [discriminator loss: 0.530309, acc: 0.734375] [adversarial loss: 1.484961, acc: 0.093750]\n",
      "6642: [discriminator loss: 0.556615, acc: 0.703125] [adversarial loss: 0.959356, acc: 0.359375]\n",
      "6643: [discriminator loss: 0.624816, acc: 0.640625] [adversarial loss: 1.432891, acc: 0.125000]\n",
      "6644: [discriminator loss: 0.567044, acc: 0.726562] [adversarial loss: 1.007398, acc: 0.375000]\n",
      "6645: [discriminator loss: 0.552019, acc: 0.734375] [adversarial loss: 1.604392, acc: 0.140625]\n",
      "6646: [discriminator loss: 0.587650, acc: 0.687500] [adversarial loss: 0.966664, acc: 0.328125]\n",
      "6647: [discriminator loss: 0.465015, acc: 0.789062] [adversarial loss: 1.397452, acc: 0.265625]\n",
      "6648: [discriminator loss: 0.533254, acc: 0.726562] [adversarial loss: 1.170973, acc: 0.281250]\n",
      "6649: [discriminator loss: 0.594656, acc: 0.664062] [adversarial loss: 1.347518, acc: 0.140625]\n",
      "6650: [discriminator loss: 0.521529, acc: 0.734375] [adversarial loss: 1.184908, acc: 0.218750]\n",
      "6651: [discriminator loss: 0.478725, acc: 0.804688] [adversarial loss: 1.241512, acc: 0.171875]\n",
      "6652: [discriminator loss: 0.605828, acc: 0.664062] [adversarial loss: 1.156615, acc: 0.296875]\n",
      "6653: [discriminator loss: 0.553205, acc: 0.703125] [adversarial loss: 1.231634, acc: 0.250000]\n",
      "6654: [discriminator loss: 0.526326, acc: 0.734375] [adversarial loss: 0.855515, acc: 0.453125]\n",
      "6655: [discriminator loss: 0.558294, acc: 0.703125] [adversarial loss: 1.266959, acc: 0.234375]\n",
      "6656: [discriminator loss: 0.555947, acc: 0.718750] [adversarial loss: 1.088681, acc: 0.312500]\n",
      "6657: [discriminator loss: 0.555900, acc: 0.742188] [adversarial loss: 1.323907, acc: 0.140625]\n",
      "6658: [discriminator loss: 0.485679, acc: 0.804688] [adversarial loss: 1.183068, acc: 0.203125]\n",
      "6659: [discriminator loss: 0.567407, acc: 0.695312] [adversarial loss: 1.300976, acc: 0.250000]\n",
      "6660: [discriminator loss: 0.535735, acc: 0.757812] [adversarial loss: 0.740651, acc: 0.531250]\n",
      "6661: [discriminator loss: 0.559426, acc: 0.695312] [adversarial loss: 1.454998, acc: 0.062500]\n",
      "6662: [discriminator loss: 0.599746, acc: 0.687500] [adversarial loss: 1.002481, acc: 0.359375]\n",
      "6663: [discriminator loss: 0.545708, acc: 0.695312] [adversarial loss: 1.468969, acc: 0.156250]\n",
      "6664: [discriminator loss: 0.506265, acc: 0.718750] [adversarial loss: 1.142173, acc: 0.234375]\n",
      "6665: [discriminator loss: 0.506186, acc: 0.796875] [adversarial loss: 1.378323, acc: 0.156250]\n",
      "6666: [discriminator loss: 0.525581, acc: 0.710938] [adversarial loss: 0.905517, acc: 0.453125]\n",
      "6667: [discriminator loss: 0.661259, acc: 0.609375] [adversarial loss: 1.391463, acc: 0.203125]\n",
      "6668: [discriminator loss: 0.502217, acc: 0.742188] [adversarial loss: 1.044054, acc: 0.359375]\n",
      "6669: [discriminator loss: 0.576459, acc: 0.656250] [adversarial loss: 1.090266, acc: 0.203125]\n",
      "6670: [discriminator loss: 0.492925, acc: 0.773438] [adversarial loss: 1.381782, acc: 0.171875]\n",
      "6671: [discriminator loss: 0.525677, acc: 0.750000] [adversarial loss: 1.066868, acc: 0.312500]\n",
      "6672: [discriminator loss: 0.523796, acc: 0.757812] [adversarial loss: 1.452405, acc: 0.125000]\n",
      "6673: [discriminator loss: 0.538407, acc: 0.742188] [adversarial loss: 1.105301, acc: 0.296875]\n",
      "6674: [discriminator loss: 0.502253, acc: 0.734375] [adversarial loss: 1.331156, acc: 0.218750]\n",
      "6675: [discriminator loss: 0.445062, acc: 0.757812] [adversarial loss: 1.492705, acc: 0.125000]\n",
      "6676: [discriminator loss: 0.524610, acc: 0.718750] [adversarial loss: 1.216117, acc: 0.250000]\n",
      "6677: [discriminator loss: 0.503383, acc: 0.742188] [adversarial loss: 1.146777, acc: 0.203125]\n",
      "6678: [discriminator loss: 0.493841, acc: 0.773438] [adversarial loss: 0.970090, acc: 0.328125]\n",
      "6679: [discriminator loss: 0.609174, acc: 0.687500] [adversarial loss: 1.440312, acc: 0.156250]\n",
      "6680: [discriminator loss: 0.541612, acc: 0.757812] [adversarial loss: 1.122006, acc: 0.281250]\n",
      "6681: [discriminator loss: 0.600378, acc: 0.679688] [adversarial loss: 1.193546, acc: 0.250000]\n",
      "6682: [discriminator loss: 0.572094, acc: 0.726562] [adversarial loss: 1.268094, acc: 0.156250]\n",
      "6683: [discriminator loss: 0.567236, acc: 0.687500] [adversarial loss: 1.765910, acc: 0.093750]\n",
      "6684: [discriminator loss: 0.490329, acc: 0.750000] [adversarial loss: 0.903888, acc: 0.281250]\n",
      "6685: [discriminator loss: 0.561330, acc: 0.742188] [adversarial loss: 1.370884, acc: 0.234375]\n",
      "6686: [discriminator loss: 0.589043, acc: 0.687500] [adversarial loss: 1.013732, acc: 0.296875]\n",
      "6687: [discriminator loss: 0.583378, acc: 0.679688] [adversarial loss: 1.197911, acc: 0.203125]\n",
      "6688: [discriminator loss: 0.482698, acc: 0.757812] [adversarial loss: 1.248712, acc: 0.250000]\n",
      "6689: [discriminator loss: 0.500552, acc: 0.757812] [adversarial loss: 1.035460, acc: 0.328125]\n",
      "6690: [discriminator loss: 0.550256, acc: 0.726562] [adversarial loss: 1.736952, acc: 0.156250]\n",
      "6691: [discriminator loss: 0.538750, acc: 0.687500] [adversarial loss: 0.978971, acc: 0.343750]\n",
      "6692: [discriminator loss: 0.564304, acc: 0.687500] [adversarial loss: 1.254700, acc: 0.187500]\n",
      "6693: [discriminator loss: 0.553339, acc: 0.664062] [adversarial loss: 1.108915, acc: 0.234375]\n",
      "6694: [discriminator loss: 0.594067, acc: 0.656250] [adversarial loss: 1.158507, acc: 0.343750]\n",
      "6695: [discriminator loss: 0.524340, acc: 0.726562] [adversarial loss: 1.218202, acc: 0.281250]\n",
      "6696: [discriminator loss: 0.476656, acc: 0.789062] [adversarial loss: 1.620267, acc: 0.093750]\n",
      "6697: [discriminator loss: 0.503755, acc: 0.726562] [adversarial loss: 0.886208, acc: 0.468750]\n",
      "6698: [discriminator loss: 0.555042, acc: 0.695312] [adversarial loss: 1.646168, acc: 0.109375]\n",
      "6699: [discriminator loss: 0.614221, acc: 0.632812] [adversarial loss: 0.799128, acc: 0.453125]\n",
      "6700: [discriminator loss: 0.603628, acc: 0.664062] [adversarial loss: 1.539800, acc: 0.062500]\n",
      "6701: [discriminator loss: 0.543692, acc: 0.734375] [adversarial loss: 1.081886, acc: 0.281250]\n",
      "6702: [discriminator loss: 0.530963, acc: 0.718750] [adversarial loss: 1.623548, acc: 0.062500]\n",
      "6703: [discriminator loss: 0.500145, acc: 0.742188] [adversarial loss: 1.129192, acc: 0.281250]\n",
      "6704: [discriminator loss: 0.642833, acc: 0.656250] [adversarial loss: 1.392115, acc: 0.203125]\n",
      "6705: [discriminator loss: 0.467909, acc: 0.796875] [adversarial loss: 1.262999, acc: 0.234375]\n",
      "6706: [discriminator loss: 0.586321, acc: 0.679688] [adversarial loss: 1.139727, acc: 0.281250]\n",
      "6707: [discriminator loss: 0.541211, acc: 0.734375] [adversarial loss: 1.271742, acc: 0.171875]\n",
      "6708: [discriminator loss: 0.590023, acc: 0.703125] [adversarial loss: 1.059559, acc: 0.343750]\n",
      "6709: [discriminator loss: 0.554561, acc: 0.726562] [adversarial loss: 1.543567, acc: 0.140625]\n",
      "6710: [discriminator loss: 0.592807, acc: 0.695312] [adversarial loss: 0.787612, acc: 0.453125]\n",
      "6711: [discriminator loss: 0.627484, acc: 0.687500] [adversarial loss: 1.585828, acc: 0.046875]\n",
      "6712: [discriminator loss: 0.568071, acc: 0.687500] [adversarial loss: 0.957438, acc: 0.390625]\n",
      "6713: [discriminator loss: 0.537240, acc: 0.734375] [adversarial loss: 1.522096, acc: 0.156250]\n",
      "6714: [discriminator loss: 0.534228, acc: 0.726562] [adversarial loss: 0.931683, acc: 0.343750]\n",
      "6715: [discriminator loss: 0.581041, acc: 0.648438] [adversarial loss: 1.033686, acc: 0.296875]\n",
      "6716: [discriminator loss: 0.566548, acc: 0.695312] [adversarial loss: 1.164893, acc: 0.203125]\n",
      "6717: [discriminator loss: 0.501307, acc: 0.757812] [adversarial loss: 1.276417, acc: 0.140625]\n",
      "6718: [discriminator loss: 0.484734, acc: 0.781250] [adversarial loss: 1.162962, acc: 0.218750]\n",
      "6719: [discriminator loss: 0.539071, acc: 0.703125] [adversarial loss: 0.964066, acc: 0.421875]\n",
      "6720: [discriminator loss: 0.556750, acc: 0.695312] [adversarial loss: 1.348571, acc: 0.125000]\n",
      "6721: [discriminator loss: 0.481841, acc: 0.765625] [adversarial loss: 0.849205, acc: 0.390625]\n",
      "6722: [discriminator loss: 0.538422, acc: 0.773438] [adversarial loss: 1.432375, acc: 0.140625]\n",
      "6723: [discriminator loss: 0.544448, acc: 0.679688] [adversarial loss: 0.886871, acc: 0.484375]\n",
      "6724: [discriminator loss: 0.547064, acc: 0.726562] [adversarial loss: 1.652744, acc: 0.140625]\n",
      "6725: [discriminator loss: 0.555099, acc: 0.695312] [adversarial loss: 0.995086, acc: 0.359375]\n",
      "6726: [discriminator loss: 0.491222, acc: 0.773438] [adversarial loss: 1.015165, acc: 0.375000]\n",
      "6727: [discriminator loss: 0.528866, acc: 0.757812] [adversarial loss: 1.475621, acc: 0.125000]\n",
      "6728: [discriminator loss: 0.512940, acc: 0.734375] [adversarial loss: 0.927243, acc: 0.312500]\n",
      "6729: [discriminator loss: 0.559496, acc: 0.703125] [adversarial loss: 1.590719, acc: 0.093750]\n",
      "6730: [discriminator loss: 0.442506, acc: 0.789062] [adversarial loss: 1.130034, acc: 0.281250]\n",
      "6731: [discriminator loss: 0.482487, acc: 0.796875] [adversarial loss: 1.479680, acc: 0.187500]\n",
      "6732: [discriminator loss: 0.463659, acc: 0.820312] [adversarial loss: 1.271928, acc: 0.218750]\n",
      "6733: [discriminator loss: 0.607943, acc: 0.656250] [adversarial loss: 1.152716, acc: 0.281250]\n",
      "6734: [discriminator loss: 0.499872, acc: 0.734375] [adversarial loss: 1.177494, acc: 0.218750]\n",
      "6735: [discriminator loss: 0.458006, acc: 0.804688] [adversarial loss: 1.033934, acc: 0.250000]\n",
      "6736: [discriminator loss: 0.538712, acc: 0.734375] [adversarial loss: 0.842793, acc: 0.453125]\n",
      "6737: [discriminator loss: 0.511774, acc: 0.742188] [adversarial loss: 1.551289, acc: 0.125000]\n",
      "6738: [discriminator loss: 0.656891, acc: 0.625000] [adversarial loss: 0.609789, acc: 0.656250]\n",
      "6739: [discriminator loss: 0.690290, acc: 0.656250] [adversarial loss: 1.276479, acc: 0.218750]\n",
      "6740: [discriminator loss: 0.479961, acc: 0.796875] [adversarial loss: 1.058693, acc: 0.281250]\n",
      "6741: [discriminator loss: 0.543527, acc: 0.742188] [adversarial loss: 1.292638, acc: 0.203125]\n",
      "6742: [discriminator loss: 0.549076, acc: 0.695312] [adversarial loss: 0.774694, acc: 0.500000]\n",
      "6743: [discriminator loss: 0.587705, acc: 0.656250] [adversarial loss: 1.256292, acc: 0.187500]\n",
      "6744: [discriminator loss: 0.514953, acc: 0.726562] [adversarial loss: 1.253631, acc: 0.140625]\n",
      "6745: [discriminator loss: 0.436267, acc: 0.796875] [adversarial loss: 1.197493, acc: 0.187500]\n",
      "6746: [discriminator loss: 0.548109, acc: 0.710938] [adversarial loss: 1.289849, acc: 0.218750]\n",
      "6747: [discriminator loss: 0.601740, acc: 0.664062] [adversarial loss: 1.142396, acc: 0.250000]\n",
      "6748: [discriminator loss: 0.512716, acc: 0.742188] [adversarial loss: 1.095324, acc: 0.312500]\n",
      "6749: [discriminator loss: 0.601416, acc: 0.664062] [adversarial loss: 0.922776, acc: 0.468750]\n",
      "6750: [discriminator loss: 0.562031, acc: 0.703125] [adversarial loss: 1.540055, acc: 0.125000]\n",
      "6751: [discriminator loss: 0.589584, acc: 0.687500] [adversarial loss: 0.757551, acc: 0.500000]\n",
      "6752: [discriminator loss: 0.529233, acc: 0.734375] [adversarial loss: 1.521841, acc: 0.156250]\n",
      "6753: [discriminator loss: 0.601758, acc: 0.664062] [adversarial loss: 0.826415, acc: 0.484375]\n",
      "6754: [discriminator loss: 0.602495, acc: 0.632812] [adversarial loss: 1.404046, acc: 0.171875]\n",
      "6755: [discriminator loss: 0.577704, acc: 0.640625] [adversarial loss: 1.039689, acc: 0.343750]\n",
      "6756: [discriminator loss: 0.579549, acc: 0.664062] [adversarial loss: 1.118755, acc: 0.296875]\n",
      "6757: [discriminator loss: 0.484180, acc: 0.796875] [adversarial loss: 1.308797, acc: 0.203125]\n",
      "6758: [discriminator loss: 0.521926, acc: 0.734375] [adversarial loss: 1.320948, acc: 0.218750]\n",
      "6759: [discriminator loss: 0.556181, acc: 0.726562] [adversarial loss: 1.130366, acc: 0.218750]\n",
      "6760: [discriminator loss: 0.523267, acc: 0.710938] [adversarial loss: 1.288355, acc: 0.250000]\n",
      "6761: [discriminator loss: 0.480178, acc: 0.828125] [adversarial loss: 1.155795, acc: 0.171875]\n",
      "6762: [discriminator loss: 0.530106, acc: 0.703125] [adversarial loss: 1.174994, acc: 0.250000]\n",
      "6763: [discriminator loss: 0.502434, acc: 0.718750] [adversarial loss: 1.294331, acc: 0.250000]\n",
      "6764: [discriminator loss: 0.552984, acc: 0.710938] [adversarial loss: 1.117078, acc: 0.265625]\n",
      "6765: [discriminator loss: 0.490921, acc: 0.742188] [adversarial loss: 1.351865, acc: 0.156250]\n",
      "6766: [discriminator loss: 0.518642, acc: 0.718750] [adversarial loss: 1.104907, acc: 0.203125]\n",
      "6767: [discriminator loss: 0.484093, acc: 0.773438] [adversarial loss: 1.262940, acc: 0.125000]\n",
      "6768: [discriminator loss: 0.527274, acc: 0.718750] [adversarial loss: 1.260086, acc: 0.218750]\n",
      "6769: [discriminator loss: 0.590913, acc: 0.671875] [adversarial loss: 1.161609, acc: 0.218750]\n",
      "6770: [discriminator loss: 0.549861, acc: 0.703125] [adversarial loss: 1.171676, acc: 0.250000]\n",
      "6771: [discriminator loss: 0.562496, acc: 0.687500] [adversarial loss: 1.201666, acc: 0.203125]\n",
      "6772: [discriminator loss: 0.535503, acc: 0.765625] [adversarial loss: 1.370795, acc: 0.218750]\n",
      "6773: [discriminator loss: 0.501976, acc: 0.750000] [adversarial loss: 1.324607, acc: 0.140625]\n",
      "6774: [discriminator loss: 0.492360, acc: 0.742188] [adversarial loss: 1.167958, acc: 0.281250]\n",
      "6775: [discriminator loss: 0.515719, acc: 0.710938] [adversarial loss: 1.061215, acc: 0.234375]\n",
      "6776: [discriminator loss: 0.449746, acc: 0.789062] [adversarial loss: 1.402840, acc: 0.187500]\n",
      "6777: [discriminator loss: 0.542436, acc: 0.718750] [adversarial loss: 1.150238, acc: 0.250000]\n",
      "6778: [discriminator loss: 0.513007, acc: 0.703125] [adversarial loss: 1.332782, acc: 0.187500]\n",
      "6779: [discriminator loss: 0.520617, acc: 0.742188] [adversarial loss: 0.948225, acc: 0.359375]\n",
      "6780: [discriminator loss: 0.544294, acc: 0.710938] [adversarial loss: 1.814242, acc: 0.000000]\n",
      "6781: [discriminator loss: 0.604586, acc: 0.656250] [adversarial loss: 0.759507, acc: 0.515625]\n",
      "6782: [discriminator loss: 0.607272, acc: 0.648438] [adversarial loss: 1.655864, acc: 0.078125]\n",
      "6783: [discriminator loss: 0.572481, acc: 0.734375] [adversarial loss: 1.013062, acc: 0.375000]\n",
      "6784: [discriminator loss: 0.484630, acc: 0.734375] [adversarial loss: 1.434111, acc: 0.125000]\n",
      "6785: [discriminator loss: 0.535901, acc: 0.734375] [adversarial loss: 0.958897, acc: 0.343750]\n",
      "6786: [discriminator loss: 0.601397, acc: 0.726562] [adversarial loss: 1.461349, acc: 0.109375]\n",
      "6787: [discriminator loss: 0.511733, acc: 0.757812] [adversarial loss: 0.981919, acc: 0.343750]\n",
      "6788: [discriminator loss: 0.585288, acc: 0.718750] [adversarial loss: 1.502817, acc: 0.140625]\n",
      "6789: [discriminator loss: 0.530220, acc: 0.734375] [adversarial loss: 1.161519, acc: 0.250000]\n",
      "6790: [discriminator loss: 0.505247, acc: 0.773438] [adversarial loss: 1.210528, acc: 0.281250]\n",
      "6791: [discriminator loss: 0.523759, acc: 0.718750] [adversarial loss: 1.279932, acc: 0.218750]\n",
      "6792: [discriminator loss: 0.510743, acc: 0.726562] [adversarial loss: 1.455293, acc: 0.093750]\n",
      "6793: [discriminator loss: 0.494634, acc: 0.734375] [adversarial loss: 1.074268, acc: 0.359375]\n",
      "6794: [discriminator loss: 0.490615, acc: 0.742188] [adversarial loss: 1.502505, acc: 0.109375]\n",
      "6795: [discriminator loss: 0.522023, acc: 0.757812] [adversarial loss: 0.859587, acc: 0.468750]\n",
      "6796: [discriminator loss: 0.553109, acc: 0.679688] [adversarial loss: 1.820849, acc: 0.078125]\n",
      "6797: [discriminator loss: 0.505134, acc: 0.687500] [adversarial loss: 1.076622, acc: 0.328125]\n",
      "6798: [discriminator loss: 0.511279, acc: 0.765625] [adversarial loss: 1.530623, acc: 0.078125]\n",
      "6799: [discriminator loss: 0.585645, acc: 0.664062] [adversarial loss: 0.905837, acc: 0.421875]\n",
      "6800: [discriminator loss: 0.600564, acc: 0.679688] [adversarial loss: 1.581374, acc: 0.125000]\n",
      "6801: [discriminator loss: 0.499506, acc: 0.750000] [adversarial loss: 1.116171, acc: 0.296875]\n",
      "6802: [discriminator loss: 0.586169, acc: 0.718750] [adversarial loss: 1.233702, acc: 0.296875]\n",
      "6803: [discriminator loss: 0.528975, acc: 0.718750] [adversarial loss: 0.854422, acc: 0.437500]\n",
      "6804: [discriminator loss: 0.579918, acc: 0.695312] [adversarial loss: 1.414220, acc: 0.140625]\n",
      "6805: [discriminator loss: 0.489733, acc: 0.742188] [adversarial loss: 1.277740, acc: 0.187500]\n",
      "6806: [discriminator loss: 0.497949, acc: 0.750000] [adversarial loss: 1.098085, acc: 0.296875]\n",
      "6807: [discriminator loss: 0.626720, acc: 0.695312] [adversarial loss: 1.220465, acc: 0.250000]\n",
      "6808: [discriminator loss: 0.591602, acc: 0.695312] [adversarial loss: 1.028544, acc: 0.234375]\n",
      "6809: [discriminator loss: 0.531465, acc: 0.742188] [adversarial loss: 1.401682, acc: 0.156250]\n",
      "6810: [discriminator loss: 0.548932, acc: 0.757812] [adversarial loss: 1.053955, acc: 0.343750]\n",
      "6811: [discriminator loss: 0.565238, acc: 0.695312] [adversarial loss: 1.509904, acc: 0.125000]\n",
      "6812: [discriminator loss: 0.518453, acc: 0.742188] [adversarial loss: 1.345496, acc: 0.156250]\n",
      "6813: [discriminator loss: 0.536811, acc: 0.734375] [adversarial loss: 1.135360, acc: 0.312500]\n",
      "6814: [discriminator loss: 0.519225, acc: 0.718750] [adversarial loss: 1.007601, acc: 0.312500]\n",
      "6815: [discriminator loss: 0.530266, acc: 0.718750] [adversarial loss: 0.824887, acc: 0.453125]\n",
      "6816: [discriminator loss: 0.538369, acc: 0.789062] [adversarial loss: 1.330963, acc: 0.093750]\n",
      "6817: [discriminator loss: 0.476288, acc: 0.726562] [adversarial loss: 0.873378, acc: 0.453125]\n",
      "6818: [discriminator loss: 0.608246, acc: 0.640625] [adversarial loss: 1.488232, acc: 0.062500]\n",
      "6819: [discriminator loss: 0.582785, acc: 0.648438] [adversarial loss: 1.122425, acc: 0.328125]\n",
      "6820: [discriminator loss: 0.556180, acc: 0.726562] [adversarial loss: 1.494062, acc: 0.078125]\n",
      "6821: [discriminator loss: 0.449478, acc: 0.796875] [adversarial loss: 1.376331, acc: 0.218750]\n",
      "6822: [discriminator loss: 0.521091, acc: 0.742188] [adversarial loss: 1.104319, acc: 0.281250]\n",
      "6823: [discriminator loss: 0.559944, acc: 0.671875] [adversarial loss: 1.421169, acc: 0.234375]\n",
      "6824: [discriminator loss: 0.492266, acc: 0.734375] [adversarial loss: 1.029650, acc: 0.359375]\n",
      "6825: [discriminator loss: 0.572932, acc: 0.734375] [adversarial loss: 1.210248, acc: 0.187500]\n",
      "6826: [discriminator loss: 0.542080, acc: 0.710938] [adversarial loss: 1.071539, acc: 0.328125]\n",
      "6827: [discriminator loss: 0.544995, acc: 0.695312] [adversarial loss: 1.443048, acc: 0.203125]\n",
      "6828: [discriminator loss: 0.599787, acc: 0.656250] [adversarial loss: 0.883137, acc: 0.437500]\n",
      "6829: [discriminator loss: 0.500904, acc: 0.757812] [adversarial loss: 1.442017, acc: 0.140625]\n",
      "6830: [discriminator loss: 0.581962, acc: 0.656250] [adversarial loss: 0.876199, acc: 0.343750]\n",
      "6831: [discriminator loss: 0.522620, acc: 0.734375] [adversarial loss: 1.259545, acc: 0.203125]\n",
      "6832: [discriminator loss: 0.548298, acc: 0.734375] [adversarial loss: 1.345243, acc: 0.187500]\n",
      "6833: [discriminator loss: 0.479301, acc: 0.781250] [adversarial loss: 1.025872, acc: 0.328125]\n",
      "6834: [discriminator loss: 0.625529, acc: 0.617188] [adversarial loss: 1.275545, acc: 0.234375]\n",
      "6835: [discriminator loss: 0.521159, acc: 0.703125] [adversarial loss: 1.213741, acc: 0.234375]\n",
      "6836: [discriminator loss: 0.546315, acc: 0.718750] [adversarial loss: 1.049130, acc: 0.312500]\n",
      "6837: [discriminator loss: 0.534450, acc: 0.773438] [adversarial loss: 1.279148, acc: 0.234375]\n",
      "6838: [discriminator loss: 0.529057, acc: 0.742188] [adversarial loss: 0.996860, acc: 0.390625]\n",
      "6839: [discriminator loss: 0.535903, acc: 0.750000] [adversarial loss: 1.490458, acc: 0.062500]\n",
      "6840: [discriminator loss: 0.585510, acc: 0.679688] [adversarial loss: 0.752849, acc: 0.562500]\n",
      "6841: [discriminator loss: 0.577147, acc: 0.671875] [adversarial loss: 1.610733, acc: 0.078125]\n",
      "6842: [discriminator loss: 0.562886, acc: 0.695312] [adversarial loss: 0.855512, acc: 0.546875]\n",
      "6843: [discriminator loss: 0.599557, acc: 0.703125] [adversarial loss: 1.345518, acc: 0.156250]\n",
      "6844: [discriminator loss: 0.510668, acc: 0.734375] [adversarial loss: 1.273761, acc: 0.203125]\n",
      "6845: [discriminator loss: 0.481385, acc: 0.773438] [adversarial loss: 1.020593, acc: 0.359375]\n",
      "6846: [discriminator loss: 0.550168, acc: 0.687500] [adversarial loss: 1.132793, acc: 0.203125]\n",
      "6847: [discriminator loss: 0.547930, acc: 0.703125] [adversarial loss: 1.146506, acc: 0.328125]\n",
      "6848: [discriminator loss: 0.576493, acc: 0.703125] [adversarial loss: 1.233046, acc: 0.234375]\n",
      "6849: [discriminator loss: 0.484113, acc: 0.773438] [adversarial loss: 0.935552, acc: 0.375000]\n",
      "6850: [discriminator loss: 0.618742, acc: 0.664062] [adversarial loss: 1.468663, acc: 0.156250]\n",
      "6851: [discriminator loss: 0.525850, acc: 0.765625] [adversarial loss: 1.040289, acc: 0.250000]\n",
      "6852: [discriminator loss: 0.526826, acc: 0.734375] [adversarial loss: 1.576485, acc: 0.078125]\n",
      "6853: [discriminator loss: 0.528593, acc: 0.750000] [adversarial loss: 1.026602, acc: 0.359375]\n",
      "6854: [discriminator loss: 0.544759, acc: 0.695312] [adversarial loss: 1.232249, acc: 0.218750]\n",
      "6855: [discriminator loss: 0.497426, acc: 0.734375] [adversarial loss: 1.069605, acc: 0.250000]\n",
      "6856: [discriminator loss: 0.562678, acc: 0.742188] [adversarial loss: 1.202919, acc: 0.265625]\n",
      "6857: [discriminator loss: 0.525703, acc: 0.804688] [adversarial loss: 1.005994, acc: 0.375000]\n",
      "6858: [discriminator loss: 0.509086, acc: 0.773438] [adversarial loss: 1.201065, acc: 0.218750]\n",
      "6859: [discriminator loss: 0.537376, acc: 0.718750] [adversarial loss: 1.022515, acc: 0.312500]\n",
      "6860: [discriminator loss: 0.485322, acc: 0.742188] [adversarial loss: 1.103836, acc: 0.218750]\n",
      "6861: [discriminator loss: 0.492838, acc: 0.718750] [adversarial loss: 1.601165, acc: 0.093750]\n",
      "6862: [discriminator loss: 0.543547, acc: 0.718750] [adversarial loss: 0.748435, acc: 0.546875]\n",
      "6863: [discriminator loss: 0.655496, acc: 0.632812] [adversarial loss: 1.631266, acc: 0.078125]\n",
      "6864: [discriminator loss: 0.566971, acc: 0.703125] [adversarial loss: 0.938587, acc: 0.359375]\n",
      "6865: [discriminator loss: 0.605855, acc: 0.679688] [adversarial loss: 1.256046, acc: 0.234375]\n",
      "6866: [discriminator loss: 0.559811, acc: 0.726562] [adversarial loss: 1.189278, acc: 0.281250]\n",
      "6867: [discriminator loss: 0.569986, acc: 0.734375] [adversarial loss: 0.932259, acc: 0.359375]\n",
      "6868: [discriminator loss: 0.501613, acc: 0.773438] [adversarial loss: 1.449189, acc: 0.265625]\n",
      "6869: [discriminator loss: 0.566809, acc: 0.718750] [adversarial loss: 1.016454, acc: 0.343750]\n",
      "6870: [discriminator loss: 0.583132, acc: 0.695312] [adversarial loss: 1.257742, acc: 0.250000]\n",
      "6871: [discriminator loss: 0.531275, acc: 0.742188] [adversarial loss: 1.116180, acc: 0.281250]\n",
      "6872: [discriminator loss: 0.511652, acc: 0.687500] [adversarial loss: 1.022058, acc: 0.250000]\n",
      "6873: [discriminator loss: 0.619542, acc: 0.609375] [adversarial loss: 1.174908, acc: 0.234375]\n",
      "6874: [discriminator loss: 0.536986, acc: 0.718750] [adversarial loss: 1.386281, acc: 0.125000]\n",
      "6875: [discriminator loss: 0.565949, acc: 0.679688] [adversarial loss: 0.990278, acc: 0.359375]\n",
      "6876: [discriminator loss: 0.526633, acc: 0.742188] [adversarial loss: 1.553988, acc: 0.109375]\n",
      "6877: [discriminator loss: 0.601718, acc: 0.664062] [adversarial loss: 0.956893, acc: 0.390625]\n",
      "6878: [discriminator loss: 0.561056, acc: 0.695312] [adversarial loss: 1.151396, acc: 0.234375]\n",
      "6879: [discriminator loss: 0.552891, acc: 0.742188] [adversarial loss: 1.060471, acc: 0.375000]\n",
      "6880: [discriminator loss: 0.454274, acc: 0.804688] [adversarial loss: 1.421571, acc: 0.171875]\n",
      "6881: [discriminator loss: 0.496033, acc: 0.773438] [adversarial loss: 0.956945, acc: 0.421875]\n",
      "6882: [discriminator loss: 0.579060, acc: 0.687500] [adversarial loss: 1.528924, acc: 0.125000]\n",
      "6883: [discriminator loss: 0.518282, acc: 0.742188] [adversarial loss: 1.082142, acc: 0.296875]\n",
      "6884: [discriminator loss: 0.558100, acc: 0.718750] [adversarial loss: 1.330415, acc: 0.171875]\n",
      "6885: [discriminator loss: 0.575396, acc: 0.671875] [adversarial loss: 0.911216, acc: 0.421875]\n",
      "6886: [discriminator loss: 0.562892, acc: 0.671875] [adversarial loss: 1.394490, acc: 0.156250]\n",
      "6887: [discriminator loss: 0.568516, acc: 0.687500] [adversarial loss: 0.894323, acc: 0.390625]\n",
      "6888: [discriminator loss: 0.515598, acc: 0.750000] [adversarial loss: 1.760614, acc: 0.109375]\n",
      "6889: [discriminator loss: 0.555839, acc: 0.703125] [adversarial loss: 0.836313, acc: 0.484375]\n",
      "6890: [discriminator loss: 0.572404, acc: 0.726562] [adversarial loss: 1.367069, acc: 0.156250]\n",
      "6891: [discriminator loss: 0.543855, acc: 0.695312] [adversarial loss: 0.735688, acc: 0.484375]\n",
      "6892: [discriminator loss: 0.576120, acc: 0.679688] [adversarial loss: 1.532921, acc: 0.125000]\n",
      "6893: [discriminator loss: 0.566674, acc: 0.718750] [adversarial loss: 0.897858, acc: 0.453125]\n",
      "6894: [discriminator loss: 0.611269, acc: 0.679688] [adversarial loss: 1.371929, acc: 0.140625]\n",
      "6895: [discriminator loss: 0.519839, acc: 0.765625] [adversarial loss: 1.267437, acc: 0.171875]\n",
      "6896: [discriminator loss: 0.551428, acc: 0.710938] [adversarial loss: 1.010408, acc: 0.265625]\n",
      "6897: [discriminator loss: 0.602975, acc: 0.710938] [adversarial loss: 1.512772, acc: 0.062500]\n",
      "6898: [discriminator loss: 0.575290, acc: 0.695312] [adversarial loss: 1.031360, acc: 0.234375]\n",
      "6899: [discriminator loss: 0.531280, acc: 0.726562] [adversarial loss: 1.183383, acc: 0.203125]\n",
      "6900: [discriminator loss: 0.536271, acc: 0.718750] [adversarial loss: 1.574260, acc: 0.156250]\n",
      "6901: [discriminator loss: 0.629166, acc: 0.687500] [adversarial loss: 0.971951, acc: 0.406250]\n",
      "6902: [discriminator loss: 0.508201, acc: 0.718750] [adversarial loss: 1.336043, acc: 0.234375]\n",
      "6903: [discriminator loss: 0.543867, acc: 0.695312] [adversarial loss: 1.014820, acc: 0.281250]\n",
      "6904: [discriminator loss: 0.590504, acc: 0.648438] [adversarial loss: 1.393579, acc: 0.125000]\n",
      "6905: [discriminator loss: 0.556035, acc: 0.718750] [adversarial loss: 1.017932, acc: 0.312500]\n",
      "6906: [discriminator loss: 0.478238, acc: 0.757812] [adversarial loss: 1.015767, acc: 0.296875]\n",
      "6907: [discriminator loss: 0.527101, acc: 0.765625] [adversarial loss: 1.189717, acc: 0.156250]\n",
      "6908: [discriminator loss: 0.566694, acc: 0.718750] [adversarial loss: 1.134486, acc: 0.234375]\n",
      "6909: [discriminator loss: 0.535428, acc: 0.710938] [adversarial loss: 1.205384, acc: 0.203125]\n",
      "6910: [discriminator loss: 0.556248, acc: 0.734375] [adversarial loss: 1.218042, acc: 0.265625]\n",
      "6911: [discriminator loss: 0.488698, acc: 0.765625] [adversarial loss: 1.167871, acc: 0.281250]\n",
      "6912: [discriminator loss: 0.520865, acc: 0.718750] [adversarial loss: 1.026226, acc: 0.328125]\n",
      "6913: [discriminator loss: 0.539562, acc: 0.695312] [adversarial loss: 1.258224, acc: 0.250000]\n",
      "6914: [discriminator loss: 0.574274, acc: 0.718750] [adversarial loss: 1.004410, acc: 0.359375]\n",
      "6915: [discriminator loss: 0.567529, acc: 0.710938] [adversarial loss: 1.686152, acc: 0.093750]\n",
      "6916: [discriminator loss: 0.542249, acc: 0.734375] [adversarial loss: 1.056243, acc: 0.296875]\n",
      "6917: [discriminator loss: 0.538871, acc: 0.710938] [adversarial loss: 1.335396, acc: 0.156250]\n",
      "6918: [discriminator loss: 0.661078, acc: 0.664062] [adversarial loss: 0.745866, acc: 0.531250]\n",
      "6919: [discriminator loss: 0.588943, acc: 0.703125] [adversarial loss: 1.467895, acc: 0.109375]\n",
      "6920: [discriminator loss: 0.575150, acc: 0.687500] [adversarial loss: 0.930291, acc: 0.375000]\n",
      "6921: [discriminator loss: 0.510016, acc: 0.742188] [adversarial loss: 1.258967, acc: 0.265625]\n",
      "6922: [discriminator loss: 0.506150, acc: 0.804688] [adversarial loss: 1.249724, acc: 0.218750]\n",
      "6923: [discriminator loss: 0.464882, acc: 0.781250] [adversarial loss: 1.373791, acc: 0.187500]\n",
      "6924: [discriminator loss: 0.534312, acc: 0.734375] [adversarial loss: 1.109236, acc: 0.296875]\n",
      "6925: [discriminator loss: 0.508290, acc: 0.703125] [adversarial loss: 1.330676, acc: 0.125000]\n",
      "6926: [discriminator loss: 0.598865, acc: 0.703125] [adversarial loss: 1.403267, acc: 0.140625]\n",
      "6927: [discriminator loss: 0.571044, acc: 0.710938] [adversarial loss: 0.961926, acc: 0.421875]\n",
      "6928: [discriminator loss: 0.553361, acc: 0.703125] [adversarial loss: 1.746800, acc: 0.031250]\n",
      "6929: [discriminator loss: 0.540064, acc: 0.703125] [adversarial loss: 0.905312, acc: 0.375000]\n",
      "6930: [discriminator loss: 0.546624, acc: 0.703125] [adversarial loss: 1.283158, acc: 0.156250]\n",
      "6931: [discriminator loss: 0.558024, acc: 0.679688] [adversarial loss: 1.142250, acc: 0.234375]\n",
      "6932: [discriminator loss: 0.515477, acc: 0.765625] [adversarial loss: 1.282711, acc: 0.218750]\n",
      "6933: [discriminator loss: 0.493599, acc: 0.773438] [adversarial loss: 1.342837, acc: 0.156250]\n",
      "6934: [discriminator loss: 0.570996, acc: 0.695312] [adversarial loss: 1.240852, acc: 0.234375]\n",
      "6935: [discriminator loss: 0.501310, acc: 0.789062] [adversarial loss: 1.076389, acc: 0.328125]\n",
      "6936: [discriminator loss: 0.559093, acc: 0.703125] [adversarial loss: 1.696510, acc: 0.046875]\n",
      "6937: [discriminator loss: 0.581155, acc: 0.687500] [adversarial loss: 1.303803, acc: 0.171875]\n",
      "6938: [discriminator loss: 0.543945, acc: 0.679688] [adversarial loss: 1.352907, acc: 0.171875]\n",
      "6939: [discriminator loss: 0.469562, acc: 0.796875] [adversarial loss: 1.062742, acc: 0.281250]\n",
      "6940: [discriminator loss: 0.537184, acc: 0.710938] [adversarial loss: 1.514030, acc: 0.093750]\n",
      "6941: [discriminator loss: 0.596534, acc: 0.687500] [adversarial loss: 1.161299, acc: 0.234375]\n",
      "6942: [discriminator loss: 0.530722, acc: 0.710938] [adversarial loss: 1.269228, acc: 0.171875]\n",
      "6943: [discriminator loss: 0.534297, acc: 0.656250] [adversarial loss: 1.065183, acc: 0.296875]\n",
      "6944: [discriminator loss: 0.464964, acc: 0.781250] [adversarial loss: 1.342651, acc: 0.234375]\n",
      "6945: [discriminator loss: 0.498830, acc: 0.742188] [adversarial loss: 0.994942, acc: 0.390625]\n",
      "6946: [discriminator loss: 0.506649, acc: 0.695312] [adversarial loss: 1.290111, acc: 0.218750]\n",
      "6947: [discriminator loss: 0.583887, acc: 0.679688] [adversarial loss: 1.120852, acc: 0.250000]\n",
      "6948: [discriminator loss: 0.504665, acc: 0.773438] [adversarial loss: 1.672684, acc: 0.062500]\n",
      "6949: [discriminator loss: 0.595534, acc: 0.664062] [adversarial loss: 0.640281, acc: 0.578125]\n",
      "6950: [discriminator loss: 0.574758, acc: 0.656250] [adversarial loss: 1.362003, acc: 0.125000]\n",
      "6951: [discriminator loss: 0.554404, acc: 0.710938] [adversarial loss: 1.075383, acc: 0.218750]\n",
      "6952: [discriminator loss: 0.506359, acc: 0.781250] [adversarial loss: 1.096778, acc: 0.203125]\n",
      "6953: [discriminator loss: 0.462956, acc: 0.851562] [adversarial loss: 1.477274, acc: 0.171875]\n",
      "6954: [discriminator loss: 0.568130, acc: 0.695312] [adversarial loss: 0.946736, acc: 0.328125]\n",
      "6955: [discriminator loss: 0.624284, acc: 0.656250] [adversarial loss: 1.593605, acc: 0.078125]\n",
      "6956: [discriminator loss: 0.590659, acc: 0.679688] [adversarial loss: 0.896132, acc: 0.406250]\n",
      "6957: [discriminator loss: 0.560101, acc: 0.679688] [adversarial loss: 1.273918, acc: 0.187500]\n",
      "6958: [discriminator loss: 0.544743, acc: 0.726562] [adversarial loss: 1.306369, acc: 0.234375]\n",
      "6959: [discriminator loss: 0.465831, acc: 0.796875] [adversarial loss: 1.275868, acc: 0.156250]\n",
      "6960: [discriminator loss: 0.545892, acc: 0.710938] [adversarial loss: 1.008179, acc: 0.375000]\n",
      "6961: [discriminator loss: 0.503981, acc: 0.765625] [adversarial loss: 1.359019, acc: 0.125000]\n",
      "6962: [discriminator loss: 0.517767, acc: 0.734375] [adversarial loss: 1.303291, acc: 0.156250]\n",
      "6963: [discriminator loss: 0.587132, acc: 0.703125] [adversarial loss: 1.343852, acc: 0.171875]\n",
      "6964: [discriminator loss: 0.586622, acc: 0.695312] [adversarial loss: 0.889486, acc: 0.421875]\n",
      "6965: [discriminator loss: 0.522067, acc: 0.710938] [adversarial loss: 1.278456, acc: 0.265625]\n",
      "6966: [discriminator loss: 0.590327, acc: 0.695312] [adversarial loss: 1.174175, acc: 0.265625]\n",
      "6967: [discriminator loss: 0.576911, acc: 0.710938] [adversarial loss: 1.374469, acc: 0.125000]\n",
      "6968: [discriminator loss: 0.534272, acc: 0.710938] [adversarial loss: 1.045566, acc: 0.343750]\n",
      "6969: [discriminator loss: 0.560217, acc: 0.718750] [adversarial loss: 1.257252, acc: 0.171875]\n",
      "6970: [discriminator loss: 0.635691, acc: 0.664062] [adversarial loss: 1.248434, acc: 0.218750]\n",
      "6971: [discriminator loss: 0.528991, acc: 0.718750] [adversarial loss: 1.252738, acc: 0.156250]\n",
      "6972: [discriminator loss: 0.517168, acc: 0.726562] [adversarial loss: 1.272950, acc: 0.109375]\n",
      "6973: [discriminator loss: 0.512549, acc: 0.742188] [adversarial loss: 0.907533, acc: 0.437500]\n",
      "6974: [discriminator loss: 0.490147, acc: 0.765625] [adversarial loss: 1.403363, acc: 0.140625]\n",
      "6975: [discriminator loss: 0.565823, acc: 0.703125] [adversarial loss: 0.920234, acc: 0.453125]\n",
      "6976: [discriminator loss: 0.538512, acc: 0.703125] [adversarial loss: 1.688164, acc: 0.093750]\n",
      "6977: [discriminator loss: 0.578688, acc: 0.742188] [adversarial loss: 0.930714, acc: 0.406250]\n",
      "6978: [discriminator loss: 0.550355, acc: 0.773438] [adversarial loss: 1.331378, acc: 0.187500]\n",
      "6979: [discriminator loss: 0.554006, acc: 0.718750] [adversarial loss: 1.046056, acc: 0.328125]\n",
      "6980: [discriminator loss: 0.647390, acc: 0.601562] [adversarial loss: 1.206613, acc: 0.234375]\n",
      "6981: [discriminator loss: 0.548389, acc: 0.710938] [adversarial loss: 0.978743, acc: 0.375000]\n",
      "6982: [discriminator loss: 0.596777, acc: 0.656250] [adversarial loss: 1.476012, acc: 0.109375]\n",
      "6983: [discriminator loss: 0.541821, acc: 0.687500] [adversarial loss: 1.058785, acc: 0.250000]\n",
      "6984: [discriminator loss: 0.576923, acc: 0.679688] [adversarial loss: 1.593311, acc: 0.171875]\n",
      "6985: [discriminator loss: 0.593428, acc: 0.679688] [adversarial loss: 0.832170, acc: 0.406250]\n",
      "6986: [discriminator loss: 0.580390, acc: 0.664062] [adversarial loss: 1.384171, acc: 0.187500]\n",
      "6987: [discriminator loss: 0.601185, acc: 0.703125] [adversarial loss: 0.896735, acc: 0.406250]\n",
      "6988: [discriminator loss: 0.589998, acc: 0.703125] [adversarial loss: 1.315086, acc: 0.187500]\n",
      "6989: [discriminator loss: 0.530835, acc: 0.718750] [adversarial loss: 1.042856, acc: 0.312500]\n",
      "6990: [discriminator loss: 0.591674, acc: 0.695312] [adversarial loss: 1.237604, acc: 0.265625]\n",
      "6991: [discriminator loss: 0.547682, acc: 0.679688] [adversarial loss: 1.124366, acc: 0.296875]\n",
      "6992: [discriminator loss: 0.486832, acc: 0.773438] [adversarial loss: 1.282434, acc: 0.140625]\n",
      "6993: [discriminator loss: 0.592669, acc: 0.656250] [adversarial loss: 1.181139, acc: 0.250000]\n",
      "6994: [discriminator loss: 0.512533, acc: 0.765625] [adversarial loss: 1.060001, acc: 0.250000]\n",
      "6995: [discriminator loss: 0.578033, acc: 0.687500] [adversarial loss: 1.413731, acc: 0.125000]\n",
      "6996: [discriminator loss: 0.610998, acc: 0.632812] [adversarial loss: 1.000840, acc: 0.437500]\n",
      "6997: [discriminator loss: 0.584074, acc: 0.679688] [adversarial loss: 1.464909, acc: 0.109375]\n",
      "6998: [discriminator loss: 0.528712, acc: 0.726562] [adversarial loss: 1.102049, acc: 0.312500]\n",
      "6999: [discriminator loss: 0.542866, acc: 0.687500] [adversarial loss: 1.138026, acc: 0.265625]\n",
      "7000: [discriminator loss: 0.504357, acc: 0.757812] [adversarial loss: 1.183029, acc: 0.203125]\n",
      "7001: [discriminator loss: 0.589414, acc: 0.710938] [adversarial loss: 1.117473, acc: 0.171875]\n",
      "7002: [discriminator loss: 0.564967, acc: 0.710938] [adversarial loss: 1.020674, acc: 0.359375]\n",
      "7003: [discriminator loss: 0.543122, acc: 0.726562] [adversarial loss: 1.249469, acc: 0.125000]\n",
      "7004: [discriminator loss: 0.516655, acc: 0.710938] [adversarial loss: 0.975474, acc: 0.312500]\n",
      "7005: [discriminator loss: 0.590014, acc: 0.695312] [adversarial loss: 1.496342, acc: 0.156250]\n",
      "7006: [discriminator loss: 0.601621, acc: 0.632812] [adversarial loss: 0.901493, acc: 0.375000]\n",
      "7007: [discriminator loss: 0.684896, acc: 0.640625] [adversarial loss: 1.616186, acc: 0.062500]\n",
      "7008: [discriminator loss: 0.581188, acc: 0.695312] [adversarial loss: 0.869815, acc: 0.437500]\n",
      "7009: [discriminator loss: 0.536500, acc: 0.773438] [adversarial loss: 1.306972, acc: 0.171875]\n",
      "7010: [discriminator loss: 0.504136, acc: 0.765625] [adversarial loss: 0.964942, acc: 0.359375]\n",
      "7011: [discriminator loss: 0.537275, acc: 0.718750] [adversarial loss: 1.149424, acc: 0.234375]\n",
      "7012: [discriminator loss: 0.505717, acc: 0.742188] [adversarial loss: 1.115186, acc: 0.250000]\n",
      "7013: [discriminator loss: 0.544095, acc: 0.710938] [adversarial loss: 1.298357, acc: 0.171875]\n",
      "7014: [discriminator loss: 0.487702, acc: 0.726562] [adversarial loss: 1.108533, acc: 0.250000]\n",
      "7015: [discriminator loss: 0.514603, acc: 0.726562] [adversarial loss: 1.033561, acc: 0.296875]\n",
      "7016: [discriminator loss: 0.572805, acc: 0.710938] [adversarial loss: 1.370130, acc: 0.187500]\n",
      "7017: [discriminator loss: 0.524240, acc: 0.765625] [adversarial loss: 1.175890, acc: 0.296875]\n",
      "7018: [discriminator loss: 0.532207, acc: 0.726562] [adversarial loss: 1.215907, acc: 0.171875]\n",
      "7019: [discriminator loss: 0.547394, acc: 0.695312] [adversarial loss: 1.150596, acc: 0.296875]\n",
      "7020: [discriminator loss: 0.492147, acc: 0.773438] [adversarial loss: 1.279485, acc: 0.171875]\n",
      "7021: [discriminator loss: 0.496101, acc: 0.765625] [adversarial loss: 1.172348, acc: 0.218750]\n",
      "7022: [discriminator loss: 0.548178, acc: 0.679688] [adversarial loss: 1.590588, acc: 0.156250]\n",
      "7023: [discriminator loss: 0.525971, acc: 0.703125] [adversarial loss: 1.035240, acc: 0.312500]\n",
      "7024: [discriminator loss: 0.464342, acc: 0.796875] [adversarial loss: 1.103260, acc: 0.343750]\n",
      "7025: [discriminator loss: 0.635705, acc: 0.671875] [adversarial loss: 1.377706, acc: 0.171875]\n",
      "7026: [discriminator loss: 0.553489, acc: 0.726562] [adversarial loss: 1.052022, acc: 0.296875]\n",
      "7027: [discriminator loss: 0.551960, acc: 0.726562] [adversarial loss: 1.250044, acc: 0.281250]\n",
      "7028: [discriminator loss: 0.553173, acc: 0.742188] [adversarial loss: 1.031866, acc: 0.296875]\n",
      "7029: [discriminator loss: 0.506551, acc: 0.773438] [adversarial loss: 1.281098, acc: 0.140625]\n",
      "7030: [discriminator loss: 0.502956, acc: 0.734375] [adversarial loss: 1.039642, acc: 0.296875]\n",
      "7031: [discriminator loss: 0.497735, acc: 0.773438] [adversarial loss: 0.903584, acc: 0.421875]\n",
      "7032: [discriminator loss: 0.525286, acc: 0.726562] [adversarial loss: 1.287334, acc: 0.203125]\n",
      "7033: [discriminator loss: 0.610969, acc: 0.671875] [adversarial loss: 0.988479, acc: 0.375000]\n",
      "7034: [discriminator loss: 0.513310, acc: 0.742188] [adversarial loss: 1.485926, acc: 0.140625]\n",
      "7035: [discriminator loss: 0.549112, acc: 0.695312] [adversarial loss: 0.931404, acc: 0.359375]\n",
      "7036: [discriminator loss: 0.591624, acc: 0.679688] [adversarial loss: 1.583330, acc: 0.078125]\n",
      "7037: [discriminator loss: 0.538583, acc: 0.726562] [adversarial loss: 1.133333, acc: 0.281250]\n",
      "7038: [discriminator loss: 0.536687, acc: 0.695312] [adversarial loss: 1.536995, acc: 0.187500]\n",
      "7039: [discriminator loss: 0.565358, acc: 0.695312] [adversarial loss: 1.011868, acc: 0.328125]\n",
      "7040: [discriminator loss: 0.553680, acc: 0.726562] [adversarial loss: 1.349555, acc: 0.187500]\n",
      "7041: [discriminator loss: 0.508794, acc: 0.750000] [adversarial loss: 1.177833, acc: 0.281250]\n",
      "7042: [discriminator loss: 0.530391, acc: 0.726562] [adversarial loss: 0.934884, acc: 0.312500]\n",
      "7043: [discriminator loss: 0.530206, acc: 0.757812] [adversarial loss: 1.372009, acc: 0.125000]\n",
      "7044: [discriminator loss: 0.540867, acc: 0.710938] [adversarial loss: 1.013092, acc: 0.328125]\n",
      "7045: [discriminator loss: 0.547758, acc: 0.796875] [adversarial loss: 1.244922, acc: 0.140625]\n",
      "7046: [discriminator loss: 0.604860, acc: 0.671875] [adversarial loss: 1.367339, acc: 0.203125]\n",
      "7047: [discriminator loss: 0.509007, acc: 0.734375] [adversarial loss: 1.104050, acc: 0.265625]\n",
      "7048: [discriminator loss: 0.550268, acc: 0.703125] [adversarial loss: 1.214856, acc: 0.156250]\n",
      "7049: [discriminator loss: 0.534883, acc: 0.718750] [adversarial loss: 1.132665, acc: 0.265625]\n",
      "7050: [discriminator loss: 0.576821, acc: 0.695312] [adversarial loss: 1.416623, acc: 0.140625]\n",
      "7051: [discriminator loss: 0.614087, acc: 0.640625] [adversarial loss: 0.891738, acc: 0.437500]\n",
      "7052: [discriminator loss: 0.565127, acc: 0.671875] [adversarial loss: 1.209112, acc: 0.218750]\n",
      "7053: [discriminator loss: 0.554329, acc: 0.718750] [adversarial loss: 1.210494, acc: 0.218750]\n",
      "7054: [discriminator loss: 0.558756, acc: 0.757812] [adversarial loss: 1.101781, acc: 0.234375]\n",
      "7055: [discriminator loss: 0.498195, acc: 0.750000] [adversarial loss: 1.235834, acc: 0.234375]\n",
      "7056: [discriminator loss: 0.478483, acc: 0.765625] [adversarial loss: 1.070242, acc: 0.265625]\n",
      "7057: [discriminator loss: 0.556216, acc: 0.734375] [adversarial loss: 1.375903, acc: 0.140625]\n",
      "7058: [discriminator loss: 0.537235, acc: 0.710938] [adversarial loss: 1.070730, acc: 0.406250]\n",
      "7059: [discriminator loss: 0.523322, acc: 0.710938] [adversarial loss: 1.404664, acc: 0.156250]\n",
      "7060: [discriminator loss: 0.539536, acc: 0.726562] [adversarial loss: 1.111995, acc: 0.265625]\n",
      "7061: [discriminator loss: 0.581850, acc: 0.695312] [adversarial loss: 1.283665, acc: 0.218750]\n",
      "7062: [discriminator loss: 0.517133, acc: 0.757812] [adversarial loss: 0.814892, acc: 0.515625]\n",
      "7063: [discriminator loss: 0.576336, acc: 0.703125] [adversarial loss: 1.568245, acc: 0.156250]\n",
      "7064: [discriminator loss: 0.593282, acc: 0.710938] [adversarial loss: 0.717625, acc: 0.609375]\n",
      "7065: [discriminator loss: 0.611384, acc: 0.640625] [adversarial loss: 1.384853, acc: 0.078125]\n",
      "7066: [discriminator loss: 0.564083, acc: 0.648438] [adversarial loss: 1.189882, acc: 0.171875]\n",
      "7067: [discriminator loss: 0.504290, acc: 0.750000] [adversarial loss: 1.180747, acc: 0.250000]\n",
      "7068: [discriminator loss: 0.501295, acc: 0.812500] [adversarial loss: 1.008733, acc: 0.265625]\n",
      "7069: [discriminator loss: 0.484303, acc: 0.781250] [adversarial loss: 1.326550, acc: 0.109375]\n",
      "7070: [discriminator loss: 0.550735, acc: 0.710938] [adversarial loss: 1.077712, acc: 0.218750]\n",
      "7071: [discriminator loss: 0.536968, acc: 0.750000] [adversarial loss: 1.405583, acc: 0.140625]\n",
      "7072: [discriminator loss: 0.519951, acc: 0.734375] [adversarial loss: 1.049899, acc: 0.265625]\n",
      "7073: [discriminator loss: 0.529684, acc: 0.742188] [adversarial loss: 1.203994, acc: 0.171875]\n",
      "7074: [discriminator loss: 0.528961, acc: 0.765625] [adversarial loss: 1.383431, acc: 0.156250]\n",
      "7075: [discriminator loss: 0.540904, acc: 0.703125] [adversarial loss: 0.970897, acc: 0.343750]\n",
      "7076: [discriminator loss: 0.488941, acc: 0.796875] [adversarial loss: 1.347818, acc: 0.234375]\n",
      "7077: [discriminator loss: 0.598327, acc: 0.695312] [adversarial loss: 1.073448, acc: 0.359375]\n",
      "7078: [discriminator loss: 0.581517, acc: 0.687500] [adversarial loss: 1.527080, acc: 0.078125]\n",
      "7079: [discriminator loss: 0.533936, acc: 0.656250] [adversarial loss: 1.185885, acc: 0.234375]\n",
      "7080: [discriminator loss: 0.515866, acc: 0.742188] [adversarial loss: 1.300198, acc: 0.140625]\n",
      "7081: [discriminator loss: 0.574675, acc: 0.757812] [adversarial loss: 0.912675, acc: 0.437500]\n",
      "7082: [discriminator loss: 0.526486, acc: 0.750000] [adversarial loss: 1.359768, acc: 0.125000]\n",
      "7083: [discriminator loss: 0.539670, acc: 0.710938] [adversarial loss: 1.301441, acc: 0.203125]\n",
      "7084: [discriminator loss: 0.570199, acc: 0.671875] [adversarial loss: 1.136368, acc: 0.312500]\n",
      "7085: [discriminator loss: 0.553200, acc: 0.703125] [adversarial loss: 0.987147, acc: 0.406250]\n",
      "7086: [discriminator loss: 0.574495, acc: 0.718750] [adversarial loss: 1.477364, acc: 0.156250]\n",
      "7087: [discriminator loss: 0.551246, acc: 0.703125] [adversarial loss: 1.254514, acc: 0.125000]\n",
      "7088: [discriminator loss: 0.589909, acc: 0.671875] [adversarial loss: 1.287248, acc: 0.281250]\n",
      "7089: [discriminator loss: 0.552649, acc: 0.750000] [adversarial loss: 0.930735, acc: 0.453125]\n",
      "7090: [discriminator loss: 0.548820, acc: 0.710938] [adversarial loss: 1.069206, acc: 0.265625]\n",
      "7091: [discriminator loss: 0.501435, acc: 0.750000] [adversarial loss: 1.247314, acc: 0.187500]\n",
      "7092: [discriminator loss: 0.524271, acc: 0.734375] [adversarial loss: 1.123053, acc: 0.281250]\n",
      "7093: [discriminator loss: 0.528584, acc: 0.703125] [adversarial loss: 1.218683, acc: 0.218750]\n",
      "7094: [discriminator loss: 0.608951, acc: 0.656250] [adversarial loss: 1.004182, acc: 0.265625]\n",
      "7095: [discriminator loss: 0.634133, acc: 0.648438] [adversarial loss: 1.326027, acc: 0.203125]\n",
      "7096: [discriminator loss: 0.546411, acc: 0.734375] [adversarial loss: 0.903623, acc: 0.359375]\n",
      "7097: [discriminator loss: 0.515649, acc: 0.726562] [adversarial loss: 1.396013, acc: 0.062500]\n",
      "7098: [discriminator loss: 0.591937, acc: 0.632812] [adversarial loss: 0.949198, acc: 0.390625]\n",
      "7099: [discriminator loss: 0.610087, acc: 0.687500] [adversarial loss: 1.569252, acc: 0.140625]\n",
      "7100: [discriminator loss: 0.508410, acc: 0.750000] [adversarial loss: 1.056508, acc: 0.406250]\n",
      "7101: [discriminator loss: 0.586115, acc: 0.664062] [adversarial loss: 1.495543, acc: 0.109375]\n",
      "7102: [discriminator loss: 0.517598, acc: 0.695312] [adversarial loss: 1.040327, acc: 0.312500]\n",
      "7103: [discriminator loss: 0.577684, acc: 0.703125] [adversarial loss: 1.182864, acc: 0.203125]\n",
      "7104: [discriminator loss: 0.548395, acc: 0.734375] [adversarial loss: 0.962841, acc: 0.343750]\n",
      "7105: [discriminator loss: 0.539685, acc: 0.710938] [adversarial loss: 1.429119, acc: 0.218750]\n",
      "7106: [discriminator loss: 0.550424, acc: 0.695312] [adversarial loss: 1.010094, acc: 0.265625]\n",
      "7107: [discriminator loss: 0.540238, acc: 0.710938] [adversarial loss: 1.337108, acc: 0.140625]\n",
      "7108: [discriminator loss: 0.521047, acc: 0.781250] [adversarial loss: 0.982967, acc: 0.281250]\n",
      "7109: [discriminator loss: 0.574022, acc: 0.703125] [adversarial loss: 1.280964, acc: 0.203125]\n",
      "7110: [discriminator loss: 0.536870, acc: 0.718750] [adversarial loss: 1.182123, acc: 0.156250]\n",
      "7111: [discriminator loss: 0.523587, acc: 0.750000] [adversarial loss: 1.183348, acc: 0.203125]\n",
      "7112: [discriminator loss: 0.523950, acc: 0.726562] [adversarial loss: 1.009109, acc: 0.281250]\n",
      "7113: [discriminator loss: 0.518829, acc: 0.742188] [adversarial loss: 1.216370, acc: 0.203125]\n",
      "7114: [discriminator loss: 0.588742, acc: 0.640625] [adversarial loss: 1.041958, acc: 0.343750]\n",
      "7115: [discriminator loss: 0.510044, acc: 0.726562] [adversarial loss: 1.584090, acc: 0.015625]\n",
      "7116: [discriminator loss: 0.598980, acc: 0.695312] [adversarial loss: 1.032095, acc: 0.281250]\n",
      "7117: [discriminator loss: 0.532480, acc: 0.734375] [adversarial loss: 1.357275, acc: 0.234375]\n",
      "7118: [discriminator loss: 0.489220, acc: 0.781250] [adversarial loss: 1.096568, acc: 0.187500]\n",
      "7119: [discriminator loss: 0.505254, acc: 0.765625] [adversarial loss: 1.310851, acc: 0.187500]\n",
      "7120: [discriminator loss: 0.562162, acc: 0.679688] [adversarial loss: 1.004119, acc: 0.343750]\n",
      "7121: [discriminator loss: 0.592568, acc: 0.679688] [adversarial loss: 1.165558, acc: 0.281250]\n",
      "7122: [discriminator loss: 0.552514, acc: 0.742188] [adversarial loss: 1.235750, acc: 0.328125]\n",
      "7123: [discriminator loss: 0.538531, acc: 0.687500] [adversarial loss: 1.313268, acc: 0.125000]\n",
      "7124: [discriminator loss: 0.543794, acc: 0.765625] [adversarial loss: 1.426178, acc: 0.171875]\n",
      "7125: [discriminator loss: 0.557105, acc: 0.734375] [adversarial loss: 1.013636, acc: 0.265625]\n",
      "7126: [discriminator loss: 0.582011, acc: 0.718750] [adversarial loss: 1.345587, acc: 0.140625]\n",
      "7127: [discriminator loss: 0.510896, acc: 0.757812] [adversarial loss: 1.078822, acc: 0.203125]\n",
      "7128: [discriminator loss: 0.485309, acc: 0.812500] [adversarial loss: 1.297214, acc: 0.281250]\n",
      "7129: [discriminator loss: 0.477988, acc: 0.773438] [adversarial loss: 0.913999, acc: 0.343750]\n",
      "7130: [discriminator loss: 0.627909, acc: 0.632812] [adversarial loss: 1.437501, acc: 0.171875]\n",
      "7131: [discriminator loss: 0.518948, acc: 0.718750] [adversarial loss: 0.840094, acc: 0.468750]\n",
      "7132: [discriminator loss: 0.577482, acc: 0.703125] [adversarial loss: 1.538654, acc: 0.109375]\n",
      "7133: [discriminator loss: 0.640959, acc: 0.648438] [adversarial loss: 0.856587, acc: 0.421875]\n",
      "7134: [discriminator loss: 0.514821, acc: 0.742188] [adversarial loss: 1.362999, acc: 0.203125]\n",
      "7135: [discriminator loss: 0.598026, acc: 0.687500] [adversarial loss: 1.087524, acc: 0.265625]\n",
      "7136: [discriminator loss: 0.503821, acc: 0.757812] [adversarial loss: 1.306223, acc: 0.171875]\n",
      "7137: [discriminator loss: 0.652819, acc: 0.695312] [adversarial loss: 1.082188, acc: 0.265625]\n",
      "7138: [discriminator loss: 0.496357, acc: 0.773438] [adversarial loss: 1.241852, acc: 0.234375]\n",
      "7139: [discriminator loss: 0.589282, acc: 0.710938] [adversarial loss: 1.371686, acc: 0.156250]\n",
      "7140: [discriminator loss: 0.546740, acc: 0.750000] [adversarial loss: 0.943980, acc: 0.390625]\n",
      "7141: [discriminator loss: 0.508784, acc: 0.765625] [adversarial loss: 1.334531, acc: 0.203125]\n",
      "7142: [discriminator loss: 0.531967, acc: 0.718750] [adversarial loss: 0.830729, acc: 0.484375]\n",
      "7143: [discriminator loss: 0.619057, acc: 0.648438] [adversarial loss: 1.440974, acc: 0.187500]\n",
      "7144: [discriminator loss: 0.589165, acc: 0.734375] [adversarial loss: 1.118459, acc: 0.296875]\n",
      "7145: [discriminator loss: 0.553816, acc: 0.734375] [adversarial loss: 1.261941, acc: 0.187500]\n",
      "7146: [discriminator loss: 0.490563, acc: 0.781250] [adversarial loss: 1.139067, acc: 0.296875]\n",
      "7147: [discriminator loss: 0.504075, acc: 0.750000] [adversarial loss: 1.372546, acc: 0.187500]\n",
      "7148: [discriminator loss: 0.489349, acc: 0.742188] [adversarial loss: 0.811271, acc: 0.531250]\n",
      "7149: [discriminator loss: 0.586002, acc: 0.656250] [adversarial loss: 1.633328, acc: 0.093750]\n",
      "7150: [discriminator loss: 0.507489, acc: 0.773438] [adversarial loss: 1.196766, acc: 0.218750]\n",
      "7151: [discriminator loss: 0.561216, acc: 0.703125] [adversarial loss: 1.359159, acc: 0.203125]\n",
      "7152: [discriminator loss: 0.509132, acc: 0.757812] [adversarial loss: 1.150673, acc: 0.234375]\n",
      "7153: [discriminator loss: 0.536773, acc: 0.710938] [adversarial loss: 1.145768, acc: 0.218750]\n",
      "7154: [discriminator loss: 0.536671, acc: 0.726562] [adversarial loss: 1.243687, acc: 0.171875]\n",
      "7155: [discriminator loss: 0.588289, acc: 0.679688] [adversarial loss: 1.193901, acc: 0.265625]\n",
      "7156: [discriminator loss: 0.518626, acc: 0.726562] [adversarial loss: 1.203048, acc: 0.187500]\n",
      "7157: [discriminator loss: 0.571445, acc: 0.687500] [adversarial loss: 1.127555, acc: 0.203125]\n",
      "7158: [discriminator loss: 0.550998, acc: 0.742188] [adversarial loss: 1.024549, acc: 0.375000]\n",
      "7159: [discriminator loss: 0.580747, acc: 0.687500] [adversarial loss: 1.345087, acc: 0.156250]\n",
      "7160: [discriminator loss: 0.515891, acc: 0.750000] [adversarial loss: 1.010072, acc: 0.250000]\n",
      "7161: [discriminator loss: 0.534812, acc: 0.757812] [adversarial loss: 1.315227, acc: 0.234375]\n",
      "7162: [discriminator loss: 0.478737, acc: 0.757812] [adversarial loss: 0.835790, acc: 0.468750]\n",
      "7163: [discriminator loss: 0.500215, acc: 0.757812] [adversarial loss: 1.319261, acc: 0.187500]\n",
      "7164: [discriminator loss: 0.542343, acc: 0.695312] [adversarial loss: 1.118784, acc: 0.265625]\n",
      "7165: [discriminator loss: 0.501631, acc: 0.796875] [adversarial loss: 0.970126, acc: 0.390625]\n",
      "7166: [discriminator loss: 0.500813, acc: 0.726562] [adversarial loss: 1.295756, acc: 0.296875]\n",
      "7167: [discriminator loss: 0.556955, acc: 0.726562] [adversarial loss: 1.189479, acc: 0.328125]\n",
      "7168: [discriminator loss: 0.540176, acc: 0.734375] [adversarial loss: 1.194704, acc: 0.218750]\n",
      "7169: [discriminator loss: 0.444749, acc: 0.828125] [adversarial loss: 1.199478, acc: 0.250000]\n",
      "7170: [discriminator loss: 0.589246, acc: 0.632812] [adversarial loss: 1.020780, acc: 0.328125]\n",
      "7171: [discriminator loss: 0.573735, acc: 0.695312] [adversarial loss: 1.277849, acc: 0.156250]\n",
      "7172: [discriminator loss: 0.655731, acc: 0.664062] [adversarial loss: 0.849570, acc: 0.406250]\n",
      "7173: [discriminator loss: 0.587964, acc: 0.710938] [adversarial loss: 1.638342, acc: 0.140625]\n",
      "7174: [discriminator loss: 0.576750, acc: 0.656250] [adversarial loss: 0.896178, acc: 0.375000]\n",
      "7175: [discriminator loss: 0.507752, acc: 0.765625] [adversarial loss: 1.205622, acc: 0.203125]\n",
      "7176: [discriminator loss: 0.548773, acc: 0.703125] [adversarial loss: 1.305788, acc: 0.093750]\n",
      "7177: [discriminator loss: 0.552649, acc: 0.695312] [adversarial loss: 0.880463, acc: 0.484375]\n",
      "7178: [discriminator loss: 0.517295, acc: 0.726562] [adversarial loss: 1.373000, acc: 0.125000]\n",
      "7179: [discriminator loss: 0.529521, acc: 0.718750] [adversarial loss: 0.939363, acc: 0.421875]\n",
      "7180: [discriminator loss: 0.544213, acc: 0.703125] [adversarial loss: 1.521950, acc: 0.109375]\n",
      "7181: [discriminator loss: 0.532342, acc: 0.734375] [adversarial loss: 1.001939, acc: 0.265625]\n",
      "7182: [discriminator loss: 0.474995, acc: 0.789062] [adversarial loss: 1.380281, acc: 0.093750]\n",
      "7183: [discriminator loss: 0.541448, acc: 0.710938] [adversarial loss: 0.937140, acc: 0.375000]\n",
      "7184: [discriminator loss: 0.505976, acc: 0.734375] [adversarial loss: 1.713878, acc: 0.093750]\n",
      "7185: [discriminator loss: 0.692075, acc: 0.625000] [adversarial loss: 0.662869, acc: 0.593750]\n",
      "7186: [discriminator loss: 0.598384, acc: 0.648438] [adversarial loss: 1.163290, acc: 0.250000]\n",
      "7187: [discriminator loss: 0.550974, acc: 0.687500] [adversarial loss: 1.016433, acc: 0.265625]\n",
      "7188: [discriminator loss: 0.552492, acc: 0.718750] [adversarial loss: 1.464392, acc: 0.140625]\n",
      "7189: [discriminator loss: 0.542377, acc: 0.718750] [adversarial loss: 0.988554, acc: 0.296875]\n",
      "7190: [discriminator loss: 0.548729, acc: 0.695312] [adversarial loss: 1.212660, acc: 0.250000]\n",
      "7191: [discriminator loss: 0.520039, acc: 0.703125] [adversarial loss: 1.030981, acc: 0.265625]\n",
      "7192: [discriminator loss: 0.444298, acc: 0.820312] [adversarial loss: 1.004476, acc: 0.250000]\n",
      "7193: [discriminator loss: 0.531822, acc: 0.695312] [adversarial loss: 1.091392, acc: 0.281250]\n",
      "7194: [discriminator loss: 0.588266, acc: 0.726562] [adversarial loss: 1.239595, acc: 0.203125]\n",
      "7195: [discriminator loss: 0.531427, acc: 0.710938] [adversarial loss: 1.397658, acc: 0.187500]\n",
      "7196: [discriminator loss: 0.541079, acc: 0.703125] [adversarial loss: 1.026227, acc: 0.296875]\n",
      "7197: [discriminator loss: 0.563450, acc: 0.687500] [adversarial loss: 1.075086, acc: 0.265625]\n",
      "7198: [discriminator loss: 0.593078, acc: 0.671875] [adversarial loss: 1.093289, acc: 0.234375]\n",
      "7199: [discriminator loss: 0.568923, acc: 0.687500] [adversarial loss: 1.485292, acc: 0.187500]\n",
      "7200: [discriminator loss: 0.569827, acc: 0.679688] [adversarial loss: 1.009369, acc: 0.296875]\n",
      "7201: [discriminator loss: 0.494073, acc: 0.789062] [adversarial loss: 1.159895, acc: 0.218750]\n",
      "7202: [discriminator loss: 0.516353, acc: 0.718750] [adversarial loss: 1.526781, acc: 0.078125]\n",
      "7203: [discriminator loss: 0.500165, acc: 0.765625] [adversarial loss: 0.878055, acc: 0.390625]\n",
      "7204: [discriminator loss: 0.538415, acc: 0.695312] [adversarial loss: 1.439121, acc: 0.062500]\n",
      "7205: [discriminator loss: 0.510876, acc: 0.750000] [adversarial loss: 0.820888, acc: 0.453125]\n",
      "7206: [discriminator loss: 0.495812, acc: 0.789062] [adversarial loss: 1.323684, acc: 0.187500]\n",
      "7207: [discriminator loss: 0.562245, acc: 0.734375] [adversarial loss: 0.989506, acc: 0.375000]\n",
      "7208: [discriminator loss: 0.499106, acc: 0.734375] [adversarial loss: 1.351120, acc: 0.234375]\n",
      "7209: [discriminator loss: 0.525695, acc: 0.765625] [adversarial loss: 0.969800, acc: 0.390625]\n",
      "7210: [discriminator loss: 0.516181, acc: 0.679688] [adversarial loss: 1.337585, acc: 0.234375]\n",
      "7211: [discriminator loss: 0.604844, acc: 0.648438] [adversarial loss: 1.054141, acc: 0.375000]\n",
      "7212: [discriminator loss: 0.567217, acc: 0.710938] [adversarial loss: 1.216651, acc: 0.234375]\n",
      "7213: [discriminator loss: 0.533236, acc: 0.718750] [adversarial loss: 0.817469, acc: 0.468750]\n",
      "7214: [discriminator loss: 0.535298, acc: 0.726562] [adversarial loss: 1.379366, acc: 0.093750]\n",
      "7215: [discriminator loss: 0.527764, acc: 0.710938] [adversarial loss: 1.039414, acc: 0.343750]\n",
      "7216: [discriminator loss: 0.564559, acc: 0.726562] [adversarial loss: 1.457641, acc: 0.187500]\n",
      "7217: [discriminator loss: 0.650287, acc: 0.640625] [adversarial loss: 0.843763, acc: 0.437500]\n",
      "7218: [discriminator loss: 0.559977, acc: 0.726562] [adversarial loss: 1.635076, acc: 0.046875]\n",
      "7219: [discriminator loss: 0.534237, acc: 0.726562] [adversarial loss: 1.013016, acc: 0.328125]\n",
      "7220: [discriminator loss: 0.551846, acc: 0.726562] [adversarial loss: 1.069481, acc: 0.328125]\n",
      "7221: [discriminator loss: 0.517746, acc: 0.742188] [adversarial loss: 1.062579, acc: 0.234375]\n",
      "7222: [discriminator loss: 0.527214, acc: 0.734375] [adversarial loss: 1.382246, acc: 0.109375]\n",
      "7223: [discriminator loss: 0.546950, acc: 0.750000] [adversarial loss: 1.309675, acc: 0.171875]\n",
      "7224: [discriminator loss: 0.617797, acc: 0.656250] [adversarial loss: 0.930698, acc: 0.421875]\n",
      "7225: [discriminator loss: 0.483435, acc: 0.757812] [adversarial loss: 1.456333, acc: 0.187500]\n",
      "7226: [discriminator loss: 0.640286, acc: 0.617188] [adversarial loss: 0.985436, acc: 0.375000]\n",
      "7227: [discriminator loss: 0.674651, acc: 0.601562] [adversarial loss: 1.753947, acc: 0.093750]\n",
      "7228: [discriminator loss: 0.642056, acc: 0.664062] [adversarial loss: 0.831580, acc: 0.453125]\n",
      "7229: [discriminator loss: 0.598991, acc: 0.703125] [adversarial loss: 1.441191, acc: 0.078125]\n",
      "7230: [discriminator loss: 0.658453, acc: 0.632812] [adversarial loss: 0.875995, acc: 0.406250]\n",
      "7231: [discriminator loss: 0.514369, acc: 0.734375] [adversarial loss: 1.271785, acc: 0.156250]\n",
      "7232: [discriminator loss: 0.565220, acc: 0.703125] [adversarial loss: 1.192963, acc: 0.265625]\n",
      "7233: [discriminator loss: 0.508535, acc: 0.789062] [adversarial loss: 1.129813, acc: 0.187500]\n",
      "7234: [discriminator loss: 0.542405, acc: 0.773438] [adversarial loss: 0.995523, acc: 0.281250]\n",
      "7235: [discriminator loss: 0.568256, acc: 0.679688] [adversarial loss: 1.204921, acc: 0.218750]\n",
      "7236: [discriminator loss: 0.480924, acc: 0.765625] [adversarial loss: 1.223990, acc: 0.203125]\n",
      "7237: [discriminator loss: 0.556048, acc: 0.734375] [adversarial loss: 0.955632, acc: 0.359375]\n",
      "7238: [discriminator loss: 0.540278, acc: 0.734375] [adversarial loss: 1.158212, acc: 0.281250]\n",
      "7239: [discriminator loss: 0.478719, acc: 0.773438] [adversarial loss: 1.201784, acc: 0.218750]\n",
      "7240: [discriminator loss: 0.549127, acc: 0.718750] [adversarial loss: 1.169097, acc: 0.250000]\n",
      "7241: [discriminator loss: 0.503784, acc: 0.773438] [adversarial loss: 1.355169, acc: 0.156250]\n",
      "7242: [discriminator loss: 0.500866, acc: 0.757812] [adversarial loss: 1.113695, acc: 0.281250]\n",
      "7243: [discriminator loss: 0.537227, acc: 0.742188] [adversarial loss: 1.394988, acc: 0.171875]\n",
      "7244: [discriminator loss: 0.691244, acc: 0.593750] [adversarial loss: 0.862230, acc: 0.453125]\n",
      "7245: [discriminator loss: 0.575105, acc: 0.703125] [adversarial loss: 1.460429, acc: 0.078125]\n",
      "7246: [discriminator loss: 0.586990, acc: 0.703125] [adversarial loss: 1.022668, acc: 0.281250]\n",
      "7247: [discriminator loss: 0.517627, acc: 0.742188] [adversarial loss: 1.187464, acc: 0.187500]\n",
      "7248: [discriminator loss: 0.518461, acc: 0.742188] [adversarial loss: 1.022673, acc: 0.296875]\n",
      "7249: [discriminator loss: 0.465792, acc: 0.796875] [adversarial loss: 1.372722, acc: 0.093750]\n",
      "7250: [discriminator loss: 0.532519, acc: 0.687500] [adversarial loss: 1.003708, acc: 0.343750]\n",
      "7251: [discriminator loss: 0.533480, acc: 0.710938] [adversarial loss: 1.171233, acc: 0.265625]\n",
      "7252: [discriminator loss: 0.521233, acc: 0.687500] [adversarial loss: 1.290255, acc: 0.109375]\n",
      "7253: [discriminator loss: 0.487482, acc: 0.750000] [adversarial loss: 1.077302, acc: 0.312500]\n",
      "7254: [discriminator loss: 0.604359, acc: 0.679688] [adversarial loss: 0.969080, acc: 0.343750]\n",
      "7255: [discriminator loss: 0.464263, acc: 0.781250] [adversarial loss: 1.284372, acc: 0.125000]\n",
      "7256: [discriminator loss: 0.483409, acc: 0.773438] [adversarial loss: 1.193223, acc: 0.265625]\n",
      "7257: [discriminator loss: 0.532870, acc: 0.656250] [adversarial loss: 1.396635, acc: 0.187500]\n",
      "7258: [discriminator loss: 0.540426, acc: 0.757812] [adversarial loss: 1.016883, acc: 0.296875]\n",
      "7259: [discriminator loss: 0.529713, acc: 0.703125] [adversarial loss: 1.183399, acc: 0.234375]\n",
      "7260: [discriminator loss: 0.470617, acc: 0.742188] [adversarial loss: 1.044822, acc: 0.328125]\n",
      "7261: [discriminator loss: 0.577723, acc: 0.664062] [adversarial loss: 1.256632, acc: 0.234375]\n",
      "7262: [discriminator loss: 0.472960, acc: 0.773438] [adversarial loss: 0.972878, acc: 0.312500]\n",
      "7263: [discriminator loss: 0.548058, acc: 0.710938] [adversarial loss: 1.665227, acc: 0.171875]\n",
      "7264: [discriminator loss: 0.587283, acc: 0.632812] [adversarial loss: 0.796014, acc: 0.546875]\n",
      "7265: [discriminator loss: 0.608035, acc: 0.656250] [adversarial loss: 1.421849, acc: 0.187500]\n",
      "7266: [discriminator loss: 0.571623, acc: 0.656250] [adversarial loss: 0.815881, acc: 0.515625]\n",
      "7267: [discriminator loss: 0.694896, acc: 0.625000] [adversarial loss: 1.510069, acc: 0.171875]\n",
      "7268: [discriminator loss: 0.570041, acc: 0.718750] [adversarial loss: 1.039743, acc: 0.312500]\n",
      "7269: [discriminator loss: 0.592938, acc: 0.632812] [adversarial loss: 1.194078, acc: 0.250000]\n",
      "7270: [discriminator loss: 0.570208, acc: 0.671875] [adversarial loss: 0.944628, acc: 0.328125]\n",
      "7271: [discriminator loss: 0.446621, acc: 0.812500] [adversarial loss: 0.999490, acc: 0.343750]\n",
      "7272: [discriminator loss: 0.536185, acc: 0.695312] [adversarial loss: 1.035157, acc: 0.296875]\n",
      "7273: [discriminator loss: 0.563101, acc: 0.710938] [adversarial loss: 1.339355, acc: 0.171875]\n",
      "7274: [discriminator loss: 0.529914, acc: 0.757812] [adversarial loss: 0.952956, acc: 0.359375]\n",
      "7275: [discriminator loss: 0.551159, acc: 0.757812] [adversarial loss: 1.333551, acc: 0.093750]\n",
      "7276: [discriminator loss: 0.555644, acc: 0.742188] [adversarial loss: 0.780482, acc: 0.437500]\n",
      "7277: [discriminator loss: 0.565361, acc: 0.742188] [adversarial loss: 1.339991, acc: 0.171875]\n",
      "7278: [discriminator loss: 0.518973, acc: 0.750000] [adversarial loss: 1.314384, acc: 0.156250]\n",
      "7279: [discriminator loss: 0.555578, acc: 0.726562] [adversarial loss: 1.180179, acc: 0.171875]\n",
      "7280: [discriminator loss: 0.538596, acc: 0.679688] [adversarial loss: 1.135925, acc: 0.234375]\n",
      "7281: [discriminator loss: 0.598479, acc: 0.734375] [adversarial loss: 1.344579, acc: 0.187500]\n",
      "7282: [discriminator loss: 0.502130, acc: 0.773438] [adversarial loss: 1.169328, acc: 0.218750]\n",
      "7283: [discriminator loss: 0.556659, acc: 0.757812] [adversarial loss: 1.103254, acc: 0.234375]\n",
      "7284: [discriminator loss: 0.585412, acc: 0.703125] [adversarial loss: 1.004679, acc: 0.296875]\n",
      "7285: [discriminator loss: 0.588921, acc: 0.742188] [adversarial loss: 1.440511, acc: 0.171875]\n",
      "7286: [discriminator loss: 0.542842, acc: 0.703125] [adversarial loss: 0.881426, acc: 0.437500]\n",
      "7287: [discriminator loss: 0.486410, acc: 0.757812] [adversarial loss: 1.404007, acc: 0.109375]\n",
      "7288: [discriminator loss: 0.501753, acc: 0.726562] [adversarial loss: 1.097349, acc: 0.265625]\n",
      "7289: [discriminator loss: 0.504287, acc: 0.750000] [adversarial loss: 1.187984, acc: 0.218750]\n",
      "7290: [discriminator loss: 0.601699, acc: 0.664062] [adversarial loss: 0.863101, acc: 0.406250]\n",
      "7291: [discriminator loss: 0.541441, acc: 0.664062] [adversarial loss: 1.565353, acc: 0.125000]\n",
      "7292: [discriminator loss: 0.555598, acc: 0.679688] [adversarial loss: 1.056611, acc: 0.250000]\n",
      "7293: [discriminator loss: 0.517718, acc: 0.781250] [adversarial loss: 0.932462, acc: 0.375000]\n",
      "7294: [discriminator loss: 0.551253, acc: 0.734375] [adversarial loss: 1.417697, acc: 0.078125]\n",
      "7295: [discriminator loss: 0.645911, acc: 0.632812] [adversarial loss: 0.765629, acc: 0.468750]\n",
      "7296: [discriminator loss: 0.594947, acc: 0.703125] [adversarial loss: 1.542992, acc: 0.109375]\n",
      "7297: [discriminator loss: 0.598726, acc: 0.687500] [adversarial loss: 0.848861, acc: 0.484375]\n",
      "7298: [discriminator loss: 0.575264, acc: 0.718750] [adversarial loss: 1.522021, acc: 0.078125]\n",
      "7299: [discriminator loss: 0.530802, acc: 0.734375] [adversarial loss: 0.956627, acc: 0.406250]\n",
      "7300: [discriminator loss: 0.528798, acc: 0.671875] [adversarial loss: 1.250056, acc: 0.203125]\n",
      "7301: [discriminator loss: 0.560368, acc: 0.671875] [adversarial loss: 0.955234, acc: 0.406250]\n",
      "7302: [discriminator loss: 0.662501, acc: 0.617188] [adversarial loss: 1.418882, acc: 0.093750]\n",
      "7303: [discriminator loss: 0.548561, acc: 0.687500] [adversarial loss: 1.019762, acc: 0.265625]\n",
      "7304: [discriminator loss: 0.529808, acc: 0.710938] [adversarial loss: 1.385564, acc: 0.125000]\n",
      "7305: [discriminator loss: 0.484431, acc: 0.796875] [adversarial loss: 1.021000, acc: 0.328125]\n",
      "7306: [discriminator loss: 0.539592, acc: 0.726562] [adversarial loss: 1.354688, acc: 0.203125]\n",
      "7307: [discriminator loss: 0.532945, acc: 0.710938] [adversarial loss: 0.926732, acc: 0.390625]\n",
      "7308: [discriminator loss: 0.589444, acc: 0.703125] [adversarial loss: 1.192758, acc: 0.265625]\n",
      "7309: [discriminator loss: 0.504390, acc: 0.781250] [adversarial loss: 1.377168, acc: 0.125000]\n",
      "7310: [discriminator loss: 0.554395, acc: 0.750000] [adversarial loss: 1.368956, acc: 0.156250]\n",
      "7311: [discriminator loss: 0.510909, acc: 0.750000] [adversarial loss: 1.121706, acc: 0.156250]\n",
      "7312: [discriminator loss: 0.617984, acc: 0.734375] [adversarial loss: 1.182638, acc: 0.250000]\n",
      "7313: [discriminator loss: 0.607776, acc: 0.625000] [adversarial loss: 1.252729, acc: 0.281250]\n",
      "7314: [discriminator loss: 0.603390, acc: 0.648438] [adversarial loss: 1.555673, acc: 0.046875]\n",
      "7315: [discriminator loss: 0.565063, acc: 0.734375] [adversarial loss: 1.076633, acc: 0.359375]\n",
      "7316: [discriminator loss: 0.529222, acc: 0.710938] [adversarial loss: 1.160385, acc: 0.187500]\n",
      "7317: [discriminator loss: 0.532554, acc: 0.757812] [adversarial loss: 1.484628, acc: 0.187500]\n",
      "7318: [discriminator loss: 0.538268, acc: 0.710938] [adversarial loss: 0.886751, acc: 0.406250]\n",
      "7319: [discriminator loss: 0.571028, acc: 0.687500] [adversarial loss: 1.291642, acc: 0.125000]\n",
      "7320: [discriminator loss: 0.518729, acc: 0.703125] [adversarial loss: 0.872092, acc: 0.312500]\n",
      "7321: [discriminator loss: 0.484747, acc: 0.734375] [adversarial loss: 1.433752, acc: 0.156250]\n",
      "7322: [discriminator loss: 0.573836, acc: 0.703125] [adversarial loss: 1.076038, acc: 0.343750]\n",
      "7323: [discriminator loss: 0.533932, acc: 0.757812] [adversarial loss: 1.428742, acc: 0.156250]\n",
      "7324: [discriminator loss: 0.498993, acc: 0.765625] [adversarial loss: 0.875684, acc: 0.484375]\n",
      "7325: [discriminator loss: 0.599162, acc: 0.679688] [adversarial loss: 1.272911, acc: 0.109375]\n",
      "7326: [discriminator loss: 0.554928, acc: 0.687500] [adversarial loss: 1.146815, acc: 0.171875]\n",
      "7327: [discriminator loss: 0.526491, acc: 0.757812] [adversarial loss: 0.966268, acc: 0.328125]\n",
      "7328: [discriminator loss: 0.536018, acc: 0.718750] [adversarial loss: 1.080423, acc: 0.281250]\n",
      "7329: [discriminator loss: 0.520981, acc: 0.742188] [adversarial loss: 1.161919, acc: 0.265625]\n",
      "7330: [discriminator loss: 0.508528, acc: 0.726562] [adversarial loss: 1.203164, acc: 0.218750]\n",
      "7331: [discriminator loss: 0.544933, acc: 0.726562] [adversarial loss: 0.997850, acc: 0.359375]\n",
      "7332: [discriminator loss: 0.536096, acc: 0.710938] [adversarial loss: 1.460612, acc: 0.140625]\n",
      "7333: [discriminator loss: 0.465243, acc: 0.734375] [adversarial loss: 1.190121, acc: 0.250000]\n",
      "7334: [discriminator loss: 0.517899, acc: 0.734375] [adversarial loss: 1.464372, acc: 0.171875]\n",
      "7335: [discriminator loss: 0.521408, acc: 0.695312] [adversarial loss: 0.886172, acc: 0.437500]\n",
      "7336: [discriminator loss: 0.605513, acc: 0.671875] [adversarial loss: 1.540027, acc: 0.156250]\n",
      "7337: [discriminator loss: 0.533721, acc: 0.718750] [adversarial loss: 1.045961, acc: 0.406250]\n",
      "7338: [discriminator loss: 0.556607, acc: 0.671875] [adversarial loss: 1.518353, acc: 0.078125]\n",
      "7339: [discriminator loss: 0.523165, acc: 0.773438] [adversarial loss: 1.073285, acc: 0.328125]\n",
      "7340: [discriminator loss: 0.538781, acc: 0.742188] [adversarial loss: 1.269989, acc: 0.171875]\n",
      "7341: [discriminator loss: 0.525558, acc: 0.742188] [adversarial loss: 0.837125, acc: 0.453125]\n",
      "7342: [discriminator loss: 0.566744, acc: 0.687500] [adversarial loss: 1.204701, acc: 0.218750]\n",
      "7343: [discriminator loss: 0.550288, acc: 0.687500] [adversarial loss: 0.912326, acc: 0.421875]\n",
      "7344: [discriminator loss: 0.569847, acc: 0.664062] [adversarial loss: 1.449644, acc: 0.109375]\n",
      "7345: [discriminator loss: 0.585948, acc: 0.648438] [adversarial loss: 1.135828, acc: 0.265625]\n",
      "7346: [discriminator loss: 0.549354, acc: 0.703125] [adversarial loss: 1.278985, acc: 0.125000]\n",
      "7347: [discriminator loss: 0.594998, acc: 0.648438] [adversarial loss: 0.862487, acc: 0.468750]\n",
      "7348: [discriminator loss: 0.569713, acc: 0.687500] [adversarial loss: 1.504326, acc: 0.156250]\n",
      "7349: [discriminator loss: 0.623205, acc: 0.632812] [adversarial loss: 0.828174, acc: 0.453125]\n",
      "7350: [discriminator loss: 0.590905, acc: 0.703125] [adversarial loss: 1.469945, acc: 0.140625]\n",
      "7351: [discriminator loss: 0.601046, acc: 0.718750] [adversarial loss: 0.902560, acc: 0.359375]\n",
      "7352: [discriminator loss: 0.549067, acc: 0.726562] [adversarial loss: 1.223751, acc: 0.187500]\n",
      "7353: [discriminator loss: 0.568092, acc: 0.695312] [adversarial loss: 1.019463, acc: 0.312500]\n",
      "7354: [discriminator loss: 0.563402, acc: 0.734375] [adversarial loss: 1.177940, acc: 0.156250]\n",
      "7355: [discriminator loss: 0.515465, acc: 0.781250] [adversarial loss: 1.008979, acc: 0.343750]\n",
      "7356: [discriminator loss: 0.513806, acc: 0.750000] [adversarial loss: 1.251084, acc: 0.171875]\n",
      "7357: [discriminator loss: 0.557056, acc: 0.726562] [adversarial loss: 1.331043, acc: 0.171875]\n",
      "7358: [discriminator loss: 0.546902, acc: 0.734375] [adversarial loss: 0.875027, acc: 0.453125]\n",
      "7359: [discriminator loss: 0.485273, acc: 0.742188] [adversarial loss: 1.403191, acc: 0.109375]\n",
      "7360: [discriminator loss: 0.446401, acc: 0.820312] [adversarial loss: 1.036175, acc: 0.359375]\n",
      "7361: [discriminator loss: 0.595888, acc: 0.656250] [adversarial loss: 1.494615, acc: 0.140625]\n",
      "7362: [discriminator loss: 0.550857, acc: 0.726562] [adversarial loss: 0.934189, acc: 0.390625]\n",
      "7363: [discriminator loss: 0.591137, acc: 0.710938] [adversarial loss: 1.360141, acc: 0.125000]\n",
      "7364: [discriminator loss: 0.524161, acc: 0.734375] [adversarial loss: 0.865401, acc: 0.375000]\n",
      "7365: [discriminator loss: 0.459093, acc: 0.789062] [adversarial loss: 1.334228, acc: 0.250000]\n",
      "7366: [discriminator loss: 0.537297, acc: 0.726562] [adversarial loss: 0.842812, acc: 0.406250]\n",
      "7367: [discriminator loss: 0.520113, acc: 0.734375] [adversarial loss: 1.349696, acc: 0.156250]\n",
      "7368: [discriminator loss: 0.540325, acc: 0.710938] [adversarial loss: 0.938476, acc: 0.421875]\n",
      "7369: [discriminator loss: 0.500591, acc: 0.757812] [adversarial loss: 1.577497, acc: 0.078125]\n",
      "7370: [discriminator loss: 0.544195, acc: 0.679688] [adversarial loss: 0.901249, acc: 0.406250]\n",
      "7371: [discriminator loss: 0.517502, acc: 0.703125] [adversarial loss: 1.450729, acc: 0.125000]\n",
      "7372: [discriminator loss: 0.507313, acc: 0.742188] [adversarial loss: 0.972460, acc: 0.328125]\n",
      "7373: [discriminator loss: 0.569061, acc: 0.765625] [adversarial loss: 1.484117, acc: 0.125000]\n",
      "7374: [discriminator loss: 0.534849, acc: 0.671875] [adversarial loss: 1.236179, acc: 0.171875]\n",
      "7375: [discriminator loss: 0.522827, acc: 0.695312] [adversarial loss: 1.359013, acc: 0.187500]\n",
      "7376: [discriminator loss: 0.527671, acc: 0.757812] [adversarial loss: 1.080223, acc: 0.187500]\n",
      "7377: [discriminator loss: 0.449371, acc: 0.804688] [adversarial loss: 1.257384, acc: 0.234375]\n",
      "7378: [discriminator loss: 0.545716, acc: 0.671875] [adversarial loss: 1.065583, acc: 0.312500]\n",
      "7379: [discriminator loss: 0.500861, acc: 0.757812] [adversarial loss: 1.261122, acc: 0.218750]\n",
      "7380: [discriminator loss: 0.564448, acc: 0.703125] [adversarial loss: 0.750054, acc: 0.531250]\n",
      "7381: [discriminator loss: 0.638211, acc: 0.679688] [adversarial loss: 1.491394, acc: 0.187500]\n",
      "7382: [discriminator loss: 0.522073, acc: 0.710938] [adversarial loss: 1.153907, acc: 0.296875]\n",
      "7383: [discriminator loss: 0.556658, acc: 0.695312] [adversarial loss: 1.455042, acc: 0.125000]\n",
      "7384: [discriminator loss: 0.590685, acc: 0.656250] [adversarial loss: 1.014123, acc: 0.343750]\n",
      "7385: [discriminator loss: 0.572996, acc: 0.742188] [adversarial loss: 1.254586, acc: 0.171875]\n",
      "7386: [discriminator loss: 0.548713, acc: 0.734375] [adversarial loss: 1.110573, acc: 0.187500]\n",
      "7387: [discriminator loss: 0.514179, acc: 0.750000] [adversarial loss: 1.173291, acc: 0.234375]\n",
      "7388: [discriminator loss: 0.502475, acc: 0.734375] [adversarial loss: 1.162366, acc: 0.281250]\n",
      "7389: [discriminator loss: 0.587078, acc: 0.679688] [adversarial loss: 1.260248, acc: 0.109375]\n",
      "7390: [discriminator loss: 0.549845, acc: 0.718750] [adversarial loss: 1.417242, acc: 0.093750]\n",
      "7391: [discriminator loss: 0.550541, acc: 0.703125] [adversarial loss: 0.963336, acc: 0.359375]\n",
      "7392: [discriminator loss: 0.527792, acc: 0.703125] [adversarial loss: 1.751100, acc: 0.093750]\n",
      "7393: [discriminator loss: 0.538625, acc: 0.742188] [adversarial loss: 0.807749, acc: 0.437500]\n",
      "7394: [discriminator loss: 0.583135, acc: 0.679688] [adversarial loss: 1.384249, acc: 0.140625]\n",
      "7395: [discriminator loss: 0.567807, acc: 0.648438] [adversarial loss: 1.003352, acc: 0.296875]\n",
      "7396: [discriminator loss: 0.485119, acc: 0.781250] [adversarial loss: 1.528942, acc: 0.171875]\n",
      "7397: [discriminator loss: 0.573563, acc: 0.679688] [adversarial loss: 1.044889, acc: 0.328125]\n",
      "7398: [discriminator loss: 0.561362, acc: 0.671875] [adversarial loss: 1.440486, acc: 0.250000]\n",
      "7399: [discriminator loss: 0.550527, acc: 0.695312] [adversarial loss: 0.837462, acc: 0.468750]\n",
      "7400: [discriminator loss: 0.456693, acc: 0.789062] [adversarial loss: 1.501278, acc: 0.109375]\n",
      "7401: [discriminator loss: 0.653558, acc: 0.703125] [adversarial loss: 0.786219, acc: 0.500000]\n",
      "7402: [discriminator loss: 0.597385, acc: 0.695312] [adversarial loss: 1.118835, acc: 0.281250]\n",
      "7403: [discriminator loss: 0.533404, acc: 0.726562] [adversarial loss: 1.251926, acc: 0.234375]\n",
      "7404: [discriminator loss: 0.505383, acc: 0.742188] [adversarial loss: 1.078715, acc: 0.281250]\n",
      "7405: [discriminator loss: 0.545858, acc: 0.718750] [adversarial loss: 1.397341, acc: 0.171875]\n",
      "7406: [discriminator loss: 0.588341, acc: 0.664062] [adversarial loss: 0.934849, acc: 0.421875]\n",
      "7407: [discriminator loss: 0.601080, acc: 0.687500] [adversarial loss: 1.486564, acc: 0.125000]\n",
      "7408: [discriminator loss: 0.547009, acc: 0.718750] [adversarial loss: 1.049689, acc: 0.265625]\n",
      "7409: [discriminator loss: 0.586663, acc: 0.703125] [adversarial loss: 1.485688, acc: 0.171875]\n",
      "7410: [discriminator loss: 0.542811, acc: 0.718750] [adversarial loss: 0.883522, acc: 0.328125]\n",
      "7411: [discriminator loss: 0.511551, acc: 0.734375] [adversarial loss: 1.569849, acc: 0.109375]\n",
      "7412: [discriminator loss: 0.552141, acc: 0.687500] [adversarial loss: 0.668280, acc: 0.531250]\n",
      "7413: [discriminator loss: 0.606514, acc: 0.687500] [adversarial loss: 1.382968, acc: 0.140625]\n",
      "7414: [discriminator loss: 0.527930, acc: 0.734375] [adversarial loss: 1.280760, acc: 0.140625]\n",
      "7415: [discriminator loss: 0.553836, acc: 0.710938] [adversarial loss: 1.213610, acc: 0.140625]\n",
      "7416: [discriminator loss: 0.464971, acc: 0.781250] [adversarial loss: 1.294974, acc: 0.234375]\n",
      "7417: [discriminator loss: 0.496298, acc: 0.789062] [adversarial loss: 1.420201, acc: 0.171875]\n",
      "7418: [discriminator loss: 0.602592, acc: 0.710938] [adversarial loss: 1.254110, acc: 0.203125]\n",
      "7419: [discriminator loss: 0.511927, acc: 0.757812] [adversarial loss: 1.203176, acc: 0.281250]\n",
      "7420: [discriminator loss: 0.599722, acc: 0.679688] [adversarial loss: 0.945137, acc: 0.328125]\n",
      "7421: [discriminator loss: 0.534835, acc: 0.781250] [adversarial loss: 1.456180, acc: 0.156250]\n",
      "7422: [discriminator loss: 0.479759, acc: 0.765625] [adversarial loss: 0.912280, acc: 0.359375]\n",
      "7423: [discriminator loss: 0.492319, acc: 0.765625] [adversarial loss: 1.447911, acc: 0.140625]\n",
      "7424: [discriminator loss: 0.636561, acc: 0.625000] [adversarial loss: 0.853418, acc: 0.484375]\n",
      "7425: [discriminator loss: 0.558907, acc: 0.687500] [adversarial loss: 1.429582, acc: 0.140625]\n",
      "7426: [discriminator loss: 0.553523, acc: 0.710938] [adversarial loss: 0.994795, acc: 0.281250]\n",
      "7427: [discriminator loss: 0.494858, acc: 0.718750] [adversarial loss: 1.235281, acc: 0.218750]\n",
      "7428: [discriminator loss: 0.462807, acc: 0.781250] [adversarial loss: 1.159333, acc: 0.234375]\n",
      "7429: [discriminator loss: 0.518484, acc: 0.726562] [adversarial loss: 1.151135, acc: 0.265625]\n",
      "7430: [discriminator loss: 0.542315, acc: 0.710938] [adversarial loss: 0.935831, acc: 0.343750]\n",
      "7431: [discriminator loss: 0.524466, acc: 0.695312] [adversarial loss: 1.360624, acc: 0.140625]\n",
      "7432: [discriminator loss: 0.500510, acc: 0.765625] [adversarial loss: 1.007613, acc: 0.343750]\n",
      "7433: [discriminator loss: 0.618230, acc: 0.625000] [adversarial loss: 1.254639, acc: 0.281250]\n",
      "7434: [discriminator loss: 0.527796, acc: 0.726562] [adversarial loss: 1.184238, acc: 0.265625]\n",
      "7435: [discriminator loss: 0.502938, acc: 0.773438] [adversarial loss: 1.231056, acc: 0.218750]\n",
      "7436: [discriminator loss: 0.546805, acc: 0.742188] [adversarial loss: 1.258684, acc: 0.203125]\n",
      "7437: [discriminator loss: 0.599851, acc: 0.648438] [adversarial loss: 1.114220, acc: 0.296875]\n",
      "7438: [discriminator loss: 0.508887, acc: 0.750000] [adversarial loss: 1.158846, acc: 0.218750]\n",
      "7439: [discriminator loss: 0.562080, acc: 0.726562] [adversarial loss: 1.446589, acc: 0.156250]\n",
      "7440: [discriminator loss: 0.546511, acc: 0.703125] [adversarial loss: 0.915120, acc: 0.359375]\n",
      "7441: [discriminator loss: 0.583677, acc: 0.695312] [adversarial loss: 1.067520, acc: 0.218750]\n",
      "7442: [discriminator loss: 0.571225, acc: 0.703125] [adversarial loss: 1.041526, acc: 0.281250]\n",
      "7443: [discriminator loss: 0.624173, acc: 0.656250] [adversarial loss: 1.351473, acc: 0.203125]\n",
      "7444: [discriminator loss: 0.474460, acc: 0.742188] [adversarial loss: 0.996464, acc: 0.312500]\n",
      "7445: [discriminator loss: 0.518114, acc: 0.726562] [adversarial loss: 1.604383, acc: 0.125000]\n",
      "7446: [discriminator loss: 0.557257, acc: 0.687500] [adversarial loss: 0.879709, acc: 0.421875]\n",
      "7447: [discriminator loss: 0.559371, acc: 0.687500] [adversarial loss: 1.275160, acc: 0.250000]\n",
      "7448: [discriminator loss: 0.502923, acc: 0.703125] [adversarial loss: 1.204702, acc: 0.234375]\n",
      "7449: [discriminator loss: 0.510027, acc: 0.750000] [adversarial loss: 0.998516, acc: 0.328125]\n",
      "7450: [discriminator loss: 0.550987, acc: 0.734375] [adversarial loss: 1.380631, acc: 0.156250]\n",
      "7451: [discriminator loss: 0.542640, acc: 0.734375] [adversarial loss: 0.903057, acc: 0.421875]\n",
      "7452: [discriminator loss: 0.558882, acc: 0.718750] [adversarial loss: 1.592352, acc: 0.109375]\n",
      "7453: [discriminator loss: 0.629442, acc: 0.640625] [adversarial loss: 0.882578, acc: 0.375000]\n",
      "7454: [discriminator loss: 0.580046, acc: 0.679688] [adversarial loss: 1.625164, acc: 0.078125]\n",
      "7455: [discriminator loss: 0.572531, acc: 0.648438] [adversarial loss: 0.862856, acc: 0.437500]\n",
      "7456: [discriminator loss: 0.543998, acc: 0.742188] [adversarial loss: 1.503755, acc: 0.109375]\n",
      "7457: [discriminator loss: 0.514017, acc: 0.773438] [adversarial loss: 1.083777, acc: 0.234375]\n",
      "7458: [discriminator loss: 0.566061, acc: 0.742188] [adversarial loss: 0.955030, acc: 0.296875]\n",
      "7459: [discriminator loss: 0.540723, acc: 0.687500] [adversarial loss: 1.165329, acc: 0.203125]\n",
      "7460: [discriminator loss: 0.599262, acc: 0.664062] [adversarial loss: 1.146300, acc: 0.281250]\n",
      "7461: [discriminator loss: 0.565140, acc: 0.679688] [adversarial loss: 1.332485, acc: 0.156250]\n",
      "7462: [discriminator loss: 0.515872, acc: 0.718750] [adversarial loss: 1.306546, acc: 0.203125]\n",
      "7463: [discriminator loss: 0.560563, acc: 0.679688] [adversarial loss: 1.139458, acc: 0.234375]\n",
      "7464: [discriminator loss: 0.506336, acc: 0.765625] [adversarial loss: 1.075008, acc: 0.296875]\n",
      "7465: [discriminator loss: 0.559256, acc: 0.695312] [adversarial loss: 1.304824, acc: 0.125000]\n",
      "7466: [discriminator loss: 0.522704, acc: 0.750000] [adversarial loss: 0.985692, acc: 0.312500]\n",
      "7467: [discriminator loss: 0.564081, acc: 0.679688] [adversarial loss: 1.529855, acc: 0.140625]\n",
      "7468: [discriminator loss: 0.499828, acc: 0.695312] [adversarial loss: 0.943290, acc: 0.343750]\n",
      "7469: [discriminator loss: 0.630061, acc: 0.625000] [adversarial loss: 1.262850, acc: 0.218750]\n",
      "7470: [discriminator loss: 0.564830, acc: 0.726562] [adversarial loss: 1.092168, acc: 0.265625]\n",
      "7471: [discriminator loss: 0.536086, acc: 0.742188] [adversarial loss: 1.541053, acc: 0.078125]\n",
      "7472: [discriminator loss: 0.567866, acc: 0.664062] [adversarial loss: 0.906638, acc: 0.375000]\n",
      "7473: [discriminator loss: 0.592999, acc: 0.656250] [adversarial loss: 1.633330, acc: 0.093750]\n",
      "7474: [discriminator loss: 0.558475, acc: 0.703125] [adversarial loss: 1.019886, acc: 0.390625]\n",
      "7475: [discriminator loss: 0.539299, acc: 0.718750] [adversarial loss: 1.470299, acc: 0.218750]\n",
      "7476: [discriminator loss: 0.570769, acc: 0.687500] [adversarial loss: 0.975888, acc: 0.328125]\n",
      "7477: [discriminator loss: 0.563164, acc: 0.687500] [adversarial loss: 1.083649, acc: 0.281250]\n",
      "7478: [discriminator loss: 0.590734, acc: 0.632812] [adversarial loss: 0.958071, acc: 0.390625]\n",
      "7479: [discriminator loss: 0.578483, acc: 0.671875] [adversarial loss: 1.392553, acc: 0.171875]\n",
      "7480: [discriminator loss: 0.568933, acc: 0.648438] [adversarial loss: 0.779032, acc: 0.515625]\n",
      "7481: [discriminator loss: 0.648820, acc: 0.632812] [adversarial loss: 1.236037, acc: 0.187500]\n",
      "7482: [discriminator loss: 0.616162, acc: 0.664062] [adversarial loss: 0.965790, acc: 0.359375]\n",
      "7483: [discriminator loss: 0.507279, acc: 0.773438] [adversarial loss: 1.211368, acc: 0.296875]\n",
      "7484: [discriminator loss: 0.496077, acc: 0.789062] [adversarial loss: 1.003513, acc: 0.328125]\n",
      "7485: [discriminator loss: 0.497596, acc: 0.812500] [adversarial loss: 1.309147, acc: 0.140625]\n",
      "7486: [discriminator loss: 0.607970, acc: 0.703125] [adversarial loss: 1.158323, acc: 0.281250]\n",
      "7487: [discriminator loss: 0.492962, acc: 0.742188] [adversarial loss: 1.119450, acc: 0.265625]\n",
      "7488: [discriminator loss: 0.490926, acc: 0.781250] [adversarial loss: 0.996276, acc: 0.296875]\n",
      "7489: [discriminator loss: 0.475714, acc: 0.843750] [adversarial loss: 1.264826, acc: 0.187500]\n",
      "7490: [discriminator loss: 0.484632, acc: 0.781250] [adversarial loss: 1.032536, acc: 0.359375]\n",
      "7491: [discriminator loss: 0.548328, acc: 0.710938] [adversarial loss: 1.306392, acc: 0.125000]\n",
      "7492: [discriminator loss: 0.570776, acc: 0.750000] [adversarial loss: 0.844416, acc: 0.500000]\n",
      "7493: [discriminator loss: 0.556144, acc: 0.742188] [adversarial loss: 1.383927, acc: 0.125000]\n",
      "7494: [discriminator loss: 0.589828, acc: 0.679688] [adversarial loss: 0.860506, acc: 0.468750]\n",
      "7495: [discriminator loss: 0.615180, acc: 0.687500] [adversarial loss: 1.489928, acc: 0.062500]\n",
      "7496: [discriminator loss: 0.528596, acc: 0.726562] [adversarial loss: 0.704035, acc: 0.531250]\n",
      "7497: [discriminator loss: 0.578963, acc: 0.695312] [adversarial loss: 1.377394, acc: 0.203125]\n",
      "7498: [discriminator loss: 0.563851, acc: 0.734375] [adversarial loss: 1.270690, acc: 0.187500]\n",
      "7499: [discriminator loss: 0.566022, acc: 0.726562] [adversarial loss: 1.029479, acc: 0.328125]\n",
      "7500: [discriminator loss: 0.505669, acc: 0.750000] [adversarial loss: 1.360501, acc: 0.125000]\n",
      "7501: [discriminator loss: 0.608669, acc: 0.679688] [adversarial loss: 1.018819, acc: 0.296875]\n",
      "7502: [discriminator loss: 0.493374, acc: 0.757812] [adversarial loss: 1.397671, acc: 0.125000]\n",
      "7503: [discriminator loss: 0.530688, acc: 0.710938] [adversarial loss: 0.937941, acc: 0.390625]\n",
      "7504: [discriminator loss: 0.546205, acc: 0.734375] [adversarial loss: 1.395073, acc: 0.125000]\n",
      "7505: [discriminator loss: 0.581603, acc: 0.695312] [adversarial loss: 1.181420, acc: 0.234375]\n",
      "7506: [discriminator loss: 0.553299, acc: 0.718750] [adversarial loss: 1.366065, acc: 0.171875]\n",
      "7507: [discriminator loss: 0.573178, acc: 0.734375] [adversarial loss: 1.170902, acc: 0.203125]\n",
      "7508: [discriminator loss: 0.569899, acc: 0.687500] [adversarial loss: 1.084372, acc: 0.296875]\n",
      "7509: [discriminator loss: 0.473724, acc: 0.773438] [adversarial loss: 1.158760, acc: 0.218750]\n",
      "7510: [discriminator loss: 0.514718, acc: 0.812500] [adversarial loss: 1.486090, acc: 0.203125]\n",
      "7511: [discriminator loss: 0.639014, acc: 0.625000] [adversarial loss: 0.932812, acc: 0.296875]\n",
      "7512: [discriminator loss: 0.528498, acc: 0.734375] [adversarial loss: 1.755593, acc: 0.109375]\n",
      "7513: [discriminator loss: 0.529172, acc: 0.710938] [adversarial loss: 1.169896, acc: 0.281250]\n",
      "7514: [discriminator loss: 0.559562, acc: 0.734375] [adversarial loss: 1.283367, acc: 0.109375]\n",
      "7515: [discriminator loss: 0.498869, acc: 0.742188] [adversarial loss: 1.216800, acc: 0.218750]\n",
      "7516: [discriminator loss: 0.610296, acc: 0.687500] [adversarial loss: 1.333923, acc: 0.203125]\n",
      "7517: [discriminator loss: 0.555807, acc: 0.671875] [adversarial loss: 1.190696, acc: 0.250000]\n",
      "7518: [discriminator loss: 0.563653, acc: 0.671875] [adversarial loss: 0.858069, acc: 0.500000]\n",
      "7519: [discriminator loss: 0.587993, acc: 0.656250] [adversarial loss: 1.229506, acc: 0.203125]\n",
      "7520: [discriminator loss: 0.552927, acc: 0.710938] [adversarial loss: 0.807243, acc: 0.578125]\n",
      "7521: [discriminator loss: 0.624633, acc: 0.664062] [adversarial loss: 1.598069, acc: 0.093750]\n",
      "7522: [discriminator loss: 0.628428, acc: 0.554688] [adversarial loss: 1.001547, acc: 0.406250]\n",
      "7523: [discriminator loss: 0.545578, acc: 0.765625] [adversarial loss: 1.418716, acc: 0.171875]\n",
      "7524: [discriminator loss: 0.604065, acc: 0.656250] [adversarial loss: 0.866604, acc: 0.437500]\n",
      "7525: [discriminator loss: 0.600623, acc: 0.671875] [adversarial loss: 1.305186, acc: 0.171875]\n",
      "7526: [discriminator loss: 0.536167, acc: 0.718750] [adversarial loss: 1.064166, acc: 0.250000]\n",
      "7527: [discriminator loss: 0.590704, acc: 0.695312] [adversarial loss: 1.025497, acc: 0.359375]\n",
      "7528: [discriminator loss: 0.592606, acc: 0.687500] [adversarial loss: 1.021625, acc: 0.343750]\n",
      "7529: [discriminator loss: 0.543383, acc: 0.734375] [adversarial loss: 1.580568, acc: 0.078125]\n",
      "7530: [discriminator loss: 0.493707, acc: 0.789062] [adversarial loss: 1.059642, acc: 0.218750]\n",
      "7531: [discriminator loss: 0.530361, acc: 0.710938] [adversarial loss: 1.108383, acc: 0.265625]\n",
      "7532: [discriminator loss: 0.540934, acc: 0.703125] [adversarial loss: 1.039912, acc: 0.265625]\n",
      "7533: [discriminator loss: 0.539663, acc: 0.726562] [adversarial loss: 1.483686, acc: 0.156250]\n",
      "7534: [discriminator loss: 0.570064, acc: 0.703125] [adversarial loss: 1.044320, acc: 0.281250]\n",
      "7535: [discriminator loss: 0.507947, acc: 0.742188] [adversarial loss: 1.174780, acc: 0.265625]\n",
      "7536: [discriminator loss: 0.618916, acc: 0.625000] [adversarial loss: 1.070037, acc: 0.250000]\n",
      "7537: [discriminator loss: 0.575074, acc: 0.671875] [adversarial loss: 1.178692, acc: 0.156250]\n",
      "7538: [discriminator loss: 0.555925, acc: 0.695312] [adversarial loss: 1.285829, acc: 0.203125]\n",
      "7539: [discriminator loss: 0.529638, acc: 0.718750] [adversarial loss: 1.173411, acc: 0.250000]\n",
      "7540: [discriminator loss: 0.587028, acc: 0.710938] [adversarial loss: 0.994716, acc: 0.265625]\n",
      "7541: [discriminator loss: 0.554689, acc: 0.703125] [adversarial loss: 1.167498, acc: 0.281250]\n",
      "7542: [discriminator loss: 0.564777, acc: 0.664062] [adversarial loss: 1.304310, acc: 0.140625]\n",
      "7543: [discriminator loss: 0.563787, acc: 0.687500] [adversarial loss: 1.002909, acc: 0.359375]\n",
      "7544: [discriminator loss: 0.540947, acc: 0.734375] [adversarial loss: 1.491976, acc: 0.125000]\n",
      "7545: [discriminator loss: 0.525968, acc: 0.695312] [adversarial loss: 1.228718, acc: 0.203125]\n",
      "7546: [discriminator loss: 0.459515, acc: 0.796875] [adversarial loss: 1.604771, acc: 0.093750]\n",
      "7547: [discriminator loss: 0.632535, acc: 0.687500] [adversarial loss: 0.885061, acc: 0.406250]\n",
      "7548: [discriminator loss: 0.525974, acc: 0.765625] [adversarial loss: 1.591463, acc: 0.015625]\n",
      "7549: [discriminator loss: 0.486747, acc: 0.765625] [adversarial loss: 1.150053, acc: 0.296875]\n",
      "7550: [discriminator loss: 0.555280, acc: 0.703125] [adversarial loss: 1.169329, acc: 0.359375]\n",
      "7551: [discriminator loss: 0.515129, acc: 0.703125] [adversarial loss: 0.936818, acc: 0.343750]\n",
      "7552: [discriminator loss: 0.497491, acc: 0.796875] [adversarial loss: 1.421088, acc: 0.093750]\n",
      "7553: [discriminator loss: 0.553935, acc: 0.703125] [adversarial loss: 1.284507, acc: 0.203125]\n",
      "7554: [discriminator loss: 0.484161, acc: 0.781250] [adversarial loss: 1.097124, acc: 0.281250]\n",
      "7555: [discriminator loss: 0.487721, acc: 0.765625] [adversarial loss: 1.339397, acc: 0.140625]\n",
      "7556: [discriminator loss: 0.596745, acc: 0.718750] [adversarial loss: 0.795173, acc: 0.515625]\n",
      "7557: [discriminator loss: 0.554929, acc: 0.695312] [adversarial loss: 1.456041, acc: 0.125000]\n",
      "7558: [discriminator loss: 0.582528, acc: 0.695312] [adversarial loss: 0.959109, acc: 0.328125]\n",
      "7559: [discriminator loss: 0.544001, acc: 0.718750] [adversarial loss: 1.457505, acc: 0.093750]\n",
      "7560: [discriminator loss: 0.567491, acc: 0.679688] [adversarial loss: 0.966129, acc: 0.359375]\n",
      "7561: [discriminator loss: 0.546546, acc: 0.671875] [adversarial loss: 1.452389, acc: 0.156250]\n",
      "7562: [discriminator loss: 0.645979, acc: 0.585938] [adversarial loss: 0.927028, acc: 0.390625]\n",
      "7563: [discriminator loss: 0.570264, acc: 0.679688] [adversarial loss: 1.395403, acc: 0.093750]\n",
      "7564: [discriminator loss: 0.540963, acc: 0.703125] [adversarial loss: 1.030505, acc: 0.375000]\n",
      "7565: [discriminator loss: 0.558292, acc: 0.718750] [adversarial loss: 1.205224, acc: 0.171875]\n",
      "7566: [discriminator loss: 0.497675, acc: 0.757812] [adversarial loss: 1.153106, acc: 0.187500]\n",
      "7567: [discriminator loss: 0.489380, acc: 0.789062] [adversarial loss: 1.127248, acc: 0.171875]\n",
      "7568: [discriminator loss: 0.529570, acc: 0.742188] [adversarial loss: 1.363694, acc: 0.140625]\n",
      "7569: [discriminator loss: 0.475191, acc: 0.789062] [adversarial loss: 0.957484, acc: 0.328125]\n",
      "7570: [discriminator loss: 0.537604, acc: 0.695312] [adversarial loss: 1.000934, acc: 0.343750]\n",
      "7571: [discriminator loss: 0.481046, acc: 0.750000] [adversarial loss: 1.220489, acc: 0.218750]\n",
      "7572: [discriminator loss: 0.511355, acc: 0.734375] [adversarial loss: 1.169124, acc: 0.265625]\n",
      "7573: [discriminator loss: 0.496296, acc: 0.750000] [adversarial loss: 0.876692, acc: 0.343750]\n",
      "7574: [discriminator loss: 0.506912, acc: 0.765625] [adversarial loss: 1.337173, acc: 0.187500]\n",
      "7575: [discriminator loss: 0.569056, acc: 0.687500] [adversarial loss: 1.167918, acc: 0.171875]\n",
      "7576: [discriminator loss: 0.555842, acc: 0.718750] [adversarial loss: 1.200788, acc: 0.250000]\n",
      "7577: [discriminator loss: 0.577124, acc: 0.671875] [adversarial loss: 0.888662, acc: 0.375000]\n",
      "7578: [discriminator loss: 0.570548, acc: 0.742188] [adversarial loss: 1.589784, acc: 0.125000]\n",
      "7579: [discriminator loss: 0.580352, acc: 0.695312] [adversarial loss: 1.085071, acc: 0.328125]\n",
      "7580: [discriminator loss: 0.554311, acc: 0.718750] [adversarial loss: 1.586962, acc: 0.203125]\n",
      "7581: [discriminator loss: 0.596922, acc: 0.695312] [adversarial loss: 1.143604, acc: 0.265625]\n",
      "7582: [discriminator loss: 0.529196, acc: 0.742188] [adversarial loss: 1.207117, acc: 0.218750]\n",
      "7583: [discriminator loss: 0.508877, acc: 0.695312] [adversarial loss: 1.098255, acc: 0.281250]\n",
      "7584: [discriminator loss: 0.527379, acc: 0.703125] [adversarial loss: 1.153192, acc: 0.234375]\n",
      "7585: [discriminator loss: 0.520845, acc: 0.710938] [adversarial loss: 1.116736, acc: 0.250000]\n",
      "7586: [discriminator loss: 0.516141, acc: 0.710938] [adversarial loss: 1.067142, acc: 0.406250]\n",
      "7587: [discriminator loss: 0.550598, acc: 0.703125] [adversarial loss: 1.198744, acc: 0.312500]\n",
      "7588: [discriminator loss: 0.581495, acc: 0.664062] [adversarial loss: 1.204830, acc: 0.234375]\n",
      "7589: [discriminator loss: 0.494278, acc: 0.765625] [adversarial loss: 1.215920, acc: 0.187500]\n",
      "7590: [discriminator loss: 0.541888, acc: 0.710938] [adversarial loss: 1.278450, acc: 0.187500]\n",
      "7591: [discriminator loss: 0.582764, acc: 0.671875] [adversarial loss: 0.900360, acc: 0.406250]\n",
      "7592: [discriminator loss: 0.529444, acc: 0.703125] [adversarial loss: 1.500306, acc: 0.031250]\n",
      "7593: [discriminator loss: 0.558909, acc: 0.687500] [adversarial loss: 1.187739, acc: 0.296875]\n",
      "7594: [discriminator loss: 0.543896, acc: 0.679688] [adversarial loss: 1.429437, acc: 0.093750]\n",
      "7595: [discriminator loss: 0.558376, acc: 0.703125] [adversarial loss: 1.099610, acc: 0.234375]\n",
      "7596: [discriminator loss: 0.586478, acc: 0.671875] [adversarial loss: 1.071933, acc: 0.234375]\n",
      "7597: [discriminator loss: 0.526689, acc: 0.703125] [adversarial loss: 1.250492, acc: 0.171875]\n",
      "7598: [discriminator loss: 0.518231, acc: 0.773438] [adversarial loss: 0.930743, acc: 0.375000]\n",
      "7599: [discriminator loss: 0.532068, acc: 0.750000] [adversarial loss: 1.458980, acc: 0.156250]\n",
      "7600: [discriminator loss: 0.568359, acc: 0.726562] [adversarial loss: 1.357773, acc: 0.109375]\n",
      "7601: [discriminator loss: 0.528745, acc: 0.718750] [adversarial loss: 1.571858, acc: 0.078125]\n",
      "7602: [discriminator loss: 0.524202, acc: 0.757812] [adversarial loss: 0.964542, acc: 0.359375]\n",
      "7603: [discriminator loss: 0.520126, acc: 0.734375] [adversarial loss: 1.476756, acc: 0.125000]\n",
      "7604: [discriminator loss: 0.548035, acc: 0.695312] [adversarial loss: 0.730116, acc: 0.531250]\n",
      "7605: [discriminator loss: 0.531283, acc: 0.710938] [adversarial loss: 1.569845, acc: 0.125000]\n",
      "7606: [discriminator loss: 0.584569, acc: 0.679688] [adversarial loss: 0.786200, acc: 0.453125]\n",
      "7607: [discriminator loss: 0.578353, acc: 0.671875] [adversarial loss: 1.468231, acc: 0.156250]\n",
      "7608: [discriminator loss: 0.519428, acc: 0.726562] [adversarial loss: 1.269671, acc: 0.187500]\n",
      "7609: [discriminator loss: 0.608994, acc: 0.664062] [adversarial loss: 0.824173, acc: 0.484375]\n",
      "7610: [discriminator loss: 0.569314, acc: 0.656250] [adversarial loss: 1.413908, acc: 0.109375]\n",
      "7611: [discriminator loss: 0.537078, acc: 0.726562] [adversarial loss: 0.877684, acc: 0.406250]\n",
      "7612: [discriminator loss: 0.568867, acc: 0.695312] [adversarial loss: 1.480994, acc: 0.171875]\n",
      "7613: [discriminator loss: 0.541196, acc: 0.742188] [adversarial loss: 0.829233, acc: 0.437500]\n",
      "7614: [discriminator loss: 0.588650, acc: 0.664062] [adversarial loss: 1.412853, acc: 0.171875]\n",
      "7615: [discriminator loss: 0.597846, acc: 0.679688] [adversarial loss: 0.971097, acc: 0.312500]\n",
      "7616: [discriminator loss: 0.570911, acc: 0.710938] [adversarial loss: 1.239372, acc: 0.171875]\n",
      "7617: [discriminator loss: 0.587356, acc: 0.695312] [adversarial loss: 1.043186, acc: 0.265625]\n",
      "7618: [discriminator loss: 0.528306, acc: 0.718750] [adversarial loss: 1.089838, acc: 0.234375]\n",
      "7619: [discriminator loss: 0.579549, acc: 0.679688] [adversarial loss: 1.211471, acc: 0.140625]\n",
      "7620: [discriminator loss: 0.549008, acc: 0.695312] [adversarial loss: 1.102378, acc: 0.250000]\n",
      "7621: [discriminator loss: 0.506261, acc: 0.757812] [adversarial loss: 1.101398, acc: 0.281250]\n",
      "7622: [discriminator loss: 0.462551, acc: 0.804688] [adversarial loss: 1.048394, acc: 0.281250]\n",
      "7623: [discriminator loss: 0.608738, acc: 0.695312] [adversarial loss: 1.405321, acc: 0.171875]\n",
      "7624: [discriminator loss: 0.567902, acc: 0.750000] [adversarial loss: 0.832497, acc: 0.453125]\n",
      "7625: [discriminator loss: 0.565615, acc: 0.679688] [adversarial loss: 1.619076, acc: 0.109375]\n",
      "7626: [discriminator loss: 0.513482, acc: 0.757812] [adversarial loss: 0.916338, acc: 0.375000]\n",
      "7627: [discriminator loss: 0.644615, acc: 0.640625] [adversarial loss: 1.602213, acc: 0.031250]\n",
      "7628: [discriminator loss: 0.588720, acc: 0.671875] [adversarial loss: 1.132587, acc: 0.250000]\n",
      "7629: [discriminator loss: 0.512147, acc: 0.757812] [adversarial loss: 1.151634, acc: 0.203125]\n",
      "7630: [discriminator loss: 0.585968, acc: 0.671875] [adversarial loss: 1.219907, acc: 0.218750]\n",
      "7631: [discriminator loss: 0.539568, acc: 0.679688] [adversarial loss: 1.002965, acc: 0.390625]\n",
      "7632: [discriminator loss: 0.552070, acc: 0.726562] [adversarial loss: 1.344107, acc: 0.187500]\n",
      "7633: [discriminator loss: 0.567518, acc: 0.679688] [adversarial loss: 0.842205, acc: 0.468750]\n",
      "7634: [discriminator loss: 0.592445, acc: 0.671875] [adversarial loss: 1.541804, acc: 0.171875]\n",
      "7635: [discriminator loss: 0.583010, acc: 0.625000] [adversarial loss: 0.904195, acc: 0.328125]\n",
      "7636: [discriminator loss: 0.539533, acc: 0.742188] [adversarial loss: 1.433478, acc: 0.140625]\n",
      "7637: [discriminator loss: 0.571203, acc: 0.679688] [adversarial loss: 0.802520, acc: 0.546875]\n",
      "7638: [discriminator loss: 0.580081, acc: 0.671875] [adversarial loss: 1.275666, acc: 0.187500]\n",
      "7639: [discriminator loss: 0.533015, acc: 0.726562] [adversarial loss: 0.993449, acc: 0.296875]\n",
      "7640: [discriminator loss: 0.527124, acc: 0.679688] [adversarial loss: 1.264512, acc: 0.203125]\n",
      "7641: [discriminator loss: 0.587931, acc: 0.648438] [adversarial loss: 0.963980, acc: 0.312500]\n",
      "7642: [discriminator loss: 0.514849, acc: 0.734375] [adversarial loss: 1.495939, acc: 0.125000]\n",
      "7643: [discriminator loss: 0.527214, acc: 0.773438] [adversarial loss: 1.136679, acc: 0.203125]\n",
      "7644: [discriminator loss: 0.512300, acc: 0.718750] [adversarial loss: 1.096725, acc: 0.296875]\n",
      "7645: [discriminator loss: 0.508479, acc: 0.765625] [adversarial loss: 1.342003, acc: 0.187500]\n",
      "7646: [discriminator loss: 0.524608, acc: 0.718750] [adversarial loss: 1.123877, acc: 0.234375]\n",
      "7647: [discriminator loss: 0.469886, acc: 0.812500] [adversarial loss: 1.415074, acc: 0.187500]\n",
      "7648: [discriminator loss: 0.533835, acc: 0.671875] [adversarial loss: 1.235770, acc: 0.203125]\n",
      "7649: [discriminator loss: 0.498759, acc: 0.765625] [adversarial loss: 1.193580, acc: 0.093750]\n",
      "7650: [discriminator loss: 0.546162, acc: 0.664062] [adversarial loss: 1.050381, acc: 0.250000]\n",
      "7651: [discriminator loss: 0.517947, acc: 0.750000] [adversarial loss: 1.104110, acc: 0.296875]\n",
      "7652: [discriminator loss: 0.488642, acc: 0.734375] [adversarial loss: 1.313838, acc: 0.093750]\n",
      "7653: [discriminator loss: 0.556483, acc: 0.703125] [adversarial loss: 1.223084, acc: 0.218750]\n",
      "7654: [discriminator loss: 0.548991, acc: 0.734375] [adversarial loss: 1.057875, acc: 0.343750]\n",
      "7655: [discriminator loss: 0.549502, acc: 0.765625] [adversarial loss: 1.108014, acc: 0.281250]\n",
      "7656: [discriminator loss: 0.558241, acc: 0.710938] [adversarial loss: 1.199881, acc: 0.375000]\n",
      "7657: [discriminator loss: 0.573724, acc: 0.687500] [adversarial loss: 1.012421, acc: 0.296875]\n",
      "7658: [discriminator loss: 0.619380, acc: 0.687500] [adversarial loss: 1.073923, acc: 0.250000]\n",
      "7659: [discriminator loss: 0.525609, acc: 0.718750] [adversarial loss: 1.261699, acc: 0.218750]\n",
      "7660: [discriminator loss: 0.606920, acc: 0.695312] [adversarial loss: 1.585024, acc: 0.109375]\n",
      "7661: [discriminator loss: 0.570300, acc: 0.679688] [adversarial loss: 0.911008, acc: 0.359375]\n",
      "7662: [discriminator loss: 0.546350, acc: 0.710938] [adversarial loss: 1.395682, acc: 0.125000]\n",
      "7663: [discriminator loss: 0.587089, acc: 0.695312] [adversarial loss: 1.117661, acc: 0.265625]\n",
      "7664: [discriminator loss: 0.593834, acc: 0.679688] [adversarial loss: 1.306730, acc: 0.140625]\n",
      "7665: [discriminator loss: 0.565965, acc: 0.703125] [adversarial loss: 1.164104, acc: 0.281250]\n",
      "7666: [discriminator loss: 0.584196, acc: 0.671875] [adversarial loss: 1.237219, acc: 0.265625]\n",
      "7667: [discriminator loss: 0.590692, acc: 0.695312] [adversarial loss: 1.074413, acc: 0.328125]\n",
      "7668: [discriminator loss: 0.529845, acc: 0.765625] [adversarial loss: 1.276216, acc: 0.218750]\n",
      "7669: [discriminator loss: 0.560548, acc: 0.710938] [adversarial loss: 1.127058, acc: 0.296875]\n",
      "7670: [discriminator loss: 0.519242, acc: 0.750000] [adversarial loss: 1.333323, acc: 0.203125]\n",
      "7671: [discriminator loss: 0.561023, acc: 0.742188] [adversarial loss: 1.163967, acc: 0.281250]\n",
      "7672: [discriminator loss: 0.563831, acc: 0.742188] [adversarial loss: 1.119936, acc: 0.265625]\n",
      "7673: [discriminator loss: 0.596686, acc: 0.679688] [adversarial loss: 1.086753, acc: 0.312500]\n",
      "7674: [discriminator loss: 0.658205, acc: 0.687500] [adversarial loss: 1.039215, acc: 0.218750]\n",
      "7675: [discriminator loss: 0.564973, acc: 0.687500] [adversarial loss: 1.139118, acc: 0.250000]\n",
      "7676: [discriminator loss: 0.555709, acc: 0.687500] [adversarial loss: 0.858591, acc: 0.406250]\n",
      "7677: [discriminator loss: 0.630586, acc: 0.648438] [adversarial loss: 1.212499, acc: 0.203125]\n",
      "7678: [discriminator loss: 0.506592, acc: 0.765625] [adversarial loss: 0.963687, acc: 0.281250]\n",
      "7679: [discriminator loss: 0.485408, acc: 0.781250] [adversarial loss: 1.319535, acc: 0.140625]\n",
      "7680: [discriminator loss: 0.587573, acc: 0.703125] [adversarial loss: 0.876544, acc: 0.437500]\n",
      "7681: [discriminator loss: 0.584325, acc: 0.703125] [adversarial loss: 1.437354, acc: 0.187500]\n",
      "7682: [discriminator loss: 0.514929, acc: 0.726562] [adversarial loss: 1.080899, acc: 0.296875]\n",
      "7683: [discriminator loss: 0.588695, acc: 0.679688] [adversarial loss: 1.324336, acc: 0.187500]\n",
      "7684: [discriminator loss: 0.566720, acc: 0.687500] [adversarial loss: 0.925278, acc: 0.390625]\n",
      "7685: [discriminator loss: 0.472937, acc: 0.734375] [adversarial loss: 1.276839, acc: 0.203125]\n",
      "7686: [discriminator loss: 0.523279, acc: 0.734375] [adversarial loss: 0.977046, acc: 0.312500]\n",
      "7687: [discriminator loss: 0.534078, acc: 0.734375] [adversarial loss: 1.334552, acc: 0.218750]\n",
      "7688: [discriminator loss: 0.558277, acc: 0.687500] [adversarial loss: 1.160293, acc: 0.312500]\n",
      "7689: [discriminator loss: 0.643986, acc: 0.609375] [adversarial loss: 1.191688, acc: 0.250000]\n",
      "7690: [discriminator loss: 0.581679, acc: 0.679688] [adversarial loss: 0.724377, acc: 0.593750]\n",
      "7691: [discriminator loss: 0.534465, acc: 0.703125] [adversarial loss: 1.469048, acc: 0.140625]\n",
      "7692: [discriminator loss: 0.576990, acc: 0.648438] [adversarial loss: 1.116318, acc: 0.312500]\n",
      "7693: [discriminator loss: 0.494613, acc: 0.710938] [adversarial loss: 1.336078, acc: 0.281250]\n",
      "7694: [discriminator loss: 0.540336, acc: 0.710938] [adversarial loss: 0.949867, acc: 0.375000]\n",
      "7695: [discriminator loss: 0.565594, acc: 0.687500] [adversarial loss: 1.505590, acc: 0.140625]\n",
      "7696: [discriminator loss: 0.514453, acc: 0.734375] [adversarial loss: 0.810821, acc: 0.421875]\n",
      "7697: [discriminator loss: 0.513172, acc: 0.734375] [adversarial loss: 1.561285, acc: 0.046875]\n",
      "7698: [discriminator loss: 0.661919, acc: 0.632812] [adversarial loss: 0.811077, acc: 0.484375]\n",
      "7699: [discriminator loss: 0.598418, acc: 0.679688] [adversarial loss: 1.720961, acc: 0.062500]\n",
      "7700: [discriminator loss: 0.522407, acc: 0.734375] [adversarial loss: 1.161835, acc: 0.234375]\n",
      "7701: [discriminator loss: 0.592243, acc: 0.679688] [adversarial loss: 1.045685, acc: 0.312500]\n",
      "7702: [discriminator loss: 0.510769, acc: 0.773438] [adversarial loss: 1.351026, acc: 0.203125]\n",
      "7703: [discriminator loss: 0.538981, acc: 0.710938] [adversarial loss: 1.035873, acc: 0.281250]\n",
      "7704: [discriminator loss: 0.570806, acc: 0.687500] [adversarial loss: 1.133148, acc: 0.265625]\n",
      "7705: [discriminator loss: 0.552861, acc: 0.726562] [adversarial loss: 0.997488, acc: 0.265625]\n",
      "7706: [discriminator loss: 0.507116, acc: 0.789062] [adversarial loss: 1.389669, acc: 0.109375]\n",
      "7707: [discriminator loss: 0.623201, acc: 0.695312] [adversarial loss: 1.264812, acc: 0.156250]\n",
      "7708: [discriminator loss: 0.540949, acc: 0.687500] [adversarial loss: 1.277941, acc: 0.171875]\n",
      "7709: [discriminator loss: 0.544291, acc: 0.734375] [adversarial loss: 1.134129, acc: 0.203125]\n",
      "7710: [discriminator loss: 0.587879, acc: 0.640625] [adversarial loss: 1.095444, acc: 0.265625]\n",
      "7711: [discriminator loss: 0.518422, acc: 0.757812] [adversarial loss: 1.033106, acc: 0.296875]\n",
      "7712: [discriminator loss: 0.481187, acc: 0.750000] [adversarial loss: 1.562914, acc: 0.093750]\n",
      "7713: [discriminator loss: 0.596653, acc: 0.648438] [adversarial loss: 0.874557, acc: 0.421875]\n",
      "7714: [discriminator loss: 0.615710, acc: 0.679688] [adversarial loss: 1.486385, acc: 0.140625]\n",
      "7715: [discriminator loss: 0.632339, acc: 0.687500] [adversarial loss: 0.991219, acc: 0.218750]\n",
      "7716: [discriminator loss: 0.542040, acc: 0.750000] [adversarial loss: 1.222447, acc: 0.187500]\n",
      "7717: [discriminator loss: 0.515250, acc: 0.742188] [adversarial loss: 1.223178, acc: 0.250000]\n",
      "7718: [discriminator loss: 0.559756, acc: 0.695312] [adversarial loss: 1.176857, acc: 0.281250]\n",
      "7719: [discriminator loss: 0.549485, acc: 0.742188] [adversarial loss: 1.511754, acc: 0.203125]\n",
      "7720: [discriminator loss: 0.540449, acc: 0.734375] [adversarial loss: 1.185635, acc: 0.265625]\n",
      "7721: [discriminator loss: 0.511182, acc: 0.804688] [adversarial loss: 1.189153, acc: 0.187500]\n",
      "7722: [discriminator loss: 0.536483, acc: 0.742188] [adversarial loss: 1.331225, acc: 0.187500]\n",
      "7723: [discriminator loss: 0.572846, acc: 0.710938] [adversarial loss: 1.179229, acc: 0.203125]\n",
      "7724: [discriminator loss: 0.512636, acc: 0.750000] [adversarial loss: 1.016078, acc: 0.375000]\n",
      "7725: [discriminator loss: 0.570909, acc: 0.687500] [adversarial loss: 1.272393, acc: 0.234375]\n",
      "7726: [discriminator loss: 0.552932, acc: 0.695312] [adversarial loss: 0.897843, acc: 0.468750]\n",
      "7727: [discriminator loss: 0.590371, acc: 0.664062] [adversarial loss: 1.376760, acc: 0.078125]\n",
      "7728: [discriminator loss: 0.505043, acc: 0.773438] [adversarial loss: 1.020521, acc: 0.312500]\n",
      "7729: [discriminator loss: 0.550493, acc: 0.726562] [adversarial loss: 1.507651, acc: 0.125000]\n",
      "7730: [discriminator loss: 0.609548, acc: 0.687500] [adversarial loss: 0.918260, acc: 0.343750]\n",
      "7731: [discriminator loss: 0.537201, acc: 0.718750] [adversarial loss: 1.136709, acc: 0.265625]\n",
      "7732: [discriminator loss: 0.559230, acc: 0.718750] [adversarial loss: 0.860043, acc: 0.437500]\n",
      "7733: [discriminator loss: 0.553823, acc: 0.695312] [adversarial loss: 1.418147, acc: 0.203125]\n",
      "7734: [discriminator loss: 0.590558, acc: 0.687500] [adversarial loss: 1.333888, acc: 0.171875]\n",
      "7735: [discriminator loss: 0.522779, acc: 0.765625] [adversarial loss: 1.203096, acc: 0.171875]\n",
      "7736: [discriminator loss: 0.475449, acc: 0.781250] [adversarial loss: 1.107594, acc: 0.265625]\n",
      "7737: [discriminator loss: 0.488946, acc: 0.765625] [adversarial loss: 1.019940, acc: 0.375000]\n",
      "7738: [discriminator loss: 0.572265, acc: 0.656250] [adversarial loss: 1.265628, acc: 0.281250]\n",
      "7739: [discriminator loss: 0.538874, acc: 0.718750] [adversarial loss: 1.373651, acc: 0.140625]\n",
      "7740: [discriminator loss: 0.604983, acc: 0.664062] [adversarial loss: 0.982528, acc: 0.375000]\n",
      "7741: [discriminator loss: 0.515227, acc: 0.750000] [adversarial loss: 1.355402, acc: 0.187500]\n",
      "7742: [discriminator loss: 0.557987, acc: 0.718750] [adversarial loss: 0.989670, acc: 0.406250]\n",
      "7743: [discriminator loss: 0.579269, acc: 0.679688] [adversarial loss: 1.107035, acc: 0.250000]\n",
      "7744: [discriminator loss: 0.551459, acc: 0.710938] [adversarial loss: 1.217949, acc: 0.218750]\n",
      "7745: [discriminator loss: 0.476295, acc: 0.789062] [adversarial loss: 1.434653, acc: 0.171875]\n",
      "7746: [discriminator loss: 0.598030, acc: 0.664062] [adversarial loss: 1.075528, acc: 0.250000]\n",
      "7747: [discriminator loss: 0.547356, acc: 0.710938] [adversarial loss: 1.319950, acc: 0.203125]\n",
      "7748: [discriminator loss: 0.503388, acc: 0.750000] [adversarial loss: 0.928240, acc: 0.406250]\n",
      "7749: [discriminator loss: 0.560673, acc: 0.695312] [adversarial loss: 1.450082, acc: 0.093750]\n",
      "7750: [discriminator loss: 0.542679, acc: 0.718750] [adversarial loss: 0.928751, acc: 0.343750]\n",
      "7751: [discriminator loss: 0.512058, acc: 0.710938] [adversarial loss: 1.250820, acc: 0.218750]\n",
      "7752: [discriminator loss: 0.531564, acc: 0.726562] [adversarial loss: 1.022223, acc: 0.312500]\n",
      "7753: [discriminator loss: 0.520837, acc: 0.742188] [adversarial loss: 1.335947, acc: 0.156250]\n",
      "7754: [discriminator loss: 0.597232, acc: 0.664062] [adversarial loss: 1.078032, acc: 0.265625]\n",
      "7755: [discriminator loss: 0.494567, acc: 0.781250] [adversarial loss: 1.177547, acc: 0.281250]\n",
      "7756: [discriminator loss: 0.581726, acc: 0.734375] [adversarial loss: 1.237988, acc: 0.250000]\n",
      "7757: [discriminator loss: 0.491903, acc: 0.710938] [adversarial loss: 1.298778, acc: 0.203125]\n",
      "7758: [discriminator loss: 0.478653, acc: 0.765625] [adversarial loss: 0.758028, acc: 0.500000]\n",
      "7759: [discriminator loss: 0.567099, acc: 0.687500] [adversarial loss: 1.225530, acc: 0.203125]\n",
      "7760: [discriminator loss: 0.460924, acc: 0.773438] [adversarial loss: 1.199949, acc: 0.250000]\n",
      "7761: [discriminator loss: 0.562626, acc: 0.718750] [adversarial loss: 1.270077, acc: 0.125000]\n",
      "7762: [discriminator loss: 0.537993, acc: 0.671875] [adversarial loss: 1.241637, acc: 0.296875]\n",
      "7763: [discriminator loss: 0.516505, acc: 0.718750] [adversarial loss: 1.414855, acc: 0.171875]\n",
      "7764: [discriminator loss: 0.495782, acc: 0.750000] [adversarial loss: 1.068925, acc: 0.281250]\n",
      "7765: [discriminator loss: 0.576348, acc: 0.695312] [adversarial loss: 0.908167, acc: 0.406250]\n",
      "7766: [discriminator loss: 0.616266, acc: 0.679688] [adversarial loss: 1.736881, acc: 0.046875]\n",
      "7767: [discriminator loss: 0.577870, acc: 0.710938] [adversarial loss: 0.742541, acc: 0.500000]\n",
      "7768: [discriminator loss: 0.523940, acc: 0.750000] [adversarial loss: 1.380283, acc: 0.187500]\n",
      "7769: [discriminator loss: 0.564533, acc: 0.703125] [adversarial loss: 1.021341, acc: 0.312500]\n",
      "7770: [discriminator loss: 0.493832, acc: 0.750000] [adversarial loss: 1.550165, acc: 0.125000]\n",
      "7771: [discriminator loss: 0.590126, acc: 0.687500] [adversarial loss: 0.799761, acc: 0.500000]\n",
      "7772: [discriminator loss: 0.591125, acc: 0.648438] [adversarial loss: 1.319347, acc: 0.203125]\n",
      "7773: [discriminator loss: 0.507012, acc: 0.726562] [adversarial loss: 1.116648, acc: 0.265625]\n",
      "7774: [discriminator loss: 0.565571, acc: 0.687500] [adversarial loss: 1.280657, acc: 0.187500]\n",
      "7775: [discriminator loss: 0.574860, acc: 0.687500] [adversarial loss: 1.013880, acc: 0.343750]\n",
      "7776: [discriminator loss: 0.532396, acc: 0.695312] [adversarial loss: 1.299892, acc: 0.203125]\n",
      "7777: [discriminator loss: 0.506055, acc: 0.757812] [adversarial loss: 1.502995, acc: 0.156250]\n",
      "7778: [discriminator loss: 0.547820, acc: 0.703125] [adversarial loss: 1.071990, acc: 0.312500]\n",
      "7779: [discriminator loss: 0.515620, acc: 0.734375] [adversarial loss: 1.192556, acc: 0.234375]\n",
      "7780: [discriminator loss: 0.597546, acc: 0.656250] [adversarial loss: 1.041493, acc: 0.312500]\n",
      "7781: [discriminator loss: 0.507071, acc: 0.757812] [adversarial loss: 1.658756, acc: 0.093750]\n",
      "7782: [discriminator loss: 0.638881, acc: 0.656250] [adversarial loss: 1.023537, acc: 0.296875]\n",
      "7783: [discriminator loss: 0.536714, acc: 0.750000] [adversarial loss: 1.211311, acc: 0.234375]\n",
      "7784: [discriminator loss: 0.549096, acc: 0.671875] [adversarial loss: 0.997940, acc: 0.390625]\n",
      "7785: [discriminator loss: 0.507413, acc: 0.750000] [adversarial loss: 1.213162, acc: 0.156250]\n",
      "7786: [discriminator loss: 0.485691, acc: 0.757812] [adversarial loss: 1.198860, acc: 0.296875]\n",
      "7787: [discriminator loss: 0.477502, acc: 0.765625] [adversarial loss: 1.120387, acc: 0.312500]\n",
      "7788: [discriminator loss: 0.610777, acc: 0.648438] [adversarial loss: 1.241722, acc: 0.234375]\n",
      "7789: [discriminator loss: 0.502601, acc: 0.726562] [adversarial loss: 1.014967, acc: 0.359375]\n",
      "7790: [discriminator loss: 0.517257, acc: 0.750000] [adversarial loss: 1.286564, acc: 0.171875]\n",
      "7791: [discriminator loss: 0.519941, acc: 0.734375] [adversarial loss: 1.126417, acc: 0.296875]\n",
      "7792: [discriminator loss: 0.556379, acc: 0.687500] [adversarial loss: 1.261382, acc: 0.203125]\n",
      "7793: [discriminator loss: 0.544719, acc: 0.664062] [adversarial loss: 0.712752, acc: 0.500000]\n",
      "7794: [discriminator loss: 0.591406, acc: 0.664062] [adversarial loss: 1.660611, acc: 0.093750]\n",
      "7795: [discriminator loss: 0.642034, acc: 0.664062] [adversarial loss: 0.827772, acc: 0.468750]\n",
      "7796: [discriminator loss: 0.571416, acc: 0.710938] [adversarial loss: 1.607402, acc: 0.046875]\n",
      "7797: [discriminator loss: 0.602611, acc: 0.656250] [adversarial loss: 1.065280, acc: 0.296875]\n",
      "7798: [discriminator loss: 0.530906, acc: 0.734375] [adversarial loss: 1.437722, acc: 0.125000]\n",
      "7799: [discriminator loss: 0.504556, acc: 0.742188] [adversarial loss: 1.264957, acc: 0.187500]\n",
      "7800: [discriminator loss: 0.580363, acc: 0.687500] [adversarial loss: 1.212734, acc: 0.265625]\n",
      "7801: [discriminator loss: 0.489768, acc: 0.781250] [adversarial loss: 1.310596, acc: 0.156250]\n",
      "7802: [discriminator loss: 0.516862, acc: 0.718750] [adversarial loss: 1.144004, acc: 0.218750]\n",
      "7803: [discriminator loss: 0.544409, acc: 0.734375] [adversarial loss: 0.989876, acc: 0.343750]\n",
      "7804: [discriminator loss: 0.586823, acc: 0.656250] [adversarial loss: 1.486741, acc: 0.078125]\n",
      "7805: [discriminator loss: 0.517397, acc: 0.687500] [adversarial loss: 0.995437, acc: 0.265625]\n",
      "7806: [discriminator loss: 0.524514, acc: 0.734375] [adversarial loss: 1.582222, acc: 0.109375]\n",
      "7807: [discriminator loss: 0.508534, acc: 0.718750] [adversarial loss: 0.960671, acc: 0.328125]\n",
      "7808: [discriminator loss: 0.556144, acc: 0.726562] [adversarial loss: 1.232592, acc: 0.234375]\n",
      "7809: [discriminator loss: 0.587105, acc: 0.664062] [adversarial loss: 1.002620, acc: 0.328125]\n",
      "7810: [discriminator loss: 0.556729, acc: 0.671875] [adversarial loss: 1.338455, acc: 0.187500]\n",
      "7811: [discriminator loss: 0.558170, acc: 0.718750] [adversarial loss: 0.823843, acc: 0.390625]\n",
      "7812: [discriminator loss: 0.570025, acc: 0.687500] [adversarial loss: 1.188404, acc: 0.250000]\n",
      "7813: [discriminator loss: 0.523068, acc: 0.726562] [adversarial loss: 0.994403, acc: 0.375000]\n",
      "7814: [discriminator loss: 0.516267, acc: 0.757812] [adversarial loss: 1.285193, acc: 0.187500]\n",
      "7815: [discriminator loss: 0.601642, acc: 0.640625] [adversarial loss: 1.070954, acc: 0.265625]\n",
      "7816: [discriminator loss: 0.584665, acc: 0.664062] [adversarial loss: 1.306413, acc: 0.218750]\n",
      "7817: [discriminator loss: 0.581766, acc: 0.671875] [adversarial loss: 0.955930, acc: 0.390625]\n",
      "7818: [discriminator loss: 0.534103, acc: 0.710938] [adversarial loss: 1.336316, acc: 0.140625]\n",
      "7819: [discriminator loss: 0.493360, acc: 0.765625] [adversarial loss: 0.953446, acc: 0.359375]\n",
      "7820: [discriminator loss: 0.547162, acc: 0.695312] [adversarial loss: 1.189956, acc: 0.218750]\n",
      "7821: [discriminator loss: 0.532165, acc: 0.734375] [adversarial loss: 0.933573, acc: 0.343750]\n",
      "7822: [discriminator loss: 0.557111, acc: 0.664062] [adversarial loss: 1.204752, acc: 0.281250]\n",
      "7823: [discriminator loss: 0.556365, acc: 0.687500] [adversarial loss: 1.317511, acc: 0.140625]\n",
      "7824: [discriminator loss: 0.583176, acc: 0.703125] [adversarial loss: 1.127904, acc: 0.312500]\n",
      "7825: [discriminator loss: 0.628584, acc: 0.609375] [adversarial loss: 1.289136, acc: 0.218750]\n",
      "7826: [discriminator loss: 0.537228, acc: 0.742188] [adversarial loss: 1.142037, acc: 0.250000]\n",
      "7827: [discriminator loss: 0.520008, acc: 0.718750] [adversarial loss: 1.168032, acc: 0.281250]\n",
      "7828: [discriminator loss: 0.530425, acc: 0.726562] [adversarial loss: 1.177012, acc: 0.250000]\n",
      "7829: [discriminator loss: 0.573943, acc: 0.695312] [adversarial loss: 1.303820, acc: 0.203125]\n",
      "7830: [discriminator loss: 0.509006, acc: 0.757812] [adversarial loss: 0.918968, acc: 0.390625]\n",
      "7831: [discriminator loss: 0.608221, acc: 0.664062] [adversarial loss: 1.458253, acc: 0.078125]\n",
      "7832: [discriminator loss: 0.548274, acc: 0.718750] [adversarial loss: 1.056671, acc: 0.312500]\n",
      "7833: [discriminator loss: 0.490480, acc: 0.757812] [adversarial loss: 1.137430, acc: 0.250000]\n",
      "7834: [discriminator loss: 0.487770, acc: 0.796875] [adversarial loss: 1.331637, acc: 0.125000]\n",
      "7835: [discriminator loss: 0.556687, acc: 0.734375] [adversarial loss: 1.004337, acc: 0.328125]\n",
      "7836: [discriminator loss: 0.564640, acc: 0.695312] [adversarial loss: 1.203283, acc: 0.187500]\n",
      "7837: [discriminator loss: 0.595575, acc: 0.648438] [adversarial loss: 1.096349, acc: 0.328125]\n",
      "7838: [discriminator loss: 0.531993, acc: 0.773438] [adversarial loss: 1.293830, acc: 0.187500]\n",
      "7839: [discriminator loss: 0.499324, acc: 0.773438] [adversarial loss: 1.125002, acc: 0.187500]\n",
      "7840: [discriminator loss: 0.528242, acc: 0.710938] [adversarial loss: 1.190751, acc: 0.203125]\n",
      "7841: [discriminator loss: 0.549430, acc: 0.664062] [adversarial loss: 0.974380, acc: 0.375000]\n",
      "7842: [discriminator loss: 0.527454, acc: 0.765625] [adversarial loss: 1.357722, acc: 0.171875]\n",
      "7843: [discriminator loss: 0.586098, acc: 0.695312] [adversarial loss: 0.941948, acc: 0.375000]\n",
      "7844: [discriminator loss: 0.563518, acc: 0.710938] [adversarial loss: 1.491918, acc: 0.125000]\n",
      "7845: [discriminator loss: 0.568421, acc: 0.687500] [adversarial loss: 1.025725, acc: 0.312500]\n",
      "7846: [discriminator loss: 0.532244, acc: 0.742188] [adversarial loss: 1.590302, acc: 0.031250]\n",
      "7847: [discriminator loss: 0.549682, acc: 0.718750] [adversarial loss: 1.039545, acc: 0.312500]\n",
      "7848: [discriminator loss: 0.517703, acc: 0.703125] [adversarial loss: 1.574287, acc: 0.046875]\n",
      "7849: [discriminator loss: 0.525538, acc: 0.718750] [adversarial loss: 1.085119, acc: 0.234375]\n",
      "7850: [discriminator loss: 0.578149, acc: 0.671875] [adversarial loss: 1.540590, acc: 0.078125]\n",
      "7851: [discriminator loss: 0.512317, acc: 0.710938] [adversarial loss: 1.130741, acc: 0.187500]\n",
      "7852: [discriminator loss: 0.538696, acc: 0.679688] [adversarial loss: 0.962965, acc: 0.390625]\n",
      "7853: [discriminator loss: 0.505206, acc: 0.742188] [adversarial loss: 1.309705, acc: 0.140625]\n",
      "7854: [discriminator loss: 0.503088, acc: 0.742188] [adversarial loss: 1.106761, acc: 0.265625]\n",
      "7855: [discriminator loss: 0.552212, acc: 0.710938] [adversarial loss: 1.282223, acc: 0.156250]\n",
      "7856: [discriminator loss: 0.501917, acc: 0.726562] [adversarial loss: 1.083941, acc: 0.218750]\n",
      "7857: [discriminator loss: 0.578170, acc: 0.687500] [adversarial loss: 1.264422, acc: 0.218750]\n",
      "7858: [discriminator loss: 0.547479, acc: 0.703125] [adversarial loss: 0.937396, acc: 0.296875]\n",
      "7859: [discriminator loss: 0.532110, acc: 0.718750] [adversarial loss: 1.402116, acc: 0.140625]\n",
      "7860: [discriminator loss: 0.564788, acc: 0.703125] [adversarial loss: 0.986848, acc: 0.375000]\n",
      "7861: [discriminator loss: 0.532263, acc: 0.750000] [adversarial loss: 1.479542, acc: 0.171875]\n",
      "7862: [discriminator loss: 0.601290, acc: 0.656250] [adversarial loss: 0.885997, acc: 0.421875]\n",
      "7863: [discriminator loss: 0.606269, acc: 0.648438] [adversarial loss: 1.303001, acc: 0.187500]\n",
      "7864: [discriminator loss: 0.571344, acc: 0.679688] [adversarial loss: 1.222862, acc: 0.218750]\n",
      "7865: [discriminator loss: 0.502789, acc: 0.750000] [adversarial loss: 1.125046, acc: 0.171875]\n",
      "7866: [discriminator loss: 0.567839, acc: 0.687500] [adversarial loss: 1.161084, acc: 0.218750]\n",
      "7867: [discriminator loss: 0.504838, acc: 0.726562] [adversarial loss: 1.119780, acc: 0.296875]\n",
      "7868: [discriminator loss: 0.553199, acc: 0.671875] [adversarial loss: 1.159584, acc: 0.203125]\n",
      "7869: [discriminator loss: 0.529885, acc: 0.734375] [adversarial loss: 1.207471, acc: 0.218750]\n",
      "7870: [discriminator loss: 0.513336, acc: 0.734375] [adversarial loss: 0.945077, acc: 0.359375]\n",
      "7871: [discriminator loss: 0.503816, acc: 0.773438] [adversarial loss: 1.291148, acc: 0.203125]\n",
      "7872: [discriminator loss: 0.599858, acc: 0.656250] [adversarial loss: 1.184695, acc: 0.265625]\n",
      "7873: [discriminator loss: 0.476607, acc: 0.757812] [adversarial loss: 1.092625, acc: 0.312500]\n",
      "7874: [discriminator loss: 0.521047, acc: 0.750000] [adversarial loss: 1.229797, acc: 0.187500]\n",
      "7875: [discriminator loss: 0.509711, acc: 0.726562] [adversarial loss: 1.017454, acc: 0.359375]\n",
      "7876: [discriminator loss: 0.505213, acc: 0.765625] [adversarial loss: 1.323919, acc: 0.187500]\n",
      "7877: [discriminator loss: 0.578993, acc: 0.687500] [adversarial loss: 0.897173, acc: 0.312500]\n",
      "7878: [discriminator loss: 0.554706, acc: 0.679688] [adversarial loss: 1.386353, acc: 0.156250]\n",
      "7879: [discriminator loss: 0.511156, acc: 0.742188] [adversarial loss: 0.985032, acc: 0.296875]\n",
      "7880: [discriminator loss: 0.433767, acc: 0.750000] [adversarial loss: 1.563232, acc: 0.156250]\n",
      "7881: [discriminator loss: 0.598711, acc: 0.687500] [adversarial loss: 0.952659, acc: 0.484375]\n",
      "7882: [discriminator loss: 0.635701, acc: 0.648438] [adversarial loss: 1.514121, acc: 0.125000]\n",
      "7883: [discriminator loss: 0.575756, acc: 0.718750] [adversarial loss: 0.974768, acc: 0.390625]\n",
      "7884: [discriminator loss: 0.604969, acc: 0.656250] [adversarial loss: 1.460856, acc: 0.109375]\n",
      "7885: [discriminator loss: 0.522107, acc: 0.734375] [adversarial loss: 0.885617, acc: 0.421875]\n",
      "7886: [discriminator loss: 0.559432, acc: 0.726562] [adversarial loss: 1.302336, acc: 0.203125]\n",
      "7887: [discriminator loss: 0.553896, acc: 0.742188] [adversarial loss: 1.224303, acc: 0.171875]\n",
      "7888: [discriminator loss: 0.516793, acc: 0.726562] [adversarial loss: 0.985860, acc: 0.296875]\n",
      "7889: [discriminator loss: 0.495288, acc: 0.765625] [adversarial loss: 1.335244, acc: 0.187500]\n",
      "7890: [discriminator loss: 0.523750, acc: 0.687500] [adversarial loss: 1.034930, acc: 0.312500]\n",
      "7891: [discriminator loss: 0.588431, acc: 0.679688] [adversarial loss: 1.355487, acc: 0.140625]\n",
      "7892: [discriminator loss: 0.595342, acc: 0.718750] [adversarial loss: 0.953703, acc: 0.343750]\n",
      "7893: [discriminator loss: 0.593960, acc: 0.648438] [adversarial loss: 1.319177, acc: 0.125000]\n",
      "7894: [discriminator loss: 0.505352, acc: 0.742188] [adversarial loss: 1.049405, acc: 0.203125]\n",
      "7895: [discriminator loss: 0.575816, acc: 0.703125] [adversarial loss: 1.028210, acc: 0.250000]\n",
      "7896: [discriminator loss: 0.542861, acc: 0.718750] [adversarial loss: 1.040585, acc: 0.296875]\n",
      "7897: [discriminator loss: 0.525864, acc: 0.710938] [adversarial loss: 1.344766, acc: 0.187500]\n",
      "7898: [discriminator loss: 0.574363, acc: 0.664062] [adversarial loss: 1.310683, acc: 0.265625]\n",
      "7899: [discriminator loss: 0.467994, acc: 0.796875] [adversarial loss: 1.131324, acc: 0.234375]\n",
      "7900: [discriminator loss: 0.529804, acc: 0.750000] [adversarial loss: 1.263350, acc: 0.171875]\n",
      "7901: [discriminator loss: 0.507086, acc: 0.734375] [adversarial loss: 1.170655, acc: 0.265625]\n",
      "7902: [discriminator loss: 0.582966, acc: 0.679688] [adversarial loss: 1.298101, acc: 0.171875]\n",
      "7903: [discriminator loss: 0.579871, acc: 0.687500] [adversarial loss: 0.856444, acc: 0.406250]\n",
      "7904: [discriminator loss: 0.628917, acc: 0.671875] [adversarial loss: 1.278450, acc: 0.203125]\n",
      "7905: [discriminator loss: 0.552899, acc: 0.726562] [adversarial loss: 1.045906, acc: 0.296875]\n",
      "7906: [discriminator loss: 0.579852, acc: 0.671875] [adversarial loss: 1.184565, acc: 0.156250]\n",
      "7907: [discriminator loss: 0.430380, acc: 0.835938] [adversarial loss: 1.366928, acc: 0.234375]\n",
      "7908: [discriminator loss: 0.561250, acc: 0.695312] [adversarial loss: 1.163973, acc: 0.281250]\n",
      "7909: [discriminator loss: 0.512056, acc: 0.757812] [adversarial loss: 1.333053, acc: 0.187500]\n",
      "7910: [discriminator loss: 0.530287, acc: 0.742188] [adversarial loss: 0.859691, acc: 0.390625]\n",
      "7911: [discriminator loss: 0.555577, acc: 0.695312] [adversarial loss: 1.472735, acc: 0.140625]\n",
      "7912: [discriminator loss: 0.525475, acc: 0.734375] [adversarial loss: 0.803124, acc: 0.515625]\n",
      "7913: [discriminator loss: 0.592215, acc: 0.679688] [adversarial loss: 1.265140, acc: 0.234375]\n",
      "7914: [discriminator loss: 0.552350, acc: 0.742188] [adversarial loss: 0.938292, acc: 0.390625]\n",
      "7915: [discriminator loss: 0.609026, acc: 0.687500] [adversarial loss: 1.542107, acc: 0.156250]\n",
      "7916: [discriminator loss: 0.573380, acc: 0.710938] [adversarial loss: 1.009810, acc: 0.328125]\n",
      "7917: [discriminator loss: 0.577776, acc: 0.703125] [adversarial loss: 1.191130, acc: 0.203125]\n",
      "7918: [discriminator loss: 0.551284, acc: 0.710938] [adversarial loss: 0.873171, acc: 0.406250]\n",
      "7919: [discriminator loss: 0.581739, acc: 0.679688] [adversarial loss: 1.325644, acc: 0.187500]\n",
      "7920: [discriminator loss: 0.518622, acc: 0.703125] [adversarial loss: 1.218172, acc: 0.187500]\n",
      "7921: [discriminator loss: 0.538369, acc: 0.750000] [adversarial loss: 1.159589, acc: 0.250000]\n",
      "7922: [discriminator loss: 0.501091, acc: 0.742188] [adversarial loss: 1.435641, acc: 0.203125]\n",
      "7923: [discriminator loss: 0.573643, acc: 0.695312] [adversarial loss: 1.053477, acc: 0.343750]\n",
      "7924: [discriminator loss: 0.534790, acc: 0.726562] [adversarial loss: 1.312249, acc: 0.203125]\n",
      "7925: [discriminator loss: 0.572751, acc: 0.710938] [adversarial loss: 1.038666, acc: 0.453125]\n",
      "7926: [discriminator loss: 0.528104, acc: 0.734375] [adversarial loss: 1.314779, acc: 0.156250]\n",
      "7927: [discriminator loss: 0.561211, acc: 0.695312] [adversarial loss: 1.145521, acc: 0.234375]\n",
      "7928: [discriminator loss: 0.540218, acc: 0.718750] [adversarial loss: 0.964375, acc: 0.406250]\n",
      "7929: [discriminator loss: 0.519492, acc: 0.765625] [adversarial loss: 1.237121, acc: 0.218750]\n",
      "7930: [discriminator loss: 0.521202, acc: 0.757812] [adversarial loss: 0.979531, acc: 0.375000]\n",
      "7931: [discriminator loss: 0.588938, acc: 0.710938] [adversarial loss: 1.243086, acc: 0.203125]\n",
      "7932: [discriminator loss: 0.551018, acc: 0.664062] [adversarial loss: 0.980943, acc: 0.343750]\n",
      "7933: [discriminator loss: 0.511744, acc: 0.742188] [adversarial loss: 1.581196, acc: 0.156250]\n",
      "7934: [discriminator loss: 0.556226, acc: 0.695312] [adversarial loss: 0.745396, acc: 0.468750]\n",
      "7935: [discriminator loss: 0.567807, acc: 0.687500] [adversarial loss: 1.676289, acc: 0.093750]\n",
      "7936: [discriminator loss: 0.513655, acc: 0.687500] [adversarial loss: 1.175808, acc: 0.250000]\n",
      "7937: [discriminator loss: 0.533729, acc: 0.757812] [adversarial loss: 1.065230, acc: 0.328125]\n",
      "7938: [discriminator loss: 0.615955, acc: 0.656250] [adversarial loss: 1.108658, acc: 0.296875]\n",
      "7939: [discriminator loss: 0.526326, acc: 0.773438] [adversarial loss: 1.440941, acc: 0.109375]\n",
      "7940: [discriminator loss: 0.564617, acc: 0.695312] [adversarial loss: 1.004215, acc: 0.250000]\n",
      "7941: [discriminator loss: 0.630108, acc: 0.671875] [adversarial loss: 1.053216, acc: 0.281250]\n",
      "7942: [discriminator loss: 0.548253, acc: 0.726562] [adversarial loss: 1.341076, acc: 0.125000]\n",
      "7943: [discriminator loss: 0.488575, acc: 0.781250] [adversarial loss: 1.006210, acc: 0.390625]\n",
      "7944: [discriminator loss: 0.590610, acc: 0.648438] [adversarial loss: 1.487599, acc: 0.062500]\n",
      "7945: [discriminator loss: 0.626265, acc: 0.609375] [adversarial loss: 1.091174, acc: 0.203125]\n",
      "7946: [discriminator loss: 0.529781, acc: 0.718750] [adversarial loss: 1.332857, acc: 0.156250]\n",
      "7947: [discriminator loss: 0.566063, acc: 0.671875] [adversarial loss: 1.367436, acc: 0.125000]\n",
      "7948: [discriminator loss: 0.555350, acc: 0.687500] [adversarial loss: 0.892348, acc: 0.437500]\n",
      "7949: [discriminator loss: 0.488619, acc: 0.742188] [adversarial loss: 1.180036, acc: 0.187500]\n",
      "7950: [discriminator loss: 0.521440, acc: 0.664062] [adversarial loss: 1.170367, acc: 0.265625]\n",
      "7951: [discriminator loss: 0.575249, acc: 0.656250] [adversarial loss: 1.138269, acc: 0.171875]\n",
      "7952: [discriminator loss: 0.522900, acc: 0.718750] [adversarial loss: 1.238962, acc: 0.250000]\n",
      "7953: [discriminator loss: 0.575824, acc: 0.687500] [adversarial loss: 1.149234, acc: 0.265625]\n",
      "7954: [discriminator loss: 0.488355, acc: 0.773438] [adversarial loss: 1.458278, acc: 0.203125]\n",
      "7955: [discriminator loss: 0.631510, acc: 0.656250] [adversarial loss: 0.826665, acc: 0.468750]\n",
      "7956: [discriminator loss: 0.561771, acc: 0.718750] [adversarial loss: 1.504611, acc: 0.078125]\n",
      "7957: [discriminator loss: 0.476936, acc: 0.750000] [adversarial loss: 1.025648, acc: 0.359375]\n",
      "7958: [discriminator loss: 0.597070, acc: 0.687500] [adversarial loss: 1.674526, acc: 0.078125]\n",
      "7959: [discriminator loss: 0.601520, acc: 0.687500] [adversarial loss: 0.858940, acc: 0.484375]\n",
      "7960: [discriminator loss: 0.586335, acc: 0.640625] [adversarial loss: 1.463495, acc: 0.109375]\n",
      "7961: [discriminator loss: 0.657499, acc: 0.632812] [adversarial loss: 0.842735, acc: 0.437500]\n",
      "7962: [discriminator loss: 0.545699, acc: 0.703125] [adversarial loss: 1.394207, acc: 0.093750]\n",
      "7963: [discriminator loss: 0.469791, acc: 0.750000] [adversarial loss: 1.061802, acc: 0.265625]\n",
      "7964: [discriminator loss: 0.582897, acc: 0.710938] [adversarial loss: 1.163961, acc: 0.281250]\n",
      "7965: [discriminator loss: 0.544255, acc: 0.734375] [adversarial loss: 1.180839, acc: 0.218750]\n",
      "7966: [discriminator loss: 0.561776, acc: 0.710938] [adversarial loss: 1.116843, acc: 0.250000]\n",
      "7967: [discriminator loss: 0.538068, acc: 0.726562] [adversarial loss: 1.462348, acc: 0.109375]\n",
      "7968: [discriminator loss: 0.618929, acc: 0.632812] [adversarial loss: 0.925624, acc: 0.375000]\n",
      "7969: [discriminator loss: 0.616939, acc: 0.609375] [adversarial loss: 1.551666, acc: 0.109375]\n",
      "7970: [discriminator loss: 0.497891, acc: 0.742188] [adversarial loss: 0.939028, acc: 0.343750]\n",
      "7971: [discriminator loss: 0.566060, acc: 0.671875] [adversarial loss: 1.134878, acc: 0.343750]\n",
      "7972: [discriminator loss: 0.517979, acc: 0.750000] [adversarial loss: 1.331750, acc: 0.140625]\n",
      "7973: [discriminator loss: 0.467856, acc: 0.789062] [adversarial loss: 1.114898, acc: 0.218750]\n",
      "7974: [discriminator loss: 0.649144, acc: 0.656250] [adversarial loss: 1.254311, acc: 0.203125]\n",
      "7975: [discriminator loss: 0.544518, acc: 0.679688] [adversarial loss: 1.051399, acc: 0.328125]\n",
      "7976: [discriminator loss: 0.518029, acc: 0.726562] [adversarial loss: 1.362953, acc: 0.140625]\n",
      "7977: [discriminator loss: 0.530052, acc: 0.695312] [adversarial loss: 1.121998, acc: 0.296875]\n",
      "7978: [discriminator loss: 0.560075, acc: 0.679688] [adversarial loss: 1.280006, acc: 0.109375]\n",
      "7979: [discriminator loss: 0.546636, acc: 0.687500] [adversarial loss: 0.947587, acc: 0.390625]\n",
      "7980: [discriminator loss: 0.574471, acc: 0.726562] [adversarial loss: 1.533497, acc: 0.109375]\n",
      "7981: [discriminator loss: 0.624357, acc: 0.671875] [adversarial loss: 0.815701, acc: 0.437500]\n",
      "7982: [discriminator loss: 0.586992, acc: 0.671875] [adversarial loss: 1.582085, acc: 0.140625]\n",
      "7983: [discriminator loss: 0.502623, acc: 0.742188] [adversarial loss: 1.038329, acc: 0.328125]\n",
      "7984: [discriminator loss: 0.504526, acc: 0.710938] [adversarial loss: 1.440825, acc: 0.171875]\n",
      "7985: [discriminator loss: 0.558069, acc: 0.695312] [adversarial loss: 1.194770, acc: 0.234375]\n",
      "7986: [discriminator loss: 0.551801, acc: 0.750000] [adversarial loss: 1.250983, acc: 0.187500]\n",
      "7987: [discriminator loss: 0.545917, acc: 0.734375] [adversarial loss: 1.148238, acc: 0.171875]\n",
      "7988: [discriminator loss: 0.553245, acc: 0.679688] [adversarial loss: 1.128729, acc: 0.250000]\n",
      "7989: [discriminator loss: 0.507093, acc: 0.750000] [adversarial loss: 1.233765, acc: 0.203125]\n",
      "7990: [discriminator loss: 0.581061, acc: 0.671875] [adversarial loss: 1.074444, acc: 0.328125]\n",
      "7991: [discriminator loss: 0.561705, acc: 0.656250] [adversarial loss: 1.175526, acc: 0.187500]\n",
      "7992: [discriminator loss: 0.600457, acc: 0.632812] [adversarial loss: 1.174306, acc: 0.265625]\n",
      "7993: [discriminator loss: 0.486743, acc: 0.734375] [adversarial loss: 1.263287, acc: 0.140625]\n",
      "7994: [discriminator loss: 0.498184, acc: 0.781250] [adversarial loss: 0.917235, acc: 0.359375]\n",
      "7995: [discriminator loss: 0.535703, acc: 0.742188] [adversarial loss: 1.361153, acc: 0.187500]\n",
      "7996: [discriminator loss: 0.549562, acc: 0.695312] [adversarial loss: 0.892073, acc: 0.343750]\n",
      "7997: [discriminator loss: 0.505377, acc: 0.773438] [adversarial loss: 1.520087, acc: 0.140625]\n",
      "7998: [discriminator loss: 0.507777, acc: 0.750000] [adversarial loss: 0.952106, acc: 0.343750]\n",
      "7999: [discriminator loss: 0.558001, acc: 0.718750] [adversarial loss: 1.440803, acc: 0.109375]\n",
      "8000: [discriminator loss: 0.575581, acc: 0.710938] [adversarial loss: 0.694022, acc: 0.640625]\n",
      "8001: [discriminator loss: 0.545507, acc: 0.718750] [adversarial loss: 1.442054, acc: 0.109375]\n",
      "8002: [discriminator loss: 0.488736, acc: 0.789062] [adversarial loss: 1.127987, acc: 0.312500]\n",
      "8003: [discriminator loss: 0.570125, acc: 0.679688] [adversarial loss: 1.125326, acc: 0.281250]\n",
      "8004: [discriminator loss: 0.489015, acc: 0.742188] [adversarial loss: 1.254653, acc: 0.234375]\n",
      "8005: [discriminator loss: 0.539956, acc: 0.703125] [adversarial loss: 1.335188, acc: 0.171875]\n",
      "8006: [discriminator loss: 0.494625, acc: 0.710938] [adversarial loss: 1.144940, acc: 0.296875]\n",
      "8007: [discriminator loss: 0.574966, acc: 0.671875] [adversarial loss: 1.220564, acc: 0.218750]\n",
      "8008: [discriminator loss: 0.539032, acc: 0.742188] [adversarial loss: 1.080732, acc: 0.390625]\n",
      "8009: [discriminator loss: 0.519255, acc: 0.718750] [adversarial loss: 1.256567, acc: 0.203125]\n",
      "8010: [discriminator loss: 0.570342, acc: 0.695312] [adversarial loss: 1.078819, acc: 0.281250]\n",
      "8011: [discriminator loss: 0.530627, acc: 0.726562] [adversarial loss: 1.458773, acc: 0.187500]\n",
      "8012: [discriminator loss: 0.599515, acc: 0.703125] [adversarial loss: 1.144818, acc: 0.234375]\n",
      "8013: [discriminator loss: 0.564010, acc: 0.679688] [adversarial loss: 1.199609, acc: 0.234375]\n",
      "8014: [discriminator loss: 0.541658, acc: 0.710938] [adversarial loss: 1.479410, acc: 0.156250]\n",
      "8015: [discriminator loss: 0.605497, acc: 0.671875] [adversarial loss: 0.746649, acc: 0.437500]\n",
      "8016: [discriminator loss: 0.571252, acc: 0.703125] [adversarial loss: 1.522332, acc: 0.109375]\n",
      "8017: [discriminator loss: 0.609252, acc: 0.648438] [adversarial loss: 0.868979, acc: 0.421875]\n",
      "8018: [discriminator loss: 0.607365, acc: 0.703125] [adversarial loss: 1.336569, acc: 0.125000]\n",
      "8019: [discriminator loss: 0.594722, acc: 0.687500] [adversarial loss: 1.089570, acc: 0.359375]\n",
      "8020: [discriminator loss: 0.557165, acc: 0.679688] [adversarial loss: 1.368382, acc: 0.125000]\n",
      "8021: [discriminator loss: 0.508601, acc: 0.757812] [adversarial loss: 1.171158, acc: 0.265625]\n",
      "8022: [discriminator loss: 0.545361, acc: 0.718750] [adversarial loss: 1.018619, acc: 0.281250]\n",
      "8023: [discriminator loss: 0.521668, acc: 0.742188] [adversarial loss: 1.387402, acc: 0.125000]\n",
      "8024: [discriminator loss: 0.570624, acc: 0.695312] [adversarial loss: 0.998271, acc: 0.328125]\n",
      "8025: [discriminator loss: 0.504341, acc: 0.757812] [adversarial loss: 1.440360, acc: 0.125000]\n",
      "8026: [discriminator loss: 0.543599, acc: 0.750000] [adversarial loss: 0.947476, acc: 0.406250]\n",
      "8027: [discriminator loss: 0.487724, acc: 0.781250] [adversarial loss: 1.361151, acc: 0.140625]\n",
      "8028: [discriminator loss: 0.584929, acc: 0.695312] [adversarial loss: 1.118660, acc: 0.187500]\n",
      "8029: [discriminator loss: 0.576030, acc: 0.710938] [adversarial loss: 1.207684, acc: 0.187500]\n",
      "8030: [discriminator loss: 0.517259, acc: 0.734375] [adversarial loss: 1.136160, acc: 0.203125]\n",
      "8031: [discriminator loss: 0.612188, acc: 0.695312] [adversarial loss: 1.157248, acc: 0.281250]\n",
      "8032: [discriminator loss: 0.517557, acc: 0.750000] [adversarial loss: 0.988573, acc: 0.359375]\n",
      "8033: [discriminator loss: 0.653453, acc: 0.640625] [adversarial loss: 1.830031, acc: 0.046875]\n",
      "8034: [discriminator loss: 0.595865, acc: 0.671875] [adversarial loss: 1.003058, acc: 0.406250]\n",
      "8035: [discriminator loss: 0.527524, acc: 0.757812] [adversarial loss: 1.019227, acc: 0.296875]\n",
      "8036: [discriminator loss: 0.513716, acc: 0.781250] [adversarial loss: 1.317716, acc: 0.187500]\n",
      "8037: [discriminator loss: 0.631009, acc: 0.617188] [adversarial loss: 1.184835, acc: 0.187500]\n",
      "8038: [discriminator loss: 0.606561, acc: 0.687500] [adversarial loss: 1.190847, acc: 0.218750]\n",
      "8039: [discriminator loss: 0.546330, acc: 0.710938] [adversarial loss: 1.107893, acc: 0.312500]\n",
      "8040: [discriminator loss: 0.550065, acc: 0.695312] [adversarial loss: 1.153622, acc: 0.234375]\n",
      "8041: [discriminator loss: 0.491133, acc: 0.757812] [adversarial loss: 1.221310, acc: 0.250000]\n",
      "8042: [discriminator loss: 0.538500, acc: 0.734375] [adversarial loss: 1.172783, acc: 0.218750]\n",
      "8043: [discriminator loss: 0.540334, acc: 0.734375] [adversarial loss: 0.992603, acc: 0.390625]\n",
      "8044: [discriminator loss: 0.604714, acc: 0.640625] [adversarial loss: 1.441048, acc: 0.140625]\n",
      "8045: [discriminator loss: 0.597198, acc: 0.640625] [adversarial loss: 0.847388, acc: 0.468750]\n",
      "8046: [discriminator loss: 0.624304, acc: 0.648438] [adversarial loss: 1.523186, acc: 0.171875]\n",
      "8047: [discriminator loss: 0.511242, acc: 0.765625] [adversarial loss: 1.098643, acc: 0.234375]\n",
      "8048: [discriminator loss: 0.511680, acc: 0.710938] [adversarial loss: 1.138591, acc: 0.218750]\n",
      "8049: [discriminator loss: 0.506732, acc: 0.757812] [adversarial loss: 1.478955, acc: 0.156250]\n",
      "8050: [discriminator loss: 0.587247, acc: 0.656250] [adversarial loss: 1.350540, acc: 0.156250]\n",
      "8051: [discriminator loss: 0.505756, acc: 0.765625] [adversarial loss: 1.029873, acc: 0.265625]\n",
      "8052: [discriminator loss: 0.538328, acc: 0.726562] [adversarial loss: 1.183073, acc: 0.265625]\n",
      "8053: [discriminator loss: 0.534270, acc: 0.726562] [adversarial loss: 1.077937, acc: 0.203125]\n",
      "8054: [discriminator loss: 0.542526, acc: 0.750000] [adversarial loss: 1.181606, acc: 0.250000]\n",
      "8055: [discriminator loss: 0.608777, acc: 0.664062] [adversarial loss: 0.930856, acc: 0.328125]\n",
      "8056: [discriminator loss: 0.592247, acc: 0.671875] [adversarial loss: 1.592762, acc: 0.109375]\n",
      "8057: [discriminator loss: 0.594609, acc: 0.664062] [adversarial loss: 0.835843, acc: 0.468750]\n",
      "8058: [discriminator loss: 0.556218, acc: 0.703125] [adversarial loss: 1.318832, acc: 0.234375]\n",
      "8059: [discriminator loss: 0.513269, acc: 0.734375] [adversarial loss: 1.192075, acc: 0.218750]\n",
      "8060: [discriminator loss: 0.533732, acc: 0.703125] [adversarial loss: 1.094494, acc: 0.265625]\n",
      "8061: [discriminator loss: 0.492319, acc: 0.718750] [adversarial loss: 1.082413, acc: 0.296875]\n",
      "8062: [discriminator loss: 0.559027, acc: 0.664062] [adversarial loss: 1.093851, acc: 0.312500]\n",
      "8063: [discriminator loss: 0.583100, acc: 0.710938] [adversarial loss: 1.380804, acc: 0.171875]\n",
      "8064: [discriminator loss: 0.523038, acc: 0.687500] [adversarial loss: 1.044528, acc: 0.281250]\n",
      "8065: [discriminator loss: 0.539832, acc: 0.710938] [adversarial loss: 1.079922, acc: 0.250000]\n",
      "8066: [discriminator loss: 0.626525, acc: 0.640625] [adversarial loss: 1.360737, acc: 0.171875]\n",
      "8067: [discriminator loss: 0.521222, acc: 0.750000] [adversarial loss: 1.140723, acc: 0.187500]\n",
      "8068: [discriminator loss: 0.516486, acc: 0.703125] [adversarial loss: 0.858088, acc: 0.453125]\n",
      "8069: [discriminator loss: 0.590506, acc: 0.679688] [adversarial loss: 1.510495, acc: 0.140625]\n",
      "8070: [discriminator loss: 0.574804, acc: 0.703125] [adversarial loss: 1.027997, acc: 0.343750]\n",
      "8071: [discriminator loss: 0.537469, acc: 0.703125] [adversarial loss: 1.501268, acc: 0.078125]\n",
      "8072: [discriminator loss: 0.513263, acc: 0.750000] [adversarial loss: 1.012594, acc: 0.328125]\n",
      "8073: [discriminator loss: 0.571026, acc: 0.656250] [adversarial loss: 1.234771, acc: 0.171875]\n",
      "8074: [discriminator loss: 0.534197, acc: 0.703125] [adversarial loss: 1.326829, acc: 0.156250]\n",
      "8075: [discriminator loss: 0.511536, acc: 0.757812] [adversarial loss: 1.141530, acc: 0.234375]\n",
      "8076: [discriminator loss: 0.483531, acc: 0.773438] [adversarial loss: 0.991329, acc: 0.265625]\n",
      "8077: [discriminator loss: 0.630727, acc: 0.664062] [adversarial loss: 1.137394, acc: 0.234375]\n",
      "8078: [discriminator loss: 0.563378, acc: 0.703125] [adversarial loss: 1.020585, acc: 0.265625]\n",
      "8079: [discriminator loss: 0.544744, acc: 0.671875] [adversarial loss: 1.256636, acc: 0.171875]\n",
      "8080: [discriminator loss: 0.544830, acc: 0.734375] [adversarial loss: 1.120916, acc: 0.359375]\n",
      "8081: [discriminator loss: 0.521219, acc: 0.726562] [adversarial loss: 1.127678, acc: 0.312500]\n",
      "8082: [discriminator loss: 0.586347, acc: 0.687500] [adversarial loss: 1.348926, acc: 0.203125]\n",
      "8083: [discriminator loss: 0.571397, acc: 0.656250] [adversarial loss: 0.963543, acc: 0.343750]\n",
      "8084: [discriminator loss: 0.561747, acc: 0.695312] [adversarial loss: 1.408638, acc: 0.171875]\n",
      "8085: [discriminator loss: 0.623441, acc: 0.632812] [adversarial loss: 1.068086, acc: 0.250000]\n",
      "8086: [discriminator loss: 0.495215, acc: 0.726562] [adversarial loss: 1.240531, acc: 0.156250]\n",
      "8087: [discriminator loss: 0.567028, acc: 0.648438] [adversarial loss: 1.134366, acc: 0.234375]\n",
      "8088: [discriminator loss: 0.523506, acc: 0.750000] [adversarial loss: 1.581765, acc: 0.093750]\n",
      "8089: [discriminator loss: 0.646837, acc: 0.648438] [adversarial loss: 0.877477, acc: 0.406250]\n",
      "8090: [discriminator loss: 0.617867, acc: 0.640625] [adversarial loss: 1.610288, acc: 0.078125]\n",
      "8091: [discriminator loss: 0.591178, acc: 0.679688] [adversarial loss: 0.821675, acc: 0.437500]\n",
      "8092: [discriminator loss: 0.577838, acc: 0.679688] [adversarial loss: 1.482255, acc: 0.078125]\n",
      "8093: [discriminator loss: 0.502523, acc: 0.757812] [adversarial loss: 1.157955, acc: 0.250000]\n",
      "8094: [discriminator loss: 0.483660, acc: 0.804688] [adversarial loss: 1.551497, acc: 0.078125]\n",
      "8095: [discriminator loss: 0.566394, acc: 0.718750] [adversarial loss: 1.091322, acc: 0.281250]\n",
      "8096: [discriminator loss: 0.568099, acc: 0.703125] [adversarial loss: 1.295769, acc: 0.218750]\n",
      "8097: [discriminator loss: 0.561718, acc: 0.664062] [adversarial loss: 1.129848, acc: 0.203125]\n",
      "8098: [discriminator loss: 0.522335, acc: 0.757812] [adversarial loss: 1.393885, acc: 0.234375]\n",
      "8099: [discriminator loss: 0.602958, acc: 0.632812] [adversarial loss: 1.168562, acc: 0.250000]\n",
      "8100: [discriminator loss: 0.501278, acc: 0.765625] [adversarial loss: 1.050122, acc: 0.296875]\n",
      "8101: [discriminator loss: 0.574250, acc: 0.710938] [adversarial loss: 1.274832, acc: 0.234375]\n",
      "8102: [discriminator loss: 0.555368, acc: 0.718750] [adversarial loss: 1.034486, acc: 0.359375]\n",
      "8103: [discriminator loss: 0.491244, acc: 0.757812] [adversarial loss: 1.414019, acc: 0.140625]\n",
      "8104: [discriminator loss: 0.568520, acc: 0.656250] [adversarial loss: 1.092038, acc: 0.296875]\n",
      "8105: [discriminator loss: 0.539437, acc: 0.734375] [adversarial loss: 1.126533, acc: 0.187500]\n",
      "8106: [discriminator loss: 0.501356, acc: 0.765625] [adversarial loss: 1.174289, acc: 0.218750]\n",
      "8107: [discriminator loss: 0.618327, acc: 0.671875] [adversarial loss: 1.164812, acc: 0.203125]\n",
      "8108: [discriminator loss: 0.531385, acc: 0.742188] [adversarial loss: 1.143913, acc: 0.187500]\n",
      "8109: [discriminator loss: 0.556173, acc: 0.703125] [adversarial loss: 1.299570, acc: 0.156250]\n",
      "8110: [discriminator loss: 0.527635, acc: 0.773438] [adversarial loss: 0.964952, acc: 0.359375]\n",
      "8111: [discriminator loss: 0.576207, acc: 0.656250] [adversarial loss: 1.404392, acc: 0.203125]\n",
      "8112: [discriminator loss: 0.603094, acc: 0.664062] [adversarial loss: 0.868762, acc: 0.468750]\n",
      "8113: [discriminator loss: 0.548868, acc: 0.687500] [adversarial loss: 1.323684, acc: 0.125000]\n",
      "8114: [discriminator loss: 0.546338, acc: 0.742188] [adversarial loss: 0.961850, acc: 0.453125]\n",
      "8115: [discriminator loss: 0.485673, acc: 0.804688] [adversarial loss: 1.263282, acc: 0.187500]\n",
      "8116: [discriminator loss: 0.578063, acc: 0.671875] [adversarial loss: 1.018310, acc: 0.250000]\n",
      "8117: [discriminator loss: 0.564433, acc: 0.726562] [adversarial loss: 1.292730, acc: 0.234375]\n",
      "8118: [discriminator loss: 0.561795, acc: 0.679688] [adversarial loss: 1.046365, acc: 0.234375]\n",
      "8119: [discriminator loss: 0.563785, acc: 0.648438] [adversarial loss: 1.332637, acc: 0.125000]\n",
      "8120: [discriminator loss: 0.532289, acc: 0.726562] [adversarial loss: 0.918746, acc: 0.343750]\n",
      "8121: [discriminator loss: 0.541492, acc: 0.750000] [adversarial loss: 1.608193, acc: 0.093750]\n",
      "8122: [discriminator loss: 0.617777, acc: 0.687500] [adversarial loss: 0.713287, acc: 0.515625]\n",
      "8123: [discriminator loss: 0.573914, acc: 0.671875] [adversarial loss: 1.477036, acc: 0.109375]\n",
      "8124: [discriminator loss: 0.584569, acc: 0.648438] [adversarial loss: 0.829113, acc: 0.406250]\n",
      "8125: [discriminator loss: 0.600769, acc: 0.695312] [adversarial loss: 1.307166, acc: 0.156250]\n",
      "8126: [discriminator loss: 0.504493, acc: 0.726562] [adversarial loss: 0.971738, acc: 0.343750]\n",
      "8127: [discriminator loss: 0.559005, acc: 0.703125] [adversarial loss: 1.470699, acc: 0.125000]\n",
      "8128: [discriminator loss: 0.520105, acc: 0.765625] [adversarial loss: 1.019347, acc: 0.375000]\n",
      "8129: [discriminator loss: 0.550576, acc: 0.687500] [adversarial loss: 1.238326, acc: 0.234375]\n",
      "8130: [discriminator loss: 0.535443, acc: 0.710938] [adversarial loss: 1.219489, acc: 0.250000]\n",
      "8131: [discriminator loss: 0.467725, acc: 0.750000] [adversarial loss: 1.267111, acc: 0.265625]\n",
      "8132: [discriminator loss: 0.501739, acc: 0.703125] [adversarial loss: 1.143634, acc: 0.296875]\n",
      "8133: [discriminator loss: 0.547821, acc: 0.679688] [adversarial loss: 1.027469, acc: 0.375000]\n",
      "8134: [discriminator loss: 0.532065, acc: 0.710938] [adversarial loss: 1.147739, acc: 0.250000]\n",
      "8135: [discriminator loss: 0.524059, acc: 0.703125] [adversarial loss: 1.190316, acc: 0.156250]\n",
      "8136: [discriminator loss: 0.484890, acc: 0.710938] [adversarial loss: 1.006806, acc: 0.328125]\n",
      "8137: [discriminator loss: 0.601161, acc: 0.671875] [adversarial loss: 1.180390, acc: 0.218750]\n",
      "8138: [discriminator loss: 0.491431, acc: 0.750000] [adversarial loss: 1.056906, acc: 0.328125]\n",
      "8139: [discriminator loss: 0.606588, acc: 0.656250] [adversarial loss: 1.132874, acc: 0.296875]\n",
      "8140: [discriminator loss: 0.589835, acc: 0.679688] [adversarial loss: 0.881951, acc: 0.421875]\n",
      "8141: [discriminator loss: 0.627154, acc: 0.648438] [adversarial loss: 1.446590, acc: 0.109375]\n",
      "8142: [discriminator loss: 0.513523, acc: 0.710938] [adversarial loss: 1.060865, acc: 0.390625]\n",
      "8143: [discriminator loss: 0.558826, acc: 0.718750] [adversarial loss: 1.442642, acc: 0.234375]\n",
      "8144: [discriminator loss: 0.527899, acc: 0.742188] [adversarial loss: 1.540805, acc: 0.156250]\n",
      "8145: [discriminator loss: 0.521801, acc: 0.726562] [adversarial loss: 0.790269, acc: 0.453125]\n",
      "8146: [discriminator loss: 0.578728, acc: 0.687500] [adversarial loss: 1.286623, acc: 0.171875]\n",
      "8147: [discriminator loss: 0.613037, acc: 0.648438] [adversarial loss: 0.915842, acc: 0.375000]\n",
      "8148: [discriminator loss: 0.614645, acc: 0.632812] [adversarial loss: 1.248893, acc: 0.171875]\n",
      "8149: [discriminator loss: 0.583406, acc: 0.703125] [adversarial loss: 0.976582, acc: 0.328125]\n",
      "8150: [discriminator loss: 0.517757, acc: 0.796875] [adversarial loss: 1.314275, acc: 0.156250]\n",
      "8151: [discriminator loss: 0.513081, acc: 0.726562] [adversarial loss: 0.915187, acc: 0.390625]\n",
      "8152: [discriminator loss: 0.594555, acc: 0.609375] [adversarial loss: 1.390657, acc: 0.140625]\n",
      "8153: [discriminator loss: 0.579253, acc: 0.679688] [adversarial loss: 0.905581, acc: 0.406250]\n",
      "8154: [discriminator loss: 0.630294, acc: 0.664062] [adversarial loss: 1.567354, acc: 0.093750]\n",
      "8155: [discriminator loss: 0.598833, acc: 0.625000] [adversarial loss: 0.977908, acc: 0.343750]\n",
      "8156: [discriminator loss: 0.556748, acc: 0.726562] [adversarial loss: 1.605857, acc: 0.078125]\n",
      "8157: [discriminator loss: 0.597504, acc: 0.718750] [adversarial loss: 1.064744, acc: 0.375000]\n",
      "8158: [discriminator loss: 0.512579, acc: 0.757812] [adversarial loss: 1.256284, acc: 0.156250]\n",
      "8159: [discriminator loss: 0.509827, acc: 0.718750] [adversarial loss: 1.216461, acc: 0.218750]\n",
      "8160: [discriminator loss: 0.580202, acc: 0.703125] [adversarial loss: 1.001882, acc: 0.265625]\n",
      "8161: [discriminator loss: 0.608172, acc: 0.648438] [adversarial loss: 1.212964, acc: 0.156250]\n",
      "8162: [discriminator loss: 0.563473, acc: 0.695312] [adversarial loss: 1.069365, acc: 0.343750]\n",
      "8163: [discriminator loss: 0.596595, acc: 0.640625] [adversarial loss: 1.128072, acc: 0.343750]\n",
      "8164: [discriminator loss: 0.557732, acc: 0.703125] [adversarial loss: 1.295681, acc: 0.125000]\n",
      "8165: [discriminator loss: 0.489700, acc: 0.765625] [adversarial loss: 1.263931, acc: 0.203125]\n",
      "8166: [discriminator loss: 0.519302, acc: 0.710938] [adversarial loss: 1.130713, acc: 0.281250]\n",
      "8167: [discriminator loss: 0.441388, acc: 0.820312] [adversarial loss: 1.227548, acc: 0.250000]\n",
      "8168: [discriminator loss: 0.624728, acc: 0.671875] [adversarial loss: 0.943625, acc: 0.328125]\n",
      "8169: [discriminator loss: 0.506932, acc: 0.742188] [adversarial loss: 1.303008, acc: 0.171875]\n",
      "8170: [discriminator loss: 0.490791, acc: 0.773438] [adversarial loss: 1.235569, acc: 0.218750]\n",
      "8171: [discriminator loss: 0.584665, acc: 0.679688] [adversarial loss: 1.221764, acc: 0.203125]\n",
      "8172: [discriminator loss: 0.500171, acc: 0.804688] [adversarial loss: 1.006562, acc: 0.328125]\n",
      "8173: [discriminator loss: 0.538658, acc: 0.734375] [adversarial loss: 1.104201, acc: 0.343750]\n",
      "8174: [discriminator loss: 0.609777, acc: 0.656250] [adversarial loss: 1.236593, acc: 0.234375]\n",
      "8175: [discriminator loss: 0.599728, acc: 0.703125] [adversarial loss: 1.188214, acc: 0.281250]\n",
      "8176: [discriminator loss: 0.542378, acc: 0.757812] [adversarial loss: 1.217750, acc: 0.265625]\n",
      "8177: [discriminator loss: 0.525688, acc: 0.757812] [adversarial loss: 1.218307, acc: 0.281250]\n",
      "8178: [discriminator loss: 0.603803, acc: 0.664062] [adversarial loss: 1.020016, acc: 0.359375]\n",
      "8179: [discriminator loss: 0.532693, acc: 0.765625] [adversarial loss: 1.449940, acc: 0.156250]\n",
      "8180: [discriminator loss: 0.566904, acc: 0.687500] [adversarial loss: 0.843580, acc: 0.437500]\n",
      "8181: [discriminator loss: 0.481295, acc: 0.734375] [adversarial loss: 1.501480, acc: 0.109375]\n",
      "8182: [discriminator loss: 0.565793, acc: 0.671875] [adversarial loss: 1.030698, acc: 0.312500]\n",
      "8183: [discriminator loss: 0.593579, acc: 0.710938] [adversarial loss: 1.359982, acc: 0.187500]\n",
      "8184: [discriminator loss: 0.599426, acc: 0.703125] [adversarial loss: 0.924027, acc: 0.421875]\n",
      "8185: [discriminator loss: 0.488086, acc: 0.789062] [adversarial loss: 1.239534, acc: 0.125000]\n",
      "8186: [discriminator loss: 0.560509, acc: 0.664062] [adversarial loss: 1.104249, acc: 0.234375]\n",
      "8187: [discriminator loss: 0.551551, acc: 0.726562] [adversarial loss: 1.125575, acc: 0.359375]\n",
      "8188: [discriminator loss: 0.539065, acc: 0.695312] [adversarial loss: 0.899728, acc: 0.406250]\n",
      "8189: [discriminator loss: 0.532508, acc: 0.726562] [adversarial loss: 1.276791, acc: 0.218750]\n",
      "8190: [discriminator loss: 0.545736, acc: 0.695312] [adversarial loss: 0.644776, acc: 0.656250]\n",
      "8191: [discriminator loss: 0.671298, acc: 0.625000] [adversarial loss: 1.647813, acc: 0.125000]\n",
      "8192: [discriminator loss: 0.498258, acc: 0.765625] [adversarial loss: 1.013593, acc: 0.343750]\n",
      "8193: [discriminator loss: 0.525206, acc: 0.726562] [adversarial loss: 1.041624, acc: 0.328125]\n",
      "8194: [discriminator loss: 0.555316, acc: 0.742188] [adversarial loss: 1.124317, acc: 0.187500]\n",
      "8195: [discriminator loss: 0.562591, acc: 0.695312] [adversarial loss: 1.194048, acc: 0.203125]\n",
      "8196: [discriminator loss: 0.487398, acc: 0.781250] [adversarial loss: 1.419979, acc: 0.125000]\n",
      "8197: [discriminator loss: 0.550881, acc: 0.695312] [adversarial loss: 0.870567, acc: 0.375000]\n",
      "8198: [discriminator loss: 0.580488, acc: 0.679688] [adversarial loss: 1.279044, acc: 0.187500]\n",
      "8199: [discriminator loss: 0.539119, acc: 0.734375] [adversarial loss: 0.906069, acc: 0.453125]\n",
      "8200: [discriminator loss: 0.486813, acc: 0.750000] [adversarial loss: 1.769504, acc: 0.078125]\n",
      "8201: [discriminator loss: 0.574660, acc: 0.671875] [adversarial loss: 1.028086, acc: 0.328125]\n",
      "8202: [discriminator loss: 0.500046, acc: 0.812500] [adversarial loss: 1.398798, acc: 0.171875]\n",
      "8203: [discriminator loss: 0.596797, acc: 0.687500] [adversarial loss: 0.877118, acc: 0.437500]\n",
      "8204: [discriminator loss: 0.518015, acc: 0.757812] [adversarial loss: 1.372116, acc: 0.203125]\n",
      "8205: [discriminator loss: 0.587299, acc: 0.687500] [adversarial loss: 1.131407, acc: 0.187500]\n",
      "8206: [discriminator loss: 0.526534, acc: 0.765625] [adversarial loss: 1.226095, acc: 0.156250]\n",
      "8207: [discriminator loss: 0.529650, acc: 0.687500] [adversarial loss: 1.157869, acc: 0.281250]\n",
      "8208: [discriminator loss: 0.528454, acc: 0.726562] [adversarial loss: 1.226220, acc: 0.203125]\n",
      "8209: [discriminator loss: 0.540192, acc: 0.710938] [adversarial loss: 1.018317, acc: 0.421875]\n",
      "8210: [discriminator loss: 0.485128, acc: 0.789062] [adversarial loss: 1.279314, acc: 0.156250]\n",
      "8211: [discriminator loss: 0.535357, acc: 0.710938] [adversarial loss: 1.061461, acc: 0.359375]\n",
      "8212: [discriminator loss: 0.506895, acc: 0.726562] [adversarial loss: 1.212263, acc: 0.140625]\n",
      "8213: [discriminator loss: 0.588688, acc: 0.656250] [adversarial loss: 1.164183, acc: 0.281250]\n",
      "8214: [discriminator loss: 0.612038, acc: 0.695312] [adversarial loss: 1.288173, acc: 0.187500]\n",
      "8215: [discriminator loss: 0.526719, acc: 0.773438] [adversarial loss: 1.115271, acc: 0.265625]\n",
      "8216: [discriminator loss: 0.493389, acc: 0.757812] [adversarial loss: 1.204563, acc: 0.187500]\n",
      "8217: [discriminator loss: 0.547249, acc: 0.695312] [adversarial loss: 1.162897, acc: 0.218750]\n",
      "8218: [discriminator loss: 0.534692, acc: 0.710938] [adversarial loss: 1.178527, acc: 0.171875]\n",
      "8219: [discriminator loss: 0.541710, acc: 0.718750] [adversarial loss: 1.311883, acc: 0.109375]\n",
      "8220: [discriminator loss: 0.537183, acc: 0.742188] [adversarial loss: 0.952152, acc: 0.375000]\n",
      "8221: [discriminator loss: 0.437965, acc: 0.804688] [adversarial loss: 1.292900, acc: 0.218750]\n",
      "8222: [discriminator loss: 0.494546, acc: 0.773438] [adversarial loss: 0.923031, acc: 0.359375]\n",
      "8223: [discriminator loss: 0.546189, acc: 0.671875] [adversarial loss: 1.394702, acc: 0.171875]\n",
      "8224: [discriminator loss: 0.530744, acc: 0.726562] [adversarial loss: 1.168843, acc: 0.312500]\n",
      "8225: [discriminator loss: 0.509180, acc: 0.703125] [adversarial loss: 1.129629, acc: 0.265625]\n",
      "8226: [discriminator loss: 0.592349, acc: 0.671875] [adversarial loss: 1.322723, acc: 0.125000]\n",
      "8227: [discriminator loss: 0.559278, acc: 0.687500] [adversarial loss: 0.845263, acc: 0.437500]\n",
      "8228: [discriminator loss: 0.610686, acc: 0.640625] [adversarial loss: 1.466466, acc: 0.156250]\n",
      "8229: [discriminator loss: 0.591514, acc: 0.625000] [adversarial loss: 1.066446, acc: 0.359375]\n",
      "8230: [discriminator loss: 0.638607, acc: 0.656250] [adversarial loss: 1.424385, acc: 0.125000]\n",
      "8231: [discriminator loss: 0.586713, acc: 0.687500] [adversarial loss: 0.920309, acc: 0.390625]\n",
      "8232: [discriminator loss: 0.594451, acc: 0.679688] [adversarial loss: 1.257387, acc: 0.203125]\n",
      "8233: [discriminator loss: 0.502422, acc: 0.734375] [adversarial loss: 1.100819, acc: 0.265625]\n",
      "8234: [discriminator loss: 0.548071, acc: 0.726562] [adversarial loss: 1.113013, acc: 0.234375]\n",
      "8235: [discriminator loss: 0.524092, acc: 0.781250] [adversarial loss: 0.975480, acc: 0.359375]\n",
      "8236: [discriminator loss: 0.543787, acc: 0.718750] [adversarial loss: 1.255584, acc: 0.203125]\n",
      "8237: [discriminator loss: 0.562974, acc: 0.726562] [adversarial loss: 1.000266, acc: 0.312500]\n",
      "8238: [discriminator loss: 0.549421, acc: 0.679688] [adversarial loss: 1.348342, acc: 0.125000]\n",
      "8239: [discriminator loss: 0.541240, acc: 0.726562] [adversarial loss: 1.039121, acc: 0.296875]\n",
      "8240: [discriminator loss: 0.525584, acc: 0.742188] [adversarial loss: 1.183400, acc: 0.125000]\n",
      "8241: [discriminator loss: 0.569204, acc: 0.671875] [adversarial loss: 1.083764, acc: 0.250000]\n",
      "8242: [discriminator loss: 0.489699, acc: 0.726562] [adversarial loss: 1.085612, acc: 0.296875]\n",
      "8243: [discriminator loss: 0.564771, acc: 0.703125] [adversarial loss: 1.137006, acc: 0.265625]\n",
      "8244: [discriminator loss: 0.577268, acc: 0.703125] [adversarial loss: 1.375334, acc: 0.171875]\n",
      "8245: [discriminator loss: 0.547792, acc: 0.664062] [adversarial loss: 1.046792, acc: 0.234375]\n",
      "8246: [discriminator loss: 0.477254, acc: 0.781250] [adversarial loss: 1.185842, acc: 0.234375]\n",
      "8247: [discriminator loss: 0.537928, acc: 0.703125] [adversarial loss: 1.467839, acc: 0.093750]\n",
      "8248: [discriminator loss: 0.614848, acc: 0.656250] [adversarial loss: 0.844799, acc: 0.453125]\n",
      "8249: [discriminator loss: 0.575652, acc: 0.679688] [adversarial loss: 1.578973, acc: 0.109375]\n",
      "8250: [discriminator loss: 0.608000, acc: 0.687500] [adversarial loss: 0.832831, acc: 0.421875]\n",
      "8251: [discriminator loss: 0.604338, acc: 0.648438] [adversarial loss: 1.121651, acc: 0.281250]\n",
      "8252: [discriminator loss: 0.538853, acc: 0.765625] [adversarial loss: 0.879969, acc: 0.453125]\n",
      "8253: [discriminator loss: 0.552701, acc: 0.750000] [adversarial loss: 1.266319, acc: 0.218750]\n",
      "8254: [discriminator loss: 0.521296, acc: 0.796875] [adversarial loss: 1.214775, acc: 0.234375]\n",
      "8255: [discriminator loss: 0.526261, acc: 0.718750] [adversarial loss: 1.079906, acc: 0.250000]\n",
      "8256: [discriminator loss: 0.509998, acc: 0.718750] [adversarial loss: 1.116998, acc: 0.203125]\n",
      "8257: [discriminator loss: 0.572972, acc: 0.734375] [adversarial loss: 0.808070, acc: 0.484375]\n",
      "8258: [discriminator loss: 0.517388, acc: 0.742188] [adversarial loss: 1.617308, acc: 0.062500]\n",
      "8259: [discriminator loss: 0.512738, acc: 0.726562] [adversarial loss: 1.210449, acc: 0.203125]\n",
      "8260: [discriminator loss: 0.588143, acc: 0.679688] [adversarial loss: 1.177919, acc: 0.125000]\n",
      "8261: [discriminator loss: 0.513109, acc: 0.742188] [adversarial loss: 1.088654, acc: 0.343750]\n",
      "8262: [discriminator loss: 0.609825, acc: 0.640625] [adversarial loss: 1.502297, acc: 0.093750]\n",
      "8263: [discriminator loss: 0.540915, acc: 0.718750] [adversarial loss: 0.939130, acc: 0.390625]\n",
      "8264: [discriminator loss: 0.546138, acc: 0.695312] [adversarial loss: 1.242222, acc: 0.234375]\n",
      "8265: [discriminator loss: 0.482822, acc: 0.718750] [adversarial loss: 0.932228, acc: 0.359375]\n",
      "8266: [discriminator loss: 0.581003, acc: 0.671875] [adversarial loss: 1.327152, acc: 0.171875]\n",
      "8267: [discriminator loss: 0.624754, acc: 0.625000] [adversarial loss: 0.988229, acc: 0.359375]\n",
      "8268: [discriminator loss: 0.578669, acc: 0.679688] [adversarial loss: 1.416380, acc: 0.171875]\n",
      "8269: [discriminator loss: 0.497626, acc: 0.765625] [adversarial loss: 1.138321, acc: 0.234375]\n",
      "8270: [discriminator loss: 0.560931, acc: 0.718750] [adversarial loss: 1.165358, acc: 0.234375]\n",
      "8271: [discriminator loss: 0.530787, acc: 0.750000] [adversarial loss: 1.130294, acc: 0.234375]\n",
      "8272: [discriminator loss: 0.545005, acc: 0.687500] [adversarial loss: 1.188787, acc: 0.187500]\n",
      "8273: [discriminator loss: 0.561494, acc: 0.718750] [adversarial loss: 0.955643, acc: 0.375000]\n",
      "8274: [discriminator loss: 0.536742, acc: 0.695312] [adversarial loss: 1.568469, acc: 0.109375]\n",
      "8275: [discriminator loss: 0.541324, acc: 0.742188] [adversarial loss: 0.936497, acc: 0.390625]\n",
      "8276: [discriminator loss: 0.547746, acc: 0.734375] [adversarial loss: 1.460721, acc: 0.140625]\n",
      "8277: [discriminator loss: 0.580337, acc: 0.664062] [adversarial loss: 0.783091, acc: 0.593750]\n",
      "8278: [discriminator loss: 0.610416, acc: 0.648438] [adversarial loss: 1.397011, acc: 0.218750]\n",
      "8279: [discriminator loss: 0.506364, acc: 0.750000] [adversarial loss: 1.104620, acc: 0.171875]\n",
      "8280: [discriminator loss: 0.480538, acc: 0.742188] [adversarial loss: 1.472218, acc: 0.125000]\n",
      "8281: [discriminator loss: 0.469484, acc: 0.757812] [adversarial loss: 1.088809, acc: 0.281250]\n",
      "8282: [discriminator loss: 0.513074, acc: 0.773438] [adversarial loss: 1.267014, acc: 0.234375]\n",
      "8283: [discriminator loss: 0.563878, acc: 0.703125] [adversarial loss: 0.951001, acc: 0.343750]\n",
      "8284: [discriminator loss: 0.505421, acc: 0.718750] [adversarial loss: 1.302113, acc: 0.203125]\n",
      "8285: [discriminator loss: 0.513876, acc: 0.750000] [adversarial loss: 1.085274, acc: 0.281250]\n",
      "8286: [discriminator loss: 0.461040, acc: 0.812500] [adversarial loss: 1.333542, acc: 0.156250]\n",
      "8287: [discriminator loss: 0.525268, acc: 0.742188] [adversarial loss: 0.821636, acc: 0.468750]\n",
      "8288: [discriminator loss: 0.561336, acc: 0.757812] [adversarial loss: 1.623862, acc: 0.109375]\n",
      "8289: [discriminator loss: 0.512919, acc: 0.757812] [adversarial loss: 0.948931, acc: 0.375000]\n",
      "8290: [discriminator loss: 0.539573, acc: 0.734375] [adversarial loss: 1.251106, acc: 0.234375]\n",
      "8291: [discriminator loss: 0.558054, acc: 0.710938] [adversarial loss: 1.089362, acc: 0.281250]\n",
      "8292: [discriminator loss: 0.586751, acc: 0.718750] [adversarial loss: 1.288176, acc: 0.234375]\n",
      "8293: [discriminator loss: 0.507478, acc: 0.695312] [adversarial loss: 1.128587, acc: 0.250000]\n",
      "8294: [discriminator loss: 0.535912, acc: 0.726562] [adversarial loss: 1.090608, acc: 0.250000]\n",
      "8295: [discriminator loss: 0.574518, acc: 0.734375] [adversarial loss: 1.422554, acc: 0.140625]\n",
      "8296: [discriminator loss: 0.502069, acc: 0.726562] [adversarial loss: 1.226012, acc: 0.171875]\n",
      "8297: [discriminator loss: 0.530434, acc: 0.734375] [adversarial loss: 1.105288, acc: 0.343750]\n",
      "8298: [discriminator loss: 0.520405, acc: 0.742188] [adversarial loss: 1.057084, acc: 0.359375]\n",
      "8299: [discriminator loss: 0.560591, acc: 0.726562] [adversarial loss: 1.181690, acc: 0.250000]\n",
      "8300: [discriminator loss: 0.610190, acc: 0.656250] [adversarial loss: 0.897354, acc: 0.437500]\n",
      "8301: [discriminator loss: 0.577613, acc: 0.734375] [adversarial loss: 1.483552, acc: 0.140625]\n",
      "8302: [discriminator loss: 0.582749, acc: 0.664062] [adversarial loss: 1.213590, acc: 0.187500]\n",
      "8303: [discriminator loss: 0.512046, acc: 0.734375] [adversarial loss: 1.101408, acc: 0.234375]\n",
      "8304: [discriminator loss: 0.549049, acc: 0.750000] [adversarial loss: 1.139849, acc: 0.281250]\n",
      "8305: [discriminator loss: 0.545007, acc: 0.742188] [adversarial loss: 1.172801, acc: 0.250000]\n",
      "8306: [discriminator loss: 0.577472, acc: 0.632812] [adversarial loss: 1.482700, acc: 0.109375]\n",
      "8307: [discriminator loss: 0.613519, acc: 0.632812] [adversarial loss: 0.773217, acc: 0.500000]\n",
      "8308: [discriminator loss: 0.554529, acc: 0.695312] [adversarial loss: 1.353540, acc: 0.140625]\n",
      "8309: [discriminator loss: 0.488232, acc: 0.757812] [adversarial loss: 1.066671, acc: 0.312500]\n",
      "8310: [discriminator loss: 0.524549, acc: 0.710938] [adversarial loss: 1.570007, acc: 0.125000]\n",
      "8311: [discriminator loss: 0.534695, acc: 0.734375] [adversarial loss: 0.740664, acc: 0.515625]\n",
      "8312: [discriminator loss: 0.614813, acc: 0.656250] [adversarial loss: 1.521983, acc: 0.109375]\n",
      "8313: [discriminator loss: 0.573362, acc: 0.664062] [adversarial loss: 1.095665, acc: 0.296875]\n",
      "8314: [discriminator loss: 0.564648, acc: 0.671875] [adversarial loss: 1.308911, acc: 0.187500]\n",
      "8315: [discriminator loss: 0.551017, acc: 0.734375] [adversarial loss: 0.929163, acc: 0.421875]\n",
      "8316: [discriminator loss: 0.509396, acc: 0.781250] [adversarial loss: 1.277876, acc: 0.125000]\n",
      "8317: [discriminator loss: 0.501564, acc: 0.703125] [adversarial loss: 1.118979, acc: 0.250000]\n",
      "8318: [discriminator loss: 0.574050, acc: 0.648438] [adversarial loss: 1.231364, acc: 0.218750]\n",
      "8319: [discriminator loss: 0.573483, acc: 0.671875] [adversarial loss: 1.012570, acc: 0.328125]\n",
      "8320: [discriminator loss: 0.569592, acc: 0.695312] [adversarial loss: 1.272404, acc: 0.140625]\n",
      "8321: [discriminator loss: 0.541094, acc: 0.742188] [adversarial loss: 1.095060, acc: 0.203125]\n",
      "8322: [discriminator loss: 0.587578, acc: 0.640625] [adversarial loss: 1.430678, acc: 0.109375]\n",
      "8323: [discriminator loss: 0.522420, acc: 0.734375] [adversarial loss: 1.049189, acc: 0.265625]\n",
      "8324: [discriminator loss: 0.641728, acc: 0.640625] [adversarial loss: 1.071218, acc: 0.265625]\n",
      "8325: [discriminator loss: 0.587469, acc: 0.687500] [adversarial loss: 1.006456, acc: 0.312500]\n",
      "8326: [discriminator loss: 0.540230, acc: 0.710938] [adversarial loss: 1.639328, acc: 0.109375]\n",
      "8327: [discriminator loss: 0.590306, acc: 0.679688] [adversarial loss: 1.046503, acc: 0.328125]\n",
      "8328: [discriminator loss: 0.576535, acc: 0.703125] [adversarial loss: 1.488891, acc: 0.156250]\n",
      "8329: [discriminator loss: 0.572475, acc: 0.718750] [adversarial loss: 1.097566, acc: 0.328125]\n",
      "8330: [discriminator loss: 0.506361, acc: 0.742188] [adversarial loss: 1.375723, acc: 0.156250]\n",
      "8331: [discriminator loss: 0.552846, acc: 0.710938] [adversarial loss: 1.344360, acc: 0.187500]\n",
      "8332: [discriminator loss: 0.510746, acc: 0.750000] [adversarial loss: 1.155583, acc: 0.234375]\n",
      "8333: [discriminator loss: 0.563436, acc: 0.726562] [adversarial loss: 1.291766, acc: 0.234375]\n",
      "8334: [discriminator loss: 0.540802, acc: 0.734375] [adversarial loss: 1.289719, acc: 0.140625]\n",
      "8335: [discriminator loss: 0.542662, acc: 0.703125] [adversarial loss: 0.993687, acc: 0.250000]\n",
      "8336: [discriminator loss: 0.566063, acc: 0.640625] [adversarial loss: 1.091457, acc: 0.343750]\n",
      "8337: [discriminator loss: 0.609243, acc: 0.695312] [adversarial loss: 1.128368, acc: 0.250000]\n",
      "8338: [discriminator loss: 0.463739, acc: 0.796875] [adversarial loss: 1.542146, acc: 0.125000]\n",
      "8339: [discriminator loss: 0.558612, acc: 0.718750] [adversarial loss: 1.194250, acc: 0.234375]\n",
      "8340: [discriminator loss: 0.583021, acc: 0.718750] [adversarial loss: 1.333768, acc: 0.187500]\n",
      "8341: [discriminator loss: 0.517638, acc: 0.757812] [adversarial loss: 1.192507, acc: 0.187500]\n",
      "8342: [discriminator loss: 0.595676, acc: 0.671875] [adversarial loss: 1.245789, acc: 0.203125]\n",
      "8343: [discriminator loss: 0.548380, acc: 0.648438] [adversarial loss: 1.407486, acc: 0.203125]\n",
      "8344: [discriminator loss: 0.592398, acc: 0.695312] [adversarial loss: 1.037536, acc: 0.234375]\n",
      "8345: [discriminator loss: 0.549265, acc: 0.703125] [adversarial loss: 1.306751, acc: 0.140625]\n",
      "8346: [discriminator loss: 0.646267, acc: 0.640625] [adversarial loss: 0.997050, acc: 0.312500]\n",
      "8347: [discriminator loss: 0.587671, acc: 0.710938] [adversarial loss: 1.166247, acc: 0.265625]\n",
      "8348: [discriminator loss: 0.531471, acc: 0.750000] [adversarial loss: 1.221019, acc: 0.171875]\n",
      "8349: [discriminator loss: 0.499409, acc: 0.757812] [adversarial loss: 1.354696, acc: 0.125000]\n",
      "8350: [discriminator loss: 0.546569, acc: 0.695312] [adversarial loss: 1.015350, acc: 0.375000]\n",
      "8351: [discriminator loss: 0.543319, acc: 0.734375] [adversarial loss: 1.399679, acc: 0.078125]\n",
      "8352: [discriminator loss: 0.553776, acc: 0.695312] [adversarial loss: 1.030343, acc: 0.296875]\n",
      "8353: [discriminator loss: 0.555399, acc: 0.757812] [adversarial loss: 1.326740, acc: 0.203125]\n",
      "8354: [discriminator loss: 0.509022, acc: 0.750000] [adversarial loss: 1.237732, acc: 0.187500]\n",
      "8355: [discriminator loss: 0.499578, acc: 0.773438] [adversarial loss: 1.086372, acc: 0.281250]\n",
      "8356: [discriminator loss: 0.487462, acc: 0.781250] [adversarial loss: 1.102513, acc: 0.296875]\n",
      "8357: [discriminator loss: 0.504916, acc: 0.757812] [adversarial loss: 1.144461, acc: 0.265625]\n",
      "8358: [discriminator loss: 0.513483, acc: 0.718750] [adversarial loss: 1.212402, acc: 0.187500]\n",
      "8359: [discriminator loss: 0.538954, acc: 0.703125] [adversarial loss: 1.014907, acc: 0.375000]\n",
      "8360: [discriminator loss: 0.597450, acc: 0.687500] [adversarial loss: 1.231213, acc: 0.343750]\n",
      "8361: [discriminator loss: 0.569138, acc: 0.695312] [adversarial loss: 1.266215, acc: 0.234375]\n",
      "8362: [discriminator loss: 0.591338, acc: 0.656250] [adversarial loss: 0.808141, acc: 0.500000]\n",
      "8363: [discriminator loss: 0.497047, acc: 0.750000] [adversarial loss: 1.496918, acc: 0.109375]\n",
      "8364: [discriminator loss: 0.586474, acc: 0.679688] [adversarial loss: 0.892414, acc: 0.406250]\n",
      "8365: [discriminator loss: 0.521999, acc: 0.757812] [adversarial loss: 1.175887, acc: 0.250000]\n",
      "8366: [discriminator loss: 0.556508, acc: 0.687500] [adversarial loss: 1.318129, acc: 0.156250]\n",
      "8367: [discriminator loss: 0.585297, acc: 0.664062] [adversarial loss: 0.880608, acc: 0.406250]\n",
      "8368: [discriminator loss: 0.576974, acc: 0.664062] [adversarial loss: 1.477453, acc: 0.109375]\n",
      "8369: [discriminator loss: 0.578166, acc: 0.710938] [adversarial loss: 1.024119, acc: 0.421875]\n",
      "8370: [discriminator loss: 0.563263, acc: 0.671875] [adversarial loss: 1.186602, acc: 0.296875]\n",
      "8371: [discriminator loss: 0.491975, acc: 0.734375] [adversarial loss: 0.878670, acc: 0.406250]\n",
      "8372: [discriminator loss: 0.486005, acc: 0.781250] [adversarial loss: 1.241053, acc: 0.171875]\n",
      "8373: [discriminator loss: 0.574801, acc: 0.703125] [adversarial loss: 1.157029, acc: 0.234375]\n",
      "8374: [discriminator loss: 0.525839, acc: 0.757812] [adversarial loss: 1.330258, acc: 0.156250]\n",
      "8375: [discriminator loss: 0.526004, acc: 0.703125] [adversarial loss: 0.797499, acc: 0.531250]\n",
      "8376: [discriminator loss: 0.555898, acc: 0.710938] [adversarial loss: 1.422467, acc: 0.156250]\n",
      "8377: [discriminator loss: 0.644516, acc: 0.601562] [adversarial loss: 0.786646, acc: 0.546875]\n",
      "8378: [discriminator loss: 0.581780, acc: 0.703125] [adversarial loss: 1.536704, acc: 0.125000]\n",
      "8379: [discriminator loss: 0.565582, acc: 0.687500] [adversarial loss: 0.831593, acc: 0.515625]\n",
      "8380: [discriminator loss: 0.634520, acc: 0.648438] [adversarial loss: 1.533889, acc: 0.125000]\n",
      "8381: [discriminator loss: 0.568042, acc: 0.695312] [adversarial loss: 0.917059, acc: 0.328125]\n",
      "8382: [discriminator loss: 0.510865, acc: 0.781250] [adversarial loss: 1.237586, acc: 0.203125]\n",
      "8383: [discriminator loss: 0.515364, acc: 0.750000] [adversarial loss: 1.050413, acc: 0.312500]\n",
      "8384: [discriminator loss: 0.549518, acc: 0.718750] [adversarial loss: 1.217890, acc: 0.203125]\n",
      "8385: [discriminator loss: 0.602010, acc: 0.687500] [adversarial loss: 1.340362, acc: 0.187500]\n",
      "8386: [discriminator loss: 0.543935, acc: 0.734375] [adversarial loss: 1.011143, acc: 0.312500]\n",
      "8387: [discriminator loss: 0.554254, acc: 0.734375] [adversarial loss: 1.097970, acc: 0.187500]\n",
      "8388: [discriminator loss: 0.518697, acc: 0.765625] [adversarial loss: 1.205934, acc: 0.265625]\n",
      "8389: [discriminator loss: 0.535231, acc: 0.679688] [adversarial loss: 0.907774, acc: 0.359375]\n",
      "8390: [discriminator loss: 0.595861, acc: 0.695312] [adversarial loss: 1.318262, acc: 0.203125]\n",
      "8391: [discriminator loss: 0.511649, acc: 0.773438] [adversarial loss: 1.231871, acc: 0.125000]\n",
      "8392: [discriminator loss: 0.544207, acc: 0.710938] [adversarial loss: 1.065665, acc: 0.343750]\n",
      "8393: [discriminator loss: 0.575412, acc: 0.687500] [adversarial loss: 1.091442, acc: 0.265625]\n",
      "8394: [discriminator loss: 0.564914, acc: 0.710938] [adversarial loss: 1.150211, acc: 0.265625]\n",
      "8395: [discriminator loss: 0.543721, acc: 0.703125] [adversarial loss: 1.232579, acc: 0.218750]\n",
      "8396: [discriminator loss: 0.543714, acc: 0.695312] [adversarial loss: 1.027455, acc: 0.343750]\n",
      "8397: [discriminator loss: 0.576536, acc: 0.734375] [adversarial loss: 1.319680, acc: 0.156250]\n",
      "8398: [discriminator loss: 0.562289, acc: 0.703125] [adversarial loss: 1.153974, acc: 0.218750]\n",
      "8399: [discriminator loss: 0.562551, acc: 0.695312] [adversarial loss: 1.423027, acc: 0.156250]\n",
      "8400: [discriminator loss: 0.509560, acc: 0.781250] [adversarial loss: 1.073258, acc: 0.265625]\n",
      "8401: [discriminator loss: 0.607256, acc: 0.703125] [adversarial loss: 1.267093, acc: 0.265625]\n",
      "8402: [discriminator loss: 0.493404, acc: 0.750000] [adversarial loss: 1.043915, acc: 0.328125]\n",
      "8403: [discriminator loss: 0.519869, acc: 0.726562] [adversarial loss: 1.129229, acc: 0.265625]\n",
      "8404: [discriminator loss: 0.550861, acc: 0.703125] [adversarial loss: 0.849669, acc: 0.468750]\n",
      "8405: [discriminator loss: 0.475507, acc: 0.765625] [adversarial loss: 1.532436, acc: 0.140625]\n",
      "8406: [discriminator loss: 0.495275, acc: 0.710938] [adversarial loss: 1.248729, acc: 0.203125]\n",
      "8407: [discriminator loss: 0.500296, acc: 0.765625] [adversarial loss: 1.243515, acc: 0.250000]\n",
      "8408: [discriminator loss: 0.553240, acc: 0.718750] [adversarial loss: 1.110466, acc: 0.250000]\n",
      "8409: [discriminator loss: 0.585885, acc: 0.718750] [adversarial loss: 1.367262, acc: 0.187500]\n",
      "8410: [discriminator loss: 0.545950, acc: 0.703125] [adversarial loss: 1.111053, acc: 0.187500]\n",
      "8411: [discriminator loss: 0.573230, acc: 0.695312] [adversarial loss: 1.026062, acc: 0.375000]\n",
      "8412: [discriminator loss: 0.510582, acc: 0.773438] [adversarial loss: 1.165990, acc: 0.234375]\n",
      "8413: [discriminator loss: 0.564037, acc: 0.695312] [adversarial loss: 1.202129, acc: 0.187500]\n",
      "8414: [discriminator loss: 0.550789, acc: 0.687500] [adversarial loss: 1.196702, acc: 0.265625]\n",
      "8415: [discriminator loss: 0.525994, acc: 0.726562] [adversarial loss: 0.802200, acc: 0.421875]\n",
      "8416: [discriminator loss: 0.613034, acc: 0.648438] [adversarial loss: 1.481784, acc: 0.171875]\n",
      "8417: [discriminator loss: 0.587755, acc: 0.656250] [adversarial loss: 0.942365, acc: 0.406250]\n",
      "8418: [discriminator loss: 0.557731, acc: 0.718750] [adversarial loss: 1.228660, acc: 0.156250]\n",
      "8419: [discriminator loss: 0.550560, acc: 0.742188] [adversarial loss: 1.134341, acc: 0.312500]\n",
      "8420: [discriminator loss: 0.581303, acc: 0.703125] [adversarial loss: 1.197117, acc: 0.265625]\n",
      "8421: [discriminator loss: 0.521510, acc: 0.718750] [adversarial loss: 1.004970, acc: 0.328125]\n",
      "8422: [discriminator loss: 0.532148, acc: 0.750000] [adversarial loss: 1.152748, acc: 0.203125]\n",
      "8423: [discriminator loss: 0.601253, acc: 0.710938] [adversarial loss: 1.493212, acc: 0.187500]\n",
      "8424: [discriminator loss: 0.572643, acc: 0.718750] [adversarial loss: 0.898012, acc: 0.406250]\n",
      "8425: [discriminator loss: 0.535668, acc: 0.695312] [adversarial loss: 1.531052, acc: 0.109375]\n",
      "8426: [discriminator loss: 0.632169, acc: 0.648438] [adversarial loss: 1.057450, acc: 0.375000]\n",
      "8427: [discriminator loss: 0.537763, acc: 0.687500] [adversarial loss: 1.206105, acc: 0.250000]\n",
      "8428: [discriminator loss: 0.626525, acc: 0.640625] [adversarial loss: 1.191676, acc: 0.203125]\n",
      "8429: [discriminator loss: 0.523151, acc: 0.710938] [adversarial loss: 1.218768, acc: 0.140625]\n",
      "8430: [discriminator loss: 0.562436, acc: 0.710938] [adversarial loss: 0.835183, acc: 0.453125]\n",
      "8431: [discriminator loss: 0.484599, acc: 0.742188] [adversarial loss: 1.162262, acc: 0.234375]\n",
      "8432: [discriminator loss: 0.569457, acc: 0.687500] [adversarial loss: 1.023067, acc: 0.359375]\n",
      "8433: [discriminator loss: 0.583582, acc: 0.718750] [adversarial loss: 1.265464, acc: 0.250000]\n",
      "8434: [discriminator loss: 0.581806, acc: 0.718750] [adversarial loss: 1.174600, acc: 0.265625]\n",
      "8435: [discriminator loss: 0.547119, acc: 0.695312] [adversarial loss: 1.191071, acc: 0.218750]\n",
      "8436: [discriminator loss: 0.542046, acc: 0.718750] [adversarial loss: 1.199531, acc: 0.234375]\n",
      "8437: [discriminator loss: 0.548185, acc: 0.757812] [adversarial loss: 0.997936, acc: 0.343750]\n",
      "8438: [discriminator loss: 0.436570, acc: 0.828125] [adversarial loss: 1.274000, acc: 0.140625]\n",
      "8439: [discriminator loss: 0.566224, acc: 0.695312] [adversarial loss: 1.074610, acc: 0.343750]\n",
      "8440: [discriminator loss: 0.583410, acc: 0.703125] [adversarial loss: 1.373960, acc: 0.250000]\n",
      "8441: [discriminator loss: 0.575602, acc: 0.632812] [adversarial loss: 0.913315, acc: 0.375000]\n",
      "8442: [discriminator loss: 0.509228, acc: 0.718750] [adversarial loss: 1.497599, acc: 0.109375]\n",
      "8443: [discriminator loss: 0.593739, acc: 0.679688] [adversarial loss: 0.937175, acc: 0.406250]\n",
      "8444: [discriminator loss: 0.514970, acc: 0.734375] [adversarial loss: 1.469617, acc: 0.156250]\n",
      "8445: [discriminator loss: 0.615847, acc: 0.679688] [adversarial loss: 1.106599, acc: 0.343750]\n",
      "8446: [discriminator loss: 0.514343, acc: 0.757812] [adversarial loss: 1.328426, acc: 0.171875]\n",
      "8447: [discriminator loss: 0.553096, acc: 0.695312] [adversarial loss: 0.879627, acc: 0.421875]\n",
      "8448: [discriminator loss: 0.539329, acc: 0.726562] [adversarial loss: 1.599377, acc: 0.125000]\n",
      "8449: [discriminator loss: 0.555817, acc: 0.710938] [adversarial loss: 1.032753, acc: 0.312500]\n",
      "8450: [discriminator loss: 0.605894, acc: 0.695312] [adversarial loss: 1.462589, acc: 0.093750]\n",
      "8451: [discriminator loss: 0.542748, acc: 0.671875] [adversarial loss: 1.063463, acc: 0.296875]\n",
      "8452: [discriminator loss: 0.570918, acc: 0.664062] [adversarial loss: 1.272712, acc: 0.281250]\n",
      "8453: [discriminator loss: 0.535270, acc: 0.695312] [adversarial loss: 1.184280, acc: 0.171875]\n",
      "8454: [discriminator loss: 0.530800, acc: 0.781250] [adversarial loss: 1.143347, acc: 0.250000]\n",
      "8455: [discriminator loss: 0.497543, acc: 0.742188] [adversarial loss: 1.108720, acc: 0.218750]\n",
      "8456: [discriminator loss: 0.585497, acc: 0.656250] [adversarial loss: 1.230892, acc: 0.218750]\n",
      "8457: [discriminator loss: 0.571407, acc: 0.695312] [adversarial loss: 1.444809, acc: 0.109375]\n",
      "8458: [discriminator loss: 0.481676, acc: 0.757812] [adversarial loss: 1.154535, acc: 0.218750]\n",
      "8459: [discriminator loss: 0.511341, acc: 0.703125] [adversarial loss: 1.149694, acc: 0.343750]\n",
      "8460: [discriminator loss: 0.578384, acc: 0.703125] [adversarial loss: 1.223056, acc: 0.296875]\n",
      "8461: [discriminator loss: 0.525333, acc: 0.765625] [adversarial loss: 1.305917, acc: 0.218750]\n",
      "8462: [discriminator loss: 0.526788, acc: 0.734375] [adversarial loss: 0.961524, acc: 0.328125]\n",
      "8463: [discriminator loss: 0.582446, acc: 0.679688] [adversarial loss: 1.375100, acc: 0.093750]\n",
      "8464: [discriminator loss: 0.597955, acc: 0.648438] [adversarial loss: 0.866305, acc: 0.328125]\n",
      "8465: [discriminator loss: 0.542678, acc: 0.718750] [adversarial loss: 1.303300, acc: 0.234375]\n",
      "8466: [discriminator loss: 0.570339, acc: 0.679688] [adversarial loss: 0.993085, acc: 0.375000]\n",
      "8467: [discriminator loss: 0.502485, acc: 0.789062] [adversarial loss: 1.395547, acc: 0.156250]\n",
      "8468: [discriminator loss: 0.577370, acc: 0.710938] [adversarial loss: 0.934917, acc: 0.406250]\n",
      "8469: [discriminator loss: 0.529910, acc: 0.726562] [adversarial loss: 1.242444, acc: 0.187500]\n",
      "8470: [discriminator loss: 0.491791, acc: 0.828125] [adversarial loss: 1.228203, acc: 0.156250]\n",
      "8471: [discriminator loss: 0.539832, acc: 0.734375] [adversarial loss: 0.927254, acc: 0.375000]\n",
      "8472: [discriminator loss: 0.590542, acc: 0.664062] [adversarial loss: 1.191881, acc: 0.203125]\n",
      "8473: [discriminator loss: 0.532003, acc: 0.750000] [adversarial loss: 0.988381, acc: 0.312500]\n",
      "8474: [discriminator loss: 0.592870, acc: 0.703125] [adversarial loss: 1.424769, acc: 0.109375]\n",
      "8475: [discriminator loss: 0.537605, acc: 0.757812] [adversarial loss: 1.071121, acc: 0.281250]\n",
      "8476: [discriminator loss: 0.554729, acc: 0.710938] [adversarial loss: 1.460837, acc: 0.156250]\n",
      "8477: [discriminator loss: 0.642618, acc: 0.617188] [adversarial loss: 0.743405, acc: 0.578125]\n",
      "8478: [discriminator loss: 0.538859, acc: 0.679688] [adversarial loss: 1.340029, acc: 0.078125]\n",
      "8479: [discriminator loss: 0.577315, acc: 0.664062] [adversarial loss: 0.953162, acc: 0.390625]\n",
      "8480: [discriminator loss: 0.591052, acc: 0.687500] [adversarial loss: 1.315493, acc: 0.156250]\n",
      "8481: [discriminator loss: 0.501730, acc: 0.718750] [adversarial loss: 1.259793, acc: 0.125000]\n",
      "8482: [discriminator loss: 0.606337, acc: 0.625000] [adversarial loss: 1.073677, acc: 0.234375]\n",
      "8483: [discriminator loss: 0.531894, acc: 0.734375] [adversarial loss: 1.296885, acc: 0.218750]\n",
      "8484: [discriminator loss: 0.578678, acc: 0.656250] [adversarial loss: 0.924809, acc: 0.328125]\n",
      "8485: [discriminator loss: 0.574580, acc: 0.687500] [adversarial loss: 1.471802, acc: 0.093750]\n",
      "8486: [discriminator loss: 0.536119, acc: 0.750000] [adversarial loss: 0.851187, acc: 0.390625]\n",
      "8487: [discriminator loss: 0.572115, acc: 0.687500] [adversarial loss: 1.519063, acc: 0.109375]\n",
      "8488: [discriminator loss: 0.549814, acc: 0.734375] [adversarial loss: 1.050615, acc: 0.281250]\n",
      "8489: [discriminator loss: 0.533895, acc: 0.757812] [adversarial loss: 1.165816, acc: 0.234375]\n",
      "8490: [discriminator loss: 0.494573, acc: 0.757812] [adversarial loss: 1.456367, acc: 0.125000]\n",
      "8491: [discriminator loss: 0.596377, acc: 0.671875] [adversarial loss: 1.026183, acc: 0.375000]\n",
      "8492: [discriminator loss: 0.531952, acc: 0.695312] [adversarial loss: 1.208976, acc: 0.156250]\n",
      "8493: [discriminator loss: 0.554517, acc: 0.734375] [adversarial loss: 1.122669, acc: 0.281250]\n",
      "8494: [discriminator loss: 0.492610, acc: 0.757812] [adversarial loss: 1.086880, acc: 0.265625]\n",
      "8495: [discriminator loss: 0.495394, acc: 0.750000] [adversarial loss: 1.310181, acc: 0.234375]\n",
      "8496: [discriminator loss: 0.635212, acc: 0.640625] [adversarial loss: 0.848518, acc: 0.453125]\n",
      "8497: [discriminator loss: 0.648288, acc: 0.648438] [adversarial loss: 1.422484, acc: 0.140625]\n",
      "8498: [discriminator loss: 0.559777, acc: 0.656250] [adversarial loss: 0.898489, acc: 0.484375]\n",
      "8499: [discriminator loss: 0.543263, acc: 0.734375] [adversarial loss: 1.260744, acc: 0.203125]\n",
      "8500: [discriminator loss: 0.524836, acc: 0.734375] [adversarial loss: 1.216109, acc: 0.250000]\n",
      "8501: [discriminator loss: 0.612560, acc: 0.679688] [adversarial loss: 1.208406, acc: 0.171875]\n",
      "8502: [discriminator loss: 0.529432, acc: 0.687500] [adversarial loss: 1.064678, acc: 0.234375]\n",
      "8503: [discriminator loss: 0.536300, acc: 0.742188] [adversarial loss: 1.250431, acc: 0.109375]\n",
      "8504: [discriminator loss: 0.586327, acc: 0.718750] [adversarial loss: 1.212266, acc: 0.203125]\n",
      "8505: [discriminator loss: 0.580985, acc: 0.679688] [adversarial loss: 1.224976, acc: 0.171875]\n",
      "8506: [discriminator loss: 0.521272, acc: 0.750000] [adversarial loss: 0.900092, acc: 0.375000]\n",
      "8507: [discriminator loss: 0.550397, acc: 0.648438] [adversarial loss: 1.053667, acc: 0.187500]\n",
      "8508: [discriminator loss: 0.542368, acc: 0.656250] [adversarial loss: 1.099512, acc: 0.312500]\n",
      "8509: [discriminator loss: 0.604484, acc: 0.617188] [adversarial loss: 1.460545, acc: 0.078125]\n",
      "8510: [discriminator loss: 0.524040, acc: 0.703125] [adversarial loss: 1.017787, acc: 0.250000]\n",
      "8511: [discriminator loss: 0.584428, acc: 0.664062] [adversarial loss: 1.331367, acc: 0.156250]\n",
      "8512: [discriminator loss: 0.559862, acc: 0.742188] [adversarial loss: 1.159151, acc: 0.250000]\n",
      "8513: [discriminator loss: 0.491545, acc: 0.781250] [adversarial loss: 1.352244, acc: 0.171875]\n",
      "8514: [discriminator loss: 0.548723, acc: 0.695312] [adversarial loss: 1.096519, acc: 0.265625]\n",
      "8515: [discriminator loss: 0.553230, acc: 0.710938] [adversarial loss: 1.305461, acc: 0.125000]\n",
      "8516: [discriminator loss: 0.480504, acc: 0.781250] [adversarial loss: 0.997940, acc: 0.359375]\n",
      "8517: [discriminator loss: 0.470140, acc: 0.796875] [adversarial loss: 1.417931, acc: 0.109375]\n",
      "8518: [discriminator loss: 0.569800, acc: 0.687500] [adversarial loss: 0.751808, acc: 0.484375]\n",
      "8519: [discriminator loss: 0.618934, acc: 0.671875] [adversarial loss: 1.332285, acc: 0.171875]\n",
      "8520: [discriminator loss: 0.510040, acc: 0.781250] [adversarial loss: 0.934339, acc: 0.328125]\n",
      "8521: [discriminator loss: 0.527420, acc: 0.710938] [adversarial loss: 1.290352, acc: 0.156250]\n",
      "8522: [discriminator loss: 0.546842, acc: 0.718750] [adversarial loss: 1.363454, acc: 0.218750]\n",
      "8523: [discriminator loss: 0.601391, acc: 0.671875] [adversarial loss: 1.203081, acc: 0.171875]\n",
      "8524: [discriminator loss: 0.505963, acc: 0.789062] [adversarial loss: 1.369783, acc: 0.156250]\n",
      "8525: [discriminator loss: 0.582454, acc: 0.718750] [adversarial loss: 0.901780, acc: 0.359375]\n",
      "8526: [discriminator loss: 0.480239, acc: 0.789062] [adversarial loss: 1.590638, acc: 0.109375]\n",
      "8527: [discriminator loss: 0.516189, acc: 0.710938] [adversarial loss: 0.945055, acc: 0.390625]\n",
      "8528: [discriminator loss: 0.516984, acc: 0.750000] [adversarial loss: 1.150879, acc: 0.312500]\n",
      "8529: [discriminator loss: 0.659725, acc: 0.640625] [adversarial loss: 1.068237, acc: 0.250000]\n",
      "8530: [discriminator loss: 0.635454, acc: 0.648438] [adversarial loss: 1.357725, acc: 0.125000]\n",
      "8531: [discriminator loss: 0.612756, acc: 0.656250] [adversarial loss: 1.029761, acc: 0.296875]\n",
      "8532: [discriminator loss: 0.623549, acc: 0.703125] [adversarial loss: 1.014415, acc: 0.281250]\n",
      "8533: [discriminator loss: 0.594602, acc: 0.640625] [adversarial loss: 0.953611, acc: 0.375000]\n",
      "8534: [discriminator loss: 0.538845, acc: 0.726562] [adversarial loss: 1.316483, acc: 0.187500]\n",
      "8535: [discriminator loss: 0.568017, acc: 0.734375] [adversarial loss: 1.014036, acc: 0.312500]\n",
      "8536: [discriminator loss: 0.570636, acc: 0.679688] [adversarial loss: 1.232534, acc: 0.203125]\n",
      "8537: [discriminator loss: 0.566310, acc: 0.750000] [adversarial loss: 0.822311, acc: 0.406250]\n",
      "8538: [discriminator loss: 0.560958, acc: 0.742188] [adversarial loss: 1.175881, acc: 0.218750]\n",
      "8539: [discriminator loss: 0.573545, acc: 0.710938] [adversarial loss: 0.823396, acc: 0.437500]\n",
      "8540: [discriminator loss: 0.538790, acc: 0.734375] [adversarial loss: 1.342104, acc: 0.218750]\n",
      "8541: [discriminator loss: 0.501504, acc: 0.750000] [adversarial loss: 1.313227, acc: 0.171875]\n",
      "8542: [discriminator loss: 0.547851, acc: 0.710938] [adversarial loss: 1.240111, acc: 0.203125]\n",
      "8543: [discriminator loss: 0.528901, acc: 0.765625] [adversarial loss: 1.008309, acc: 0.359375]\n",
      "8544: [discriminator loss: 0.543211, acc: 0.718750] [adversarial loss: 1.334927, acc: 0.187500]\n",
      "8545: [discriminator loss: 0.562452, acc: 0.718750] [adversarial loss: 1.151818, acc: 0.312500]\n",
      "8546: [discriminator loss: 0.529086, acc: 0.679688] [adversarial loss: 1.544211, acc: 0.078125]\n",
      "8547: [discriminator loss: 0.582989, acc: 0.710938] [adversarial loss: 0.832748, acc: 0.468750]\n",
      "8548: [discriminator loss: 0.568507, acc: 0.710938] [adversarial loss: 1.168862, acc: 0.281250]\n",
      "8549: [discriminator loss: 0.534707, acc: 0.718750] [adversarial loss: 0.941873, acc: 0.359375]\n",
      "8550: [discriminator loss: 0.629426, acc: 0.640625] [adversarial loss: 1.317895, acc: 0.296875]\n",
      "8551: [discriminator loss: 0.552183, acc: 0.695312] [adversarial loss: 0.970163, acc: 0.437500]\n",
      "8552: [discriminator loss: 0.507710, acc: 0.718750] [adversarial loss: 1.424188, acc: 0.156250]\n",
      "8553: [discriminator loss: 0.533591, acc: 0.703125] [adversarial loss: 0.992418, acc: 0.359375]\n",
      "8554: [discriminator loss: 0.556224, acc: 0.726562] [adversarial loss: 1.165772, acc: 0.187500]\n",
      "8555: [discriminator loss: 0.511338, acc: 0.718750] [adversarial loss: 1.046300, acc: 0.296875]\n",
      "8556: [discriminator loss: 0.485113, acc: 0.757812] [adversarial loss: 1.583750, acc: 0.109375]\n",
      "8557: [discriminator loss: 0.662999, acc: 0.656250] [adversarial loss: 0.900099, acc: 0.406250]\n",
      "8558: [discriminator loss: 0.590562, acc: 0.695312] [adversarial loss: 1.523018, acc: 0.093750]\n",
      "8559: [discriminator loss: 0.596605, acc: 0.703125] [adversarial loss: 0.962745, acc: 0.421875]\n",
      "8560: [discriminator loss: 0.499305, acc: 0.734375] [adversarial loss: 1.099943, acc: 0.312500]\n",
      "8561: [discriminator loss: 0.547360, acc: 0.734375] [adversarial loss: 1.170022, acc: 0.312500]\n",
      "8562: [discriminator loss: 0.615142, acc: 0.640625] [adversarial loss: 1.055563, acc: 0.359375]\n",
      "8563: [discriminator loss: 0.528175, acc: 0.742188] [adversarial loss: 0.927153, acc: 0.343750]\n",
      "8564: [discriminator loss: 0.496349, acc: 0.796875] [adversarial loss: 1.121999, acc: 0.250000]\n",
      "8565: [discriminator loss: 0.514220, acc: 0.726562] [adversarial loss: 1.171208, acc: 0.171875]\n",
      "8566: [discriminator loss: 0.589259, acc: 0.671875] [adversarial loss: 1.264839, acc: 0.171875]\n",
      "8567: [discriminator loss: 0.570535, acc: 0.671875] [adversarial loss: 1.101920, acc: 0.328125]\n",
      "8568: [discriminator loss: 0.581922, acc: 0.664062] [adversarial loss: 1.276879, acc: 0.156250]\n",
      "8569: [discriminator loss: 0.555174, acc: 0.695312] [adversarial loss: 1.030003, acc: 0.281250]\n",
      "8570: [discriminator loss: 0.510252, acc: 0.765625] [adversarial loss: 1.525186, acc: 0.140625]\n",
      "8571: [discriminator loss: 0.547825, acc: 0.750000] [adversarial loss: 0.839247, acc: 0.375000]\n",
      "8572: [discriminator loss: 0.560652, acc: 0.671875] [adversarial loss: 1.419211, acc: 0.125000]\n",
      "8573: [discriminator loss: 0.584509, acc: 0.710938] [adversarial loss: 0.923362, acc: 0.375000]\n",
      "8574: [discriminator loss: 0.596036, acc: 0.695312] [adversarial loss: 1.223395, acc: 0.265625]\n",
      "8575: [discriminator loss: 0.629863, acc: 0.617188] [adversarial loss: 1.001066, acc: 0.343750]\n",
      "8576: [discriminator loss: 0.449853, acc: 0.789062] [adversarial loss: 1.309423, acc: 0.218750]\n",
      "8577: [discriminator loss: 0.479779, acc: 0.804688] [adversarial loss: 1.324384, acc: 0.109375]\n",
      "8578: [discriminator loss: 0.566387, acc: 0.734375] [adversarial loss: 1.217109, acc: 0.296875]\n",
      "8579: [discriminator loss: 0.589376, acc: 0.648438] [adversarial loss: 1.061975, acc: 0.250000]\n",
      "8580: [discriminator loss: 0.554207, acc: 0.734375] [adversarial loss: 1.092337, acc: 0.296875]\n",
      "8581: [discriminator loss: 0.544749, acc: 0.734375] [adversarial loss: 1.009968, acc: 0.359375]\n",
      "8582: [discriminator loss: 0.552150, acc: 0.718750] [adversarial loss: 1.138108, acc: 0.250000]\n",
      "8583: [discriminator loss: 0.561544, acc: 0.718750] [adversarial loss: 1.220287, acc: 0.265625]\n",
      "8584: [discriminator loss: 0.592691, acc: 0.695312] [adversarial loss: 0.789945, acc: 0.484375]\n",
      "8585: [discriminator loss: 0.541469, acc: 0.742188] [adversarial loss: 1.324229, acc: 0.140625]\n",
      "8586: [discriminator loss: 0.586271, acc: 0.679688] [adversarial loss: 1.141782, acc: 0.234375]\n",
      "8587: [discriminator loss: 0.650510, acc: 0.617188] [adversarial loss: 1.304260, acc: 0.140625]\n",
      "8588: [discriminator loss: 0.623971, acc: 0.671875] [adversarial loss: 0.957825, acc: 0.375000]\n",
      "8589: [discriminator loss: 0.568682, acc: 0.726562] [adversarial loss: 1.062588, acc: 0.343750]\n",
      "8590: [discriminator loss: 0.515666, acc: 0.765625] [adversarial loss: 1.295884, acc: 0.187500]\n",
      "8591: [discriminator loss: 0.559898, acc: 0.695312] [adversarial loss: 0.877476, acc: 0.406250]\n",
      "8592: [discriminator loss: 0.548928, acc: 0.718750] [adversarial loss: 1.429574, acc: 0.078125]\n",
      "8593: [discriminator loss: 0.594011, acc: 0.671875] [adversarial loss: 0.704765, acc: 0.609375]\n",
      "8594: [discriminator loss: 0.585703, acc: 0.703125] [adversarial loss: 1.319878, acc: 0.156250]\n",
      "8595: [discriminator loss: 0.547132, acc: 0.726562] [adversarial loss: 1.060696, acc: 0.218750]\n",
      "8596: [discriminator loss: 0.545514, acc: 0.757812] [adversarial loss: 1.237880, acc: 0.140625]\n",
      "8597: [discriminator loss: 0.567462, acc: 0.710938] [adversarial loss: 1.092797, acc: 0.265625]\n",
      "8598: [discriminator loss: 0.593667, acc: 0.703125] [adversarial loss: 1.432001, acc: 0.125000]\n",
      "8599: [discriminator loss: 0.576848, acc: 0.710938] [adversarial loss: 0.940159, acc: 0.375000]\n",
      "8600: [discriminator loss: 0.500659, acc: 0.757812] [adversarial loss: 1.405143, acc: 0.125000]\n",
      "8601: [discriminator loss: 0.588200, acc: 0.687500] [adversarial loss: 0.980118, acc: 0.312500]\n",
      "8602: [discriminator loss: 0.653819, acc: 0.656250] [adversarial loss: 1.297449, acc: 0.187500]\n",
      "8603: [discriminator loss: 0.562473, acc: 0.671875] [adversarial loss: 1.166353, acc: 0.250000]\n",
      "8604: [discriminator loss: 0.540184, acc: 0.687500] [adversarial loss: 1.177988, acc: 0.234375]\n",
      "8605: [discriminator loss: 0.581490, acc: 0.695312] [adversarial loss: 1.167844, acc: 0.203125]\n",
      "8606: [discriminator loss: 0.522371, acc: 0.710938] [adversarial loss: 1.363427, acc: 0.156250]\n",
      "8607: [discriminator loss: 0.542437, acc: 0.750000] [adversarial loss: 1.080230, acc: 0.234375]\n",
      "8608: [discriminator loss: 0.600770, acc: 0.671875] [adversarial loss: 1.292463, acc: 0.109375]\n",
      "8609: [discriminator loss: 0.621878, acc: 0.679688] [adversarial loss: 1.235685, acc: 0.125000]\n",
      "8610: [discriminator loss: 0.584532, acc: 0.671875] [adversarial loss: 1.014643, acc: 0.296875]\n",
      "8611: [discriminator loss: 0.542874, acc: 0.710938] [adversarial loss: 1.215494, acc: 0.187500]\n",
      "8612: [discriminator loss: 0.606047, acc: 0.671875] [adversarial loss: 1.181243, acc: 0.234375]\n",
      "8613: [discriminator loss: 0.614499, acc: 0.640625] [adversarial loss: 1.268700, acc: 0.171875]\n",
      "8614: [discriminator loss: 0.500460, acc: 0.742188] [adversarial loss: 1.230921, acc: 0.187500]\n",
      "8615: [discriminator loss: 0.529546, acc: 0.742188] [adversarial loss: 1.091971, acc: 0.359375]\n",
      "8616: [discriminator loss: 0.581944, acc: 0.687500] [adversarial loss: 1.396444, acc: 0.140625]\n",
      "8617: [discriminator loss: 0.574414, acc: 0.710938] [adversarial loss: 0.964233, acc: 0.312500]\n",
      "8618: [discriminator loss: 0.498473, acc: 0.742188] [adversarial loss: 1.316070, acc: 0.218750]\n",
      "8619: [discriminator loss: 0.497257, acc: 0.765625] [adversarial loss: 1.016771, acc: 0.312500]\n",
      "8620: [discriminator loss: 0.579172, acc: 0.671875] [adversarial loss: 1.460414, acc: 0.125000]\n",
      "8621: [discriminator loss: 0.518156, acc: 0.742188] [adversarial loss: 0.895154, acc: 0.421875]\n",
      "8622: [discriminator loss: 0.536318, acc: 0.750000] [adversarial loss: 1.371691, acc: 0.156250]\n",
      "8623: [discriminator loss: 0.569311, acc: 0.695312] [adversarial loss: 0.984754, acc: 0.390625]\n",
      "8624: [discriminator loss: 0.515181, acc: 0.718750] [adversarial loss: 1.183236, acc: 0.265625]\n",
      "8625: [discriminator loss: 0.511310, acc: 0.742188] [adversarial loss: 1.091789, acc: 0.203125]\n",
      "8626: [discriminator loss: 0.633292, acc: 0.632812] [adversarial loss: 1.073926, acc: 0.296875]\n",
      "8627: [discriminator loss: 0.580972, acc: 0.703125] [adversarial loss: 1.024490, acc: 0.390625]\n",
      "8628: [discriminator loss: 0.592963, acc: 0.664062] [adversarial loss: 1.413215, acc: 0.140625]\n",
      "8629: [discriminator loss: 0.537105, acc: 0.695312] [adversarial loss: 1.030771, acc: 0.296875]\n",
      "8630: [discriminator loss: 0.588230, acc: 0.679688] [adversarial loss: 1.250869, acc: 0.218750]\n",
      "8631: [discriminator loss: 0.482324, acc: 0.781250] [adversarial loss: 1.144260, acc: 0.234375]\n",
      "8632: [discriminator loss: 0.527389, acc: 0.710938] [adversarial loss: 1.495607, acc: 0.109375]\n",
      "8633: [discriminator loss: 0.603666, acc: 0.609375] [adversarial loss: 0.855887, acc: 0.453125]\n",
      "8634: [discriminator loss: 0.563224, acc: 0.710938] [adversarial loss: 1.529151, acc: 0.093750]\n",
      "8635: [discriminator loss: 0.612856, acc: 0.687500] [adversarial loss: 0.948731, acc: 0.390625]\n",
      "8636: [discriminator loss: 0.560162, acc: 0.726562] [adversarial loss: 1.429830, acc: 0.171875]\n",
      "8637: [discriminator loss: 0.590189, acc: 0.679688] [adversarial loss: 0.932203, acc: 0.312500]\n",
      "8638: [discriminator loss: 0.555307, acc: 0.703125] [adversarial loss: 1.618291, acc: 0.062500]\n",
      "8639: [discriminator loss: 0.539844, acc: 0.679688] [adversarial loss: 0.813914, acc: 0.515625]\n",
      "8640: [discriminator loss: 0.582428, acc: 0.726562] [adversarial loss: 1.476169, acc: 0.078125]\n",
      "8641: [discriminator loss: 0.533255, acc: 0.703125] [adversarial loss: 0.947953, acc: 0.421875]\n",
      "8642: [discriminator loss: 0.580184, acc: 0.679688] [adversarial loss: 1.122664, acc: 0.234375]\n",
      "8643: [discriminator loss: 0.614393, acc: 0.664062] [adversarial loss: 1.232498, acc: 0.296875]\n",
      "8644: [discriminator loss: 0.518655, acc: 0.718750] [adversarial loss: 1.060734, acc: 0.312500]\n",
      "8645: [discriminator loss: 0.598473, acc: 0.703125] [adversarial loss: 1.154033, acc: 0.140625]\n",
      "8646: [discriminator loss: 0.574176, acc: 0.664062] [adversarial loss: 0.998701, acc: 0.328125]\n",
      "8647: [discriminator loss: 0.500688, acc: 0.718750] [adversarial loss: 1.077052, acc: 0.296875]\n",
      "8648: [discriminator loss: 0.563518, acc: 0.656250] [adversarial loss: 1.561201, acc: 0.046875]\n",
      "8649: [discriminator loss: 0.602590, acc: 0.625000] [adversarial loss: 1.023656, acc: 0.250000]\n",
      "8650: [discriminator loss: 0.535558, acc: 0.718750] [adversarial loss: 1.182075, acc: 0.187500]\n",
      "8651: [discriminator loss: 0.571598, acc: 0.679688] [adversarial loss: 1.164002, acc: 0.265625]\n",
      "8652: [discriminator loss: 0.562229, acc: 0.718750] [adversarial loss: 1.182837, acc: 0.187500]\n",
      "8653: [discriminator loss: 0.575464, acc: 0.695312] [adversarial loss: 1.005840, acc: 0.296875]\n",
      "8654: [discriminator loss: 0.564735, acc: 0.695312] [adversarial loss: 1.069973, acc: 0.250000]\n",
      "8655: [discriminator loss: 0.536180, acc: 0.765625] [adversarial loss: 0.948555, acc: 0.343750]\n",
      "8656: [discriminator loss: 0.527690, acc: 0.718750] [adversarial loss: 1.227080, acc: 0.203125]\n",
      "8657: [discriminator loss: 0.568464, acc: 0.718750] [adversarial loss: 0.962652, acc: 0.343750]\n",
      "8658: [discriminator loss: 0.555698, acc: 0.750000] [adversarial loss: 1.316039, acc: 0.140625]\n",
      "8659: [discriminator loss: 0.455343, acc: 0.820312] [adversarial loss: 1.228798, acc: 0.218750]\n",
      "8660: [discriminator loss: 0.567198, acc: 0.671875] [adversarial loss: 1.420050, acc: 0.187500]\n",
      "8661: [discriminator loss: 0.547421, acc: 0.703125] [adversarial loss: 0.900301, acc: 0.421875]\n",
      "8662: [discriminator loss: 0.530193, acc: 0.710938] [adversarial loss: 1.570930, acc: 0.109375]\n",
      "8663: [discriminator loss: 0.488095, acc: 0.757812] [adversarial loss: 1.038536, acc: 0.281250]\n",
      "8664: [discriminator loss: 0.613282, acc: 0.656250] [adversarial loss: 1.450215, acc: 0.125000]\n",
      "8665: [discriminator loss: 0.567738, acc: 0.656250] [adversarial loss: 0.853903, acc: 0.421875]\n",
      "8666: [discriminator loss: 0.535864, acc: 0.726562] [adversarial loss: 1.514552, acc: 0.109375]\n",
      "8667: [discriminator loss: 0.539816, acc: 0.734375] [adversarial loss: 1.172910, acc: 0.234375]\n",
      "8668: [discriminator loss: 0.543723, acc: 0.703125] [adversarial loss: 1.574529, acc: 0.046875]\n",
      "8669: [discriminator loss: 0.585052, acc: 0.695312] [adversarial loss: 0.954091, acc: 0.390625]\n",
      "8670: [discriminator loss: 0.535543, acc: 0.734375] [adversarial loss: 1.372139, acc: 0.171875]\n",
      "8671: [discriminator loss: 0.528198, acc: 0.734375] [adversarial loss: 1.017609, acc: 0.312500]\n",
      "8672: [discriminator loss: 0.600137, acc: 0.601562] [adversarial loss: 1.589759, acc: 0.140625]\n",
      "8673: [discriminator loss: 0.571470, acc: 0.687500] [adversarial loss: 0.900822, acc: 0.390625]\n",
      "8674: [discriminator loss: 0.495280, acc: 0.750000] [adversarial loss: 1.234361, acc: 0.125000]\n",
      "8675: [discriminator loss: 0.570424, acc: 0.695312] [adversarial loss: 0.971095, acc: 0.312500]\n",
      "8676: [discriminator loss: 0.585385, acc: 0.648438] [adversarial loss: 1.200720, acc: 0.203125]\n",
      "8677: [discriminator loss: 0.585310, acc: 0.679688] [adversarial loss: 0.895213, acc: 0.406250]\n",
      "8678: [discriminator loss: 0.519182, acc: 0.750000] [adversarial loss: 1.370902, acc: 0.125000]\n",
      "8679: [discriminator loss: 0.537566, acc: 0.718750] [adversarial loss: 0.969362, acc: 0.328125]\n",
      "8680: [discriminator loss: 0.573993, acc: 0.695312] [adversarial loss: 1.342733, acc: 0.156250]\n",
      "8681: [discriminator loss: 0.521514, acc: 0.726562] [adversarial loss: 1.099578, acc: 0.359375]\n",
      "8682: [discriminator loss: 0.563730, acc: 0.687500] [adversarial loss: 1.385335, acc: 0.156250]\n",
      "8683: [discriminator loss: 0.446349, acc: 0.781250] [adversarial loss: 1.335947, acc: 0.187500]\n",
      "8684: [discriminator loss: 0.554242, acc: 0.750000] [adversarial loss: 1.188881, acc: 0.218750]\n",
      "8685: [discriminator loss: 0.547407, acc: 0.710938] [adversarial loss: 0.999953, acc: 0.296875]\n",
      "8686: [discriminator loss: 0.548477, acc: 0.703125] [adversarial loss: 1.479076, acc: 0.125000]\n",
      "8687: [discriminator loss: 0.510480, acc: 0.726562] [adversarial loss: 0.917247, acc: 0.375000]\n",
      "8688: [discriminator loss: 0.540698, acc: 0.695312] [adversarial loss: 0.976661, acc: 0.359375]\n",
      "8689: [discriminator loss: 0.585683, acc: 0.695312] [adversarial loss: 1.189496, acc: 0.234375]\n",
      "8690: [discriminator loss: 0.553059, acc: 0.695312] [adversarial loss: 1.128971, acc: 0.328125]\n",
      "8691: [discriminator loss: 0.666975, acc: 0.601562] [adversarial loss: 1.037471, acc: 0.234375]\n",
      "8692: [discriminator loss: 0.579283, acc: 0.656250] [adversarial loss: 1.575050, acc: 0.109375]\n",
      "8693: [discriminator loss: 0.593060, acc: 0.648438] [adversarial loss: 0.737675, acc: 0.531250]\n",
      "8694: [discriminator loss: 0.599045, acc: 0.664062] [adversarial loss: 1.547997, acc: 0.062500]\n",
      "8695: [discriminator loss: 0.552812, acc: 0.726562] [adversarial loss: 0.948517, acc: 0.421875]\n",
      "8696: [discriminator loss: 0.564085, acc: 0.742188] [adversarial loss: 0.990301, acc: 0.281250]\n",
      "8697: [discriminator loss: 0.521041, acc: 0.734375] [adversarial loss: 1.050049, acc: 0.390625]\n",
      "8698: [discriminator loss: 0.548120, acc: 0.750000] [adversarial loss: 1.362861, acc: 0.125000]\n",
      "8699: [discriminator loss: 0.586909, acc: 0.625000] [adversarial loss: 1.220580, acc: 0.203125]\n",
      "8700: [discriminator loss: 0.517322, acc: 0.726562] [adversarial loss: 1.011875, acc: 0.265625]\n",
      "8701: [discriminator loss: 0.528158, acc: 0.742188] [adversarial loss: 0.905762, acc: 0.359375]\n",
      "8702: [discriminator loss: 0.508614, acc: 0.742188] [adversarial loss: 1.382864, acc: 0.093750]\n",
      "8703: [discriminator loss: 0.545702, acc: 0.742188] [adversarial loss: 0.964813, acc: 0.437500]\n",
      "8704: [discriminator loss: 0.564888, acc: 0.718750] [adversarial loss: 1.159708, acc: 0.265625]\n",
      "8705: [discriminator loss: 0.544646, acc: 0.718750] [adversarial loss: 1.040944, acc: 0.343750]\n",
      "8706: [discriminator loss: 0.615643, acc: 0.640625] [adversarial loss: 1.197985, acc: 0.234375]\n",
      "8707: [discriminator loss: 0.520883, acc: 0.726562] [adversarial loss: 1.138181, acc: 0.218750]\n",
      "8708: [discriminator loss: 0.536515, acc: 0.710938] [adversarial loss: 1.234118, acc: 0.265625]\n",
      "8709: [discriminator loss: 0.543436, acc: 0.742188] [adversarial loss: 1.279321, acc: 0.218750]\n",
      "8710: [discriminator loss: 0.530502, acc: 0.750000] [adversarial loss: 1.308311, acc: 0.171875]\n",
      "8711: [discriminator loss: 0.579378, acc: 0.710938] [adversarial loss: 1.055374, acc: 0.359375]\n",
      "8712: [discriminator loss: 0.571532, acc: 0.703125] [adversarial loss: 1.228641, acc: 0.234375]\n",
      "8713: [discriminator loss: 0.520019, acc: 0.726562] [adversarial loss: 1.022953, acc: 0.281250]\n",
      "8714: [discriminator loss: 0.521682, acc: 0.687500] [adversarial loss: 1.363582, acc: 0.187500]\n",
      "8715: [discriminator loss: 0.553333, acc: 0.664062] [adversarial loss: 1.025448, acc: 0.375000]\n",
      "8716: [discriminator loss: 0.514346, acc: 0.757812] [adversarial loss: 1.393505, acc: 0.156250]\n",
      "8717: [discriminator loss: 0.576369, acc: 0.742188] [adversarial loss: 1.010259, acc: 0.359375]\n",
      "8718: [discriminator loss: 0.556257, acc: 0.703125] [adversarial loss: 1.213040, acc: 0.218750]\n",
      "8719: [discriminator loss: 0.586905, acc: 0.687500] [adversarial loss: 0.886902, acc: 0.468750]\n",
      "8720: [discriminator loss: 0.570338, acc: 0.710938] [adversarial loss: 1.185298, acc: 0.296875]\n",
      "8721: [discriminator loss: 0.543735, acc: 0.757812] [adversarial loss: 1.003013, acc: 0.343750]\n",
      "8722: [discriminator loss: 0.618337, acc: 0.664062] [adversarial loss: 1.637665, acc: 0.093750]\n",
      "8723: [discriminator loss: 0.616330, acc: 0.687500] [adversarial loss: 0.919162, acc: 0.406250]\n",
      "8724: [discriminator loss: 0.554797, acc: 0.703125] [adversarial loss: 0.998227, acc: 0.312500]\n",
      "8725: [discriminator loss: 0.589367, acc: 0.625000] [adversarial loss: 1.345445, acc: 0.125000]\n",
      "8726: [discriminator loss: 0.534279, acc: 0.671875] [adversarial loss: 0.967417, acc: 0.296875]\n",
      "8727: [discriminator loss: 0.607474, acc: 0.656250] [adversarial loss: 1.142166, acc: 0.218750]\n",
      "8728: [discriminator loss: 0.595280, acc: 0.671875] [adversarial loss: 0.955696, acc: 0.406250]\n",
      "8729: [discriminator loss: 0.614694, acc: 0.679688] [adversarial loss: 1.090524, acc: 0.203125]\n",
      "8730: [discriminator loss: 0.496629, acc: 0.750000] [adversarial loss: 1.196184, acc: 0.234375]\n",
      "8731: [discriminator loss: 0.539393, acc: 0.703125] [adversarial loss: 1.178371, acc: 0.234375]\n",
      "8732: [discriminator loss: 0.581194, acc: 0.703125] [adversarial loss: 1.152002, acc: 0.234375]\n",
      "8733: [discriminator loss: 0.572069, acc: 0.687500] [adversarial loss: 1.125631, acc: 0.265625]\n",
      "8734: [discriminator loss: 0.542177, acc: 0.726562] [adversarial loss: 0.888684, acc: 0.406250]\n",
      "8735: [discriminator loss: 0.504632, acc: 0.750000] [adversarial loss: 1.396421, acc: 0.109375]\n",
      "8736: [discriminator loss: 0.531347, acc: 0.765625] [adversarial loss: 0.790354, acc: 0.515625]\n",
      "8737: [discriminator loss: 0.583275, acc: 0.679688] [adversarial loss: 1.504070, acc: 0.171875]\n",
      "8738: [discriminator loss: 0.555121, acc: 0.679688] [adversarial loss: 0.838478, acc: 0.437500]\n",
      "8739: [discriminator loss: 0.519309, acc: 0.703125] [adversarial loss: 1.157789, acc: 0.265625]\n",
      "8740: [discriminator loss: 0.524548, acc: 0.703125] [adversarial loss: 1.078761, acc: 0.265625]\n",
      "8741: [discriminator loss: 0.571195, acc: 0.664062] [adversarial loss: 1.364626, acc: 0.125000]\n",
      "8742: [discriminator loss: 0.593871, acc: 0.640625] [adversarial loss: 0.947954, acc: 0.328125]\n",
      "8743: [discriminator loss: 0.619817, acc: 0.617188] [adversarial loss: 1.077899, acc: 0.250000]\n",
      "8744: [discriminator loss: 0.531285, acc: 0.679688] [adversarial loss: 1.058344, acc: 0.281250]\n",
      "8745: [discriminator loss: 0.576397, acc: 0.679688] [adversarial loss: 1.147236, acc: 0.234375]\n",
      "8746: [discriminator loss: 0.584070, acc: 0.609375] [adversarial loss: 1.127176, acc: 0.234375]\n",
      "8747: [discriminator loss: 0.562451, acc: 0.710938] [adversarial loss: 1.246804, acc: 0.156250]\n",
      "8748: [discriminator loss: 0.521477, acc: 0.781250] [adversarial loss: 1.130117, acc: 0.265625]\n",
      "8749: [discriminator loss: 0.571433, acc: 0.679688] [adversarial loss: 1.049972, acc: 0.250000]\n",
      "8750: [discriminator loss: 0.589011, acc: 0.679688] [adversarial loss: 1.164023, acc: 0.203125]\n",
      "8751: [discriminator loss: 0.576369, acc: 0.695312] [adversarial loss: 0.958938, acc: 0.328125]\n",
      "8752: [discriminator loss: 0.506411, acc: 0.781250] [adversarial loss: 1.392483, acc: 0.109375]\n",
      "8753: [discriminator loss: 0.581046, acc: 0.710938] [adversarial loss: 0.981219, acc: 0.359375]\n",
      "8754: [discriminator loss: 0.553071, acc: 0.710938] [adversarial loss: 1.454635, acc: 0.140625]\n",
      "8755: [discriminator loss: 0.560945, acc: 0.687500] [adversarial loss: 1.140361, acc: 0.234375]\n",
      "8756: [discriminator loss: 0.492606, acc: 0.742188] [adversarial loss: 1.263972, acc: 0.109375]\n",
      "8757: [discriminator loss: 0.548368, acc: 0.710938] [adversarial loss: 0.974321, acc: 0.296875]\n",
      "8758: [discriminator loss: 0.563653, acc: 0.710938] [adversarial loss: 1.283231, acc: 0.171875]\n",
      "8759: [discriminator loss: 0.602504, acc: 0.648438] [adversarial loss: 0.915431, acc: 0.421875]\n",
      "8760: [discriminator loss: 0.566430, acc: 0.671875] [adversarial loss: 1.207651, acc: 0.171875]\n",
      "8761: [discriminator loss: 0.509639, acc: 0.742188] [adversarial loss: 1.377518, acc: 0.125000]\n",
      "8762: [discriminator loss: 0.532106, acc: 0.734375] [adversarial loss: 1.133486, acc: 0.218750]\n",
      "8763: [discriminator loss: 0.523080, acc: 0.734375] [adversarial loss: 1.191170, acc: 0.234375]\n",
      "8764: [discriminator loss: 0.514966, acc: 0.789062] [adversarial loss: 1.123452, acc: 0.234375]\n",
      "8765: [discriminator loss: 0.564648, acc: 0.742188] [adversarial loss: 1.505053, acc: 0.078125]\n",
      "8766: [discriminator loss: 0.555890, acc: 0.671875] [adversarial loss: 0.776835, acc: 0.468750]\n",
      "8767: [discriminator loss: 0.576881, acc: 0.710938] [adversarial loss: 1.443846, acc: 0.078125]\n",
      "8768: [discriminator loss: 0.578226, acc: 0.656250] [adversarial loss: 0.786806, acc: 0.578125]\n",
      "8769: [discriminator loss: 0.624048, acc: 0.656250] [adversarial loss: 1.246223, acc: 0.187500]\n",
      "8770: [discriminator loss: 0.580545, acc: 0.710938] [adversarial loss: 1.153898, acc: 0.218750]\n",
      "8771: [discriminator loss: 0.560185, acc: 0.710938] [adversarial loss: 1.389970, acc: 0.218750]\n",
      "8772: [discriminator loss: 0.510731, acc: 0.750000] [adversarial loss: 1.045206, acc: 0.359375]\n",
      "8773: [discriminator loss: 0.579895, acc: 0.679688] [adversarial loss: 1.132991, acc: 0.234375]\n",
      "8774: [discriminator loss: 0.549655, acc: 0.710938] [adversarial loss: 1.105116, acc: 0.265625]\n",
      "8775: [discriminator loss: 0.503538, acc: 0.734375] [adversarial loss: 0.953443, acc: 0.375000]\n",
      "8776: [discriminator loss: 0.563228, acc: 0.710938] [adversarial loss: 1.374605, acc: 0.171875]\n",
      "8777: [discriminator loss: 0.586408, acc: 0.679688] [adversarial loss: 1.223882, acc: 0.187500]\n",
      "8778: [discriminator loss: 0.587683, acc: 0.671875] [adversarial loss: 1.209068, acc: 0.218750]\n",
      "8779: [discriminator loss: 0.573217, acc: 0.671875] [adversarial loss: 1.089471, acc: 0.296875]\n",
      "8780: [discriminator loss: 0.515752, acc: 0.742188] [adversarial loss: 1.349540, acc: 0.156250]\n",
      "8781: [discriminator loss: 0.527759, acc: 0.679688] [adversarial loss: 1.058018, acc: 0.296875]\n",
      "8782: [discriminator loss: 0.511581, acc: 0.710938] [adversarial loss: 1.095680, acc: 0.281250]\n",
      "8783: [discriminator loss: 0.568066, acc: 0.710938] [adversarial loss: 1.224570, acc: 0.281250]\n",
      "8784: [discriminator loss: 0.574250, acc: 0.664062] [adversarial loss: 0.925701, acc: 0.437500]\n",
      "8785: [discriminator loss: 0.566049, acc: 0.632812] [adversarial loss: 1.506653, acc: 0.140625]\n",
      "8786: [discriminator loss: 0.549045, acc: 0.703125] [adversarial loss: 1.258498, acc: 0.171875]\n",
      "8787: [discriminator loss: 0.584820, acc: 0.679688] [adversarial loss: 1.079636, acc: 0.218750]\n",
      "8788: [discriminator loss: 0.627823, acc: 0.648438] [adversarial loss: 1.111200, acc: 0.250000]\n",
      "8789: [discriminator loss: 0.537452, acc: 0.742188] [adversarial loss: 0.996882, acc: 0.328125]\n",
      "8790: [discriminator loss: 0.522403, acc: 0.703125] [adversarial loss: 1.254719, acc: 0.156250]\n",
      "8791: [discriminator loss: 0.535123, acc: 0.710938] [adversarial loss: 1.030992, acc: 0.328125]\n",
      "8792: [discriminator loss: 0.503263, acc: 0.726562] [adversarial loss: 1.268758, acc: 0.171875]\n",
      "8793: [discriminator loss: 0.575527, acc: 0.703125] [adversarial loss: 1.170848, acc: 0.187500]\n",
      "8794: [discriminator loss: 0.628287, acc: 0.632812] [adversarial loss: 1.279211, acc: 0.218750]\n",
      "8795: [discriminator loss: 0.551097, acc: 0.664062] [adversarial loss: 0.942354, acc: 0.359375]\n",
      "8796: [discriminator loss: 0.560083, acc: 0.710938] [adversarial loss: 1.405993, acc: 0.140625]\n",
      "8797: [discriminator loss: 0.515215, acc: 0.726562] [adversarial loss: 0.946887, acc: 0.359375]\n",
      "8798: [discriminator loss: 0.580064, acc: 0.648438] [adversarial loss: 1.363853, acc: 0.062500]\n",
      "8799: [discriminator loss: 0.555515, acc: 0.765625] [adversarial loss: 0.850038, acc: 0.421875]\n",
      "8800: [discriminator loss: 0.549018, acc: 0.742188] [adversarial loss: 1.394139, acc: 0.140625]\n",
      "8801: [discriminator loss: 0.632614, acc: 0.609375] [adversarial loss: 0.677616, acc: 0.578125]\n",
      "8802: [discriminator loss: 0.606216, acc: 0.632812] [adversarial loss: 1.425344, acc: 0.093750]\n",
      "8803: [discriminator loss: 0.594441, acc: 0.648438] [adversarial loss: 1.089355, acc: 0.281250]\n",
      "8804: [discriminator loss: 0.606952, acc: 0.679688] [adversarial loss: 1.293287, acc: 0.187500]\n",
      "8805: [discriminator loss: 0.556689, acc: 0.687500] [adversarial loss: 1.136317, acc: 0.187500]\n",
      "8806: [discriminator loss: 0.499429, acc: 0.781250] [adversarial loss: 1.087177, acc: 0.343750]\n",
      "8807: [discriminator loss: 0.565492, acc: 0.710938] [adversarial loss: 1.064066, acc: 0.250000]\n",
      "8808: [discriminator loss: 0.598643, acc: 0.703125] [adversarial loss: 1.159183, acc: 0.203125]\n",
      "8809: [discriminator loss: 0.593560, acc: 0.664062] [adversarial loss: 1.154064, acc: 0.234375]\n",
      "8810: [discriminator loss: 0.486258, acc: 0.765625] [adversarial loss: 1.538899, acc: 0.093750]\n",
      "8811: [discriminator loss: 0.507011, acc: 0.750000] [adversarial loss: 0.843042, acc: 0.500000]\n",
      "8812: [discriminator loss: 0.567227, acc: 0.679688] [adversarial loss: 1.270484, acc: 0.140625]\n",
      "8813: [discriminator loss: 0.516852, acc: 0.773438] [adversarial loss: 1.131703, acc: 0.250000]\n",
      "8814: [discriminator loss: 0.536498, acc: 0.750000] [adversarial loss: 0.920053, acc: 0.343750]\n",
      "8815: [discriminator loss: 0.538843, acc: 0.703125] [adversarial loss: 1.241353, acc: 0.328125]\n",
      "8816: [discriminator loss: 0.542846, acc: 0.718750] [adversarial loss: 0.891803, acc: 0.375000]\n",
      "8817: [discriminator loss: 0.512375, acc: 0.765625] [adversarial loss: 1.548187, acc: 0.125000]\n",
      "8818: [discriminator loss: 0.681609, acc: 0.609375] [adversarial loss: 0.961347, acc: 0.359375]\n",
      "8819: [discriminator loss: 0.568682, acc: 0.703125] [adversarial loss: 1.067213, acc: 0.281250]\n",
      "8820: [discriminator loss: 0.640528, acc: 0.593750] [adversarial loss: 1.076221, acc: 0.328125]\n",
      "8821: [discriminator loss: 0.623109, acc: 0.703125] [adversarial loss: 1.037466, acc: 0.312500]\n",
      "8822: [discriminator loss: 0.516956, acc: 0.710938] [adversarial loss: 1.150144, acc: 0.187500]\n",
      "8823: [discriminator loss: 0.540851, acc: 0.726562] [adversarial loss: 1.180277, acc: 0.265625]\n",
      "8824: [discriminator loss: 0.556848, acc: 0.687500] [adversarial loss: 1.007601, acc: 0.406250]\n",
      "8825: [discriminator loss: 0.535850, acc: 0.750000] [adversarial loss: 1.408327, acc: 0.140625]\n",
      "8826: [discriminator loss: 0.562708, acc: 0.664062] [adversarial loss: 1.011528, acc: 0.312500]\n",
      "8827: [discriminator loss: 0.559885, acc: 0.718750] [adversarial loss: 1.509905, acc: 0.109375]\n",
      "8828: [discriminator loss: 0.576732, acc: 0.656250] [adversarial loss: 0.855525, acc: 0.484375]\n",
      "8829: [discriminator loss: 0.544034, acc: 0.757812] [adversarial loss: 1.267235, acc: 0.218750]\n",
      "8830: [discriminator loss: 0.508533, acc: 0.718750] [adversarial loss: 1.112349, acc: 0.234375]\n",
      "8831: [discriminator loss: 0.496835, acc: 0.757812] [adversarial loss: 1.312295, acc: 0.171875]\n",
      "8832: [discriminator loss: 0.522878, acc: 0.710938] [adversarial loss: 0.735507, acc: 0.484375]\n",
      "8833: [discriminator loss: 0.573206, acc: 0.703125] [adversarial loss: 1.338658, acc: 0.218750]\n",
      "8834: [discriminator loss: 0.566254, acc: 0.695312] [adversarial loss: 1.071838, acc: 0.328125]\n",
      "8835: [discriminator loss: 0.540272, acc: 0.750000] [adversarial loss: 1.126521, acc: 0.265625]\n",
      "8836: [discriminator loss: 0.503690, acc: 0.781250] [adversarial loss: 1.108448, acc: 0.156250]\n",
      "8837: [discriminator loss: 0.522265, acc: 0.710938] [adversarial loss: 1.155441, acc: 0.218750]\n",
      "8838: [discriminator loss: 0.503557, acc: 0.734375] [adversarial loss: 1.172851, acc: 0.203125]\n",
      "8839: [discriminator loss: 0.548359, acc: 0.703125] [adversarial loss: 0.860520, acc: 0.468750]\n",
      "8840: [discriminator loss: 0.574885, acc: 0.710938] [adversarial loss: 1.526364, acc: 0.125000]\n",
      "8841: [discriminator loss: 0.521282, acc: 0.734375] [adversarial loss: 0.899939, acc: 0.375000]\n",
      "8842: [discriminator loss: 0.540826, acc: 0.718750] [adversarial loss: 1.324251, acc: 0.187500]\n",
      "8843: [discriminator loss: 0.496328, acc: 0.773438] [adversarial loss: 1.203505, acc: 0.265625]\n",
      "8844: [discriminator loss: 0.520564, acc: 0.757812] [adversarial loss: 1.088382, acc: 0.281250]\n",
      "8845: [discriminator loss: 0.550350, acc: 0.687500] [adversarial loss: 1.155066, acc: 0.218750]\n",
      "8846: [discriminator loss: 0.514204, acc: 0.718750] [adversarial loss: 1.372527, acc: 0.156250]\n",
      "8847: [discriminator loss: 0.548753, acc: 0.742188] [adversarial loss: 1.093210, acc: 0.296875]\n",
      "8848: [discriminator loss: 0.580713, acc: 0.679688] [adversarial loss: 1.334424, acc: 0.156250]\n",
      "8849: [discriminator loss: 0.498229, acc: 0.773438] [adversarial loss: 1.161227, acc: 0.203125]\n",
      "8850: [discriminator loss: 0.561394, acc: 0.734375] [adversarial loss: 1.160882, acc: 0.265625]\n",
      "8851: [discriminator loss: 0.653843, acc: 0.601562] [adversarial loss: 1.325123, acc: 0.187500]\n",
      "8852: [discriminator loss: 0.540128, acc: 0.734375] [adversarial loss: 0.989946, acc: 0.375000]\n",
      "8853: [discriminator loss: 0.623109, acc: 0.664062] [adversarial loss: 1.123380, acc: 0.328125]\n",
      "8854: [discriminator loss: 0.481734, acc: 0.781250] [adversarial loss: 1.433869, acc: 0.078125]\n",
      "8855: [discriminator loss: 0.549843, acc: 0.703125] [adversarial loss: 0.990184, acc: 0.328125]\n",
      "8856: [discriminator loss: 0.529130, acc: 0.765625] [adversarial loss: 1.680508, acc: 0.093750]\n",
      "8857: [discriminator loss: 0.616066, acc: 0.679688] [adversarial loss: 0.861010, acc: 0.500000]\n",
      "8858: [discriminator loss: 0.538650, acc: 0.726562] [adversarial loss: 1.662166, acc: 0.078125]\n",
      "8859: [discriminator loss: 0.564773, acc: 0.671875] [adversarial loss: 0.699235, acc: 0.546875]\n",
      "8860: [discriminator loss: 0.614481, acc: 0.664062] [adversarial loss: 1.302113, acc: 0.265625]\n",
      "8861: [discriminator loss: 0.546136, acc: 0.679688] [adversarial loss: 0.906452, acc: 0.390625]\n",
      "8862: [discriminator loss: 0.518087, acc: 0.710938] [adversarial loss: 1.446135, acc: 0.156250]\n",
      "8863: [discriminator loss: 0.589643, acc: 0.695312] [adversarial loss: 0.811555, acc: 0.468750]\n",
      "8864: [discriminator loss: 0.503803, acc: 0.781250] [adversarial loss: 1.260926, acc: 0.203125]\n",
      "8865: [discriminator loss: 0.549400, acc: 0.671875] [adversarial loss: 1.044354, acc: 0.296875]\n",
      "8866: [discriminator loss: 0.560681, acc: 0.695312] [adversarial loss: 1.250628, acc: 0.140625]\n",
      "8867: [discriminator loss: 0.543399, acc: 0.718750] [adversarial loss: 1.032028, acc: 0.312500]\n",
      "8868: [discriminator loss: 0.577836, acc: 0.726562] [adversarial loss: 1.249018, acc: 0.250000]\n",
      "8869: [discriminator loss: 0.555566, acc: 0.687500] [adversarial loss: 0.987310, acc: 0.359375]\n",
      "8870: [discriminator loss: 0.624374, acc: 0.648438] [adversarial loss: 1.494177, acc: 0.109375]\n",
      "8871: [discriminator loss: 0.517853, acc: 0.726562] [adversarial loss: 1.101874, acc: 0.250000]\n",
      "8872: [discriminator loss: 0.543683, acc: 0.734375] [adversarial loss: 1.200729, acc: 0.250000]\n",
      "8873: [discriminator loss: 0.568100, acc: 0.710938] [adversarial loss: 1.044468, acc: 0.296875]\n",
      "8874: [discriminator loss: 0.491939, acc: 0.804688] [adversarial loss: 1.578564, acc: 0.140625]\n",
      "8875: [discriminator loss: 0.587060, acc: 0.703125] [adversarial loss: 1.266064, acc: 0.187500]\n",
      "8876: [discriminator loss: 0.514605, acc: 0.757812] [adversarial loss: 1.087007, acc: 0.218750]\n",
      "8877: [discriminator loss: 0.539948, acc: 0.687500] [adversarial loss: 1.027554, acc: 0.390625]\n",
      "8878: [discriminator loss: 0.483087, acc: 0.796875] [adversarial loss: 1.070467, acc: 0.296875]\n",
      "8879: [discriminator loss: 0.533688, acc: 0.734375] [adversarial loss: 0.910558, acc: 0.406250]\n",
      "8880: [discriminator loss: 0.585797, acc: 0.687500] [adversarial loss: 1.407611, acc: 0.234375]\n",
      "8881: [discriminator loss: 0.600329, acc: 0.664062] [adversarial loss: 0.943868, acc: 0.406250]\n",
      "8882: [discriminator loss: 0.554085, acc: 0.679688] [adversarial loss: 1.584836, acc: 0.046875]\n",
      "8883: [discriminator loss: 0.528190, acc: 0.695312] [adversarial loss: 0.903925, acc: 0.390625]\n",
      "8884: [discriminator loss: 0.597617, acc: 0.664062] [adversarial loss: 1.284512, acc: 0.171875]\n",
      "8885: [discriminator loss: 0.515559, acc: 0.750000] [adversarial loss: 0.810943, acc: 0.484375]\n",
      "8886: [discriminator loss: 0.582410, acc: 0.695312] [adversarial loss: 1.492616, acc: 0.062500]\n",
      "8887: [discriminator loss: 0.552365, acc: 0.664062] [adversarial loss: 0.962752, acc: 0.359375]\n",
      "8888: [discriminator loss: 0.556114, acc: 0.679688] [adversarial loss: 1.457960, acc: 0.093750]\n",
      "8889: [discriminator loss: 0.633913, acc: 0.593750] [adversarial loss: 0.911162, acc: 0.406250]\n",
      "8890: [discriminator loss: 0.629714, acc: 0.671875] [adversarial loss: 1.280567, acc: 0.109375]\n",
      "8891: [discriminator loss: 0.544004, acc: 0.742188] [adversarial loss: 1.016829, acc: 0.359375]\n",
      "8892: [discriminator loss: 0.536882, acc: 0.765625] [adversarial loss: 1.276736, acc: 0.234375]\n",
      "8893: [discriminator loss: 0.563368, acc: 0.679688] [adversarial loss: 1.150408, acc: 0.250000]\n",
      "8894: [discriminator loss: 0.539487, acc: 0.750000] [adversarial loss: 1.324788, acc: 0.203125]\n",
      "8895: [discriminator loss: 0.627957, acc: 0.632812] [adversarial loss: 0.979671, acc: 0.375000]\n",
      "8896: [discriminator loss: 0.607171, acc: 0.601562] [adversarial loss: 1.007240, acc: 0.343750]\n",
      "8897: [discriminator loss: 0.565657, acc: 0.679688] [adversarial loss: 1.241190, acc: 0.250000]\n",
      "8898: [discriminator loss: 0.552675, acc: 0.726562] [adversarial loss: 1.224230, acc: 0.250000]\n",
      "8899: [discriminator loss: 0.548082, acc: 0.765625] [adversarial loss: 1.130600, acc: 0.171875]\n",
      "8900: [discriminator loss: 0.568074, acc: 0.710938] [adversarial loss: 1.120819, acc: 0.296875]\n",
      "8901: [discriminator loss: 0.569477, acc: 0.687500] [adversarial loss: 1.125693, acc: 0.234375]\n",
      "8902: [discriminator loss: 0.555841, acc: 0.703125] [adversarial loss: 1.210500, acc: 0.234375]\n",
      "8903: [discriminator loss: 0.544877, acc: 0.710938] [adversarial loss: 1.076724, acc: 0.328125]\n",
      "8904: [discriminator loss: 0.488890, acc: 0.757812] [adversarial loss: 1.367305, acc: 0.156250]\n",
      "8905: [discriminator loss: 0.561258, acc: 0.664062] [adversarial loss: 0.845252, acc: 0.437500]\n",
      "8906: [discriminator loss: 0.636860, acc: 0.625000] [adversarial loss: 1.622197, acc: 0.078125]\n",
      "8907: [discriminator loss: 0.617502, acc: 0.656250] [adversarial loss: 0.852964, acc: 0.421875]\n",
      "8908: [discriminator loss: 0.602164, acc: 0.718750] [adversarial loss: 1.494726, acc: 0.078125]\n",
      "8909: [discriminator loss: 0.534549, acc: 0.726562] [adversarial loss: 1.094344, acc: 0.218750]\n",
      "8910: [discriminator loss: 0.543877, acc: 0.703125] [adversarial loss: 1.392339, acc: 0.109375]\n",
      "8911: [discriminator loss: 0.522491, acc: 0.710938] [adversarial loss: 1.070533, acc: 0.218750]\n",
      "8912: [discriminator loss: 0.576940, acc: 0.664062] [adversarial loss: 0.969402, acc: 0.328125]\n",
      "8913: [discriminator loss: 0.505014, acc: 0.750000] [adversarial loss: 1.075837, acc: 0.265625]\n",
      "8914: [discriminator loss: 0.572424, acc: 0.640625] [adversarial loss: 1.269326, acc: 0.218750]\n",
      "8915: [discriminator loss: 0.537140, acc: 0.734375] [adversarial loss: 0.959466, acc: 0.343750]\n",
      "8916: [discriminator loss: 0.598423, acc: 0.695312] [adversarial loss: 1.428590, acc: 0.125000]\n",
      "8917: [discriminator loss: 0.548537, acc: 0.695312] [adversarial loss: 0.929105, acc: 0.421875]\n",
      "8918: [discriminator loss: 0.568152, acc: 0.687500] [adversarial loss: 1.053566, acc: 0.343750]\n",
      "8919: [discriminator loss: 0.552639, acc: 0.718750] [adversarial loss: 1.242102, acc: 0.156250]\n",
      "8920: [discriminator loss: 0.485208, acc: 0.718750] [adversarial loss: 1.391923, acc: 0.125000]\n",
      "8921: [discriminator loss: 0.591724, acc: 0.640625] [adversarial loss: 0.914209, acc: 0.390625]\n",
      "8922: [discriminator loss: 0.528893, acc: 0.750000] [adversarial loss: 1.226174, acc: 0.187500]\n",
      "8923: [discriminator loss: 0.566967, acc: 0.703125] [adversarial loss: 0.967229, acc: 0.390625]\n",
      "8924: [discriminator loss: 0.569513, acc: 0.695312] [adversarial loss: 1.563401, acc: 0.125000]\n",
      "8925: [discriminator loss: 0.584775, acc: 0.695312] [adversarial loss: 0.806659, acc: 0.484375]\n",
      "8926: [discriminator loss: 0.517459, acc: 0.750000] [adversarial loss: 1.219680, acc: 0.250000]\n",
      "8927: [discriminator loss: 0.511625, acc: 0.742188] [adversarial loss: 1.389969, acc: 0.171875]\n",
      "8928: [discriminator loss: 0.530171, acc: 0.710938] [adversarial loss: 1.162905, acc: 0.218750]\n",
      "8929: [discriminator loss: 0.568183, acc: 0.718750] [adversarial loss: 1.207550, acc: 0.218750]\n",
      "8930: [discriminator loss: 0.650457, acc: 0.609375] [adversarial loss: 0.984187, acc: 0.390625]\n",
      "8931: [discriminator loss: 0.587390, acc: 0.703125] [adversarial loss: 1.413987, acc: 0.187500]\n",
      "8932: [discriminator loss: 0.539312, acc: 0.695312] [adversarial loss: 0.958826, acc: 0.343750]\n",
      "8933: [discriminator loss: 0.528083, acc: 0.695312] [adversarial loss: 1.504134, acc: 0.109375]\n",
      "8934: [discriminator loss: 0.652035, acc: 0.640625] [adversarial loss: 0.736544, acc: 0.546875]\n",
      "8935: [discriminator loss: 0.524599, acc: 0.679688] [adversarial loss: 1.203232, acc: 0.187500]\n",
      "8936: [discriminator loss: 0.548918, acc: 0.695312] [adversarial loss: 1.024097, acc: 0.359375]\n",
      "8937: [discriminator loss: 0.499159, acc: 0.742188] [adversarial loss: 1.296173, acc: 0.187500]\n",
      "8938: [discriminator loss: 0.546850, acc: 0.742188] [adversarial loss: 1.152868, acc: 0.187500]\n",
      "8939: [discriminator loss: 0.493461, acc: 0.750000] [adversarial loss: 1.263632, acc: 0.156250]\n",
      "8940: [discriminator loss: 0.558385, acc: 0.718750] [adversarial loss: 1.083030, acc: 0.265625]\n",
      "8941: [discriminator loss: 0.545949, acc: 0.718750] [adversarial loss: 1.185990, acc: 0.171875]\n",
      "8942: [discriminator loss: 0.540031, acc: 0.757812] [adversarial loss: 0.912829, acc: 0.343750]\n",
      "8943: [discriminator loss: 0.511285, acc: 0.750000] [adversarial loss: 1.230525, acc: 0.156250]\n",
      "8944: [discriminator loss: 0.585923, acc: 0.687500] [adversarial loss: 0.843595, acc: 0.437500]\n",
      "8945: [discriminator loss: 0.613709, acc: 0.640625] [adversarial loss: 1.469891, acc: 0.156250]\n",
      "8946: [discriminator loss: 0.547400, acc: 0.726562] [adversarial loss: 0.905880, acc: 0.375000]\n",
      "8947: [discriminator loss: 0.598938, acc: 0.648438] [adversarial loss: 1.174760, acc: 0.140625]\n",
      "8948: [discriminator loss: 0.606592, acc: 0.656250] [adversarial loss: 1.014749, acc: 0.343750]\n",
      "8949: [discriminator loss: 0.586763, acc: 0.664062] [adversarial loss: 1.183431, acc: 0.250000]\n",
      "8950: [discriminator loss: 0.518781, acc: 0.695312] [adversarial loss: 1.247025, acc: 0.140625]\n",
      "8951: [discriminator loss: 0.508051, acc: 0.789062] [adversarial loss: 1.023358, acc: 0.359375]\n",
      "8952: [discriminator loss: 0.558538, acc: 0.718750] [adversarial loss: 1.213085, acc: 0.250000]\n",
      "8953: [discriminator loss: 0.512093, acc: 0.710938] [adversarial loss: 0.971688, acc: 0.296875]\n",
      "8954: [discriminator loss: 0.504669, acc: 0.742188] [adversarial loss: 1.267882, acc: 0.203125]\n",
      "8955: [discriminator loss: 0.515740, acc: 0.687500] [adversarial loss: 0.906429, acc: 0.328125]\n",
      "8956: [discriminator loss: 0.505157, acc: 0.750000] [adversarial loss: 1.266177, acc: 0.203125]\n",
      "8957: [discriminator loss: 0.583464, acc: 0.734375] [adversarial loss: 1.114520, acc: 0.265625]\n",
      "8958: [discriminator loss: 0.541426, acc: 0.703125] [adversarial loss: 1.064555, acc: 0.281250]\n",
      "8959: [discriminator loss: 0.541158, acc: 0.695312] [adversarial loss: 1.126401, acc: 0.250000]\n",
      "8960: [discriminator loss: 0.579356, acc: 0.664062] [adversarial loss: 0.944663, acc: 0.281250]\n",
      "8961: [discriminator loss: 0.556162, acc: 0.710938] [adversarial loss: 1.110770, acc: 0.296875]\n",
      "8962: [discriminator loss: 0.485449, acc: 0.750000] [adversarial loss: 0.969801, acc: 0.406250]\n",
      "8963: [discriminator loss: 0.573030, acc: 0.734375] [adversarial loss: 1.503844, acc: 0.109375]\n",
      "8964: [discriminator loss: 0.678509, acc: 0.554688] [adversarial loss: 0.784243, acc: 0.531250]\n",
      "8965: [discriminator loss: 0.604214, acc: 0.632812] [adversarial loss: 1.583347, acc: 0.078125]\n",
      "8966: [discriminator loss: 0.627473, acc: 0.679688] [adversarial loss: 0.716379, acc: 0.562500]\n",
      "8967: [discriminator loss: 0.577018, acc: 0.703125] [adversarial loss: 1.457571, acc: 0.093750]\n",
      "8968: [discriminator loss: 0.520355, acc: 0.710938] [adversarial loss: 1.026138, acc: 0.265625]\n",
      "8969: [discriminator loss: 0.506408, acc: 0.750000] [adversarial loss: 1.140081, acc: 0.265625]\n",
      "8970: [discriminator loss: 0.438427, acc: 0.773438] [adversarial loss: 0.944532, acc: 0.437500]\n",
      "8971: [discriminator loss: 0.524606, acc: 0.703125] [adversarial loss: 1.284250, acc: 0.171875]\n",
      "8972: [discriminator loss: 0.558859, acc: 0.664062] [adversarial loss: 1.140558, acc: 0.312500]\n",
      "8973: [discriminator loss: 0.595150, acc: 0.664062] [adversarial loss: 1.145476, acc: 0.281250]\n",
      "8974: [discriminator loss: 0.553739, acc: 0.695312] [adversarial loss: 1.158206, acc: 0.140625]\n",
      "8975: [discriminator loss: 0.640579, acc: 0.679688] [adversarial loss: 1.057953, acc: 0.265625]\n",
      "8976: [discriminator loss: 0.538547, acc: 0.726562] [adversarial loss: 1.095165, acc: 0.218750]\n",
      "8977: [discriminator loss: 0.534531, acc: 0.710938] [adversarial loss: 1.496695, acc: 0.062500]\n",
      "8978: [discriminator loss: 0.567570, acc: 0.695312] [adversarial loss: 1.358213, acc: 0.156250]\n",
      "8979: [discriminator loss: 0.542291, acc: 0.734375] [adversarial loss: 1.165941, acc: 0.234375]\n",
      "8980: [discriminator loss: 0.644825, acc: 0.710938] [adversarial loss: 1.160340, acc: 0.250000]\n",
      "8981: [discriminator loss: 0.511632, acc: 0.757812] [adversarial loss: 1.013663, acc: 0.328125]\n",
      "8982: [discriminator loss: 0.523016, acc: 0.695312] [adversarial loss: 1.252769, acc: 0.203125]\n",
      "8983: [discriminator loss: 0.566512, acc: 0.695312] [adversarial loss: 1.080070, acc: 0.218750]\n",
      "8984: [discriminator loss: 0.608570, acc: 0.687500] [adversarial loss: 1.087417, acc: 0.234375]\n",
      "8985: [discriminator loss: 0.501081, acc: 0.773438] [adversarial loss: 1.162576, acc: 0.250000]\n",
      "8986: [discriminator loss: 0.456075, acc: 0.781250] [adversarial loss: 1.311399, acc: 0.125000]\n",
      "8987: [discriminator loss: 0.586449, acc: 0.664062] [adversarial loss: 1.328522, acc: 0.109375]\n",
      "8988: [discriminator loss: 0.544491, acc: 0.726562] [adversarial loss: 1.082573, acc: 0.250000]\n",
      "8989: [discriminator loss: 0.509437, acc: 0.781250] [adversarial loss: 1.474344, acc: 0.156250]\n",
      "8990: [discriminator loss: 0.506631, acc: 0.757812] [adversarial loss: 0.855913, acc: 0.359375]\n",
      "8991: [discriminator loss: 0.502728, acc: 0.757812] [adversarial loss: 1.667105, acc: 0.046875]\n",
      "8992: [discriminator loss: 0.558800, acc: 0.671875] [adversarial loss: 0.903848, acc: 0.437500]\n",
      "8993: [discriminator loss: 0.570360, acc: 0.703125] [adversarial loss: 1.500717, acc: 0.062500]\n",
      "8994: [discriminator loss: 0.568149, acc: 0.695312] [adversarial loss: 0.810984, acc: 0.406250]\n",
      "8995: [discriminator loss: 0.584473, acc: 0.671875] [adversarial loss: 1.778610, acc: 0.062500]\n",
      "8996: [discriminator loss: 0.534813, acc: 0.742188] [adversarial loss: 0.825584, acc: 0.421875]\n",
      "8997: [discriminator loss: 0.559583, acc: 0.718750] [adversarial loss: 0.924734, acc: 0.437500]\n",
      "8998: [discriminator loss: 0.555125, acc: 0.710938] [adversarial loss: 1.409974, acc: 0.156250]\n",
      "8999: [discriminator loss: 0.541150, acc: 0.726562] [adversarial loss: 0.891489, acc: 0.343750]\n",
      "9000: [discriminator loss: 0.535949, acc: 0.710938] [adversarial loss: 1.311214, acc: 0.187500]\n",
      "9001: [discriminator loss: 0.573398, acc: 0.656250] [adversarial loss: 0.999612, acc: 0.328125]\n",
      "9002: [discriminator loss: 0.593044, acc: 0.695312] [adversarial loss: 1.502406, acc: 0.109375]\n",
      "9003: [discriminator loss: 0.586974, acc: 0.679688] [adversarial loss: 0.914745, acc: 0.359375]\n",
      "9004: [discriminator loss: 0.652104, acc: 0.671875] [adversarial loss: 1.305557, acc: 0.140625]\n",
      "9005: [discriminator loss: 0.522808, acc: 0.695312] [adversarial loss: 0.896880, acc: 0.484375]\n",
      "9006: [discriminator loss: 0.520280, acc: 0.718750] [adversarial loss: 1.301701, acc: 0.234375]\n",
      "9007: [discriminator loss: 0.584378, acc: 0.664062] [adversarial loss: 1.033174, acc: 0.312500]\n",
      "9008: [discriminator loss: 0.435285, acc: 0.828125] [adversarial loss: 1.301580, acc: 0.218750]\n",
      "9009: [discriminator loss: 0.577566, acc: 0.656250] [adversarial loss: 1.093265, acc: 0.156250]\n",
      "9010: [discriminator loss: 0.509167, acc: 0.750000] [adversarial loss: 1.401484, acc: 0.109375]\n",
      "9011: [discriminator loss: 0.501377, acc: 0.773438] [adversarial loss: 0.858259, acc: 0.406250]\n",
      "9012: [discriminator loss: 0.518073, acc: 0.765625] [adversarial loss: 1.370832, acc: 0.125000]\n",
      "9013: [discriminator loss: 0.533483, acc: 0.710938] [adversarial loss: 1.117911, acc: 0.156250]\n",
      "9014: [discriminator loss: 0.542039, acc: 0.726562] [adversarial loss: 1.414242, acc: 0.156250]\n",
      "9015: [discriminator loss: 0.617322, acc: 0.656250] [adversarial loss: 1.116899, acc: 0.296875]\n",
      "9016: [discriminator loss: 0.509865, acc: 0.726562] [adversarial loss: 1.434293, acc: 0.156250]\n",
      "9017: [discriminator loss: 0.542832, acc: 0.742188] [adversarial loss: 1.004259, acc: 0.234375]\n",
      "9018: [discriminator loss: 0.510825, acc: 0.750000] [adversarial loss: 1.329060, acc: 0.171875]\n",
      "9019: [discriminator loss: 0.506087, acc: 0.695312] [adversarial loss: 0.959758, acc: 0.312500]\n",
      "9020: [discriminator loss: 0.534753, acc: 0.718750] [adversarial loss: 1.242405, acc: 0.203125]\n",
      "9021: [discriminator loss: 0.545150, acc: 0.703125] [adversarial loss: 0.959052, acc: 0.390625]\n",
      "9022: [discriminator loss: 0.551962, acc: 0.742188] [adversarial loss: 1.325017, acc: 0.125000]\n",
      "9023: [discriminator loss: 0.597828, acc: 0.671875] [adversarial loss: 1.046266, acc: 0.281250]\n",
      "9024: [discriminator loss: 0.588639, acc: 0.632812] [adversarial loss: 1.318847, acc: 0.140625]\n",
      "9025: [discriminator loss: 0.489511, acc: 0.757812] [adversarial loss: 1.086006, acc: 0.250000]\n",
      "9026: [discriminator loss: 0.520473, acc: 0.703125] [adversarial loss: 1.146998, acc: 0.218750]\n",
      "9027: [discriminator loss: 0.565034, acc: 0.710938] [adversarial loss: 1.322497, acc: 0.203125]\n",
      "9028: [discriminator loss: 0.605114, acc: 0.671875] [adversarial loss: 0.924949, acc: 0.406250]\n",
      "9029: [discriminator loss: 0.534213, acc: 0.734375] [adversarial loss: 1.467094, acc: 0.125000]\n",
      "9030: [discriminator loss: 0.580075, acc: 0.695312] [adversarial loss: 1.136642, acc: 0.234375]\n",
      "9031: [discriminator loss: 0.614300, acc: 0.656250] [adversarial loss: 1.244891, acc: 0.171875]\n",
      "9032: [discriminator loss: 0.518011, acc: 0.710938] [adversarial loss: 1.412169, acc: 0.093750]\n",
      "9033: [discriminator loss: 0.628065, acc: 0.617188] [adversarial loss: 0.736832, acc: 0.515625]\n",
      "9034: [discriminator loss: 0.607006, acc: 0.687500] [adversarial loss: 1.344660, acc: 0.109375]\n",
      "9035: [discriminator loss: 0.597382, acc: 0.664062] [adversarial loss: 0.998496, acc: 0.328125]\n",
      "9036: [discriminator loss: 0.513216, acc: 0.710938] [adversarial loss: 1.420549, acc: 0.156250]\n",
      "9037: [discriminator loss: 0.539754, acc: 0.726562] [adversarial loss: 1.078710, acc: 0.328125]\n",
      "9038: [discriminator loss: 0.578744, acc: 0.695312] [adversarial loss: 1.160391, acc: 0.218750]\n",
      "9039: [discriminator loss: 0.516919, acc: 0.726562] [adversarial loss: 1.072257, acc: 0.250000]\n",
      "9040: [discriminator loss: 0.526687, acc: 0.718750] [adversarial loss: 1.531475, acc: 0.062500]\n",
      "9041: [discriminator loss: 0.567108, acc: 0.710938] [adversarial loss: 1.091750, acc: 0.312500]\n",
      "9042: [discriminator loss: 0.522127, acc: 0.742188] [adversarial loss: 1.013429, acc: 0.296875]\n",
      "9043: [discriminator loss: 0.634418, acc: 0.671875] [adversarial loss: 1.005887, acc: 0.359375]\n",
      "9044: [discriminator loss: 0.574744, acc: 0.703125] [adversarial loss: 1.243626, acc: 0.250000]\n",
      "9045: [discriminator loss: 0.539799, acc: 0.710938] [adversarial loss: 1.073968, acc: 0.359375]\n",
      "9046: [discriminator loss: 0.513071, acc: 0.750000] [adversarial loss: 1.591363, acc: 0.109375]\n",
      "9047: [discriminator loss: 0.561906, acc: 0.679688] [adversarial loss: 0.686984, acc: 0.531250]\n",
      "9048: [discriminator loss: 0.567855, acc: 0.687500] [adversarial loss: 1.556535, acc: 0.078125]\n",
      "9049: [discriminator loss: 0.667235, acc: 0.609375] [adversarial loss: 0.810025, acc: 0.468750]\n",
      "9050: [discriminator loss: 0.554464, acc: 0.664062] [adversarial loss: 1.240649, acc: 0.171875]\n",
      "9051: [discriminator loss: 0.566329, acc: 0.671875] [adversarial loss: 0.857768, acc: 0.484375]\n",
      "9052: [discriminator loss: 0.509825, acc: 0.757812] [adversarial loss: 1.200613, acc: 0.250000]\n",
      "9053: [discriminator loss: 0.554601, acc: 0.726562] [adversarial loss: 1.419072, acc: 0.109375]\n",
      "9054: [discriminator loss: 0.480335, acc: 0.773438] [adversarial loss: 0.995140, acc: 0.406250]\n",
      "9055: [discriminator loss: 0.515553, acc: 0.757812] [adversarial loss: 1.360755, acc: 0.234375]\n",
      "9056: [discriminator loss: 0.558139, acc: 0.703125] [adversarial loss: 1.024793, acc: 0.296875]\n",
      "9057: [discriminator loss: 0.622978, acc: 0.679688] [adversarial loss: 1.399655, acc: 0.125000]\n",
      "9058: [discriminator loss: 0.623572, acc: 0.640625] [adversarial loss: 0.920254, acc: 0.421875]\n",
      "9059: [discriminator loss: 0.597138, acc: 0.671875] [adversarial loss: 1.319336, acc: 0.125000]\n",
      "9060: [discriminator loss: 0.556617, acc: 0.726562] [adversarial loss: 1.016123, acc: 0.281250]\n",
      "9061: [discriminator loss: 0.533517, acc: 0.726562] [adversarial loss: 1.330902, acc: 0.218750]\n",
      "9062: [discriminator loss: 0.561105, acc: 0.679688] [adversarial loss: 1.189035, acc: 0.140625]\n",
      "9063: [discriminator loss: 0.532393, acc: 0.718750] [adversarial loss: 1.248533, acc: 0.125000]\n",
      "9064: [discriminator loss: 0.512257, acc: 0.789062] [adversarial loss: 1.099107, acc: 0.234375]\n",
      "9065: [discriminator loss: 0.597855, acc: 0.640625] [adversarial loss: 1.252220, acc: 0.187500]\n",
      "9066: [discriminator loss: 0.549528, acc: 0.710938] [adversarial loss: 1.086791, acc: 0.296875]\n",
      "9067: [discriminator loss: 0.535911, acc: 0.710938] [adversarial loss: 0.997789, acc: 0.359375]\n",
      "9068: [discriminator loss: 0.547526, acc: 0.718750] [adversarial loss: 1.377121, acc: 0.234375]\n",
      "9069: [discriminator loss: 0.567675, acc: 0.687500] [adversarial loss: 0.966535, acc: 0.375000]\n",
      "9070: [discriminator loss: 0.550822, acc: 0.750000] [adversarial loss: 1.329179, acc: 0.171875]\n",
      "9071: [discriminator loss: 0.573037, acc: 0.703125] [adversarial loss: 0.935602, acc: 0.343750]\n",
      "9072: [discriminator loss: 0.551867, acc: 0.718750] [adversarial loss: 1.207594, acc: 0.234375]\n",
      "9073: [discriminator loss: 0.548738, acc: 0.734375] [adversarial loss: 0.872537, acc: 0.437500]\n",
      "9074: [discriminator loss: 0.507618, acc: 0.765625] [adversarial loss: 1.435435, acc: 0.156250]\n",
      "9075: [discriminator loss: 0.505176, acc: 0.742188] [adversarial loss: 1.137620, acc: 0.234375]\n",
      "9076: [discriminator loss: 0.581605, acc: 0.718750] [adversarial loss: 1.260006, acc: 0.265625]\n",
      "9077: [discriminator loss: 0.512313, acc: 0.773438] [adversarial loss: 0.978068, acc: 0.343750]\n",
      "9078: [discriminator loss: 0.559908, acc: 0.695312] [adversarial loss: 1.423840, acc: 0.125000]\n",
      "9079: [discriminator loss: 0.598399, acc: 0.664062] [adversarial loss: 0.934310, acc: 0.328125]\n",
      "9080: [discriminator loss: 0.590946, acc: 0.687500] [adversarial loss: 1.269819, acc: 0.234375]\n",
      "9081: [discriminator loss: 0.661601, acc: 0.609375] [adversarial loss: 1.355589, acc: 0.171875]\n",
      "9082: [discriminator loss: 0.522908, acc: 0.734375] [adversarial loss: 1.308215, acc: 0.250000]\n",
      "9083: [discriminator loss: 0.498240, acc: 0.765625] [adversarial loss: 1.137155, acc: 0.250000]\n",
      "9084: [discriminator loss: 0.478264, acc: 0.781250] [adversarial loss: 1.043839, acc: 0.359375]\n",
      "9085: [discriminator loss: 0.585582, acc: 0.695312] [adversarial loss: 1.117520, acc: 0.250000]\n",
      "9086: [discriminator loss: 0.528612, acc: 0.757812] [adversarial loss: 0.973539, acc: 0.359375]\n",
      "9087: [discriminator loss: 0.555557, acc: 0.703125] [adversarial loss: 1.466158, acc: 0.093750]\n",
      "9088: [discriminator loss: 0.504479, acc: 0.710938] [adversarial loss: 1.007295, acc: 0.359375]\n",
      "9089: [discriminator loss: 0.589388, acc: 0.656250] [adversarial loss: 1.204659, acc: 0.250000]\n",
      "9090: [discriminator loss: 0.575133, acc: 0.656250] [adversarial loss: 1.622403, acc: 0.109375]\n",
      "9091: [discriminator loss: 0.637573, acc: 0.648438] [adversarial loss: 0.650127, acc: 0.562500]\n",
      "9092: [discriminator loss: 0.584400, acc: 0.703125] [adversarial loss: 1.412782, acc: 0.125000]\n",
      "9093: [discriminator loss: 0.572781, acc: 0.671875] [adversarial loss: 0.887186, acc: 0.437500]\n",
      "9094: [discriminator loss: 0.543441, acc: 0.703125] [adversarial loss: 1.209788, acc: 0.156250]\n",
      "9095: [discriminator loss: 0.561745, acc: 0.710938] [adversarial loss: 1.211716, acc: 0.140625]\n",
      "9096: [discriminator loss: 0.526184, acc: 0.703125] [adversarial loss: 1.217474, acc: 0.265625]\n",
      "9097: [discriminator loss: 0.517454, acc: 0.726562] [adversarial loss: 1.433577, acc: 0.062500]\n",
      "9098: [discriminator loss: 0.547110, acc: 0.687500] [adversarial loss: 1.012783, acc: 0.296875]\n",
      "9099: [discriminator loss: 0.589924, acc: 0.726562] [adversarial loss: 1.269175, acc: 0.250000]\n",
      "9100: [discriminator loss: 0.526483, acc: 0.750000] [adversarial loss: 0.977227, acc: 0.406250]\n",
      "9101: [discriminator loss: 0.496119, acc: 0.750000] [adversarial loss: 1.264819, acc: 0.234375]\n",
      "9102: [discriminator loss: 0.570110, acc: 0.703125] [adversarial loss: 1.123379, acc: 0.187500]\n",
      "9103: [discriminator loss: 0.532281, acc: 0.703125] [adversarial loss: 1.153118, acc: 0.250000]\n",
      "9104: [discriminator loss: 0.547601, acc: 0.757812] [adversarial loss: 1.247357, acc: 0.187500]\n",
      "9105: [discriminator loss: 0.532161, acc: 0.703125] [adversarial loss: 1.127686, acc: 0.234375]\n",
      "9106: [discriminator loss: 0.489091, acc: 0.804688] [adversarial loss: 1.354614, acc: 0.187500]\n",
      "9107: [discriminator loss: 0.558530, acc: 0.710938] [adversarial loss: 0.998198, acc: 0.328125]\n",
      "9108: [discriminator loss: 0.538767, acc: 0.710938] [adversarial loss: 1.070669, acc: 0.359375]\n",
      "9109: [discriminator loss: 0.574100, acc: 0.718750] [adversarial loss: 1.257737, acc: 0.218750]\n",
      "9110: [discriminator loss: 0.515365, acc: 0.718750] [adversarial loss: 1.099467, acc: 0.281250]\n",
      "9111: [discriminator loss: 0.551694, acc: 0.710938] [adversarial loss: 1.358014, acc: 0.140625]\n",
      "9112: [discriminator loss: 0.614152, acc: 0.656250] [adversarial loss: 0.848133, acc: 0.468750]\n",
      "9113: [discriminator loss: 0.584817, acc: 0.726562] [adversarial loss: 1.334714, acc: 0.093750]\n",
      "9114: [discriminator loss: 0.602496, acc: 0.617188] [adversarial loss: 0.835416, acc: 0.562500]\n",
      "9115: [discriminator loss: 0.569217, acc: 0.679688] [adversarial loss: 1.507455, acc: 0.109375]\n",
      "9116: [discriminator loss: 0.510451, acc: 0.757812] [adversarial loss: 0.986467, acc: 0.265625]\n",
      "9117: [discriminator loss: 0.533536, acc: 0.742188] [adversarial loss: 1.416239, acc: 0.171875]\n",
      "9118: [discriminator loss: 0.460973, acc: 0.742188] [adversarial loss: 1.099355, acc: 0.187500]\n",
      "9119: [discriminator loss: 0.546867, acc: 0.671875] [adversarial loss: 1.316039, acc: 0.125000]\n",
      "9120: [discriminator loss: 0.523285, acc: 0.703125] [adversarial loss: 1.113105, acc: 0.250000]\n",
      "9121: [discriminator loss: 0.538649, acc: 0.734375] [adversarial loss: 1.172074, acc: 0.187500]\n",
      "9122: [discriminator loss: 0.518254, acc: 0.718750] [adversarial loss: 1.051983, acc: 0.328125]\n",
      "9123: [discriminator loss: 0.569211, acc: 0.664062] [adversarial loss: 1.296942, acc: 0.109375]\n",
      "9124: [discriminator loss: 0.577112, acc: 0.656250] [adversarial loss: 0.874743, acc: 0.484375]\n",
      "9125: [discriminator loss: 0.563281, acc: 0.710938] [adversarial loss: 1.233468, acc: 0.234375]\n",
      "9126: [discriminator loss: 0.511254, acc: 0.726562] [adversarial loss: 1.345441, acc: 0.109375]\n",
      "9127: [discriminator loss: 0.608041, acc: 0.648438] [adversarial loss: 0.816942, acc: 0.437500]\n",
      "9128: [discriminator loss: 0.556282, acc: 0.710938] [adversarial loss: 1.200368, acc: 0.203125]\n",
      "9129: [discriminator loss: 0.516082, acc: 0.742188] [adversarial loss: 0.988823, acc: 0.375000]\n",
      "9130: [discriminator loss: 0.497142, acc: 0.750000] [adversarial loss: 1.481545, acc: 0.140625]\n",
      "9131: [discriminator loss: 0.565729, acc: 0.703125] [adversarial loss: 0.986904, acc: 0.312500]\n",
      "9132: [discriminator loss: 0.595716, acc: 0.671875] [adversarial loss: 1.388719, acc: 0.187500]\n",
      "9133: [discriminator loss: 0.523944, acc: 0.742188] [adversarial loss: 0.987797, acc: 0.406250]\n",
      "9134: [discriminator loss: 0.573303, acc: 0.687500] [adversarial loss: 1.606071, acc: 0.062500]\n",
      "9135: [discriminator loss: 0.576522, acc: 0.687500] [adversarial loss: 0.982366, acc: 0.312500]\n",
      "9136: [discriminator loss: 0.511161, acc: 0.757812] [adversarial loss: 1.292696, acc: 0.187500]\n",
      "9137: [discriminator loss: 0.583001, acc: 0.703125] [adversarial loss: 1.089802, acc: 0.250000]\n",
      "9138: [discriminator loss: 0.497625, acc: 0.734375] [adversarial loss: 1.379727, acc: 0.093750]\n",
      "9139: [discriminator loss: 0.549493, acc: 0.687500] [adversarial loss: 0.918275, acc: 0.390625]\n",
      "9140: [discriminator loss: 0.532530, acc: 0.695312] [adversarial loss: 1.715625, acc: 0.062500]\n",
      "9141: [discriminator loss: 0.566397, acc: 0.687500] [adversarial loss: 0.976143, acc: 0.343750]\n",
      "9142: [discriminator loss: 0.557595, acc: 0.695312] [adversarial loss: 1.084985, acc: 0.250000]\n",
      "9143: [discriminator loss: 0.627450, acc: 0.640625] [adversarial loss: 1.386582, acc: 0.093750]\n",
      "9144: [discriminator loss: 0.493905, acc: 0.742188] [adversarial loss: 1.105873, acc: 0.281250]\n",
      "9145: [discriminator loss: 0.533953, acc: 0.734375] [adversarial loss: 1.199552, acc: 0.250000]\n",
      "9146: [discriminator loss: 0.513058, acc: 0.750000] [adversarial loss: 1.277319, acc: 0.171875]\n",
      "9147: [discriminator loss: 0.492479, acc: 0.734375] [adversarial loss: 1.129757, acc: 0.203125]\n",
      "9148: [discriminator loss: 0.545501, acc: 0.679688] [adversarial loss: 1.061532, acc: 0.296875]\n",
      "9149: [discriminator loss: 0.553695, acc: 0.695312] [adversarial loss: 1.359640, acc: 0.078125]\n",
      "9150: [discriminator loss: 0.625515, acc: 0.640625] [adversarial loss: 0.995052, acc: 0.437500]\n",
      "9151: [discriminator loss: 0.494071, acc: 0.750000] [adversarial loss: 0.961389, acc: 0.375000]\n",
      "9152: [discriminator loss: 0.616511, acc: 0.609375] [adversarial loss: 1.359113, acc: 0.187500]\n",
      "9153: [discriminator loss: 0.469956, acc: 0.765625] [adversarial loss: 1.055557, acc: 0.265625]\n",
      "9154: [discriminator loss: 0.501535, acc: 0.734375] [adversarial loss: 1.342687, acc: 0.109375]\n",
      "9155: [discriminator loss: 0.503708, acc: 0.765625] [adversarial loss: 1.104138, acc: 0.250000]\n",
      "9156: [discriminator loss: 0.561499, acc: 0.679688] [adversarial loss: 1.085915, acc: 0.312500]\n",
      "9157: [discriminator loss: 0.633003, acc: 0.679688] [adversarial loss: 1.284157, acc: 0.187500]\n",
      "9158: [discriminator loss: 0.509552, acc: 0.703125] [adversarial loss: 1.335543, acc: 0.218750]\n",
      "9159: [discriminator loss: 0.525300, acc: 0.726562] [adversarial loss: 0.926906, acc: 0.421875]\n",
      "9160: [discriminator loss: 0.534799, acc: 0.710938] [adversarial loss: 1.102868, acc: 0.281250]\n",
      "9161: [discriminator loss: 0.565924, acc: 0.664062] [adversarial loss: 1.192207, acc: 0.234375]\n",
      "9162: [discriminator loss: 0.561940, acc: 0.679688] [adversarial loss: 1.044734, acc: 0.296875]\n",
      "9163: [discriminator loss: 0.634544, acc: 0.656250] [adversarial loss: 1.263248, acc: 0.203125]\n",
      "9164: [discriminator loss: 0.532955, acc: 0.703125] [adversarial loss: 0.751968, acc: 0.500000]\n",
      "9165: [discriminator loss: 0.590756, acc: 0.695312] [adversarial loss: 1.709506, acc: 0.093750]\n",
      "9166: [discriminator loss: 0.672372, acc: 0.578125] [adversarial loss: 0.982637, acc: 0.359375]\n",
      "9167: [discriminator loss: 0.559848, acc: 0.679688] [adversarial loss: 1.314010, acc: 0.156250]\n",
      "9168: [discriminator loss: 0.538783, acc: 0.718750] [adversarial loss: 1.063314, acc: 0.250000]\n",
      "9169: [discriminator loss: 0.524969, acc: 0.710938] [adversarial loss: 1.346519, acc: 0.093750]\n",
      "9170: [discriminator loss: 0.555428, acc: 0.648438] [adversarial loss: 0.977195, acc: 0.328125]\n",
      "9171: [discriminator loss: 0.438589, acc: 0.789062] [adversarial loss: 1.224119, acc: 0.234375]\n",
      "9172: [discriminator loss: 0.586135, acc: 0.687500] [adversarial loss: 1.374407, acc: 0.140625]\n",
      "9173: [discriminator loss: 0.527401, acc: 0.710938] [adversarial loss: 1.242073, acc: 0.140625]\n",
      "9174: [discriminator loss: 0.478663, acc: 0.773438] [adversarial loss: 1.220961, acc: 0.250000]\n",
      "9175: [discriminator loss: 0.630040, acc: 0.625000] [adversarial loss: 1.076700, acc: 0.359375]\n",
      "9176: [discriminator loss: 0.604864, acc: 0.703125] [adversarial loss: 1.083411, acc: 0.281250]\n",
      "9177: [discriminator loss: 0.543592, acc: 0.671875] [adversarial loss: 1.449908, acc: 0.093750]\n",
      "9178: [discriminator loss: 0.549312, acc: 0.734375] [adversarial loss: 1.036814, acc: 0.312500]\n",
      "9179: [discriminator loss: 0.578921, acc: 0.695312] [adversarial loss: 1.329868, acc: 0.156250]\n",
      "9180: [discriminator loss: 0.471425, acc: 0.796875] [adversarial loss: 1.078897, acc: 0.281250]\n",
      "9181: [discriminator loss: 0.522406, acc: 0.726562] [adversarial loss: 1.092535, acc: 0.328125]\n",
      "9182: [discriminator loss: 0.552631, acc: 0.718750] [adversarial loss: 1.110724, acc: 0.343750]\n",
      "9183: [discriminator loss: 0.508071, acc: 0.757812] [adversarial loss: 1.202436, acc: 0.312500]\n",
      "9184: [discriminator loss: 0.551016, acc: 0.695312] [adversarial loss: 0.970451, acc: 0.296875]\n",
      "9185: [discriminator loss: 0.540977, acc: 0.726562] [adversarial loss: 1.601095, acc: 0.078125]\n",
      "9186: [discriminator loss: 0.587162, acc: 0.671875] [adversarial loss: 0.740922, acc: 0.500000]\n",
      "9187: [discriminator loss: 0.595848, acc: 0.664062] [adversarial loss: 1.254611, acc: 0.218750]\n",
      "9188: [discriminator loss: 0.531406, acc: 0.765625] [adversarial loss: 0.982808, acc: 0.390625]\n",
      "9189: [discriminator loss: 0.528350, acc: 0.710938] [adversarial loss: 1.397905, acc: 0.078125]\n",
      "9190: [discriminator loss: 0.538003, acc: 0.695312] [adversarial loss: 0.883019, acc: 0.468750]\n",
      "9191: [discriminator loss: 0.564606, acc: 0.679688] [adversarial loss: 1.272369, acc: 0.156250]\n",
      "9192: [discriminator loss: 0.494116, acc: 0.765625] [adversarial loss: 1.067344, acc: 0.296875]\n",
      "9193: [discriminator loss: 0.561271, acc: 0.710938] [adversarial loss: 1.450536, acc: 0.109375]\n",
      "9194: [discriminator loss: 0.551190, acc: 0.695312] [adversarial loss: 0.978009, acc: 0.281250]\n",
      "9195: [discriminator loss: 0.562618, acc: 0.703125] [adversarial loss: 1.352359, acc: 0.171875]\n",
      "9196: [discriminator loss: 0.554499, acc: 0.703125] [adversarial loss: 1.072061, acc: 0.312500]\n",
      "9197: [discriminator loss: 0.553729, acc: 0.640625] [adversarial loss: 1.663755, acc: 0.078125]\n",
      "9198: [discriminator loss: 0.528471, acc: 0.710938] [adversarial loss: 0.833946, acc: 0.437500]\n",
      "9199: [discriminator loss: 0.558899, acc: 0.742188] [adversarial loss: 1.412153, acc: 0.234375]\n",
      "9200: [discriminator loss: 0.598918, acc: 0.679688] [adversarial loss: 1.109433, acc: 0.218750]\n",
      "9201: [discriminator loss: 0.542944, acc: 0.726562] [adversarial loss: 1.133508, acc: 0.250000]\n",
      "9202: [discriminator loss: 0.538181, acc: 0.695312] [adversarial loss: 1.155772, acc: 0.234375]\n",
      "9203: [discriminator loss: 0.559514, acc: 0.703125] [adversarial loss: 1.175274, acc: 0.281250]\n",
      "9204: [discriminator loss: 0.520884, acc: 0.750000] [adversarial loss: 1.061459, acc: 0.265625]\n",
      "9205: [discriminator loss: 0.548585, acc: 0.664062] [adversarial loss: 1.148751, acc: 0.328125]\n",
      "9206: [discriminator loss: 0.570815, acc: 0.726562] [adversarial loss: 1.367963, acc: 0.109375]\n",
      "9207: [discriminator loss: 0.583609, acc: 0.687500] [adversarial loss: 0.812121, acc: 0.515625]\n",
      "9208: [discriminator loss: 0.526303, acc: 0.695312] [adversarial loss: 1.512281, acc: 0.125000]\n",
      "9209: [discriminator loss: 0.630821, acc: 0.632812] [adversarial loss: 0.981113, acc: 0.296875]\n",
      "9210: [discriminator loss: 0.557195, acc: 0.710938] [adversarial loss: 1.513918, acc: 0.109375]\n",
      "9211: [discriminator loss: 0.566084, acc: 0.750000] [adversarial loss: 0.926356, acc: 0.390625]\n",
      "9212: [discriminator loss: 0.573659, acc: 0.703125] [adversarial loss: 1.340315, acc: 0.187500]\n",
      "9213: [discriminator loss: 0.525568, acc: 0.726562] [adversarial loss: 0.968381, acc: 0.437500]\n",
      "9214: [discriminator loss: 0.596626, acc: 0.648438] [adversarial loss: 1.258110, acc: 0.234375]\n",
      "9215: [discriminator loss: 0.511889, acc: 0.742188] [adversarial loss: 1.398468, acc: 0.156250]\n",
      "9216: [discriminator loss: 0.501863, acc: 0.742188] [adversarial loss: 1.033717, acc: 0.281250]\n",
      "9217: [discriminator loss: 0.513782, acc: 0.750000] [adversarial loss: 1.134890, acc: 0.250000]\n",
      "9218: [discriminator loss: 0.524466, acc: 0.757812] [adversarial loss: 1.291620, acc: 0.218750]\n",
      "9219: [discriminator loss: 0.527936, acc: 0.726562] [adversarial loss: 0.894364, acc: 0.406250]\n",
      "9220: [discriminator loss: 0.512593, acc: 0.757812] [adversarial loss: 1.241218, acc: 0.187500]\n",
      "9221: [discriminator loss: 0.575633, acc: 0.671875] [adversarial loss: 0.889756, acc: 0.437500]\n",
      "9222: [discriminator loss: 0.559513, acc: 0.710938] [adversarial loss: 1.418316, acc: 0.093750]\n",
      "9223: [discriminator loss: 0.512242, acc: 0.757812] [adversarial loss: 1.149725, acc: 0.234375]\n",
      "9224: [discriminator loss: 0.599455, acc: 0.671875] [adversarial loss: 1.009529, acc: 0.406250]\n",
      "9225: [discriminator loss: 0.499247, acc: 0.718750] [adversarial loss: 0.937013, acc: 0.375000]\n",
      "9226: [discriminator loss: 0.490715, acc: 0.703125] [adversarial loss: 1.226238, acc: 0.250000]\n",
      "9227: [discriminator loss: 0.621475, acc: 0.648438] [adversarial loss: 1.302348, acc: 0.218750]\n",
      "9228: [discriminator loss: 0.557534, acc: 0.687500] [adversarial loss: 1.175074, acc: 0.187500]\n",
      "9229: [discriminator loss: 0.530327, acc: 0.679688] [adversarial loss: 1.231076, acc: 0.156250]\n",
      "9230: [discriminator loss: 0.478140, acc: 0.765625] [adversarial loss: 1.325625, acc: 0.093750]\n",
      "9231: [discriminator loss: 0.542461, acc: 0.726562] [adversarial loss: 1.016505, acc: 0.312500]\n",
      "9232: [discriminator loss: 0.563442, acc: 0.671875] [adversarial loss: 1.257092, acc: 0.140625]\n",
      "9233: [discriminator loss: 0.579917, acc: 0.640625] [adversarial loss: 0.863990, acc: 0.468750]\n",
      "9234: [discriminator loss: 0.555707, acc: 0.703125] [adversarial loss: 1.395148, acc: 0.109375]\n",
      "9235: [discriminator loss: 0.570326, acc: 0.718750] [adversarial loss: 1.009403, acc: 0.359375]\n",
      "9236: [discriminator loss: 0.576677, acc: 0.710938] [adversarial loss: 1.153460, acc: 0.234375]\n",
      "9237: [discriminator loss: 0.531510, acc: 0.695312] [adversarial loss: 1.187955, acc: 0.250000]\n",
      "9238: [discriminator loss: 0.549361, acc: 0.679688] [adversarial loss: 0.932963, acc: 0.421875]\n",
      "9239: [discriminator loss: 0.524886, acc: 0.734375] [adversarial loss: 1.262365, acc: 0.234375]\n",
      "9240: [discriminator loss: 0.572533, acc: 0.703125] [adversarial loss: 1.030485, acc: 0.343750]\n",
      "9241: [discriminator loss: 0.561565, acc: 0.726562] [adversarial loss: 1.461392, acc: 0.156250]\n",
      "9242: [discriminator loss: 0.521404, acc: 0.742188] [adversarial loss: 0.960064, acc: 0.375000]\n",
      "9243: [discriminator loss: 0.568424, acc: 0.679688] [adversarial loss: 1.445914, acc: 0.140625]\n",
      "9244: [discriminator loss: 0.524794, acc: 0.726562] [adversarial loss: 0.888496, acc: 0.390625]\n",
      "9245: [discriminator loss: 0.578292, acc: 0.679688] [adversarial loss: 1.412786, acc: 0.171875]\n",
      "9246: [discriminator loss: 0.582266, acc: 0.656250] [adversarial loss: 1.024923, acc: 0.421875]\n",
      "9247: [discriminator loss: 0.620312, acc: 0.648438] [adversarial loss: 0.989478, acc: 0.375000]\n",
      "9248: [discriminator loss: 0.526649, acc: 0.781250] [adversarial loss: 1.472809, acc: 0.203125]\n",
      "9249: [discriminator loss: 0.503936, acc: 0.718750] [adversarial loss: 1.157412, acc: 0.359375]\n",
      "9250: [discriminator loss: 0.557445, acc: 0.734375] [adversarial loss: 1.431793, acc: 0.156250]\n",
      "9251: [discriminator loss: 0.541949, acc: 0.687500] [adversarial loss: 0.743042, acc: 0.515625]\n",
      "9252: [discriminator loss: 0.528077, acc: 0.742188] [adversarial loss: 1.532517, acc: 0.093750]\n",
      "9253: [discriminator loss: 0.542226, acc: 0.687500] [adversarial loss: 1.136154, acc: 0.203125]\n",
      "9254: [discriminator loss: 0.494729, acc: 0.750000] [adversarial loss: 1.284023, acc: 0.140625]\n",
      "9255: [discriminator loss: 0.440234, acc: 0.796875] [adversarial loss: 1.252104, acc: 0.218750]\n",
      "9256: [discriminator loss: 0.589904, acc: 0.640625] [adversarial loss: 1.418776, acc: 0.187500]\n",
      "9257: [discriminator loss: 0.448759, acc: 0.796875] [adversarial loss: 1.100195, acc: 0.250000]\n",
      "9258: [discriminator loss: 0.495878, acc: 0.765625] [adversarial loss: 1.617194, acc: 0.093750]\n",
      "9259: [discriminator loss: 0.545398, acc: 0.703125] [adversarial loss: 0.955666, acc: 0.453125]\n",
      "9260: [discriminator loss: 0.583219, acc: 0.687500] [adversarial loss: 1.718451, acc: 0.125000]\n",
      "9261: [discriminator loss: 0.666091, acc: 0.625000] [adversarial loss: 0.825047, acc: 0.453125]\n",
      "9262: [discriminator loss: 0.593299, acc: 0.687500] [adversarial loss: 1.309951, acc: 0.187500]\n",
      "9263: [discriminator loss: 0.644727, acc: 0.695312] [adversarial loss: 0.944122, acc: 0.437500]\n",
      "9264: [discriminator loss: 0.580712, acc: 0.671875] [adversarial loss: 1.494286, acc: 0.125000]\n",
      "9265: [discriminator loss: 0.473611, acc: 0.757812] [adversarial loss: 0.982402, acc: 0.281250]\n",
      "9266: [discriminator loss: 0.452450, acc: 0.789062] [adversarial loss: 1.521036, acc: 0.140625]\n",
      "9267: [discriminator loss: 0.538345, acc: 0.726562] [adversarial loss: 0.889702, acc: 0.531250]\n",
      "9268: [discriminator loss: 0.587801, acc: 0.679688] [adversarial loss: 1.626284, acc: 0.078125]\n",
      "9269: [discriminator loss: 0.562601, acc: 0.671875] [adversarial loss: 1.147540, acc: 0.234375]\n",
      "9270: [discriminator loss: 0.571160, acc: 0.710938] [adversarial loss: 1.251223, acc: 0.109375]\n",
      "9271: [discriminator loss: 0.599512, acc: 0.718750] [adversarial loss: 1.075297, acc: 0.312500]\n",
      "9272: [discriminator loss: 0.570531, acc: 0.687500] [adversarial loss: 1.372617, acc: 0.156250]\n",
      "9273: [discriminator loss: 0.566627, acc: 0.671875] [adversarial loss: 0.955275, acc: 0.390625]\n",
      "9274: [discriminator loss: 0.556173, acc: 0.710938] [adversarial loss: 1.296778, acc: 0.171875]\n",
      "9275: [discriminator loss: 0.587052, acc: 0.656250] [adversarial loss: 1.239914, acc: 0.234375]\n",
      "9276: [discriminator loss: 0.504794, acc: 0.726562] [adversarial loss: 1.114130, acc: 0.250000]\n",
      "9277: [discriminator loss: 0.567420, acc: 0.656250] [adversarial loss: 1.128399, acc: 0.359375]\n",
      "9278: [discriminator loss: 0.554071, acc: 0.718750] [adversarial loss: 1.141631, acc: 0.296875]\n",
      "9279: [discriminator loss: 0.483016, acc: 0.757812] [adversarial loss: 1.264553, acc: 0.218750]\n",
      "9280: [discriminator loss: 0.518335, acc: 0.765625] [adversarial loss: 0.857092, acc: 0.500000]\n",
      "9281: [discriminator loss: 0.580832, acc: 0.726562] [adversarial loss: 1.787085, acc: 0.093750]\n",
      "9282: [discriminator loss: 0.548115, acc: 0.742188] [adversarial loss: 1.068739, acc: 0.281250]\n",
      "9283: [discriminator loss: 0.522258, acc: 0.687500] [adversarial loss: 1.123165, acc: 0.156250]\n",
      "9284: [discriminator loss: 0.523489, acc: 0.757812] [adversarial loss: 1.325568, acc: 0.187500]\n",
      "9285: [discriminator loss: 0.547149, acc: 0.671875] [adversarial loss: 0.882732, acc: 0.484375]\n",
      "9286: [discriminator loss: 0.574932, acc: 0.679688] [adversarial loss: 1.069567, acc: 0.281250]\n",
      "9287: [discriminator loss: 0.504973, acc: 0.750000] [adversarial loss: 1.312713, acc: 0.250000]\n",
      "9288: [discriminator loss: 0.507502, acc: 0.765625] [adversarial loss: 1.219013, acc: 0.187500]\n",
      "9289: [discriminator loss: 0.567330, acc: 0.703125] [adversarial loss: 1.332960, acc: 0.125000]\n",
      "9290: [discriminator loss: 0.512859, acc: 0.718750] [adversarial loss: 1.010011, acc: 0.359375]\n",
      "9291: [discriminator loss: 0.615153, acc: 0.648438] [adversarial loss: 1.133145, acc: 0.234375]\n",
      "9292: [discriminator loss: 0.528394, acc: 0.726562] [adversarial loss: 0.881809, acc: 0.437500]\n",
      "9293: [discriminator loss: 0.552296, acc: 0.726562] [adversarial loss: 1.580364, acc: 0.109375]\n",
      "9294: [discriminator loss: 0.551876, acc: 0.710938] [adversarial loss: 1.147348, acc: 0.265625]\n",
      "9295: [discriminator loss: 0.568329, acc: 0.695312] [adversarial loss: 1.596186, acc: 0.109375]\n",
      "9296: [discriminator loss: 0.553941, acc: 0.718750] [adversarial loss: 0.912839, acc: 0.390625]\n",
      "9297: [discriminator loss: 0.612110, acc: 0.562500] [adversarial loss: 1.277367, acc: 0.234375]\n",
      "9298: [discriminator loss: 0.535599, acc: 0.703125] [adversarial loss: 1.037411, acc: 0.281250]\n",
      "9299: [discriminator loss: 0.611668, acc: 0.648438] [adversarial loss: 1.216278, acc: 0.265625]\n",
      "9300: [discriminator loss: 0.518851, acc: 0.750000] [adversarial loss: 1.317717, acc: 0.187500]\n",
      "9301: [discriminator loss: 0.548078, acc: 0.703125] [adversarial loss: 1.500652, acc: 0.140625]\n",
      "9302: [discriminator loss: 0.547796, acc: 0.664062] [adversarial loss: 0.845788, acc: 0.437500]\n",
      "9303: [discriminator loss: 0.518895, acc: 0.757812] [adversarial loss: 1.674378, acc: 0.093750]\n",
      "9304: [discriminator loss: 0.555488, acc: 0.687500] [adversarial loss: 0.893901, acc: 0.406250]\n",
      "9305: [discriminator loss: 0.565318, acc: 0.710938] [adversarial loss: 1.307761, acc: 0.171875]\n",
      "9306: [discriminator loss: 0.591548, acc: 0.703125] [adversarial loss: 1.211064, acc: 0.312500]\n",
      "9307: [discriminator loss: 0.606176, acc: 0.679688] [adversarial loss: 1.101933, acc: 0.265625]\n",
      "9308: [discriminator loss: 0.592020, acc: 0.679688] [adversarial loss: 1.070573, acc: 0.265625]\n",
      "9309: [discriminator loss: 0.511995, acc: 0.765625] [adversarial loss: 1.187833, acc: 0.234375]\n",
      "9310: [discriminator loss: 0.507012, acc: 0.718750] [adversarial loss: 1.122806, acc: 0.265625]\n",
      "9311: [discriminator loss: 0.484476, acc: 0.781250] [adversarial loss: 1.134634, acc: 0.281250]\n",
      "9312: [discriminator loss: 0.583814, acc: 0.687500] [adversarial loss: 1.111451, acc: 0.343750]\n",
      "9313: [discriminator loss: 0.529517, acc: 0.726562] [adversarial loss: 1.095146, acc: 0.234375]\n",
      "9314: [discriminator loss: 0.524901, acc: 0.765625] [adversarial loss: 1.147583, acc: 0.234375]\n",
      "9315: [discriminator loss: 0.530228, acc: 0.726562] [adversarial loss: 1.151412, acc: 0.234375]\n",
      "9316: [discriminator loss: 0.590397, acc: 0.656250] [adversarial loss: 1.145254, acc: 0.187500]\n",
      "9317: [discriminator loss: 0.482780, acc: 0.796875] [adversarial loss: 1.278414, acc: 0.187500]\n",
      "9318: [discriminator loss: 0.524679, acc: 0.718750] [adversarial loss: 1.036032, acc: 0.343750]\n",
      "9319: [discriminator loss: 0.530354, acc: 0.703125] [adversarial loss: 1.486979, acc: 0.093750]\n",
      "9320: [discriminator loss: 0.566571, acc: 0.695312] [adversarial loss: 0.927254, acc: 0.437500]\n",
      "9321: [discriminator loss: 0.577721, acc: 0.710938] [adversarial loss: 1.892588, acc: 0.093750]\n",
      "9322: [discriminator loss: 0.656413, acc: 0.609375] [adversarial loss: 0.780805, acc: 0.546875]\n",
      "9323: [discriminator loss: 0.560592, acc: 0.695312] [adversarial loss: 1.286455, acc: 0.218750]\n",
      "9324: [discriminator loss: 0.511629, acc: 0.757812] [adversarial loss: 1.047611, acc: 0.375000]\n",
      "9325: [discriminator loss: 0.550104, acc: 0.695312] [adversarial loss: 1.156418, acc: 0.265625]\n",
      "9326: [discriminator loss: 0.544260, acc: 0.687500] [adversarial loss: 1.074364, acc: 0.296875]\n",
      "9327: [discriminator loss: 0.492935, acc: 0.789062] [adversarial loss: 1.154333, acc: 0.203125]\n",
      "9328: [discriminator loss: 0.613131, acc: 0.625000] [adversarial loss: 1.080155, acc: 0.234375]\n",
      "9329: [discriminator loss: 0.552323, acc: 0.710938] [adversarial loss: 1.256044, acc: 0.171875]\n",
      "9330: [discriminator loss: 0.519727, acc: 0.710938] [adversarial loss: 1.170695, acc: 0.187500]\n",
      "9331: [discriminator loss: 0.531629, acc: 0.734375] [adversarial loss: 1.107834, acc: 0.218750]\n",
      "9332: [discriminator loss: 0.509378, acc: 0.789062] [adversarial loss: 1.471105, acc: 0.203125]\n",
      "9333: [discriminator loss: 0.555652, acc: 0.632812] [adversarial loss: 0.842382, acc: 0.375000]\n",
      "9334: [discriminator loss: 0.615927, acc: 0.687500] [adversarial loss: 1.521809, acc: 0.062500]\n",
      "9335: [discriminator loss: 0.577288, acc: 0.687500] [adversarial loss: 0.886918, acc: 0.421875]\n",
      "9336: [discriminator loss: 0.538974, acc: 0.718750] [adversarial loss: 1.571502, acc: 0.140625]\n",
      "9337: [discriminator loss: 0.541423, acc: 0.687500] [adversarial loss: 0.892202, acc: 0.343750]\n",
      "9338: [discriminator loss: 0.577065, acc: 0.695312] [adversarial loss: 1.277213, acc: 0.234375]\n",
      "9339: [discriminator loss: 0.551048, acc: 0.687500] [adversarial loss: 1.124129, acc: 0.265625]\n",
      "9340: [discriminator loss: 0.498633, acc: 0.789062] [adversarial loss: 1.087658, acc: 0.343750]\n",
      "9341: [discriminator loss: 0.575964, acc: 0.695312] [adversarial loss: 1.091080, acc: 0.250000]\n",
      "9342: [discriminator loss: 0.546386, acc: 0.695312] [adversarial loss: 1.416670, acc: 0.140625]\n",
      "9343: [discriminator loss: 0.512558, acc: 0.710938] [adversarial loss: 1.067974, acc: 0.250000]\n",
      "9344: [discriminator loss: 0.495973, acc: 0.726562] [adversarial loss: 1.064866, acc: 0.218750]\n",
      "9345: [discriminator loss: 0.543290, acc: 0.726562] [adversarial loss: 1.309592, acc: 0.140625]\n",
      "9346: [discriminator loss: 0.586168, acc: 0.640625] [adversarial loss: 1.045945, acc: 0.296875]\n",
      "9347: [discriminator loss: 0.547178, acc: 0.734375] [adversarial loss: 1.406984, acc: 0.156250]\n",
      "9348: [discriminator loss: 0.538096, acc: 0.703125] [adversarial loss: 0.923730, acc: 0.406250]\n",
      "9349: [discriminator loss: 0.534950, acc: 0.710938] [adversarial loss: 1.519388, acc: 0.156250]\n",
      "9350: [discriminator loss: 0.593748, acc: 0.703125] [adversarial loss: 0.968055, acc: 0.406250]\n",
      "9351: [discriminator loss: 0.529361, acc: 0.773438] [adversarial loss: 1.333130, acc: 0.171875]\n",
      "9352: [discriminator loss: 0.611026, acc: 0.695312] [adversarial loss: 0.836309, acc: 0.468750]\n",
      "9353: [discriminator loss: 0.572778, acc: 0.687500] [adversarial loss: 1.404544, acc: 0.187500]\n",
      "9354: [discriminator loss: 0.552307, acc: 0.679688] [adversarial loss: 0.804597, acc: 0.375000]\n",
      "9355: [discriminator loss: 0.531220, acc: 0.718750] [adversarial loss: 1.281636, acc: 0.218750]\n",
      "9356: [discriminator loss: 0.611926, acc: 0.648438] [adversarial loss: 1.287335, acc: 0.281250]\n",
      "9357: [discriminator loss: 0.525540, acc: 0.750000] [adversarial loss: 1.353463, acc: 0.234375]\n",
      "9358: [discriminator loss: 0.540412, acc: 0.695312] [adversarial loss: 1.178255, acc: 0.265625]\n",
      "9359: [discriminator loss: 0.571260, acc: 0.671875] [adversarial loss: 1.065483, acc: 0.203125]\n",
      "9360: [discriminator loss: 0.471516, acc: 0.781250] [adversarial loss: 1.021355, acc: 0.281250]\n",
      "9361: [discriminator loss: 0.523353, acc: 0.718750] [adversarial loss: 1.353419, acc: 0.171875]\n",
      "9362: [discriminator loss: 0.505052, acc: 0.718750] [adversarial loss: 0.846251, acc: 0.406250]\n",
      "9363: [discriminator loss: 0.552354, acc: 0.695312] [adversarial loss: 1.485217, acc: 0.218750]\n",
      "9364: [discriminator loss: 0.582075, acc: 0.617188] [adversarial loss: 0.773140, acc: 0.500000]\n",
      "9365: [discriminator loss: 0.593632, acc: 0.726562] [adversarial loss: 1.377383, acc: 0.171875]\n",
      "9366: [discriminator loss: 0.567211, acc: 0.664062] [adversarial loss: 0.962882, acc: 0.328125]\n",
      "9367: [discriminator loss: 0.565701, acc: 0.750000] [adversarial loss: 1.518413, acc: 0.078125]\n",
      "9368: [discriminator loss: 0.527291, acc: 0.734375] [adversarial loss: 0.818778, acc: 0.453125]\n",
      "9369: [discriminator loss: 0.676242, acc: 0.679688] [adversarial loss: 1.506794, acc: 0.093750]\n",
      "9370: [discriminator loss: 0.571318, acc: 0.726562] [adversarial loss: 0.938383, acc: 0.375000]\n",
      "9371: [discriminator loss: 0.567242, acc: 0.703125] [adversarial loss: 1.373932, acc: 0.125000]\n",
      "9372: [discriminator loss: 0.591782, acc: 0.679688] [adversarial loss: 1.154001, acc: 0.250000]\n",
      "9373: [discriminator loss: 0.522427, acc: 0.750000] [adversarial loss: 1.022521, acc: 0.343750]\n",
      "9374: [discriminator loss: 0.558426, acc: 0.710938] [adversarial loss: 1.232394, acc: 0.203125]\n",
      "9375: [discriminator loss: 0.593718, acc: 0.726562] [adversarial loss: 1.105538, acc: 0.312500]\n",
      "9376: [discriminator loss: 0.564865, acc: 0.703125] [adversarial loss: 1.144459, acc: 0.281250]\n",
      "9377: [discriminator loss: 0.501719, acc: 0.757812] [adversarial loss: 1.128253, acc: 0.328125]\n",
      "9378: [discriminator loss: 0.461262, acc: 0.781250] [adversarial loss: 1.208125, acc: 0.265625]\n",
      "9379: [discriminator loss: 0.570925, acc: 0.687500] [adversarial loss: 0.932739, acc: 0.390625]\n",
      "9380: [discriminator loss: 0.523037, acc: 0.750000] [adversarial loss: 1.589099, acc: 0.078125]\n",
      "9381: [discriminator loss: 0.596686, acc: 0.656250] [adversarial loss: 0.748893, acc: 0.578125]\n",
      "9382: [discriminator loss: 0.556924, acc: 0.687500] [adversarial loss: 1.518610, acc: 0.109375]\n",
      "9383: [discriminator loss: 0.571472, acc: 0.726562] [adversarial loss: 1.145475, acc: 0.265625]\n",
      "9384: [discriminator loss: 0.578185, acc: 0.648438] [adversarial loss: 1.355727, acc: 0.203125]\n",
      "9385: [discriminator loss: 0.560675, acc: 0.710938] [adversarial loss: 0.983188, acc: 0.437500]\n",
      "9386: [discriminator loss: 0.522403, acc: 0.757812] [adversarial loss: 1.169760, acc: 0.203125]\n",
      "9387: [discriminator loss: 0.514140, acc: 0.718750] [adversarial loss: 1.026811, acc: 0.250000]\n",
      "9388: [discriminator loss: 0.503390, acc: 0.726562] [adversarial loss: 1.024823, acc: 0.296875]\n",
      "9389: [discriminator loss: 0.633588, acc: 0.687500] [adversarial loss: 1.271380, acc: 0.171875]\n",
      "9390: [discriminator loss: 0.525133, acc: 0.710938] [adversarial loss: 0.914587, acc: 0.390625]\n",
      "9391: [discriminator loss: 0.530909, acc: 0.718750] [adversarial loss: 1.379068, acc: 0.125000]\n",
      "9392: [discriminator loss: 0.601415, acc: 0.679688] [adversarial loss: 1.180516, acc: 0.234375]\n",
      "9393: [discriminator loss: 0.461240, acc: 0.781250] [adversarial loss: 1.175012, acc: 0.250000]\n",
      "9394: [discriminator loss: 0.608617, acc: 0.632812] [adversarial loss: 1.151887, acc: 0.218750]\n",
      "9395: [discriminator loss: 0.633326, acc: 0.625000] [adversarial loss: 1.224970, acc: 0.250000]\n",
      "9396: [discriminator loss: 0.547450, acc: 0.703125] [adversarial loss: 0.960325, acc: 0.343750]\n",
      "9397: [discriminator loss: 0.578570, acc: 0.695312] [adversarial loss: 1.103110, acc: 0.328125]\n",
      "9398: [discriminator loss: 0.537595, acc: 0.734375] [adversarial loss: 1.406269, acc: 0.093750]\n",
      "9399: [discriminator loss: 0.556942, acc: 0.703125] [adversarial loss: 1.124405, acc: 0.312500]\n",
      "9400: [discriminator loss: 0.648987, acc: 0.593750] [adversarial loss: 1.264732, acc: 0.171875]\n",
      "9401: [discriminator loss: 0.569353, acc: 0.710938] [adversarial loss: 0.957915, acc: 0.312500]\n",
      "9402: [discriminator loss: 0.594681, acc: 0.695312] [adversarial loss: 1.416862, acc: 0.109375]\n",
      "9403: [discriminator loss: 0.548449, acc: 0.734375] [adversarial loss: 0.873174, acc: 0.375000]\n",
      "9404: [discriminator loss: 0.540458, acc: 0.671875] [adversarial loss: 1.299457, acc: 0.187500]\n",
      "9405: [discriminator loss: 0.600602, acc: 0.671875] [adversarial loss: 0.800671, acc: 0.484375]\n",
      "9406: [discriminator loss: 0.552807, acc: 0.726562] [adversarial loss: 1.335331, acc: 0.156250]\n",
      "9407: [discriminator loss: 0.518389, acc: 0.703125] [adversarial loss: 1.289786, acc: 0.171875]\n",
      "9408: [discriminator loss: 0.477876, acc: 0.828125] [adversarial loss: 1.080745, acc: 0.359375]\n",
      "9409: [discriminator loss: 0.533451, acc: 0.703125] [adversarial loss: 1.293476, acc: 0.156250]\n",
      "9410: [discriminator loss: 0.510041, acc: 0.742188] [adversarial loss: 1.178169, acc: 0.250000]\n",
      "9411: [discriminator loss: 0.573469, acc: 0.679688] [adversarial loss: 1.283007, acc: 0.203125]\n",
      "9412: [discriminator loss: 0.537738, acc: 0.710938] [adversarial loss: 1.074154, acc: 0.203125]\n",
      "9413: [discriminator loss: 0.546148, acc: 0.734375] [adversarial loss: 1.374615, acc: 0.093750]\n",
      "9414: [discriminator loss: 0.505735, acc: 0.781250] [adversarial loss: 1.056562, acc: 0.265625]\n",
      "9415: [discriminator loss: 0.572125, acc: 0.734375] [adversarial loss: 1.496313, acc: 0.156250]\n",
      "9416: [discriminator loss: 0.546275, acc: 0.703125] [adversarial loss: 1.047075, acc: 0.328125]\n",
      "9417: [discriminator loss: 0.542595, acc: 0.710938] [adversarial loss: 1.421612, acc: 0.156250]\n",
      "9418: [discriminator loss: 0.516017, acc: 0.726562] [adversarial loss: 0.811322, acc: 0.406250]\n",
      "9419: [discriminator loss: 0.600089, acc: 0.671875] [adversarial loss: 1.578476, acc: 0.109375]\n",
      "9420: [discriminator loss: 0.570464, acc: 0.679688] [adversarial loss: 1.001339, acc: 0.343750]\n",
      "9421: [discriminator loss: 0.502923, acc: 0.757812] [adversarial loss: 1.417283, acc: 0.140625]\n",
      "9422: [discriminator loss: 0.504264, acc: 0.742188] [adversarial loss: 1.038549, acc: 0.265625]\n",
      "9423: [discriminator loss: 0.565485, acc: 0.726562] [adversarial loss: 1.356698, acc: 0.140625]\n",
      "9424: [discriminator loss: 0.555305, acc: 0.679688] [adversarial loss: 1.322312, acc: 0.156250]\n",
      "9425: [discriminator loss: 0.598032, acc: 0.664062] [adversarial loss: 1.080418, acc: 0.250000]\n",
      "9426: [discriminator loss: 0.578859, acc: 0.695312] [adversarial loss: 1.266038, acc: 0.125000]\n",
      "9427: [discriminator loss: 0.535554, acc: 0.781250] [adversarial loss: 0.978926, acc: 0.312500]\n",
      "9428: [discriminator loss: 0.576629, acc: 0.679688] [adversarial loss: 1.156814, acc: 0.187500]\n",
      "9429: [discriminator loss: 0.531520, acc: 0.757812] [adversarial loss: 1.144857, acc: 0.265625]\n",
      "9430: [discriminator loss: 0.543568, acc: 0.726562] [adversarial loss: 1.282054, acc: 0.171875]\n",
      "9431: [discriminator loss: 0.562283, acc: 0.710938] [adversarial loss: 0.977965, acc: 0.328125]\n",
      "9432: [discriminator loss: 0.543724, acc: 0.734375] [adversarial loss: 1.411625, acc: 0.125000]\n",
      "9433: [discriminator loss: 0.480965, acc: 0.773438] [adversarial loss: 1.021048, acc: 0.312500]\n",
      "9434: [discriminator loss: 0.573785, acc: 0.742188] [adversarial loss: 1.249265, acc: 0.250000]\n",
      "9435: [discriminator loss: 0.567802, acc: 0.710938] [adversarial loss: 0.894368, acc: 0.390625]\n",
      "9436: [discriminator loss: 0.584607, acc: 0.710938] [adversarial loss: 1.718432, acc: 0.062500]\n",
      "9437: [discriminator loss: 0.594445, acc: 0.679688] [adversarial loss: 0.872200, acc: 0.406250]\n",
      "9438: [discriminator loss: 0.607808, acc: 0.687500] [adversarial loss: 1.723181, acc: 0.000000]\n",
      "9439: [discriminator loss: 0.531239, acc: 0.726562] [adversarial loss: 1.141278, acc: 0.281250]\n",
      "9440: [discriminator loss: 0.505735, acc: 0.718750] [adversarial loss: 1.306389, acc: 0.171875]\n",
      "9441: [discriminator loss: 0.526377, acc: 0.718750] [adversarial loss: 1.037333, acc: 0.281250]\n",
      "9442: [discriminator loss: 0.597345, acc: 0.687500] [adversarial loss: 1.214745, acc: 0.250000]\n",
      "9443: [discriminator loss: 0.546255, acc: 0.703125] [adversarial loss: 1.132182, acc: 0.234375]\n",
      "9444: [discriminator loss: 0.539422, acc: 0.726562] [adversarial loss: 1.147622, acc: 0.171875]\n",
      "9445: [discriminator loss: 0.501455, acc: 0.742188] [adversarial loss: 1.178890, acc: 0.187500]\n",
      "9446: [discriminator loss: 0.558045, acc: 0.703125] [adversarial loss: 1.050779, acc: 0.343750]\n",
      "9447: [discriminator loss: 0.487125, acc: 0.757812] [adversarial loss: 1.429092, acc: 0.156250]\n",
      "9448: [discriminator loss: 0.535320, acc: 0.734375] [adversarial loss: 1.106233, acc: 0.281250]\n",
      "9449: [discriminator loss: 0.504330, acc: 0.804688] [adversarial loss: 1.291733, acc: 0.125000]\n",
      "9450: [discriminator loss: 0.545550, acc: 0.671875] [adversarial loss: 0.920741, acc: 0.390625]\n",
      "9451: [discriminator loss: 0.523281, acc: 0.765625] [adversarial loss: 1.650631, acc: 0.031250]\n",
      "9452: [discriminator loss: 0.563618, acc: 0.671875] [adversarial loss: 0.741258, acc: 0.531250]\n",
      "9453: [discriminator loss: 0.622886, acc: 0.664062] [adversarial loss: 1.483690, acc: 0.125000]\n",
      "9454: [discriminator loss: 0.581110, acc: 0.671875] [adversarial loss: 0.865567, acc: 0.390625]\n",
      "9455: [discriminator loss: 0.700586, acc: 0.570312] [adversarial loss: 1.440254, acc: 0.125000]\n",
      "9456: [discriminator loss: 0.642812, acc: 0.656250] [adversarial loss: 1.173401, acc: 0.234375]\n",
      "9457: [discriminator loss: 0.522006, acc: 0.742188] [adversarial loss: 0.992720, acc: 0.390625]\n",
      "9458: [discriminator loss: 0.551605, acc: 0.671875] [adversarial loss: 1.331675, acc: 0.203125]\n",
      "9459: [discriminator loss: 0.511773, acc: 0.734375] [adversarial loss: 1.104618, acc: 0.281250]\n",
      "9460: [discriminator loss: 0.559076, acc: 0.726562] [adversarial loss: 1.099607, acc: 0.328125]\n",
      "9461: [discriminator loss: 0.560253, acc: 0.710938] [adversarial loss: 1.046724, acc: 0.328125]\n",
      "9462: [discriminator loss: 0.494557, acc: 0.726562] [adversarial loss: 1.063064, acc: 0.250000]\n",
      "9463: [discriminator loss: 0.537430, acc: 0.718750] [adversarial loss: 1.253630, acc: 0.218750]\n",
      "9464: [discriminator loss: 0.553775, acc: 0.703125] [adversarial loss: 1.055786, acc: 0.328125]\n",
      "9465: [discriminator loss: 0.612494, acc: 0.640625] [adversarial loss: 1.213768, acc: 0.171875]\n",
      "9466: [discriminator loss: 0.502417, acc: 0.773438] [adversarial loss: 1.175151, acc: 0.265625]\n",
      "9467: [discriminator loss: 0.586707, acc: 0.703125] [adversarial loss: 0.999300, acc: 0.343750]\n",
      "9468: [discriminator loss: 0.591076, acc: 0.671875] [adversarial loss: 1.635233, acc: 0.062500]\n",
      "9469: [discriminator loss: 0.592323, acc: 0.679688] [adversarial loss: 0.694865, acc: 0.562500]\n",
      "9470: [discriminator loss: 0.570132, acc: 0.656250] [adversarial loss: 1.531337, acc: 0.125000]\n",
      "9471: [discriminator loss: 0.583680, acc: 0.734375] [adversarial loss: 1.060283, acc: 0.250000]\n",
      "9472: [discriminator loss: 0.542682, acc: 0.695312] [adversarial loss: 1.318624, acc: 0.156250]\n",
      "9473: [discriminator loss: 0.504356, acc: 0.796875] [adversarial loss: 0.957276, acc: 0.343750]\n",
      "9474: [discriminator loss: 0.551643, acc: 0.687500] [adversarial loss: 1.210854, acc: 0.187500]\n",
      "9475: [discriminator loss: 0.547124, acc: 0.718750] [adversarial loss: 0.940057, acc: 0.406250]\n",
      "9476: [discriminator loss: 0.569701, acc: 0.703125] [adversarial loss: 1.176621, acc: 0.281250]\n",
      "9477: [discriminator loss: 0.525269, acc: 0.726562] [adversarial loss: 1.341339, acc: 0.171875]\n",
      "9478: [discriminator loss: 0.480697, acc: 0.789062] [adversarial loss: 1.021569, acc: 0.328125]\n",
      "9479: [discriminator loss: 0.505148, acc: 0.796875] [adversarial loss: 1.487271, acc: 0.093750]\n",
      "9480: [discriminator loss: 0.651927, acc: 0.648438] [adversarial loss: 0.756648, acc: 0.437500]\n",
      "9481: [discriminator loss: 0.541254, acc: 0.703125] [adversarial loss: 1.408067, acc: 0.140625]\n",
      "9482: [discriminator loss: 0.555257, acc: 0.695312] [adversarial loss: 0.948554, acc: 0.343750]\n",
      "9483: [discriminator loss: 0.534878, acc: 0.742188] [adversarial loss: 1.195203, acc: 0.171875]\n",
      "9484: [discriminator loss: 0.516747, acc: 0.718750] [adversarial loss: 0.897487, acc: 0.359375]\n",
      "9485: [discriminator loss: 0.529823, acc: 0.718750] [adversarial loss: 1.208846, acc: 0.171875]\n",
      "9486: [discriminator loss: 0.615498, acc: 0.640625] [adversarial loss: 1.040431, acc: 0.265625]\n",
      "9487: [discriminator loss: 0.512834, acc: 0.734375] [adversarial loss: 1.395732, acc: 0.156250]\n",
      "9488: [discriminator loss: 0.702765, acc: 0.585938] [adversarial loss: 0.758677, acc: 0.500000]\n",
      "9489: [discriminator loss: 0.615268, acc: 0.671875] [adversarial loss: 1.539443, acc: 0.140625]\n",
      "9490: [discriminator loss: 0.530020, acc: 0.695312] [adversarial loss: 1.092712, acc: 0.281250]\n",
      "9491: [discriminator loss: 0.490387, acc: 0.757812] [adversarial loss: 1.304396, acc: 0.125000]\n",
      "9492: [discriminator loss: 0.455147, acc: 0.835938] [adversarial loss: 1.278223, acc: 0.156250]\n",
      "9493: [discriminator loss: 0.551807, acc: 0.687500] [adversarial loss: 0.836792, acc: 0.453125]\n",
      "9494: [discriminator loss: 0.520166, acc: 0.742188] [adversarial loss: 1.375399, acc: 0.156250]\n",
      "9495: [discriminator loss: 0.556620, acc: 0.734375] [adversarial loss: 1.147673, acc: 0.171875]\n",
      "9496: [discriminator loss: 0.549058, acc: 0.742188] [adversarial loss: 1.080781, acc: 0.281250]\n",
      "9497: [discriminator loss: 0.498278, acc: 0.773438] [adversarial loss: 1.477146, acc: 0.093750]\n",
      "9498: [discriminator loss: 0.500090, acc: 0.750000] [adversarial loss: 0.922972, acc: 0.453125]\n",
      "9499: [discriminator loss: 0.531810, acc: 0.695312] [adversarial loss: 1.419397, acc: 0.109375]\n",
      "9500: [discriminator loss: 0.552153, acc: 0.679688] [adversarial loss: 0.888345, acc: 0.406250]\n",
      "9501: [discriminator loss: 0.508335, acc: 0.703125] [adversarial loss: 1.716379, acc: 0.109375]\n",
      "9502: [discriminator loss: 0.489266, acc: 0.734375] [adversarial loss: 0.986409, acc: 0.328125]\n",
      "9503: [discriminator loss: 0.524036, acc: 0.710938] [adversarial loss: 1.152863, acc: 0.234375]\n",
      "9504: [discriminator loss: 0.521319, acc: 0.664062] [adversarial loss: 1.105481, acc: 0.203125]\n",
      "9505: [discriminator loss: 0.504468, acc: 0.703125] [adversarial loss: 1.258867, acc: 0.203125]\n",
      "9506: [discriminator loss: 0.613877, acc: 0.640625] [adversarial loss: 0.967238, acc: 0.359375]\n",
      "9507: [discriminator loss: 0.609905, acc: 0.664062] [adversarial loss: 1.399047, acc: 0.171875]\n",
      "9508: [discriminator loss: 0.566737, acc: 0.679688] [adversarial loss: 1.082591, acc: 0.281250]\n",
      "9509: [discriminator loss: 0.446149, acc: 0.820312] [adversarial loss: 1.607643, acc: 0.046875]\n",
      "9510: [discriminator loss: 0.552989, acc: 0.671875] [adversarial loss: 0.925295, acc: 0.453125]\n",
      "9511: [discriminator loss: 0.527388, acc: 0.734375] [adversarial loss: 1.485227, acc: 0.093750]\n",
      "9512: [discriminator loss: 0.492869, acc: 0.734375] [adversarial loss: 1.171788, acc: 0.187500]\n",
      "9513: [discriminator loss: 0.496726, acc: 0.789062] [adversarial loss: 1.262906, acc: 0.171875]\n",
      "9514: [discriminator loss: 0.654948, acc: 0.679688] [adversarial loss: 1.240195, acc: 0.218750]\n",
      "9515: [discriminator loss: 0.546676, acc: 0.757812] [adversarial loss: 0.932761, acc: 0.437500]\n",
      "9516: [discriminator loss: 0.540881, acc: 0.718750] [adversarial loss: 1.307612, acc: 0.187500]\n",
      "9517: [discriminator loss: 0.483533, acc: 0.718750] [adversarial loss: 1.112558, acc: 0.281250]\n",
      "9518: [discriminator loss: 0.548982, acc: 0.703125] [adversarial loss: 1.058205, acc: 0.265625]\n",
      "9519: [discriminator loss: 0.543848, acc: 0.687500] [adversarial loss: 1.278085, acc: 0.187500]\n",
      "9520: [discriminator loss: 0.566911, acc: 0.726562] [adversarial loss: 1.141029, acc: 0.234375]\n",
      "9521: [discriminator loss: 0.607562, acc: 0.671875] [adversarial loss: 1.042834, acc: 0.328125]\n",
      "9522: [discriminator loss: 0.589096, acc: 0.687500] [adversarial loss: 1.084944, acc: 0.203125]\n",
      "9523: [discriminator loss: 0.503864, acc: 0.726562] [adversarial loss: 1.163316, acc: 0.171875]\n",
      "9524: [discriminator loss: 0.535648, acc: 0.742188] [adversarial loss: 1.194393, acc: 0.218750]\n",
      "9525: [discriminator loss: 0.531995, acc: 0.710938] [adversarial loss: 0.825382, acc: 0.484375]\n",
      "9526: [discriminator loss: 0.534573, acc: 0.718750] [adversarial loss: 1.337941, acc: 0.125000]\n",
      "9527: [discriminator loss: 0.522274, acc: 0.734375] [adversarial loss: 0.911349, acc: 0.359375]\n",
      "9528: [discriminator loss: 0.592603, acc: 0.695312] [adversarial loss: 1.009666, acc: 0.281250]\n",
      "9529: [discriminator loss: 0.639187, acc: 0.656250] [adversarial loss: 1.285261, acc: 0.171875]\n",
      "9530: [discriminator loss: 0.570924, acc: 0.695312] [adversarial loss: 1.119971, acc: 0.203125]\n",
      "9531: [discriminator loss: 0.562080, acc: 0.679688] [adversarial loss: 1.179385, acc: 0.281250]\n",
      "9532: [discriminator loss: 0.562870, acc: 0.695312] [adversarial loss: 1.106352, acc: 0.234375]\n",
      "9533: [discriminator loss: 0.562358, acc: 0.687500] [adversarial loss: 0.946736, acc: 0.343750]\n",
      "9534: [discriminator loss: 0.548027, acc: 0.718750] [adversarial loss: 1.073171, acc: 0.218750]\n",
      "9535: [discriminator loss: 0.664195, acc: 0.632812] [adversarial loss: 1.202837, acc: 0.171875]\n",
      "9536: [discriminator loss: 0.581465, acc: 0.687500] [adversarial loss: 1.132051, acc: 0.296875]\n",
      "9537: [discriminator loss: 0.634031, acc: 0.695312] [adversarial loss: 1.441391, acc: 0.093750]\n",
      "9538: [discriminator loss: 0.516906, acc: 0.742188] [adversarial loss: 1.024136, acc: 0.312500]\n",
      "9539: [discriminator loss: 0.608274, acc: 0.640625] [adversarial loss: 1.271649, acc: 0.203125]\n",
      "9540: [discriminator loss: 0.516748, acc: 0.750000] [adversarial loss: 1.162985, acc: 0.218750]\n",
      "9541: [discriminator loss: 0.549527, acc: 0.718750] [adversarial loss: 1.166867, acc: 0.234375]\n",
      "9542: [discriminator loss: 0.565410, acc: 0.703125] [adversarial loss: 1.606427, acc: 0.093750]\n",
      "9543: [discriminator loss: 0.623611, acc: 0.640625] [adversarial loss: 0.852533, acc: 0.484375]\n",
      "9544: [discriminator loss: 0.601251, acc: 0.703125] [adversarial loss: 1.615623, acc: 0.062500]\n",
      "9545: [discriminator loss: 0.572898, acc: 0.664062] [adversarial loss: 0.780255, acc: 0.468750]\n",
      "9546: [discriminator loss: 0.556026, acc: 0.734375] [adversarial loss: 1.477857, acc: 0.171875]\n",
      "9547: [discriminator loss: 0.523607, acc: 0.726562] [adversarial loss: 1.212062, acc: 0.218750]\n",
      "9548: [discriminator loss: 0.519113, acc: 0.726562] [adversarial loss: 1.209875, acc: 0.187500]\n",
      "9549: [discriminator loss: 0.557334, acc: 0.664062] [adversarial loss: 0.959457, acc: 0.296875]\n",
      "9550: [discriminator loss: 0.584959, acc: 0.718750] [adversarial loss: 1.644814, acc: 0.078125]\n",
      "9551: [discriminator loss: 0.577936, acc: 0.664062] [adversarial loss: 0.708528, acc: 0.546875]\n",
      "9552: [discriminator loss: 0.563810, acc: 0.734375] [adversarial loss: 1.431689, acc: 0.109375]\n",
      "9553: [discriminator loss: 0.539010, acc: 0.734375] [adversarial loss: 0.835594, acc: 0.453125]\n",
      "9554: [discriminator loss: 0.580084, acc: 0.703125] [adversarial loss: 1.481063, acc: 0.203125]\n",
      "9555: [discriminator loss: 0.620341, acc: 0.656250] [adversarial loss: 1.049487, acc: 0.250000]\n",
      "9556: [discriminator loss: 0.588264, acc: 0.664062] [adversarial loss: 1.427832, acc: 0.171875]\n",
      "9557: [discriminator loss: 0.575826, acc: 0.703125] [adversarial loss: 0.924461, acc: 0.375000]\n",
      "9558: [discriminator loss: 0.581771, acc: 0.703125] [adversarial loss: 1.092521, acc: 0.234375]\n",
      "9559: [discriminator loss: 0.592068, acc: 0.695312] [adversarial loss: 1.211940, acc: 0.312500]\n",
      "9560: [discriminator loss: 0.585054, acc: 0.632812] [adversarial loss: 1.155693, acc: 0.312500]\n",
      "9561: [discriminator loss: 0.560892, acc: 0.734375] [adversarial loss: 1.186102, acc: 0.234375]\n",
      "9562: [discriminator loss: 0.571546, acc: 0.664062] [adversarial loss: 1.108756, acc: 0.296875]\n",
      "9563: [discriminator loss: 0.483612, acc: 0.804688] [adversarial loss: 1.008139, acc: 0.343750]\n",
      "9564: [discriminator loss: 0.484526, acc: 0.757812] [adversarial loss: 1.108239, acc: 0.281250]\n",
      "9565: [discriminator loss: 0.540331, acc: 0.710938] [adversarial loss: 1.040741, acc: 0.281250]\n",
      "9566: [discriminator loss: 0.545080, acc: 0.734375] [adversarial loss: 0.989521, acc: 0.375000]\n",
      "9567: [discriminator loss: 0.545782, acc: 0.718750] [adversarial loss: 1.370354, acc: 0.156250]\n",
      "9568: [discriminator loss: 0.542086, acc: 0.687500] [adversarial loss: 0.885048, acc: 0.406250]\n",
      "9569: [discriminator loss: 0.642826, acc: 0.609375] [adversarial loss: 1.297282, acc: 0.234375]\n",
      "9570: [discriminator loss: 0.597095, acc: 0.671875] [adversarial loss: 0.952222, acc: 0.296875]\n",
      "9571: [discriminator loss: 0.506107, acc: 0.781250] [adversarial loss: 1.097712, acc: 0.218750]\n",
      "9572: [discriminator loss: 0.579869, acc: 0.671875] [adversarial loss: 1.179330, acc: 0.218750]\n",
      "9573: [discriminator loss: 0.546112, acc: 0.718750] [adversarial loss: 1.036437, acc: 0.296875]\n",
      "9574: [discriminator loss: 0.505033, acc: 0.750000] [adversarial loss: 1.039874, acc: 0.406250]\n",
      "9575: [discriminator loss: 0.530722, acc: 0.750000] [adversarial loss: 1.009390, acc: 0.343750]\n",
      "9576: [discriminator loss: 0.565113, acc: 0.640625] [adversarial loss: 1.213283, acc: 0.218750]\n",
      "9577: [discriminator loss: 0.526740, acc: 0.773438] [adversarial loss: 1.154653, acc: 0.203125]\n",
      "9578: [discriminator loss: 0.516728, acc: 0.718750] [adversarial loss: 0.994323, acc: 0.328125]\n",
      "9579: [discriminator loss: 0.587867, acc: 0.718750] [adversarial loss: 1.354306, acc: 0.125000]\n",
      "9580: [discriminator loss: 0.499706, acc: 0.718750] [adversarial loss: 1.041146, acc: 0.343750]\n",
      "9581: [discriminator loss: 0.557145, acc: 0.734375] [adversarial loss: 1.423880, acc: 0.125000]\n",
      "9582: [discriminator loss: 0.516238, acc: 0.734375] [adversarial loss: 0.789447, acc: 0.515625]\n",
      "9583: [discriminator loss: 0.546511, acc: 0.750000] [adversarial loss: 1.482321, acc: 0.093750]\n",
      "9584: [discriminator loss: 0.541673, acc: 0.710938] [adversarial loss: 0.876667, acc: 0.406250]\n",
      "9585: [discriminator loss: 0.580663, acc: 0.726562] [adversarial loss: 1.321567, acc: 0.156250]\n",
      "9586: [discriminator loss: 0.565589, acc: 0.671875] [adversarial loss: 1.108830, acc: 0.265625]\n",
      "9587: [discriminator loss: 0.505428, acc: 0.757812] [adversarial loss: 1.452100, acc: 0.109375]\n",
      "9588: [discriminator loss: 0.469374, acc: 0.781250] [adversarial loss: 1.375382, acc: 0.125000]\n",
      "9589: [discriminator loss: 0.597992, acc: 0.648438] [adversarial loss: 1.279399, acc: 0.187500]\n",
      "9590: [discriminator loss: 0.604226, acc: 0.695312] [adversarial loss: 1.421264, acc: 0.171875]\n",
      "9591: [discriminator loss: 0.609336, acc: 0.640625] [adversarial loss: 0.896859, acc: 0.421875]\n",
      "9592: [discriminator loss: 0.530869, acc: 0.710938] [adversarial loss: 1.287715, acc: 0.234375]\n",
      "9593: [discriminator loss: 0.566739, acc: 0.687500] [adversarial loss: 1.068938, acc: 0.343750]\n",
      "9594: [discriminator loss: 0.552409, acc: 0.695312] [adversarial loss: 1.248443, acc: 0.187500]\n",
      "9595: [discriminator loss: 0.537707, acc: 0.710938] [adversarial loss: 1.020365, acc: 0.359375]\n",
      "9596: [discriminator loss: 0.644599, acc: 0.601562] [adversarial loss: 1.136996, acc: 0.187500]\n",
      "9597: [discriminator loss: 0.538278, acc: 0.742188] [adversarial loss: 1.025557, acc: 0.359375]\n",
      "9598: [discriminator loss: 0.541288, acc: 0.695312] [adversarial loss: 1.275309, acc: 0.218750]\n",
      "9599: [discriminator loss: 0.574522, acc: 0.671875] [adversarial loss: 1.010343, acc: 0.296875]\n",
      "9600: [discriminator loss: 0.500620, acc: 0.757812] [adversarial loss: 1.157278, acc: 0.234375]\n",
      "9601: [discriminator loss: 0.551631, acc: 0.734375] [adversarial loss: 1.212103, acc: 0.203125]\n",
      "9602: [discriminator loss: 0.555997, acc: 0.726562] [adversarial loss: 1.330832, acc: 0.109375]\n",
      "9603: [discriminator loss: 0.558169, acc: 0.726562] [adversarial loss: 1.092083, acc: 0.312500]\n",
      "9604: [discriminator loss: 0.536671, acc: 0.773438] [adversarial loss: 1.318261, acc: 0.140625]\n",
      "9605: [discriminator loss: 0.587551, acc: 0.671875] [adversarial loss: 1.102731, acc: 0.281250]\n",
      "9606: [discriminator loss: 0.495173, acc: 0.718750] [adversarial loss: 1.192184, acc: 0.312500]\n",
      "9607: [discriminator loss: 0.493205, acc: 0.750000] [adversarial loss: 0.943991, acc: 0.375000]\n",
      "9608: [discriminator loss: 0.660969, acc: 0.617188] [adversarial loss: 1.177977, acc: 0.250000]\n",
      "9609: [discriminator loss: 0.508830, acc: 0.773438] [adversarial loss: 1.101011, acc: 0.265625]\n",
      "9610: [discriminator loss: 0.533017, acc: 0.789062] [adversarial loss: 1.141472, acc: 0.250000]\n",
      "9611: [discriminator loss: 0.453346, acc: 0.757812] [adversarial loss: 0.982626, acc: 0.406250]\n",
      "9612: [discriminator loss: 0.583974, acc: 0.718750] [adversarial loss: 0.983707, acc: 0.312500]\n",
      "9613: [discriminator loss: 0.587721, acc: 0.687500] [adversarial loss: 1.578296, acc: 0.093750]\n",
      "9614: [discriminator loss: 0.510019, acc: 0.804688] [adversarial loss: 0.852869, acc: 0.406250]\n",
      "9615: [discriminator loss: 0.588688, acc: 0.671875] [adversarial loss: 1.343220, acc: 0.171875]\n",
      "9616: [discriminator loss: 0.590656, acc: 0.695312] [adversarial loss: 0.964682, acc: 0.343750]\n",
      "9617: [discriminator loss: 0.582361, acc: 0.679688] [adversarial loss: 1.461363, acc: 0.109375]\n",
      "9618: [discriminator loss: 0.542977, acc: 0.726562] [adversarial loss: 1.123574, acc: 0.234375]\n",
      "9619: [discriminator loss: 0.592456, acc: 0.687500] [adversarial loss: 1.229813, acc: 0.125000]\n",
      "9620: [discriminator loss: 0.568638, acc: 0.687500] [adversarial loss: 1.235365, acc: 0.171875]\n",
      "9621: [discriminator loss: 0.553498, acc: 0.710938] [adversarial loss: 1.135431, acc: 0.203125]\n",
      "9622: [discriminator loss: 0.504494, acc: 0.757812] [adversarial loss: 1.130228, acc: 0.250000]\n",
      "9623: [discriminator loss: 0.500566, acc: 0.750000] [adversarial loss: 1.174085, acc: 0.265625]\n",
      "9624: [discriminator loss: 0.526661, acc: 0.710938] [adversarial loss: 1.351238, acc: 0.156250]\n",
      "9625: [discriminator loss: 0.620359, acc: 0.671875] [adversarial loss: 1.030335, acc: 0.296875]\n",
      "9626: [discriminator loss: 0.580817, acc: 0.664062] [adversarial loss: 1.747193, acc: 0.156250]\n",
      "9627: [discriminator loss: 0.579891, acc: 0.703125] [adversarial loss: 0.988153, acc: 0.390625]\n",
      "9628: [discriminator loss: 0.603951, acc: 0.687500] [adversarial loss: 1.598936, acc: 0.078125]\n",
      "9629: [discriminator loss: 0.635639, acc: 0.656250] [adversarial loss: 0.773198, acc: 0.515625]\n",
      "9630: [discriminator loss: 0.564809, acc: 0.718750] [adversarial loss: 1.303887, acc: 0.156250]\n",
      "9631: [discriminator loss: 0.527408, acc: 0.734375] [adversarial loss: 1.257274, acc: 0.187500]\n",
      "9632: [discriminator loss: 0.586880, acc: 0.656250] [adversarial loss: 1.045863, acc: 0.250000]\n",
      "9633: [discriminator loss: 0.563413, acc: 0.687500] [adversarial loss: 1.322517, acc: 0.187500]\n",
      "9634: [discriminator loss: 0.459041, acc: 0.773438] [adversarial loss: 1.232309, acc: 0.218750]\n",
      "9635: [discriminator loss: 0.591289, acc: 0.750000] [adversarial loss: 1.391146, acc: 0.187500]\n",
      "9636: [discriminator loss: 0.598830, acc: 0.695312] [adversarial loss: 1.099172, acc: 0.218750]\n",
      "9637: [discriminator loss: 0.492371, acc: 0.781250] [adversarial loss: 1.305022, acc: 0.140625]\n",
      "9638: [discriminator loss: 0.612679, acc: 0.617188] [adversarial loss: 1.048902, acc: 0.328125]\n",
      "9639: [discriminator loss: 0.601718, acc: 0.632812] [adversarial loss: 1.050448, acc: 0.281250]\n",
      "9640: [discriminator loss: 0.508695, acc: 0.750000] [adversarial loss: 1.350264, acc: 0.171875]\n",
      "9641: [discriminator loss: 0.583120, acc: 0.703125] [adversarial loss: 1.198230, acc: 0.187500]\n",
      "9642: [discriminator loss: 0.581776, acc: 0.656250] [adversarial loss: 1.045444, acc: 0.203125]\n",
      "9643: [discriminator loss: 0.594391, acc: 0.671875] [adversarial loss: 1.210971, acc: 0.250000]\n",
      "9644: [discriminator loss: 0.553262, acc: 0.703125] [adversarial loss: 0.902509, acc: 0.406250]\n",
      "9645: [discriminator loss: 0.565111, acc: 0.695312] [adversarial loss: 1.080359, acc: 0.218750]\n",
      "9646: [discriminator loss: 0.588999, acc: 0.648438] [adversarial loss: 1.212476, acc: 0.234375]\n",
      "9647: [discriminator loss: 0.606348, acc: 0.679688] [adversarial loss: 1.046886, acc: 0.218750]\n",
      "9648: [discriminator loss: 0.510625, acc: 0.718750] [adversarial loss: 1.054084, acc: 0.281250]\n",
      "9649: [discriminator loss: 0.588540, acc: 0.703125] [adversarial loss: 1.098591, acc: 0.281250]\n",
      "9650: [discriminator loss: 0.524357, acc: 0.742188] [adversarial loss: 1.212751, acc: 0.125000]\n",
      "9651: [discriminator loss: 0.526716, acc: 0.718750] [adversarial loss: 0.967286, acc: 0.328125]\n",
      "9652: [discriminator loss: 0.552823, acc: 0.671875] [adversarial loss: 1.316261, acc: 0.140625]\n",
      "9653: [discriminator loss: 0.624048, acc: 0.703125] [adversarial loss: 0.974820, acc: 0.375000]\n",
      "9654: [discriminator loss: 0.522308, acc: 0.750000] [adversarial loss: 1.237892, acc: 0.250000]\n",
      "9655: [discriminator loss: 0.583774, acc: 0.679688] [adversarial loss: 1.304919, acc: 0.187500]\n",
      "9656: [discriminator loss: 0.531962, acc: 0.695312] [adversarial loss: 1.213285, acc: 0.218750]\n",
      "9657: [discriminator loss: 0.549074, acc: 0.664062] [adversarial loss: 1.159029, acc: 0.281250]\n",
      "9658: [discriminator loss: 0.587457, acc: 0.664062] [adversarial loss: 1.026217, acc: 0.343750]\n",
      "9659: [discriminator loss: 0.537559, acc: 0.773438] [adversarial loss: 1.097443, acc: 0.203125]\n",
      "9660: [discriminator loss: 0.478659, acc: 0.789062] [adversarial loss: 1.120901, acc: 0.250000]\n",
      "9661: [discriminator loss: 0.575477, acc: 0.671875] [adversarial loss: 1.203543, acc: 0.296875]\n",
      "9662: [discriminator loss: 0.549373, acc: 0.664062] [adversarial loss: 1.358148, acc: 0.156250]\n",
      "9663: [discriminator loss: 0.641307, acc: 0.625000] [adversarial loss: 0.912926, acc: 0.390625]\n",
      "9664: [discriminator loss: 0.590511, acc: 0.656250] [adversarial loss: 1.414889, acc: 0.078125]\n",
      "9665: [discriminator loss: 0.534765, acc: 0.757812] [adversarial loss: 0.909612, acc: 0.500000]\n",
      "9666: [discriminator loss: 0.543867, acc: 0.734375] [adversarial loss: 1.472775, acc: 0.156250]\n",
      "9667: [discriminator loss: 0.562033, acc: 0.679688] [adversarial loss: 1.051176, acc: 0.328125]\n",
      "9668: [discriminator loss: 0.664498, acc: 0.648438] [adversarial loss: 1.391122, acc: 0.125000]\n",
      "9669: [discriminator loss: 0.567525, acc: 0.656250] [adversarial loss: 0.749519, acc: 0.609375]\n",
      "9670: [discriminator loss: 0.571955, acc: 0.695312] [adversarial loss: 1.377448, acc: 0.125000]\n",
      "9671: [discriminator loss: 0.613115, acc: 0.664062] [adversarial loss: 0.682938, acc: 0.640625]\n",
      "9672: [discriminator loss: 0.572741, acc: 0.656250] [adversarial loss: 1.442350, acc: 0.093750]\n",
      "9673: [discriminator loss: 0.572791, acc: 0.703125] [adversarial loss: 0.961575, acc: 0.390625]\n",
      "9674: [discriminator loss: 0.520042, acc: 0.726562] [adversarial loss: 1.074067, acc: 0.359375]\n",
      "9675: [discriminator loss: 0.487032, acc: 0.734375] [adversarial loss: 1.554941, acc: 0.140625]\n",
      "9676: [discriminator loss: 0.540835, acc: 0.726562] [adversarial loss: 0.875381, acc: 0.453125]\n",
      "9677: [discriminator loss: 0.642404, acc: 0.648438] [adversarial loss: 1.207070, acc: 0.281250]\n",
      "9678: [discriminator loss: 0.588518, acc: 0.687500] [adversarial loss: 1.113112, acc: 0.328125]\n",
      "9679: [discriminator loss: 0.531756, acc: 0.718750] [adversarial loss: 1.230518, acc: 0.187500]\n",
      "9680: [discriminator loss: 0.542869, acc: 0.742188] [adversarial loss: 1.378142, acc: 0.156250]\n",
      "9681: [discriminator loss: 0.542780, acc: 0.695312] [adversarial loss: 1.233791, acc: 0.156250]\n",
      "9682: [discriminator loss: 0.523391, acc: 0.718750] [adversarial loss: 1.269825, acc: 0.203125]\n",
      "9683: [discriminator loss: 0.541699, acc: 0.750000] [adversarial loss: 0.938195, acc: 0.390625]\n",
      "9684: [discriminator loss: 0.573659, acc: 0.679688] [adversarial loss: 1.295486, acc: 0.281250]\n",
      "9685: [discriminator loss: 0.603995, acc: 0.671875] [adversarial loss: 1.072479, acc: 0.343750]\n",
      "9686: [discriminator loss: 0.509061, acc: 0.726562] [adversarial loss: 1.336435, acc: 0.187500]\n",
      "9687: [discriminator loss: 0.605237, acc: 0.625000] [adversarial loss: 0.958177, acc: 0.296875]\n",
      "9688: [discriminator loss: 0.541908, acc: 0.718750] [adversarial loss: 1.651145, acc: 0.125000]\n",
      "9689: [discriminator loss: 0.504613, acc: 0.742188] [adversarial loss: 1.080996, acc: 0.296875]\n",
      "9690: [discriminator loss: 0.501865, acc: 0.710938] [adversarial loss: 1.245912, acc: 0.187500]\n",
      "9691: [discriminator loss: 0.479454, acc: 0.789062] [adversarial loss: 1.190645, acc: 0.343750]\n",
      "9692: [discriminator loss: 0.576821, acc: 0.734375] [adversarial loss: 1.049615, acc: 0.312500]\n",
      "9693: [discriminator loss: 0.557484, acc: 0.710938] [adversarial loss: 1.251398, acc: 0.203125]\n",
      "9694: [discriminator loss: 0.527426, acc: 0.757812] [adversarial loss: 1.145295, acc: 0.234375]\n",
      "9695: [discriminator loss: 0.524522, acc: 0.734375] [adversarial loss: 1.268907, acc: 0.218750]\n",
      "9696: [discriminator loss: 0.521152, acc: 0.765625] [adversarial loss: 0.904411, acc: 0.406250]\n",
      "9697: [discriminator loss: 0.644003, acc: 0.648438] [adversarial loss: 1.095120, acc: 0.296875]\n",
      "9698: [discriminator loss: 0.554363, acc: 0.703125] [adversarial loss: 1.104170, acc: 0.234375]\n",
      "9699: [discriminator loss: 0.535178, acc: 0.750000] [adversarial loss: 1.142692, acc: 0.281250]\n",
      "9700: [discriminator loss: 0.527888, acc: 0.750000] [adversarial loss: 0.929149, acc: 0.375000]\n",
      "9701: [discriminator loss: 0.476425, acc: 0.796875] [adversarial loss: 1.327914, acc: 0.187500]\n",
      "9702: [discriminator loss: 0.606065, acc: 0.648438] [adversarial loss: 1.049128, acc: 0.375000]\n",
      "9703: [discriminator loss: 0.632925, acc: 0.679688] [adversarial loss: 1.442606, acc: 0.203125]\n",
      "9704: [discriminator loss: 0.590365, acc: 0.703125] [adversarial loss: 0.963246, acc: 0.375000]\n",
      "9705: [discriminator loss: 0.499180, acc: 0.742188] [adversarial loss: 1.427176, acc: 0.156250]\n",
      "9706: [discriminator loss: 0.545212, acc: 0.710938] [adversarial loss: 1.136969, acc: 0.218750]\n",
      "9707: [discriminator loss: 0.589255, acc: 0.656250] [adversarial loss: 1.466163, acc: 0.109375]\n",
      "9708: [discriminator loss: 0.511206, acc: 0.718750] [adversarial loss: 0.925882, acc: 0.312500]\n",
      "9709: [discriminator loss: 0.606173, acc: 0.695312] [adversarial loss: 1.306571, acc: 0.125000]\n",
      "9710: [discriminator loss: 0.544287, acc: 0.703125] [adversarial loss: 1.021024, acc: 0.281250]\n",
      "9711: [discriminator loss: 0.520889, acc: 0.726562] [adversarial loss: 1.272250, acc: 0.187500]\n",
      "9712: [discriminator loss: 0.558755, acc: 0.687500] [adversarial loss: 1.138971, acc: 0.265625]\n",
      "9713: [discriminator loss: 0.568670, acc: 0.695312] [adversarial loss: 1.071555, acc: 0.265625]\n",
      "9714: [discriminator loss: 0.523588, acc: 0.734375] [adversarial loss: 1.121114, acc: 0.234375]\n",
      "9715: [discriminator loss: 0.525476, acc: 0.757812] [adversarial loss: 1.309692, acc: 0.156250]\n",
      "9716: [discriminator loss: 0.508596, acc: 0.750000] [adversarial loss: 1.113807, acc: 0.281250]\n",
      "9717: [discriminator loss: 0.651384, acc: 0.640625] [adversarial loss: 1.220959, acc: 0.234375]\n",
      "9718: [discriminator loss: 0.547060, acc: 0.664062] [adversarial loss: 1.026217, acc: 0.390625]\n",
      "9719: [discriminator loss: 0.509452, acc: 0.734375] [adversarial loss: 1.352881, acc: 0.140625]\n",
      "9720: [discriminator loss: 0.544030, acc: 0.718750] [adversarial loss: 0.991168, acc: 0.328125]\n",
      "9721: [discriminator loss: 0.604644, acc: 0.679688] [adversarial loss: 1.425704, acc: 0.125000]\n",
      "9722: [discriminator loss: 0.551207, acc: 0.695312] [adversarial loss: 1.038686, acc: 0.359375]\n",
      "9723: [discriminator loss: 0.544943, acc: 0.687500] [adversarial loss: 1.195535, acc: 0.156250]\n",
      "9724: [discriminator loss: 0.521564, acc: 0.726562] [adversarial loss: 1.297038, acc: 0.140625]\n",
      "9725: [discriminator loss: 0.547180, acc: 0.773438] [adversarial loss: 1.108071, acc: 0.250000]\n",
      "9726: [discriminator loss: 0.525664, acc: 0.718750] [adversarial loss: 1.085179, acc: 0.203125]\n",
      "9727: [discriminator loss: 0.567025, acc: 0.718750] [adversarial loss: 1.604574, acc: 0.125000]\n",
      "9728: [discriminator loss: 0.614923, acc: 0.671875] [adversarial loss: 0.817754, acc: 0.484375]\n",
      "9729: [discriminator loss: 0.565804, acc: 0.656250] [adversarial loss: 1.703686, acc: 0.046875]\n",
      "9730: [discriminator loss: 0.582030, acc: 0.726562] [adversarial loss: 0.790827, acc: 0.500000]\n",
      "9731: [discriminator loss: 0.535390, acc: 0.710938] [adversarial loss: 1.416669, acc: 0.093750]\n",
      "9732: [discriminator loss: 0.573640, acc: 0.632812] [adversarial loss: 0.836205, acc: 0.406250]\n",
      "9733: [discriminator loss: 0.523574, acc: 0.703125] [adversarial loss: 1.153821, acc: 0.187500]\n",
      "9734: [discriminator loss: 0.565567, acc: 0.703125] [adversarial loss: 1.185393, acc: 0.234375]\n",
      "9735: [discriminator loss: 0.510727, acc: 0.757812] [adversarial loss: 1.067967, acc: 0.265625]\n",
      "9736: [discriminator loss: 0.541705, acc: 0.664062] [adversarial loss: 1.381505, acc: 0.156250]\n",
      "9737: [discriminator loss: 0.622568, acc: 0.695312] [adversarial loss: 1.104448, acc: 0.343750]\n",
      "9738: [discriminator loss: 0.660541, acc: 0.593750] [adversarial loss: 1.435195, acc: 0.156250]\n",
      "9739: [discriminator loss: 0.496068, acc: 0.773438] [adversarial loss: 1.031987, acc: 0.328125]\n",
      "9740: [discriminator loss: 0.557729, acc: 0.695312] [adversarial loss: 1.052051, acc: 0.359375]\n",
      "9741: [discriminator loss: 0.474863, acc: 0.781250] [adversarial loss: 0.993623, acc: 0.421875]\n",
      "9742: [discriminator loss: 0.537652, acc: 0.710938] [adversarial loss: 1.364192, acc: 0.140625]\n",
      "9743: [discriminator loss: 0.590417, acc: 0.640625] [adversarial loss: 1.106219, acc: 0.250000]\n",
      "9744: [discriminator loss: 0.523739, acc: 0.718750] [adversarial loss: 1.414638, acc: 0.156250]\n",
      "9745: [discriminator loss: 0.574868, acc: 0.656250] [adversarial loss: 0.723250, acc: 0.609375]\n",
      "9746: [discriminator loss: 0.553286, acc: 0.710938] [adversarial loss: 1.370007, acc: 0.140625]\n",
      "9747: [discriminator loss: 0.516800, acc: 0.718750] [adversarial loss: 0.952910, acc: 0.359375]\n",
      "9748: [discriminator loss: 0.503457, acc: 0.765625] [adversarial loss: 1.119816, acc: 0.328125]\n",
      "9749: [discriminator loss: 0.585156, acc: 0.671875] [adversarial loss: 1.282408, acc: 0.156250]\n",
      "9750: [discriminator loss: 0.570695, acc: 0.656250] [adversarial loss: 1.098536, acc: 0.234375]\n",
      "9751: [discriminator loss: 0.585210, acc: 0.656250] [adversarial loss: 0.884743, acc: 0.531250]\n",
      "9752: [discriminator loss: 0.512885, acc: 0.773438] [adversarial loss: 1.203545, acc: 0.265625]\n",
      "9753: [discriminator loss: 0.558493, acc: 0.726562] [adversarial loss: 1.240380, acc: 0.109375]\n",
      "9754: [discriminator loss: 0.492547, acc: 0.742188] [adversarial loss: 1.327341, acc: 0.062500]\n",
      "9755: [discriminator loss: 0.547176, acc: 0.718750] [adversarial loss: 1.159311, acc: 0.203125]\n",
      "9756: [discriminator loss: 0.521860, acc: 0.750000] [adversarial loss: 1.134320, acc: 0.250000]\n",
      "9757: [discriminator loss: 0.521785, acc: 0.703125] [adversarial loss: 1.306440, acc: 0.140625]\n",
      "9758: [discriminator loss: 0.561799, acc: 0.750000] [adversarial loss: 1.018494, acc: 0.328125]\n",
      "9759: [discriminator loss: 0.510021, acc: 0.773438] [adversarial loss: 1.100535, acc: 0.281250]\n",
      "9760: [discriminator loss: 0.523659, acc: 0.742188] [adversarial loss: 1.064541, acc: 0.328125]\n",
      "9761: [discriminator loss: 0.592038, acc: 0.648438] [adversarial loss: 1.360128, acc: 0.187500]\n",
      "9762: [discriminator loss: 0.524044, acc: 0.750000] [adversarial loss: 1.032459, acc: 0.281250]\n",
      "9763: [discriminator loss: 0.569446, acc: 0.726562] [adversarial loss: 1.361943, acc: 0.125000]\n",
      "9764: [discriminator loss: 0.574793, acc: 0.718750] [adversarial loss: 0.837155, acc: 0.421875]\n",
      "9765: [discriminator loss: 0.544328, acc: 0.710938] [adversarial loss: 1.264120, acc: 0.125000]\n",
      "9766: [discriminator loss: 0.612271, acc: 0.640625] [adversarial loss: 0.864276, acc: 0.437500]\n",
      "9767: [discriminator loss: 0.567400, acc: 0.664062] [adversarial loss: 1.671138, acc: 0.046875]\n",
      "9768: [discriminator loss: 0.584573, acc: 0.687500] [adversarial loss: 0.915751, acc: 0.421875]\n",
      "9769: [discriminator loss: 0.532279, acc: 0.734375] [adversarial loss: 1.661871, acc: 0.078125]\n",
      "9770: [discriminator loss: 0.571324, acc: 0.695312] [adversarial loss: 1.046541, acc: 0.265625]\n",
      "9771: [discriminator loss: 0.608768, acc: 0.671875] [adversarial loss: 1.230997, acc: 0.234375]\n",
      "9772: [discriminator loss: 0.538076, acc: 0.710938] [adversarial loss: 0.988935, acc: 0.359375]\n",
      "9773: [discriminator loss: 0.507771, acc: 0.765625] [adversarial loss: 1.040926, acc: 0.265625]\n",
      "9774: [discriminator loss: 0.564192, acc: 0.695312] [adversarial loss: 1.133661, acc: 0.359375]\n",
      "9775: [discriminator loss: 0.527448, acc: 0.742188] [adversarial loss: 1.375166, acc: 0.203125]\n",
      "9776: [discriminator loss: 0.530176, acc: 0.726562] [adversarial loss: 0.960034, acc: 0.390625]\n",
      "9777: [discriminator loss: 0.600920, acc: 0.695312] [adversarial loss: 1.509922, acc: 0.125000]\n",
      "9778: [discriminator loss: 0.577251, acc: 0.671875] [adversarial loss: 0.896597, acc: 0.437500]\n",
      "9779: [discriminator loss: 0.604069, acc: 0.656250] [adversarial loss: 1.367903, acc: 0.250000]\n",
      "9780: [discriminator loss: 0.646825, acc: 0.687500] [adversarial loss: 1.037090, acc: 0.250000]\n",
      "9781: [discriminator loss: 0.560517, acc: 0.703125] [adversarial loss: 1.520347, acc: 0.031250]\n",
      "9782: [discriminator loss: 0.582857, acc: 0.656250] [adversarial loss: 0.892609, acc: 0.437500]\n",
      "9783: [discriminator loss: 0.565535, acc: 0.734375] [adversarial loss: 1.394573, acc: 0.156250]\n",
      "9784: [discriminator loss: 0.559885, acc: 0.687500] [adversarial loss: 1.066921, acc: 0.312500]\n",
      "9785: [discriminator loss: 0.522476, acc: 0.703125] [adversarial loss: 1.258409, acc: 0.203125]\n",
      "9786: [discriminator loss: 0.492965, acc: 0.757812] [adversarial loss: 1.153439, acc: 0.265625]\n",
      "9787: [discriminator loss: 0.502332, acc: 0.757812] [adversarial loss: 1.015323, acc: 0.343750]\n",
      "9788: [discriminator loss: 0.556415, acc: 0.687500] [adversarial loss: 1.086255, acc: 0.265625]\n",
      "9789: [discriminator loss: 0.610391, acc: 0.632812] [adversarial loss: 1.041792, acc: 0.312500]\n",
      "9790: [discriminator loss: 0.547408, acc: 0.703125] [adversarial loss: 1.178912, acc: 0.203125]\n",
      "9791: [discriminator loss: 0.509114, acc: 0.734375] [adversarial loss: 1.048551, acc: 0.328125]\n",
      "9792: [discriminator loss: 0.541587, acc: 0.734375] [adversarial loss: 1.087854, acc: 0.312500]\n",
      "9793: [discriminator loss: 0.501104, acc: 0.703125] [adversarial loss: 1.284089, acc: 0.218750]\n",
      "9794: [discriminator loss: 0.523830, acc: 0.742188] [adversarial loss: 1.095798, acc: 0.296875]\n",
      "9795: [discriminator loss: 0.480685, acc: 0.804688] [adversarial loss: 1.420482, acc: 0.109375]\n",
      "9796: [discriminator loss: 0.596500, acc: 0.687500] [adversarial loss: 0.817181, acc: 0.484375]\n",
      "9797: [discriminator loss: 0.586778, acc: 0.679688] [adversarial loss: 1.334022, acc: 0.140625]\n",
      "9798: [discriminator loss: 0.479563, acc: 0.773438] [adversarial loss: 0.972520, acc: 0.359375]\n",
      "9799: [discriminator loss: 0.559113, acc: 0.703125] [adversarial loss: 1.362269, acc: 0.203125]\n",
      "9800: [discriminator loss: 0.576036, acc: 0.664062] [adversarial loss: 0.992166, acc: 0.343750]\n",
      "9801: [discriminator loss: 0.599739, acc: 0.632812] [adversarial loss: 1.305846, acc: 0.125000]\n",
      "9802: [discriminator loss: 0.535741, acc: 0.703125] [adversarial loss: 1.214410, acc: 0.250000]\n",
      "9803: [discriminator loss: 0.546109, acc: 0.726562] [adversarial loss: 1.191973, acc: 0.203125]\n",
      "9804: [discriminator loss: 0.524822, acc: 0.734375] [adversarial loss: 1.366771, acc: 0.171875]\n",
      "9805: [discriminator loss: 0.529333, acc: 0.734375] [adversarial loss: 0.976515, acc: 0.390625]\n",
      "9806: [discriminator loss: 0.538713, acc: 0.742188] [adversarial loss: 1.170680, acc: 0.218750]\n",
      "9807: [discriminator loss: 0.582597, acc: 0.664062] [adversarial loss: 1.005564, acc: 0.328125]\n",
      "9808: [discriminator loss: 0.529368, acc: 0.757812] [adversarial loss: 1.143510, acc: 0.250000]\n",
      "9809: [discriminator loss: 0.528109, acc: 0.750000] [adversarial loss: 1.289400, acc: 0.125000]\n",
      "9810: [discriminator loss: 0.471549, acc: 0.765625] [adversarial loss: 0.940891, acc: 0.312500]\n",
      "9811: [discriminator loss: 0.562793, acc: 0.671875] [adversarial loss: 1.333344, acc: 0.187500]\n",
      "9812: [discriminator loss: 0.544241, acc: 0.703125] [adversarial loss: 0.904567, acc: 0.421875]\n",
      "9813: [discriminator loss: 0.571101, acc: 0.656250] [adversarial loss: 1.382194, acc: 0.187500]\n",
      "9814: [discriminator loss: 0.526308, acc: 0.726562] [adversarial loss: 0.799053, acc: 0.546875]\n",
      "9815: [discriminator loss: 0.584466, acc: 0.687500] [adversarial loss: 1.341790, acc: 0.156250]\n",
      "9816: [discriminator loss: 0.554708, acc: 0.734375] [adversarial loss: 0.859553, acc: 0.468750]\n",
      "9817: [discriminator loss: 0.574214, acc: 0.695312] [adversarial loss: 1.149249, acc: 0.203125]\n",
      "9818: [discriminator loss: 0.549571, acc: 0.679688] [adversarial loss: 1.295183, acc: 0.125000]\n",
      "9819: [discriminator loss: 0.524226, acc: 0.750000] [adversarial loss: 1.055018, acc: 0.250000]\n",
      "9820: [discriminator loss: 0.581152, acc: 0.695312] [adversarial loss: 0.944219, acc: 0.375000]\n",
      "9821: [discriminator loss: 0.562496, acc: 0.718750] [adversarial loss: 1.353521, acc: 0.125000]\n",
      "9822: [discriminator loss: 0.522313, acc: 0.742188] [adversarial loss: 1.191809, acc: 0.281250]\n",
      "9823: [discriminator loss: 0.511905, acc: 0.726562] [adversarial loss: 1.218152, acc: 0.218750]\n",
      "9824: [discriminator loss: 0.522445, acc: 0.789062] [adversarial loss: 1.286951, acc: 0.125000]\n",
      "9825: [discriminator loss: 0.573019, acc: 0.710938] [adversarial loss: 1.076296, acc: 0.296875]\n",
      "9826: [discriminator loss: 0.539347, acc: 0.750000] [adversarial loss: 1.297008, acc: 0.187500]\n",
      "9827: [discriminator loss: 0.548792, acc: 0.687500] [adversarial loss: 0.918832, acc: 0.390625]\n",
      "9828: [discriminator loss: 0.569225, acc: 0.679688] [adversarial loss: 1.446832, acc: 0.156250]\n",
      "9829: [discriminator loss: 0.603528, acc: 0.703125] [adversarial loss: 0.838113, acc: 0.406250]\n",
      "9830: [discriminator loss: 0.554365, acc: 0.656250] [adversarial loss: 1.531355, acc: 0.156250]\n",
      "9831: [discriminator loss: 0.614914, acc: 0.687500] [adversarial loss: 0.859303, acc: 0.453125]\n",
      "9832: [discriminator loss: 0.561405, acc: 0.703125] [adversarial loss: 1.538629, acc: 0.062500]\n",
      "9833: [discriminator loss: 0.599400, acc: 0.726562] [adversarial loss: 0.970022, acc: 0.296875]\n",
      "9834: [discriminator loss: 0.524439, acc: 0.734375] [adversarial loss: 1.364559, acc: 0.156250]\n",
      "9835: [discriminator loss: 0.520829, acc: 0.734375] [adversarial loss: 1.217778, acc: 0.234375]\n",
      "9836: [discriminator loss: 0.585526, acc: 0.679688] [adversarial loss: 1.095325, acc: 0.296875]\n",
      "9837: [discriminator loss: 0.565836, acc: 0.687500] [adversarial loss: 1.504473, acc: 0.109375]\n",
      "9838: [discriminator loss: 0.676211, acc: 0.625000] [adversarial loss: 0.848682, acc: 0.375000]\n",
      "9839: [discriminator loss: 0.558630, acc: 0.710938] [adversarial loss: 1.199872, acc: 0.187500]\n",
      "9840: [discriminator loss: 0.574426, acc: 0.703125] [adversarial loss: 0.981231, acc: 0.281250]\n",
      "9841: [discriminator loss: 0.501580, acc: 0.781250] [adversarial loss: 1.519824, acc: 0.125000]\n",
      "9842: [discriminator loss: 0.546057, acc: 0.726562] [adversarial loss: 0.806031, acc: 0.375000]\n",
      "9843: [discriminator loss: 0.547654, acc: 0.718750] [adversarial loss: 1.297020, acc: 0.218750]\n",
      "9844: [discriminator loss: 0.521040, acc: 0.726562] [adversarial loss: 1.021012, acc: 0.250000]\n",
      "9845: [discriminator loss: 0.550639, acc: 0.718750] [adversarial loss: 1.370526, acc: 0.171875]\n",
      "9846: [discriminator loss: 0.583871, acc: 0.726562] [adversarial loss: 1.026426, acc: 0.312500]\n",
      "9847: [discriminator loss: 0.523395, acc: 0.734375] [adversarial loss: 1.295798, acc: 0.171875]\n",
      "9848: [discriminator loss: 0.541206, acc: 0.734375] [adversarial loss: 1.053094, acc: 0.296875]\n",
      "9849: [discriminator loss: 0.510794, acc: 0.718750] [adversarial loss: 1.310864, acc: 0.187500]\n",
      "9850: [discriminator loss: 0.528070, acc: 0.757812] [adversarial loss: 1.020118, acc: 0.250000]\n",
      "9851: [discriminator loss: 0.534565, acc: 0.734375] [adversarial loss: 1.273459, acc: 0.171875]\n",
      "9852: [discriminator loss: 0.574624, acc: 0.671875] [adversarial loss: 1.217017, acc: 0.250000]\n",
      "9853: [discriminator loss: 0.439485, acc: 0.820312] [adversarial loss: 1.229082, acc: 0.234375]\n",
      "9854: [discriminator loss: 0.552385, acc: 0.703125] [adversarial loss: 1.098236, acc: 0.296875]\n",
      "9855: [discriminator loss: 0.502572, acc: 0.789062] [adversarial loss: 1.070507, acc: 0.265625]\n",
      "9856: [discriminator loss: 0.551895, acc: 0.703125] [adversarial loss: 1.168876, acc: 0.375000]\n",
      "9857: [discriminator loss: 0.504936, acc: 0.804688] [adversarial loss: 1.337114, acc: 0.171875]\n",
      "9858: [discriminator loss: 0.526393, acc: 0.734375] [adversarial loss: 1.464840, acc: 0.140625]\n",
      "9859: [discriminator loss: 0.565844, acc: 0.710938] [adversarial loss: 0.939322, acc: 0.328125]\n",
      "9860: [discriminator loss: 0.594016, acc: 0.687500] [adversarial loss: 1.383563, acc: 0.062500]\n",
      "9861: [discriminator loss: 0.550630, acc: 0.695312] [adversarial loss: 0.826901, acc: 0.453125]\n",
      "9862: [discriminator loss: 0.586097, acc: 0.695312] [adversarial loss: 1.230437, acc: 0.234375]\n",
      "9863: [discriminator loss: 0.551326, acc: 0.695312] [adversarial loss: 0.837049, acc: 0.421875]\n",
      "9864: [discriminator loss: 0.556481, acc: 0.695312] [adversarial loss: 1.418961, acc: 0.109375]\n",
      "9865: [discriminator loss: 0.545235, acc: 0.703125] [adversarial loss: 1.280046, acc: 0.218750]\n",
      "9866: [discriminator loss: 0.468558, acc: 0.773438] [adversarial loss: 1.262782, acc: 0.203125]\n",
      "9867: [discriminator loss: 0.485226, acc: 0.789062] [adversarial loss: 1.182769, acc: 0.218750]\n",
      "9868: [discriminator loss: 0.539428, acc: 0.710938] [adversarial loss: 1.165492, acc: 0.250000]\n",
      "9869: [discriminator loss: 0.527520, acc: 0.718750] [adversarial loss: 1.177500, acc: 0.328125]\n",
      "9870: [discriminator loss: 0.584023, acc: 0.695312] [adversarial loss: 1.275778, acc: 0.140625]\n",
      "9871: [discriminator loss: 0.497330, acc: 0.750000] [adversarial loss: 1.220576, acc: 0.296875]\n",
      "9872: [discriminator loss: 0.589425, acc: 0.687500] [adversarial loss: 1.114445, acc: 0.265625]\n",
      "9873: [discriminator loss: 0.490316, acc: 0.773438] [adversarial loss: 1.348620, acc: 0.203125]\n",
      "9874: [discriminator loss: 0.560244, acc: 0.695312] [adversarial loss: 0.924458, acc: 0.437500]\n",
      "9875: [discriminator loss: 0.493405, acc: 0.757812] [adversarial loss: 1.290860, acc: 0.171875]\n",
      "9876: [discriminator loss: 0.551977, acc: 0.695312] [adversarial loss: 1.034109, acc: 0.312500]\n",
      "9877: [discriminator loss: 0.499801, acc: 0.718750] [adversarial loss: 1.216740, acc: 0.234375]\n",
      "9878: [discriminator loss: 0.546645, acc: 0.750000] [adversarial loss: 1.035346, acc: 0.265625]\n",
      "9879: [discriminator loss: 0.527319, acc: 0.710938] [adversarial loss: 1.447397, acc: 0.109375]\n",
      "9880: [discriminator loss: 0.512772, acc: 0.718750] [adversarial loss: 0.924849, acc: 0.359375]\n",
      "9881: [discriminator loss: 0.567663, acc: 0.632812] [adversarial loss: 1.185518, acc: 0.234375]\n",
      "9882: [discriminator loss: 0.563709, acc: 0.703125] [adversarial loss: 1.061878, acc: 0.250000]\n",
      "9883: [discriminator loss: 0.565729, acc: 0.695312] [adversarial loss: 1.321868, acc: 0.203125]\n",
      "9884: [discriminator loss: 0.585210, acc: 0.695312] [adversarial loss: 1.130790, acc: 0.250000]\n",
      "9885: [discriminator loss: 0.486254, acc: 0.789062] [adversarial loss: 1.250902, acc: 0.156250]\n",
      "9886: [discriminator loss: 0.501523, acc: 0.773438] [adversarial loss: 1.214677, acc: 0.187500]\n",
      "9887: [discriminator loss: 0.618468, acc: 0.648438] [adversarial loss: 1.003607, acc: 0.359375]\n",
      "9888: [discriminator loss: 0.570699, acc: 0.734375] [adversarial loss: 1.262393, acc: 0.218750]\n",
      "9889: [discriminator loss: 0.556522, acc: 0.687500] [adversarial loss: 1.134023, acc: 0.187500]\n",
      "9890: [discriminator loss: 0.570548, acc: 0.671875] [adversarial loss: 1.224718, acc: 0.203125]\n",
      "9891: [discriminator loss: 0.594177, acc: 0.632812] [adversarial loss: 1.213847, acc: 0.234375]\n",
      "9892: [discriminator loss: 0.492330, acc: 0.742188] [adversarial loss: 1.085300, acc: 0.203125]\n",
      "9893: [discriminator loss: 0.522263, acc: 0.773438] [adversarial loss: 1.575687, acc: 0.140625]\n",
      "9894: [discriminator loss: 0.642263, acc: 0.656250] [adversarial loss: 0.818000, acc: 0.531250]\n",
      "9895: [discriminator loss: 0.595662, acc: 0.679688] [adversarial loss: 1.626811, acc: 0.093750]\n",
      "9896: [discriminator loss: 0.587765, acc: 0.640625] [adversarial loss: 0.713832, acc: 0.593750]\n",
      "9897: [discriminator loss: 0.594126, acc: 0.679688] [adversarial loss: 1.447402, acc: 0.093750]\n",
      "9898: [discriminator loss: 0.614694, acc: 0.617188] [adversarial loss: 0.931571, acc: 0.406250]\n",
      "9899: [discriminator loss: 0.602454, acc: 0.671875] [adversarial loss: 1.336241, acc: 0.156250]\n",
      "9900: [discriminator loss: 0.523181, acc: 0.757812] [adversarial loss: 1.178179, acc: 0.265625]\n",
      "9901: [discriminator loss: 0.623209, acc: 0.656250] [adversarial loss: 1.228728, acc: 0.203125]\n",
      "9902: [discriminator loss: 0.498383, acc: 0.750000] [adversarial loss: 1.106234, acc: 0.328125]\n",
      "9903: [discriminator loss: 0.480579, acc: 0.773438] [adversarial loss: 1.234786, acc: 0.281250]\n",
      "9904: [discriminator loss: 0.584647, acc: 0.679688] [adversarial loss: 1.301855, acc: 0.234375]\n",
      "9905: [discriminator loss: 0.482238, acc: 0.765625] [adversarial loss: 1.331870, acc: 0.156250]\n",
      "9906: [discriminator loss: 0.544284, acc: 0.726562] [adversarial loss: 1.114872, acc: 0.171875]\n",
      "9907: [discriminator loss: 0.556750, acc: 0.695312] [adversarial loss: 1.135298, acc: 0.343750]\n",
      "9908: [discriminator loss: 0.559018, acc: 0.656250] [adversarial loss: 1.085983, acc: 0.265625]\n",
      "9909: [discriminator loss: 0.487815, acc: 0.789062] [adversarial loss: 1.465042, acc: 0.140625]\n",
      "9910: [discriminator loss: 0.641976, acc: 0.648438] [adversarial loss: 1.134557, acc: 0.328125]\n",
      "9911: [discriminator loss: 0.513295, acc: 0.710938] [adversarial loss: 1.290805, acc: 0.218750]\n",
      "9912: [discriminator loss: 0.562337, acc: 0.726562] [adversarial loss: 1.179105, acc: 0.187500]\n",
      "9913: [discriminator loss: 0.541583, acc: 0.734375] [adversarial loss: 1.213709, acc: 0.281250]\n",
      "9914: [discriminator loss: 0.559795, acc: 0.734375] [adversarial loss: 1.483991, acc: 0.125000]\n",
      "9915: [discriminator loss: 0.567968, acc: 0.671875] [adversarial loss: 0.923563, acc: 0.390625]\n",
      "9916: [discriminator loss: 0.531958, acc: 0.757812] [adversarial loss: 1.120873, acc: 0.328125]\n",
      "9917: [discriminator loss: 0.581714, acc: 0.703125] [adversarial loss: 1.244396, acc: 0.187500]\n",
      "9918: [discriminator loss: 0.537129, acc: 0.734375] [adversarial loss: 1.127581, acc: 0.265625]\n",
      "9919: [discriminator loss: 0.589466, acc: 0.695312] [adversarial loss: 1.098548, acc: 0.312500]\n",
      "9920: [discriminator loss: 0.495746, acc: 0.757812] [adversarial loss: 1.437068, acc: 0.078125]\n",
      "9921: [discriminator loss: 0.558395, acc: 0.710938] [adversarial loss: 0.848278, acc: 0.531250]\n",
      "9922: [discriminator loss: 0.617766, acc: 0.664062] [adversarial loss: 1.502958, acc: 0.093750]\n",
      "9923: [discriminator loss: 0.586128, acc: 0.687500] [adversarial loss: 0.657890, acc: 0.625000]\n",
      "9924: [discriminator loss: 0.628020, acc: 0.632812] [adversarial loss: 1.358516, acc: 0.156250]\n",
      "9925: [discriminator loss: 0.535759, acc: 0.726562] [adversarial loss: 1.016001, acc: 0.281250]\n",
      "9926: [discriminator loss: 0.550313, acc: 0.687500] [adversarial loss: 1.445906, acc: 0.140625]\n",
      "9927: [discriminator loss: 0.576591, acc: 0.656250] [adversarial loss: 1.100996, acc: 0.250000]\n",
      "9928: [discriminator loss: 0.588352, acc: 0.679688] [adversarial loss: 1.084597, acc: 0.281250]\n",
      "9929: [discriminator loss: 0.545728, acc: 0.703125] [adversarial loss: 1.127442, acc: 0.265625]\n",
      "9930: [discriminator loss: 0.546002, acc: 0.710938] [adversarial loss: 1.409220, acc: 0.109375]\n",
      "9931: [discriminator loss: 0.523310, acc: 0.703125] [adversarial loss: 0.937683, acc: 0.437500]\n",
      "9932: [discriminator loss: 0.522495, acc: 0.726562] [adversarial loss: 1.326705, acc: 0.187500]\n",
      "9933: [discriminator loss: 0.528581, acc: 0.703125] [adversarial loss: 1.053998, acc: 0.328125]\n",
      "9934: [discriminator loss: 0.564199, acc: 0.718750] [adversarial loss: 1.328423, acc: 0.171875]\n",
      "9935: [discriminator loss: 0.636424, acc: 0.640625] [adversarial loss: 1.252989, acc: 0.218750]\n",
      "9936: [discriminator loss: 0.541803, acc: 0.718750] [adversarial loss: 1.283282, acc: 0.171875]\n",
      "9937: [discriminator loss: 0.563469, acc: 0.703125] [adversarial loss: 1.292914, acc: 0.187500]\n",
      "9938: [discriminator loss: 0.547398, acc: 0.734375] [adversarial loss: 1.116247, acc: 0.234375]\n",
      "9939: [discriminator loss: 0.582574, acc: 0.648438] [adversarial loss: 0.976067, acc: 0.343750]\n",
      "9940: [discriminator loss: 0.580402, acc: 0.687500] [adversarial loss: 1.311970, acc: 0.125000]\n",
      "9941: [discriminator loss: 0.573221, acc: 0.695312] [adversarial loss: 1.092426, acc: 0.234375]\n",
      "9942: [discriminator loss: 0.520274, acc: 0.765625] [adversarial loss: 1.246955, acc: 0.171875]\n",
      "9943: [discriminator loss: 0.497523, acc: 0.781250] [adversarial loss: 0.988717, acc: 0.375000]\n",
      "9944: [discriminator loss: 0.554183, acc: 0.726562] [adversarial loss: 1.403715, acc: 0.109375]\n",
      "9945: [discriminator loss: 0.546520, acc: 0.742188] [adversarial loss: 0.783008, acc: 0.453125]\n",
      "9946: [discriminator loss: 0.616530, acc: 0.632812] [adversarial loss: 1.566952, acc: 0.109375]\n",
      "9947: [discriminator loss: 0.550090, acc: 0.687500] [adversarial loss: 0.839828, acc: 0.437500]\n",
      "9948: [discriminator loss: 0.543592, acc: 0.671875] [adversarial loss: 1.514863, acc: 0.109375]\n",
      "9949: [discriminator loss: 0.542427, acc: 0.695312] [adversarial loss: 0.899112, acc: 0.421875]\n",
      "9950: [discriminator loss: 0.537275, acc: 0.726562] [adversarial loss: 1.312826, acc: 0.187500]\n",
      "9951: [discriminator loss: 0.531685, acc: 0.742188] [adversarial loss: 0.981211, acc: 0.421875]\n",
      "9952: [discriminator loss: 0.562759, acc: 0.695312] [adversarial loss: 1.110765, acc: 0.250000]\n",
      "9953: [discriminator loss: 0.485483, acc: 0.757812] [adversarial loss: 1.214265, acc: 0.218750]\n",
      "9954: [discriminator loss: 0.546919, acc: 0.734375] [adversarial loss: 0.958263, acc: 0.406250]\n",
      "9955: [discriminator loss: 0.521981, acc: 0.757812] [adversarial loss: 1.299512, acc: 0.234375]\n",
      "9956: [discriminator loss: 0.591781, acc: 0.695312] [adversarial loss: 0.967895, acc: 0.359375]\n",
      "9957: [discriminator loss: 0.527190, acc: 0.734375] [adversarial loss: 1.433158, acc: 0.125000]\n",
      "9958: [discriminator loss: 0.564183, acc: 0.687500] [adversarial loss: 0.786894, acc: 0.500000]\n",
      "9959: [discriminator loss: 0.557992, acc: 0.695312] [adversarial loss: 1.096761, acc: 0.265625]\n",
      "9960: [discriminator loss: 0.563086, acc: 0.648438] [adversarial loss: 0.982696, acc: 0.265625]\n",
      "9961: [discriminator loss: 0.554272, acc: 0.734375] [adversarial loss: 1.374655, acc: 0.156250]\n",
      "9962: [discriminator loss: 0.528901, acc: 0.742188] [adversarial loss: 1.150994, acc: 0.265625]\n",
      "9963: [discriminator loss: 0.602982, acc: 0.671875] [adversarial loss: 1.280286, acc: 0.250000]\n",
      "9964: [discriminator loss: 0.618034, acc: 0.695312] [adversarial loss: 0.826757, acc: 0.437500]\n",
      "9965: [discriminator loss: 0.568014, acc: 0.710938] [adversarial loss: 1.340347, acc: 0.187500]\n",
      "9966: [discriminator loss: 0.485052, acc: 0.750000] [adversarial loss: 1.050214, acc: 0.312500]\n",
      "9967: [discriminator loss: 0.552674, acc: 0.679688] [adversarial loss: 1.212133, acc: 0.281250]\n",
      "9968: [discriminator loss: 0.513540, acc: 0.742188] [adversarial loss: 0.927362, acc: 0.343750]\n",
      "9969: [discriminator loss: 0.564201, acc: 0.687500] [adversarial loss: 1.019551, acc: 0.218750]\n",
      "9970: [discriminator loss: 0.532250, acc: 0.750000] [adversarial loss: 1.231990, acc: 0.171875]\n",
      "9971: [discriminator loss: 0.535510, acc: 0.695312] [adversarial loss: 0.933304, acc: 0.390625]\n",
      "9972: [discriminator loss: 0.561406, acc: 0.726562] [adversarial loss: 1.184980, acc: 0.203125]\n",
      "9973: [discriminator loss: 0.468486, acc: 0.804688] [adversarial loss: 0.969869, acc: 0.343750]\n",
      "9974: [discriminator loss: 0.576559, acc: 0.687500] [adversarial loss: 1.446988, acc: 0.140625]\n",
      "9975: [discriminator loss: 0.523185, acc: 0.726562] [adversarial loss: 0.980893, acc: 0.375000]\n",
      "9976: [discriminator loss: 0.579401, acc: 0.710938] [adversarial loss: 1.676962, acc: 0.125000]\n",
      "9977: [discriminator loss: 0.538951, acc: 0.710938] [adversarial loss: 0.848801, acc: 0.484375]\n",
      "9978: [discriminator loss: 0.606726, acc: 0.679688] [adversarial loss: 1.104135, acc: 0.328125]\n",
      "9979: [discriminator loss: 0.615942, acc: 0.617188] [adversarial loss: 1.016092, acc: 0.312500]\n",
      "9980: [discriminator loss: 0.610573, acc: 0.671875] [adversarial loss: 1.068657, acc: 0.265625]\n",
      "9981: [discriminator loss: 0.514385, acc: 0.742188] [adversarial loss: 1.256232, acc: 0.218750]\n",
      "9982: [discriminator loss: 0.506064, acc: 0.718750] [adversarial loss: 1.126590, acc: 0.187500]\n",
      "9983: [discriminator loss: 0.533357, acc: 0.703125] [adversarial loss: 1.066591, acc: 0.281250]\n",
      "9984: [discriminator loss: 0.587638, acc: 0.687500] [adversarial loss: 1.154023, acc: 0.203125]\n",
      "9985: [discriminator loss: 0.534207, acc: 0.718750] [adversarial loss: 0.971741, acc: 0.359375]\n",
      "9986: [discriminator loss: 0.552126, acc: 0.695312] [adversarial loss: 1.406307, acc: 0.140625]\n",
      "9987: [discriminator loss: 0.574004, acc: 0.703125] [adversarial loss: 0.924780, acc: 0.437500]\n",
      "9988: [discriminator loss: 0.480801, acc: 0.781250] [adversarial loss: 1.259309, acc: 0.218750]\n",
      "9989: [discriminator loss: 0.612052, acc: 0.640625] [adversarial loss: 1.579131, acc: 0.125000]\n",
      "9990: [discriminator loss: 0.540353, acc: 0.718750] [adversarial loss: 0.975407, acc: 0.390625]\n",
      "9991: [discriminator loss: 0.528100, acc: 0.734375] [adversarial loss: 1.125638, acc: 0.296875]\n",
      "9992: [discriminator loss: 0.553796, acc: 0.750000] [adversarial loss: 1.216912, acc: 0.234375]\n",
      "9993: [discriminator loss: 0.536926, acc: 0.734375] [adversarial loss: 1.135625, acc: 0.265625]\n",
      "9994: [discriminator loss: 0.499633, acc: 0.734375] [adversarial loss: 1.065887, acc: 0.312500]\n",
      "9995: [discriminator loss: 0.569713, acc: 0.710938] [adversarial loss: 1.294031, acc: 0.187500]\n",
      "9996: [discriminator loss: 0.530884, acc: 0.820312] [adversarial loss: 1.067371, acc: 0.218750]\n",
      "9997: [discriminator loss: 0.596402, acc: 0.679688] [adversarial loss: 1.217482, acc: 0.234375]\n",
      "9998: [discriminator loss: 0.565628, acc: 0.710938] [adversarial loss: 1.170819, acc: 0.234375]\n",
      "9999: [discriminator loss: 0.476532, acc: 0.773438] [adversarial loss: 1.065920, acc: 0.343750]\n",
      "10000: [discriminator loss: 0.563668, acc: 0.687500] [adversarial loss: 1.349620, acc: 0.234375]\n",
      "10001: [discriminator loss: 0.571466, acc: 0.679688] [adversarial loss: 0.942826, acc: 0.375000]\n",
      "10002: [discriminator loss: 0.559818, acc: 0.679688] [adversarial loss: 1.356229, acc: 0.171875]\n",
      "10003: [discriminator loss: 0.521512, acc: 0.718750] [adversarial loss: 0.765512, acc: 0.515625]\n",
      "10004: [discriminator loss: 0.574654, acc: 0.726562] [adversarial loss: 1.286278, acc: 0.281250]\n",
      "10005: [discriminator loss: 0.561668, acc: 0.664062] [adversarial loss: 1.238107, acc: 0.296875]\n",
      "10006: [discriminator loss: 0.534102, acc: 0.710938] [adversarial loss: 1.160555, acc: 0.312500]\n",
      "10007: [discriminator loss: 0.582005, acc: 0.664062] [adversarial loss: 1.382982, acc: 0.171875]\n",
      "10008: [discriminator loss: 0.572662, acc: 0.703125] [adversarial loss: 0.736393, acc: 0.531250]\n",
      "10009: [discriminator loss: 0.589972, acc: 0.656250] [adversarial loss: 1.689936, acc: 0.062500]\n",
      "10010: [discriminator loss: 0.569149, acc: 0.679688] [adversarial loss: 0.721455, acc: 0.562500]\n",
      "10011: [discriminator loss: 0.575222, acc: 0.710938] [adversarial loss: 1.386208, acc: 0.046875]\n",
      "10012: [discriminator loss: 0.518546, acc: 0.718750] [adversarial loss: 1.214614, acc: 0.250000]\n",
      "10013: [discriminator loss: 0.544897, acc: 0.734375] [adversarial loss: 1.058263, acc: 0.375000]\n",
      "10014: [discriminator loss: 0.481193, acc: 0.773438] [adversarial loss: 1.336748, acc: 0.265625]\n",
      "10015: [discriminator loss: 0.584970, acc: 0.695312] [adversarial loss: 1.226335, acc: 0.187500]\n",
      "10016: [discriminator loss: 0.591403, acc: 0.648438] [adversarial loss: 1.328246, acc: 0.156250]\n",
      "10017: [discriminator loss: 0.509064, acc: 0.773438] [adversarial loss: 0.967828, acc: 0.390625]\n",
      "10018: [discriminator loss: 0.527676, acc: 0.703125] [adversarial loss: 1.331490, acc: 0.156250]\n",
      "10019: [discriminator loss: 0.504088, acc: 0.812500] [adversarial loss: 1.196262, acc: 0.250000]\n",
      "10020: [discriminator loss: 0.593369, acc: 0.679688] [adversarial loss: 1.338591, acc: 0.187500]\n",
      "10021: [discriminator loss: 0.540796, acc: 0.703125] [adversarial loss: 0.943833, acc: 0.468750]\n",
      "10022: [discriminator loss: 0.542588, acc: 0.734375] [adversarial loss: 1.293564, acc: 0.234375]\n",
      "10023: [discriminator loss: 0.658893, acc: 0.609375] [adversarial loss: 1.053982, acc: 0.312500]\n",
      "10024: [discriminator loss: 0.587147, acc: 0.648438] [adversarial loss: 1.408603, acc: 0.125000]\n",
      "10025: [discriminator loss: 0.540464, acc: 0.734375] [adversarial loss: 1.103745, acc: 0.234375]\n",
      "10026: [discriminator loss: 0.485654, acc: 0.765625] [adversarial loss: 1.087575, acc: 0.296875]\n",
      "10027: [discriminator loss: 0.529353, acc: 0.750000] [adversarial loss: 1.045697, acc: 0.343750]\n",
      "10028: [discriminator loss: 0.558866, acc: 0.718750] [adversarial loss: 1.185271, acc: 0.234375]\n",
      "10029: [discriminator loss: 0.548782, acc: 0.710938] [adversarial loss: 1.191365, acc: 0.265625]\n",
      "10030: [discriminator loss: 0.587925, acc: 0.656250] [adversarial loss: 0.931439, acc: 0.421875]\n",
      "10031: [discriminator loss: 0.533160, acc: 0.695312] [adversarial loss: 1.235850, acc: 0.156250]\n",
      "10032: [discriminator loss: 0.559325, acc: 0.671875] [adversarial loss: 1.105074, acc: 0.250000]\n",
      "10033: [discriminator loss: 0.523003, acc: 0.703125] [adversarial loss: 1.150624, acc: 0.265625]\n",
      "10034: [discriminator loss: 0.587938, acc: 0.695312] [adversarial loss: 1.259944, acc: 0.250000]\n",
      "10035: [discriminator loss: 0.490619, acc: 0.742188] [adversarial loss: 1.604365, acc: 0.062500]\n",
      "10036: [discriminator loss: 0.588437, acc: 0.679688] [adversarial loss: 0.815251, acc: 0.484375]\n",
      "10037: [discriminator loss: 0.529720, acc: 0.695312] [adversarial loss: 1.356653, acc: 0.234375]\n",
      "10038: [discriminator loss: 0.527953, acc: 0.757812] [adversarial loss: 1.036273, acc: 0.343750]\n",
      "10039: [discriminator loss: 0.563469, acc: 0.718750] [adversarial loss: 1.444626, acc: 0.125000]\n",
      "10040: [discriminator loss: 0.507305, acc: 0.742188] [adversarial loss: 1.024590, acc: 0.343750]\n",
      "10041: [discriminator loss: 0.539642, acc: 0.687500] [adversarial loss: 1.658789, acc: 0.062500]\n",
      "10042: [discriminator loss: 0.523443, acc: 0.734375] [adversarial loss: 1.117601, acc: 0.281250]\n",
      "10043: [discriminator loss: 0.532946, acc: 0.726562] [adversarial loss: 1.292299, acc: 0.218750]\n",
      "10044: [discriminator loss: 0.542161, acc: 0.695312] [adversarial loss: 0.879013, acc: 0.406250]\n",
      "10045: [discriminator loss: 0.566108, acc: 0.726562] [adversarial loss: 1.297594, acc: 0.187500]\n",
      "10046: [discriminator loss: 0.529240, acc: 0.703125] [adversarial loss: 0.967578, acc: 0.328125]\n",
      "10047: [discriminator loss: 0.506785, acc: 0.750000] [adversarial loss: 1.205075, acc: 0.203125]\n",
      "10048: [discriminator loss: 0.574559, acc: 0.648438] [adversarial loss: 0.940424, acc: 0.359375]\n",
      "10049: [discriminator loss: 0.524842, acc: 0.765625] [adversarial loss: 1.374868, acc: 0.140625]\n",
      "10050: [discriminator loss: 0.567387, acc: 0.710938] [adversarial loss: 0.981187, acc: 0.390625]\n",
      "10051: [discriminator loss: 0.579394, acc: 0.734375] [adversarial loss: 1.431264, acc: 0.125000]\n",
      "10052: [discriminator loss: 0.485589, acc: 0.765625] [adversarial loss: 0.963074, acc: 0.343750]\n",
      "10053: [discriminator loss: 0.528413, acc: 0.734375] [adversarial loss: 1.347725, acc: 0.156250]\n",
      "10054: [discriminator loss: 0.572215, acc: 0.695312] [adversarial loss: 1.006158, acc: 0.375000]\n",
      "10055: [discriminator loss: 0.598592, acc: 0.656250] [adversarial loss: 1.282045, acc: 0.187500]\n",
      "10056: [discriminator loss: 0.515663, acc: 0.710938] [adversarial loss: 0.862647, acc: 0.546875]\n",
      "10057: [discriminator loss: 0.549945, acc: 0.757812] [adversarial loss: 1.403076, acc: 0.140625]\n",
      "10058: [discriminator loss: 0.500324, acc: 0.750000] [adversarial loss: 1.153329, acc: 0.312500]\n",
      "10059: [discriminator loss: 0.508169, acc: 0.750000] [adversarial loss: 1.015851, acc: 0.312500]\n",
      "10060: [discriminator loss: 0.593420, acc: 0.726562] [adversarial loss: 1.207971, acc: 0.218750]\n",
      "10061: [discriminator loss: 0.555010, acc: 0.664062] [adversarial loss: 1.255521, acc: 0.203125]\n",
      "10062: [discriminator loss: 0.598420, acc: 0.656250] [adversarial loss: 0.867517, acc: 0.406250]\n",
      "10063: [discriminator loss: 0.583792, acc: 0.656250] [adversarial loss: 1.284075, acc: 0.265625]\n",
      "10064: [discriminator loss: 0.590845, acc: 0.656250] [adversarial loss: 1.174806, acc: 0.265625]\n",
      "10065: [discriminator loss: 0.538815, acc: 0.750000] [adversarial loss: 1.061749, acc: 0.359375]\n",
      "10066: [discriminator loss: 0.506479, acc: 0.773438] [adversarial loss: 1.213085, acc: 0.171875]\n",
      "10067: [discriminator loss: 0.597198, acc: 0.695312] [adversarial loss: 0.955085, acc: 0.406250]\n",
      "10068: [discriminator loss: 0.496121, acc: 0.757812] [adversarial loss: 1.430093, acc: 0.125000]\n",
      "10069: [discriminator loss: 0.546575, acc: 0.742188] [adversarial loss: 0.873142, acc: 0.453125]\n",
      "10070: [discriminator loss: 0.626563, acc: 0.695312] [adversarial loss: 1.386090, acc: 0.171875]\n",
      "10071: [discriminator loss: 0.597076, acc: 0.656250] [adversarial loss: 0.915563, acc: 0.390625]\n",
      "10072: [discriminator loss: 0.553471, acc: 0.695312] [adversarial loss: 1.338224, acc: 0.140625]\n",
      "10073: [discriminator loss: 0.595481, acc: 0.664062] [adversarial loss: 0.751192, acc: 0.531250]\n",
      "10074: [discriminator loss: 0.564356, acc: 0.695312] [adversarial loss: 1.204309, acc: 0.156250]\n",
      "10075: [discriminator loss: 0.540735, acc: 0.742188] [adversarial loss: 1.198199, acc: 0.234375]\n",
      "10076: [discriminator loss: 0.568618, acc: 0.671875] [adversarial loss: 1.298215, acc: 0.171875]\n",
      "10077: [discriminator loss: 0.578632, acc: 0.695312] [adversarial loss: 0.951302, acc: 0.328125]\n",
      "10078: [discriminator loss: 0.597883, acc: 0.664062] [adversarial loss: 1.292827, acc: 0.203125]\n",
      "10079: [discriminator loss: 0.610558, acc: 0.703125] [adversarial loss: 0.948462, acc: 0.359375]\n",
      "10080: [discriminator loss: 0.570970, acc: 0.664062] [adversarial loss: 1.536828, acc: 0.125000]\n",
      "10081: [discriminator loss: 0.533914, acc: 0.726562] [adversarial loss: 0.792046, acc: 0.593750]\n",
      "10082: [discriminator loss: 0.615260, acc: 0.656250] [adversarial loss: 1.450212, acc: 0.140625]\n",
      "10083: [discriminator loss: 0.550112, acc: 0.695312] [adversarial loss: 0.981813, acc: 0.265625]\n",
      "10084: [discriminator loss: 0.543551, acc: 0.710938] [adversarial loss: 1.192337, acc: 0.203125]\n",
      "10085: [discriminator loss: 0.576101, acc: 0.679688] [adversarial loss: 0.873581, acc: 0.406250]\n",
      "10086: [discriminator loss: 0.544884, acc: 0.718750] [adversarial loss: 1.489405, acc: 0.156250]\n",
      "10087: [discriminator loss: 0.519096, acc: 0.718750] [adversarial loss: 1.041711, acc: 0.328125]\n",
      "10088: [discriminator loss: 0.566342, acc: 0.710938] [adversarial loss: 1.076663, acc: 0.312500]\n",
      "10089: [discriminator loss: 0.558115, acc: 0.710938] [adversarial loss: 1.185924, acc: 0.203125]\n",
      "10090: [discriminator loss: 0.562749, acc: 0.687500] [adversarial loss: 1.156180, acc: 0.312500]\n",
      "10091: [discriminator loss: 0.559574, acc: 0.703125] [adversarial loss: 1.105951, acc: 0.234375]\n",
      "10092: [discriminator loss: 0.517935, acc: 0.742188] [adversarial loss: 1.072441, acc: 0.328125]\n",
      "10093: [discriminator loss: 0.539336, acc: 0.750000] [adversarial loss: 1.316353, acc: 0.218750]\n",
      "10094: [discriminator loss: 0.512632, acc: 0.687500] [adversarial loss: 1.307031, acc: 0.109375]\n",
      "10095: [discriminator loss: 0.571790, acc: 0.703125] [adversarial loss: 1.400446, acc: 0.171875]\n",
      "10096: [discriminator loss: 0.616244, acc: 0.695312] [adversarial loss: 0.807242, acc: 0.468750]\n",
      "10097: [discriminator loss: 0.664454, acc: 0.648438] [adversarial loss: 1.810219, acc: 0.015625]\n",
      "10098: [discriminator loss: 0.673751, acc: 0.617188] [adversarial loss: 1.039501, acc: 0.312500]\n",
      "10099: [discriminator loss: 0.550181, acc: 0.773438] [adversarial loss: 1.139512, acc: 0.281250]\n",
      "10100: [discriminator loss: 0.526901, acc: 0.718750] [adversarial loss: 1.068550, acc: 0.265625]\n",
      "10101: [discriminator loss: 0.630064, acc: 0.617188] [adversarial loss: 1.074830, acc: 0.265625]\n",
      "10102: [discriminator loss: 0.546649, acc: 0.734375] [adversarial loss: 1.068119, acc: 0.296875]\n",
      "10103: [discriminator loss: 0.558847, acc: 0.695312] [adversarial loss: 1.130891, acc: 0.296875]\n",
      "10104: [discriminator loss: 0.551929, acc: 0.726562] [adversarial loss: 1.239224, acc: 0.140625]\n",
      "10105: [discriminator loss: 0.551177, acc: 0.671875] [adversarial loss: 1.016651, acc: 0.328125]\n",
      "10106: [discriminator loss: 0.478692, acc: 0.796875] [adversarial loss: 1.225447, acc: 0.203125]\n",
      "10107: [discriminator loss: 0.522974, acc: 0.726562] [adversarial loss: 1.381066, acc: 0.187500]\n",
      "10108: [discriminator loss: 0.532062, acc: 0.679688] [adversarial loss: 0.841727, acc: 0.375000]\n",
      "10109: [discriminator loss: 0.599332, acc: 0.664062] [adversarial loss: 1.303231, acc: 0.140625]\n",
      "10110: [discriminator loss: 0.587056, acc: 0.640625] [adversarial loss: 0.924321, acc: 0.421875]\n",
      "10111: [discriminator loss: 0.531862, acc: 0.765625] [adversarial loss: 1.188048, acc: 0.140625]\n",
      "10112: [discriminator loss: 0.524362, acc: 0.695312] [adversarial loss: 1.156437, acc: 0.265625]\n",
      "10113: [discriminator loss: 0.511793, acc: 0.757812] [adversarial loss: 1.419077, acc: 0.187500]\n",
      "10114: [discriminator loss: 0.456403, acc: 0.796875] [adversarial loss: 1.070678, acc: 0.265625]\n",
      "10115: [discriminator loss: 0.581382, acc: 0.695312] [adversarial loss: 1.575307, acc: 0.109375]\n",
      "10116: [discriminator loss: 0.578614, acc: 0.695312] [adversarial loss: 1.044060, acc: 0.328125]\n",
      "10117: [discriminator loss: 0.555750, acc: 0.671875] [adversarial loss: 1.217092, acc: 0.265625]\n",
      "10118: [discriminator loss: 0.519702, acc: 0.718750] [adversarial loss: 1.473905, acc: 0.187500]\n",
      "10119: [discriminator loss: 0.549551, acc: 0.679688] [adversarial loss: 1.028558, acc: 0.375000]\n",
      "10120: [discriminator loss: 0.593113, acc: 0.671875] [adversarial loss: 1.458223, acc: 0.156250]\n",
      "10121: [discriminator loss: 0.600374, acc: 0.710938] [adversarial loss: 0.814981, acc: 0.421875]\n",
      "10122: [discriminator loss: 0.505183, acc: 0.726562] [adversarial loss: 1.283748, acc: 0.156250]\n",
      "10123: [discriminator loss: 0.540227, acc: 0.726562] [adversarial loss: 1.275845, acc: 0.125000]\n",
      "10124: [discriminator loss: 0.540296, acc: 0.750000] [adversarial loss: 1.213838, acc: 0.234375]\n",
      "10125: [discriminator loss: 0.528797, acc: 0.750000] [adversarial loss: 1.033147, acc: 0.343750]\n",
      "10126: [discriminator loss: 0.578067, acc: 0.664062] [adversarial loss: 1.252652, acc: 0.281250]\n",
      "10127: [discriminator loss: 0.548625, acc: 0.664062] [adversarial loss: 1.198605, acc: 0.218750]\n",
      "10128: [discriminator loss: 0.513347, acc: 0.718750] [adversarial loss: 0.952948, acc: 0.375000]\n",
      "10129: [discriminator loss: 0.537997, acc: 0.742188] [adversarial loss: 1.497245, acc: 0.125000]\n",
      "10130: [discriminator loss: 0.633415, acc: 0.625000] [adversarial loss: 0.974984, acc: 0.281250]\n",
      "10131: [discriminator loss: 0.570654, acc: 0.687500] [adversarial loss: 1.377682, acc: 0.187500]\n",
      "10132: [discriminator loss: 0.529516, acc: 0.710938] [adversarial loss: 0.958502, acc: 0.328125]\n",
      "10133: [discriminator loss: 0.585321, acc: 0.687500] [adversarial loss: 1.483255, acc: 0.046875]\n",
      "10134: [discriminator loss: 0.605438, acc: 0.632812] [adversarial loss: 0.931236, acc: 0.406250]\n",
      "10135: [discriminator loss: 0.477445, acc: 0.765625] [adversarial loss: 1.483422, acc: 0.109375]\n",
      "10136: [discriminator loss: 0.627262, acc: 0.640625] [adversarial loss: 0.842917, acc: 0.390625]\n",
      "10137: [discriminator loss: 0.563537, acc: 0.664062] [adversarial loss: 1.209955, acc: 0.234375]\n",
      "10138: [discriminator loss: 0.527418, acc: 0.734375] [adversarial loss: 0.819302, acc: 0.359375]\n",
      "10139: [discriminator loss: 0.526507, acc: 0.687500] [adversarial loss: 1.125657, acc: 0.187500]\n",
      "10140: [discriminator loss: 0.561186, acc: 0.687500] [adversarial loss: 1.102190, acc: 0.203125]\n",
      "10141: [discriminator loss: 0.527894, acc: 0.718750] [adversarial loss: 1.227511, acc: 0.234375]\n",
      "10142: [discriminator loss: 0.570972, acc: 0.679688] [adversarial loss: 1.180070, acc: 0.265625]\n",
      "10143: [discriminator loss: 0.525808, acc: 0.734375] [adversarial loss: 1.139143, acc: 0.250000]\n",
      "10144: [discriminator loss: 0.505864, acc: 0.757812] [adversarial loss: 1.189969, acc: 0.281250]\n",
      "10145: [discriminator loss: 0.535118, acc: 0.742188] [adversarial loss: 1.087745, acc: 0.359375]\n",
      "10146: [discriminator loss: 0.595124, acc: 0.656250] [adversarial loss: 1.373900, acc: 0.109375]\n",
      "10147: [discriminator loss: 0.603953, acc: 0.656250] [adversarial loss: 1.045089, acc: 0.328125]\n",
      "10148: [discriminator loss: 0.577080, acc: 0.625000] [adversarial loss: 1.572048, acc: 0.140625]\n",
      "10149: [discriminator loss: 0.612902, acc: 0.703125] [adversarial loss: 0.713401, acc: 0.625000]\n",
      "10150: [discriminator loss: 0.566042, acc: 0.703125] [adversarial loss: 1.334219, acc: 0.218750]\n",
      "10151: [discriminator loss: 0.526174, acc: 0.726562] [adversarial loss: 0.931206, acc: 0.328125]\n",
      "10152: [discriminator loss: 0.522718, acc: 0.781250] [adversarial loss: 1.266448, acc: 0.203125]\n",
      "10153: [discriminator loss: 0.556582, acc: 0.664062] [adversarial loss: 0.993756, acc: 0.296875]\n",
      "10154: [discriminator loss: 0.531235, acc: 0.695312] [adversarial loss: 1.094555, acc: 0.187500]\n",
      "10155: [discriminator loss: 0.545708, acc: 0.710938] [adversarial loss: 1.197001, acc: 0.203125]\n",
      "10156: [discriminator loss: 0.514260, acc: 0.750000] [adversarial loss: 1.017365, acc: 0.265625]\n",
      "10157: [discriminator loss: 0.538935, acc: 0.734375] [adversarial loss: 1.334345, acc: 0.140625]\n",
      "10158: [discriminator loss: 0.627771, acc: 0.617188] [adversarial loss: 0.804692, acc: 0.468750]\n",
      "10159: [discriminator loss: 0.535444, acc: 0.718750] [adversarial loss: 1.351910, acc: 0.171875]\n",
      "10160: [discriminator loss: 0.560410, acc: 0.687500] [adversarial loss: 1.200362, acc: 0.140625]\n",
      "10161: [discriminator loss: 0.535480, acc: 0.742188] [adversarial loss: 1.251618, acc: 0.281250]\n",
      "10162: [discriminator loss: 0.566087, acc: 0.687500] [adversarial loss: 1.094926, acc: 0.265625]\n",
      "10163: [discriminator loss: 0.586212, acc: 0.679688] [adversarial loss: 1.211872, acc: 0.140625]\n",
      "10164: [discriminator loss: 0.475481, acc: 0.773438] [adversarial loss: 1.192740, acc: 0.234375]\n",
      "10165: [discriminator loss: 0.553916, acc: 0.718750] [adversarial loss: 1.123693, acc: 0.234375]\n",
      "10166: [discriminator loss: 0.570879, acc: 0.648438] [adversarial loss: 1.286880, acc: 0.281250]\n",
      "10167: [discriminator loss: 0.531587, acc: 0.718750] [adversarial loss: 1.092698, acc: 0.328125]\n",
      "10168: [discriminator loss: 0.502707, acc: 0.796875] [adversarial loss: 1.339166, acc: 0.078125]\n",
      "10169: [discriminator loss: 0.497775, acc: 0.742188] [adversarial loss: 0.982745, acc: 0.375000]\n",
      "10170: [discriminator loss: 0.516502, acc: 0.742188] [adversarial loss: 1.102020, acc: 0.281250]\n",
      "10171: [discriminator loss: 0.558369, acc: 0.718750] [adversarial loss: 1.476132, acc: 0.156250]\n",
      "10172: [discriminator loss: 0.533066, acc: 0.687500] [adversarial loss: 1.027677, acc: 0.375000]\n",
      "10173: [discriminator loss: 0.569543, acc: 0.710938] [adversarial loss: 1.346034, acc: 0.156250]\n",
      "10174: [discriminator loss: 0.494082, acc: 0.750000] [adversarial loss: 0.852325, acc: 0.468750]\n",
      "10175: [discriminator loss: 0.599370, acc: 0.726562] [adversarial loss: 1.571387, acc: 0.093750]\n",
      "10176: [discriminator loss: 0.545140, acc: 0.726562] [adversarial loss: 0.933309, acc: 0.343750]\n",
      "10177: [discriminator loss: 0.593271, acc: 0.656250] [adversarial loss: 1.450175, acc: 0.140625]\n",
      "10178: [discriminator loss: 0.591921, acc: 0.656250] [adversarial loss: 0.954481, acc: 0.437500]\n",
      "10179: [discriminator loss: 0.634015, acc: 0.617188] [adversarial loss: 1.240608, acc: 0.171875]\n",
      "10180: [discriminator loss: 0.521365, acc: 0.773438] [adversarial loss: 1.069725, acc: 0.250000]\n",
      "10181: [discriminator loss: 0.586325, acc: 0.695312] [adversarial loss: 1.237325, acc: 0.250000]\n",
      "10182: [discriminator loss: 0.573747, acc: 0.695312] [adversarial loss: 1.257751, acc: 0.203125]\n",
      "10183: [discriminator loss: 0.486913, acc: 0.781250] [adversarial loss: 1.396028, acc: 0.171875]\n",
      "10184: [discriminator loss: 0.612566, acc: 0.687500] [adversarial loss: 0.929224, acc: 0.453125]\n",
      "10185: [discriminator loss: 0.544347, acc: 0.726562] [adversarial loss: 1.248943, acc: 0.171875]\n",
      "10186: [discriminator loss: 0.574682, acc: 0.710938] [adversarial loss: 1.080725, acc: 0.328125]\n",
      "10187: [discriminator loss: 0.585689, acc: 0.679688] [adversarial loss: 1.346079, acc: 0.171875]\n",
      "10188: [discriminator loss: 0.527917, acc: 0.718750] [adversarial loss: 1.173820, acc: 0.203125]\n",
      "10189: [discriminator loss: 0.520738, acc: 0.789062] [adversarial loss: 1.431618, acc: 0.156250]\n",
      "10190: [discriminator loss: 0.538773, acc: 0.718750] [adversarial loss: 1.001540, acc: 0.265625]\n",
      "10191: [discriminator loss: 0.581591, acc: 0.710938] [adversarial loss: 1.203611, acc: 0.187500]\n",
      "10192: [discriminator loss: 0.574166, acc: 0.710938] [adversarial loss: 1.076252, acc: 0.187500]\n",
      "10193: [discriminator loss: 0.555993, acc: 0.718750] [adversarial loss: 1.342931, acc: 0.109375]\n",
      "10194: [discriminator loss: 0.488917, acc: 0.757812] [adversarial loss: 0.837960, acc: 0.390625]\n",
      "10195: [discriminator loss: 0.574667, acc: 0.703125] [adversarial loss: 1.557783, acc: 0.093750]\n",
      "10196: [discriminator loss: 0.561899, acc: 0.703125] [adversarial loss: 0.891938, acc: 0.390625]\n",
      "10197: [discriminator loss: 0.570361, acc: 0.664062] [adversarial loss: 1.261532, acc: 0.187500]\n",
      "10198: [discriminator loss: 0.524908, acc: 0.710938] [adversarial loss: 1.006855, acc: 0.281250]\n",
      "10199: [discriminator loss: 0.529292, acc: 0.757812] [adversarial loss: 1.045601, acc: 0.281250]\n",
      "10200: [discriminator loss: 0.508554, acc: 0.765625] [adversarial loss: 1.312727, acc: 0.125000]\n",
      "10201: [discriminator loss: 0.527727, acc: 0.734375] [adversarial loss: 1.133576, acc: 0.234375]\n",
      "10202: [discriminator loss: 0.546496, acc: 0.687500] [adversarial loss: 1.360632, acc: 0.125000]\n",
      "10203: [discriminator loss: 0.584132, acc: 0.718750] [adversarial loss: 0.715564, acc: 0.531250]\n",
      "10204: [discriminator loss: 0.564345, acc: 0.695312] [adversarial loss: 1.627454, acc: 0.093750]\n",
      "10205: [discriminator loss: 0.655090, acc: 0.625000] [adversarial loss: 0.753437, acc: 0.531250]\n",
      "10206: [discriminator loss: 0.649177, acc: 0.664062] [adversarial loss: 1.320699, acc: 0.187500]\n",
      "10207: [discriminator loss: 0.568636, acc: 0.679688] [adversarial loss: 1.137165, acc: 0.234375]\n",
      "10208: [discriminator loss: 0.531582, acc: 0.773438] [adversarial loss: 1.230655, acc: 0.203125]\n",
      "10209: [discriminator loss: 0.568584, acc: 0.679688] [adversarial loss: 1.010358, acc: 0.281250]\n",
      "10210: [discriminator loss: 0.525793, acc: 0.734375] [adversarial loss: 1.211289, acc: 0.156250]\n",
      "10211: [discriminator loss: 0.494714, acc: 0.765625] [adversarial loss: 1.228707, acc: 0.187500]\n",
      "10212: [discriminator loss: 0.512446, acc: 0.750000] [adversarial loss: 1.060763, acc: 0.296875]\n",
      "10213: [discriminator loss: 0.552655, acc: 0.671875] [adversarial loss: 1.037913, acc: 0.359375]\n",
      "10214: [discriminator loss: 0.576481, acc: 0.640625] [adversarial loss: 0.923719, acc: 0.437500]\n",
      "10215: [discriminator loss: 0.587389, acc: 0.671875] [adversarial loss: 0.990815, acc: 0.312500]\n",
      "10216: [discriminator loss: 0.533836, acc: 0.757812] [adversarial loss: 1.356689, acc: 0.218750]\n",
      "10217: [discriminator loss: 0.471302, acc: 0.757812] [adversarial loss: 1.300082, acc: 0.265625]\n",
      "10218: [discriminator loss: 0.581116, acc: 0.679688] [adversarial loss: 1.524458, acc: 0.109375]\n",
      "10219: [discriminator loss: 0.565920, acc: 0.679688] [adversarial loss: 0.880110, acc: 0.453125]\n",
      "10220: [discriminator loss: 0.585756, acc: 0.718750] [adversarial loss: 1.299710, acc: 0.171875]\n",
      "10221: [discriminator loss: 0.602748, acc: 0.679688] [adversarial loss: 1.075676, acc: 0.281250]\n",
      "10222: [discriminator loss: 0.553594, acc: 0.703125] [adversarial loss: 1.002990, acc: 0.281250]\n",
      "10223: [discriminator loss: 0.554926, acc: 0.718750] [adversarial loss: 1.200073, acc: 0.203125]\n",
      "10224: [discriminator loss: 0.481184, acc: 0.750000] [adversarial loss: 0.958794, acc: 0.328125]\n",
      "10225: [discriminator loss: 0.554503, acc: 0.687500] [adversarial loss: 1.089756, acc: 0.281250]\n",
      "10226: [discriminator loss: 0.516783, acc: 0.742188] [adversarial loss: 1.064529, acc: 0.312500]\n",
      "10227: [discriminator loss: 0.580392, acc: 0.695312] [adversarial loss: 0.924851, acc: 0.421875]\n",
      "10228: [discriminator loss: 0.572517, acc: 0.679688] [adversarial loss: 1.809712, acc: 0.031250]\n",
      "10229: [discriminator loss: 0.597767, acc: 0.679688] [adversarial loss: 0.946582, acc: 0.359375]\n",
      "10230: [discriminator loss: 0.581749, acc: 0.656250] [adversarial loss: 1.604742, acc: 0.031250]\n",
      "10231: [discriminator loss: 0.507884, acc: 0.726562] [adversarial loss: 0.908873, acc: 0.390625]\n",
      "10232: [discriminator loss: 0.584409, acc: 0.671875] [adversarial loss: 1.458692, acc: 0.171875]\n",
      "10233: [discriminator loss: 0.629919, acc: 0.656250] [adversarial loss: 1.064679, acc: 0.359375]\n",
      "10234: [discriminator loss: 0.514606, acc: 0.757812] [adversarial loss: 1.036845, acc: 0.296875]\n",
      "10235: [discriminator loss: 0.608192, acc: 0.617188] [adversarial loss: 0.861223, acc: 0.453125]\n",
      "10236: [discriminator loss: 0.533473, acc: 0.718750] [adversarial loss: 1.496098, acc: 0.140625]\n",
      "10237: [discriminator loss: 0.610181, acc: 0.679688] [adversarial loss: 1.028890, acc: 0.296875]\n",
      "10238: [discriminator loss: 0.533499, acc: 0.726562] [adversarial loss: 1.272760, acc: 0.140625]\n",
      "10239: [discriminator loss: 0.584457, acc: 0.679688] [adversarial loss: 0.810086, acc: 0.531250]\n",
      "10240: [discriminator loss: 0.573982, acc: 0.687500] [adversarial loss: 1.343059, acc: 0.187500]\n",
      "10241: [discriminator loss: 0.530060, acc: 0.734375] [adversarial loss: 0.955483, acc: 0.375000]\n",
      "10242: [discriminator loss: 0.548519, acc: 0.726562] [adversarial loss: 1.299067, acc: 0.156250]\n",
      "10243: [discriminator loss: 0.530439, acc: 0.742188] [adversarial loss: 1.003041, acc: 0.328125]\n",
      "10244: [discriminator loss: 0.500846, acc: 0.765625] [adversarial loss: 1.203593, acc: 0.218750]\n",
      "10245: [discriminator loss: 0.507047, acc: 0.750000] [adversarial loss: 1.051975, acc: 0.312500]\n",
      "10246: [discriminator loss: 0.518230, acc: 0.742188] [adversarial loss: 1.299181, acc: 0.218750]\n",
      "10247: [discriminator loss: 0.545381, acc: 0.687500] [adversarial loss: 0.998214, acc: 0.312500]\n",
      "10248: [discriminator loss: 0.601348, acc: 0.648438] [adversarial loss: 1.340165, acc: 0.156250]\n",
      "10249: [discriminator loss: 0.596400, acc: 0.625000] [adversarial loss: 0.936093, acc: 0.390625]\n",
      "10250: [discriminator loss: 0.504565, acc: 0.781250] [adversarial loss: 1.113784, acc: 0.265625]\n",
      "10251: [discriminator loss: 0.495979, acc: 0.726562] [adversarial loss: 1.161386, acc: 0.343750]\n",
      "10252: [discriminator loss: 0.570811, acc: 0.703125] [adversarial loss: 1.167505, acc: 0.265625]\n",
      "10253: [discriminator loss: 0.507135, acc: 0.757812] [adversarial loss: 1.091533, acc: 0.328125]\n",
      "10254: [discriminator loss: 0.586068, acc: 0.679688] [adversarial loss: 0.902117, acc: 0.343750]\n",
      "10255: [discriminator loss: 0.465072, acc: 0.804688] [adversarial loss: 1.355779, acc: 0.156250]\n",
      "10256: [discriminator loss: 0.583279, acc: 0.703125] [adversarial loss: 1.414687, acc: 0.125000]\n",
      "10257: [discriminator loss: 0.596939, acc: 0.664062] [adversarial loss: 0.947829, acc: 0.328125]\n",
      "10258: [discriminator loss: 0.577781, acc: 0.664062] [adversarial loss: 1.595283, acc: 0.062500]\n",
      "10259: [discriminator loss: 0.547019, acc: 0.734375] [adversarial loss: 1.201322, acc: 0.203125]\n",
      "10260: [discriminator loss: 0.548254, acc: 0.703125] [adversarial loss: 1.236374, acc: 0.265625]\n",
      "10261: [discriminator loss: 0.579039, acc: 0.718750] [adversarial loss: 0.911676, acc: 0.515625]\n",
      "10262: [discriminator loss: 0.526853, acc: 0.765625] [adversarial loss: 1.409521, acc: 0.234375]\n",
      "10263: [discriminator loss: 0.565990, acc: 0.687500] [adversarial loss: 0.997012, acc: 0.281250]\n",
      "10264: [discriminator loss: 0.492248, acc: 0.742188] [adversarial loss: 0.998936, acc: 0.328125]\n",
      "10265: [discriminator loss: 0.498868, acc: 0.781250] [adversarial loss: 1.477463, acc: 0.218750]\n",
      "10266: [discriminator loss: 0.488503, acc: 0.726562] [adversarial loss: 1.013929, acc: 0.281250]\n",
      "10267: [discriminator loss: 0.526170, acc: 0.750000] [adversarial loss: 1.174795, acc: 0.281250]\n",
      "10268: [discriminator loss: 0.491244, acc: 0.750000] [adversarial loss: 0.957000, acc: 0.390625]\n",
      "10269: [discriminator loss: 0.551681, acc: 0.710938] [adversarial loss: 1.658998, acc: 0.093750]\n",
      "10270: [discriminator loss: 0.584682, acc: 0.664062] [adversarial loss: 0.860739, acc: 0.437500]\n",
      "10271: [discriminator loss: 0.518281, acc: 0.742188] [adversarial loss: 1.405044, acc: 0.156250]\n",
      "10272: [discriminator loss: 0.728851, acc: 0.554688] [adversarial loss: 0.996143, acc: 0.359375]\n",
      "10273: [discriminator loss: 0.533120, acc: 0.765625] [adversarial loss: 1.478011, acc: 0.187500]\n",
      "10274: [discriminator loss: 0.558180, acc: 0.671875] [adversarial loss: 0.884261, acc: 0.421875]\n",
      "10275: [discriminator loss: 0.561608, acc: 0.726562] [adversarial loss: 1.240486, acc: 0.203125]\n",
      "10276: [discriminator loss: 0.521772, acc: 0.742188] [adversarial loss: 1.181571, acc: 0.250000]\n",
      "10277: [discriminator loss: 0.580412, acc: 0.656250] [adversarial loss: 1.201055, acc: 0.218750]\n",
      "10278: [discriminator loss: 0.557159, acc: 0.718750] [adversarial loss: 1.346561, acc: 0.156250]\n",
      "10279: [discriminator loss: 0.567172, acc: 0.687500] [adversarial loss: 0.984979, acc: 0.343750]\n",
      "10280: [discriminator loss: 0.574234, acc: 0.679688] [adversarial loss: 1.225694, acc: 0.187500]\n",
      "10281: [discriminator loss: 0.512395, acc: 0.773438] [adversarial loss: 0.873429, acc: 0.406250]\n",
      "10282: [discriminator loss: 0.550151, acc: 0.703125] [adversarial loss: 1.432397, acc: 0.156250]\n",
      "10283: [discriminator loss: 0.515527, acc: 0.765625] [adversarial loss: 0.995065, acc: 0.281250]\n",
      "10284: [discriminator loss: 0.608176, acc: 0.656250] [adversarial loss: 1.711706, acc: 0.046875]\n",
      "10285: [discriminator loss: 0.590575, acc: 0.640625] [adversarial loss: 0.857217, acc: 0.437500]\n",
      "10286: [discriminator loss: 0.535109, acc: 0.781250] [adversarial loss: 1.414174, acc: 0.140625]\n",
      "10287: [discriminator loss: 0.598361, acc: 0.695312] [adversarial loss: 0.954211, acc: 0.375000]\n",
      "10288: [discriminator loss: 0.520361, acc: 0.703125] [adversarial loss: 1.440511, acc: 0.078125]\n",
      "10289: [discriminator loss: 0.525962, acc: 0.695312] [adversarial loss: 1.124981, acc: 0.296875]\n",
      "10290: [discriminator loss: 0.558799, acc: 0.703125] [adversarial loss: 1.139555, acc: 0.328125]\n",
      "10291: [discriminator loss: 0.509057, acc: 0.742188] [adversarial loss: 0.887896, acc: 0.437500]\n",
      "10292: [discriminator loss: 0.620022, acc: 0.687500] [adversarial loss: 1.428630, acc: 0.109375]\n",
      "10293: [discriminator loss: 0.592848, acc: 0.648438] [adversarial loss: 1.100180, acc: 0.218750]\n",
      "10294: [discriminator loss: 0.534638, acc: 0.718750] [adversarial loss: 1.028466, acc: 0.234375]\n",
      "10295: [discriminator loss: 0.468048, acc: 0.789062] [adversarial loss: 1.151134, acc: 0.328125]\n",
      "10296: [discriminator loss: 0.568552, acc: 0.710938] [adversarial loss: 1.094535, acc: 0.296875]\n",
      "10297: [discriminator loss: 0.580819, acc: 0.695312] [adversarial loss: 1.311427, acc: 0.156250]\n",
      "10298: [discriminator loss: 0.562059, acc: 0.726562] [adversarial loss: 0.929129, acc: 0.375000]\n",
      "10299: [discriminator loss: 0.540302, acc: 0.734375] [adversarial loss: 1.348909, acc: 0.171875]\n",
      "10300: [discriminator loss: 0.602736, acc: 0.671875] [adversarial loss: 1.104291, acc: 0.296875]\n",
      "10301: [discriminator loss: 0.461768, acc: 0.742188] [adversarial loss: 1.417721, acc: 0.171875]\n",
      "10302: [discriminator loss: 0.590679, acc: 0.671875] [adversarial loss: 0.961598, acc: 0.359375]\n",
      "10303: [discriminator loss: 0.562547, acc: 0.664062] [adversarial loss: 1.374181, acc: 0.218750]\n",
      "10304: [discriminator loss: 0.614128, acc: 0.664062] [adversarial loss: 1.135421, acc: 0.296875]\n",
      "10305: [discriminator loss: 0.507475, acc: 0.773438] [adversarial loss: 1.143335, acc: 0.218750]\n",
      "10306: [discriminator loss: 0.578041, acc: 0.695312] [adversarial loss: 1.373491, acc: 0.156250]\n",
      "10307: [discriminator loss: 0.490895, acc: 0.750000] [adversarial loss: 0.993914, acc: 0.328125]\n",
      "10308: [discriminator loss: 0.610538, acc: 0.679688] [adversarial loss: 1.138464, acc: 0.187500]\n",
      "10309: [discriminator loss: 0.549092, acc: 0.679688] [adversarial loss: 1.093494, acc: 0.281250]\n",
      "10310: [discriminator loss: 0.528592, acc: 0.781250] [adversarial loss: 1.282162, acc: 0.187500]\n",
      "10311: [discriminator loss: 0.546193, acc: 0.718750] [adversarial loss: 0.973079, acc: 0.375000]\n",
      "10312: [discriminator loss: 0.582661, acc: 0.671875] [adversarial loss: 1.391061, acc: 0.203125]\n",
      "10313: [discriminator loss: 0.541705, acc: 0.703125] [adversarial loss: 1.029506, acc: 0.359375]\n",
      "10314: [discriminator loss: 0.474414, acc: 0.773438] [adversarial loss: 1.436575, acc: 0.093750]\n",
      "10315: [discriminator loss: 0.610080, acc: 0.671875] [adversarial loss: 0.789447, acc: 0.453125]\n",
      "10316: [discriminator loss: 0.517394, acc: 0.726562] [adversarial loss: 1.383815, acc: 0.187500]\n",
      "10317: [discriminator loss: 0.542001, acc: 0.710938] [adversarial loss: 1.080977, acc: 0.296875]\n",
      "10318: [discriminator loss: 0.581768, acc: 0.671875] [adversarial loss: 1.190658, acc: 0.218750]\n",
      "10319: [discriminator loss: 0.522272, acc: 0.710938] [adversarial loss: 1.007273, acc: 0.312500]\n",
      "10320: [discriminator loss: 0.587047, acc: 0.664062] [adversarial loss: 1.411589, acc: 0.140625]\n",
      "10321: [discriminator loss: 0.593050, acc: 0.671875] [adversarial loss: 1.014579, acc: 0.328125]\n",
      "10322: [discriminator loss: 0.533163, acc: 0.695312] [adversarial loss: 1.323252, acc: 0.250000]\n",
      "10323: [discriminator loss: 0.530555, acc: 0.710938] [adversarial loss: 0.845039, acc: 0.468750]\n",
      "10324: [discriminator loss: 0.497557, acc: 0.789062] [adversarial loss: 1.483386, acc: 0.078125]\n",
      "10325: [discriminator loss: 0.569259, acc: 0.703125] [adversarial loss: 0.843865, acc: 0.484375]\n",
      "10326: [discriminator loss: 0.518731, acc: 0.703125] [adversarial loss: 1.402142, acc: 0.140625]\n",
      "10327: [discriminator loss: 0.585056, acc: 0.640625] [adversarial loss: 1.034281, acc: 0.265625]\n",
      "10328: [discriminator loss: 0.556850, acc: 0.742188] [adversarial loss: 1.123306, acc: 0.250000]\n",
      "10329: [discriminator loss: 0.485732, acc: 0.765625] [adversarial loss: 1.258313, acc: 0.218750]\n",
      "10330: [discriminator loss: 0.591438, acc: 0.656250] [adversarial loss: 1.090641, acc: 0.296875]\n",
      "10331: [discriminator loss: 0.488555, acc: 0.757812] [adversarial loss: 1.211831, acc: 0.218750]\n",
      "10332: [discriminator loss: 0.502424, acc: 0.750000] [adversarial loss: 0.903725, acc: 0.390625]\n",
      "10333: [discriminator loss: 0.515073, acc: 0.765625] [adversarial loss: 1.219601, acc: 0.187500]\n",
      "10334: [discriminator loss: 0.488910, acc: 0.781250] [adversarial loss: 1.131938, acc: 0.281250]\n",
      "10335: [discriminator loss: 0.483903, acc: 0.750000] [adversarial loss: 1.049765, acc: 0.328125]\n",
      "10336: [discriminator loss: 0.606410, acc: 0.687500] [adversarial loss: 1.279474, acc: 0.203125]\n",
      "10337: [discriminator loss: 0.569693, acc: 0.703125] [adversarial loss: 1.289703, acc: 0.156250]\n",
      "10338: [discriminator loss: 0.532987, acc: 0.718750] [adversarial loss: 1.205406, acc: 0.218750]\n",
      "10339: [discriminator loss: 0.513245, acc: 0.679688] [adversarial loss: 1.082230, acc: 0.328125]\n",
      "10340: [discriminator loss: 0.634117, acc: 0.648438] [adversarial loss: 1.353008, acc: 0.218750]\n",
      "10341: [discriminator loss: 0.530622, acc: 0.695312] [adversarial loss: 1.165701, acc: 0.125000]\n",
      "10342: [discriminator loss: 0.575757, acc: 0.703125] [adversarial loss: 1.421867, acc: 0.203125]\n",
      "10343: [discriminator loss: 0.584233, acc: 0.671875] [adversarial loss: 0.918828, acc: 0.343750]\n",
      "10344: [discriminator loss: 0.536191, acc: 0.734375] [adversarial loss: 1.495444, acc: 0.125000]\n",
      "10345: [discriminator loss: 0.547910, acc: 0.710938] [adversarial loss: 0.762802, acc: 0.484375]\n",
      "10346: [discriminator loss: 0.555527, acc: 0.781250] [adversarial loss: 1.347954, acc: 0.140625]\n",
      "10347: [discriminator loss: 0.492801, acc: 0.773438] [adversarial loss: 0.937217, acc: 0.421875]\n",
      "10348: [discriminator loss: 0.555413, acc: 0.710938] [adversarial loss: 1.389544, acc: 0.140625]\n",
      "10349: [discriminator loss: 0.523218, acc: 0.726562] [adversarial loss: 1.197555, acc: 0.250000]\n",
      "10350: [discriminator loss: 0.499332, acc: 0.765625] [adversarial loss: 1.349930, acc: 0.171875]\n",
      "10351: [discriminator loss: 0.588576, acc: 0.648438] [adversarial loss: 0.806009, acc: 0.484375]\n",
      "10352: [discriminator loss: 0.632987, acc: 0.625000] [adversarial loss: 1.344665, acc: 0.203125]\n",
      "10353: [discriminator loss: 0.549508, acc: 0.671875] [adversarial loss: 0.937941, acc: 0.421875]\n",
      "10354: [discriminator loss: 0.569606, acc: 0.703125] [adversarial loss: 1.211612, acc: 0.250000]\n",
      "10355: [discriminator loss: 0.588479, acc: 0.695312] [adversarial loss: 1.160734, acc: 0.265625]\n",
      "10356: [discriminator loss: 0.564399, acc: 0.695312] [adversarial loss: 1.067343, acc: 0.281250]\n",
      "10357: [discriminator loss: 0.594288, acc: 0.640625] [adversarial loss: 1.286403, acc: 0.218750]\n",
      "10358: [discriminator loss: 0.543964, acc: 0.710938] [adversarial loss: 1.019935, acc: 0.296875]\n",
      "10359: [discriminator loss: 0.535973, acc: 0.703125] [adversarial loss: 1.335330, acc: 0.218750]\n",
      "10360: [discriminator loss: 0.554635, acc: 0.703125] [adversarial loss: 1.098552, acc: 0.312500]\n",
      "10361: [discriminator loss: 0.534445, acc: 0.757812] [adversarial loss: 1.145434, acc: 0.328125]\n",
      "10362: [discriminator loss: 0.498219, acc: 0.765625] [adversarial loss: 1.164153, acc: 0.296875]\n",
      "10363: [discriminator loss: 0.498207, acc: 0.710938] [adversarial loss: 1.242249, acc: 0.265625]\n",
      "10364: [discriminator loss: 0.577358, acc: 0.703125] [adversarial loss: 1.092752, acc: 0.281250]\n",
      "10365: [discriminator loss: 0.558836, acc: 0.718750] [adversarial loss: 0.735994, acc: 0.468750]\n",
      "10366: [discriminator loss: 0.553444, acc: 0.679688] [adversarial loss: 1.837749, acc: 0.046875]\n",
      "10367: [discriminator loss: 0.601542, acc: 0.656250] [adversarial loss: 0.910494, acc: 0.437500]\n",
      "10368: [discriminator loss: 0.517223, acc: 0.742188] [adversarial loss: 1.277037, acc: 0.140625]\n",
      "10369: [discriminator loss: 0.614518, acc: 0.671875] [adversarial loss: 0.944003, acc: 0.421875]\n",
      "10370: [discriminator loss: 0.534550, acc: 0.718750] [adversarial loss: 1.231905, acc: 0.156250]\n",
      "10371: [discriminator loss: 0.519713, acc: 0.726562] [adversarial loss: 0.973192, acc: 0.390625]\n",
      "10372: [discriminator loss: 0.443381, acc: 0.812500] [adversarial loss: 1.516548, acc: 0.093750]\n",
      "10373: [discriminator loss: 0.492177, acc: 0.773438] [adversarial loss: 0.997921, acc: 0.281250]\n",
      "10374: [discriminator loss: 0.549330, acc: 0.679688] [adversarial loss: 1.298725, acc: 0.125000]\n",
      "10375: [discriminator loss: 0.599359, acc: 0.671875] [adversarial loss: 0.998191, acc: 0.281250]\n",
      "10376: [discriminator loss: 0.529232, acc: 0.757812] [adversarial loss: 1.180769, acc: 0.187500]\n",
      "10377: [discriminator loss: 0.461370, acc: 0.789062] [adversarial loss: 1.018306, acc: 0.375000]\n",
      "10378: [discriminator loss: 0.533631, acc: 0.726562] [adversarial loss: 1.200760, acc: 0.265625]\n",
      "10379: [discriminator loss: 0.582146, acc: 0.734375] [adversarial loss: 1.206294, acc: 0.234375]\n",
      "10380: [discriminator loss: 0.607299, acc: 0.679688] [adversarial loss: 0.932096, acc: 0.453125]\n",
      "10381: [discriminator loss: 0.568934, acc: 0.710938] [adversarial loss: 1.424019, acc: 0.187500]\n",
      "10382: [discriminator loss: 0.585410, acc: 0.664062] [adversarial loss: 1.016010, acc: 0.328125]\n",
      "10383: [discriminator loss: 0.588849, acc: 0.687500] [adversarial loss: 1.356458, acc: 0.140625]\n",
      "10384: [discriminator loss: 0.574783, acc: 0.671875] [adversarial loss: 0.910722, acc: 0.375000]\n",
      "10385: [discriminator loss: 0.620048, acc: 0.718750] [adversarial loss: 1.568993, acc: 0.078125]\n",
      "10386: [discriminator loss: 0.549754, acc: 0.703125] [adversarial loss: 0.994525, acc: 0.312500]\n",
      "10387: [discriminator loss: 0.548649, acc: 0.710938] [adversarial loss: 1.221422, acc: 0.203125]\n",
      "10388: [discriminator loss: 0.591674, acc: 0.601562] [adversarial loss: 1.246868, acc: 0.203125]\n",
      "10389: [discriminator loss: 0.525313, acc: 0.742188] [adversarial loss: 1.047391, acc: 0.234375]\n",
      "10390: [discriminator loss: 0.617305, acc: 0.648438] [adversarial loss: 1.152567, acc: 0.281250]\n",
      "10391: [discriminator loss: 0.518380, acc: 0.781250] [adversarial loss: 0.993102, acc: 0.312500]\n",
      "10392: [discriminator loss: 0.623147, acc: 0.687500] [adversarial loss: 1.344336, acc: 0.125000]\n",
      "10393: [discriminator loss: 0.556331, acc: 0.726562] [adversarial loss: 1.325274, acc: 0.187500]\n",
      "10394: [discriminator loss: 0.489195, acc: 0.773438] [adversarial loss: 1.031235, acc: 0.312500]\n",
      "10395: [discriminator loss: 0.562964, acc: 0.718750] [adversarial loss: 1.273163, acc: 0.156250]\n",
      "10396: [discriminator loss: 0.555351, acc: 0.703125] [adversarial loss: 0.869069, acc: 0.437500]\n",
      "10397: [discriminator loss: 0.578309, acc: 0.687500] [adversarial loss: 1.444800, acc: 0.078125]\n",
      "10398: [discriminator loss: 0.549393, acc: 0.742188] [adversarial loss: 0.761866, acc: 0.531250]\n",
      "10399: [discriminator loss: 0.604612, acc: 0.648438] [adversarial loss: 1.591969, acc: 0.046875]\n",
      "10400: [discriminator loss: 0.640053, acc: 0.632812] [adversarial loss: 1.005603, acc: 0.375000]\n",
      "10401: [discriminator loss: 0.558875, acc: 0.742188] [adversarial loss: 1.021288, acc: 0.312500]\n",
      "10402: [discriminator loss: 0.508078, acc: 0.757812] [adversarial loss: 1.224203, acc: 0.171875]\n",
      "10403: [discriminator loss: 0.529165, acc: 0.703125] [adversarial loss: 1.210131, acc: 0.203125]\n",
      "10404: [discriminator loss: 0.532526, acc: 0.757812] [adversarial loss: 1.207479, acc: 0.218750]\n",
      "10405: [discriminator loss: 0.520196, acc: 0.757812] [adversarial loss: 1.030385, acc: 0.328125]\n",
      "10406: [discriminator loss: 0.572772, acc: 0.718750] [adversarial loss: 1.145420, acc: 0.203125]\n",
      "10407: [discriminator loss: 0.480566, acc: 0.820312] [adversarial loss: 1.057064, acc: 0.250000]\n",
      "10408: [discriminator loss: 0.564190, acc: 0.726562] [adversarial loss: 1.243073, acc: 0.187500]\n",
      "10409: [discriminator loss: 0.600784, acc: 0.664062] [adversarial loss: 1.078048, acc: 0.296875]\n",
      "10410: [discriminator loss: 0.492311, acc: 0.820312] [adversarial loss: 1.305042, acc: 0.171875]\n",
      "10411: [discriminator loss: 0.507048, acc: 0.773438] [adversarial loss: 0.972430, acc: 0.375000]\n",
      "10412: [discriminator loss: 0.560216, acc: 0.687500] [adversarial loss: 1.168876, acc: 0.250000]\n",
      "10413: [discriminator loss: 0.536370, acc: 0.695312] [adversarial loss: 1.071814, acc: 0.359375]\n",
      "10414: [discriminator loss: 0.511129, acc: 0.695312] [adversarial loss: 1.247575, acc: 0.171875]\n",
      "10415: [discriminator loss: 0.549757, acc: 0.726562] [adversarial loss: 1.261555, acc: 0.125000]\n",
      "10416: [discriminator loss: 0.516786, acc: 0.742188] [adversarial loss: 0.900164, acc: 0.421875]\n",
      "10417: [discriminator loss: 0.519337, acc: 0.710938] [adversarial loss: 1.230572, acc: 0.250000]\n",
      "10418: [discriminator loss: 0.512951, acc: 0.742188] [adversarial loss: 1.177852, acc: 0.250000]\n",
      "10419: [discriminator loss: 0.478363, acc: 0.781250] [adversarial loss: 1.252224, acc: 0.218750]\n",
      "10420: [discriminator loss: 0.497739, acc: 0.773438] [adversarial loss: 0.933948, acc: 0.453125]\n",
      "10421: [discriminator loss: 0.544915, acc: 0.687500] [adversarial loss: 1.442245, acc: 0.109375]\n",
      "10422: [discriminator loss: 0.625863, acc: 0.664062] [adversarial loss: 0.691567, acc: 0.609375]\n",
      "10423: [discriminator loss: 0.559759, acc: 0.695312] [adversarial loss: 1.800704, acc: 0.046875]\n",
      "10424: [discriminator loss: 0.576171, acc: 0.710938] [adversarial loss: 0.884453, acc: 0.421875]\n",
      "10425: [discriminator loss: 0.586489, acc: 0.632812] [adversarial loss: 1.412978, acc: 0.156250]\n",
      "10426: [discriminator loss: 0.554561, acc: 0.664062] [adversarial loss: 0.868679, acc: 0.406250]\n",
      "10427: [discriminator loss: 0.519924, acc: 0.726562] [adversarial loss: 1.381623, acc: 0.171875]\n",
      "10428: [discriminator loss: 0.522510, acc: 0.750000] [adversarial loss: 1.078362, acc: 0.265625]\n",
      "10429: [discriminator loss: 0.535802, acc: 0.734375] [adversarial loss: 0.947256, acc: 0.390625]\n",
      "10430: [discriminator loss: 0.537964, acc: 0.726562] [adversarial loss: 1.160518, acc: 0.265625]\n",
      "10431: [discriminator loss: 0.559909, acc: 0.718750] [adversarial loss: 1.162564, acc: 0.234375]\n",
      "10432: [discriminator loss: 0.485760, acc: 0.796875] [adversarial loss: 1.223804, acc: 0.250000]\n",
      "10433: [discriminator loss: 0.589478, acc: 0.671875] [adversarial loss: 1.051556, acc: 0.359375]\n",
      "10434: [discriminator loss: 0.585276, acc: 0.664062] [adversarial loss: 1.203880, acc: 0.265625]\n",
      "10435: [discriminator loss: 0.529601, acc: 0.703125] [adversarial loss: 1.187675, acc: 0.312500]\n",
      "10436: [discriminator loss: 0.516257, acc: 0.718750] [adversarial loss: 1.185422, acc: 0.187500]\n",
      "10437: [discriminator loss: 0.570904, acc: 0.726562] [adversarial loss: 1.230603, acc: 0.187500]\n",
      "10438: [discriminator loss: 0.605974, acc: 0.656250] [adversarial loss: 1.199251, acc: 0.203125]\n",
      "10439: [discriminator loss: 0.522783, acc: 0.710938] [adversarial loss: 1.012376, acc: 0.375000]\n",
      "10440: [discriminator loss: 0.594215, acc: 0.625000] [adversarial loss: 1.524339, acc: 0.109375]\n",
      "10441: [discriminator loss: 0.560493, acc: 0.718750] [adversarial loss: 1.003200, acc: 0.343750]\n",
      "10442: [discriminator loss: 0.510752, acc: 0.765625] [adversarial loss: 1.365165, acc: 0.125000]\n",
      "10443: [discriminator loss: 0.596508, acc: 0.664062] [adversarial loss: 0.840635, acc: 0.359375]\n",
      "10444: [discriminator loss: 0.521365, acc: 0.726562] [adversarial loss: 1.312244, acc: 0.218750]\n",
      "10445: [discriminator loss: 0.531475, acc: 0.710938] [adversarial loss: 1.166208, acc: 0.156250]\n",
      "10446: [discriminator loss: 0.579024, acc: 0.703125] [adversarial loss: 1.087845, acc: 0.281250]\n",
      "10447: [discriminator loss: 0.514929, acc: 0.750000] [adversarial loss: 1.377444, acc: 0.171875]\n",
      "10448: [discriminator loss: 0.521698, acc: 0.726562] [adversarial loss: 1.051140, acc: 0.312500]\n",
      "10449: [discriminator loss: 0.563762, acc: 0.703125] [adversarial loss: 1.194574, acc: 0.187500]\n",
      "10450: [discriminator loss: 0.502751, acc: 0.757812] [adversarial loss: 0.856233, acc: 0.468750]\n",
      "10451: [discriminator loss: 0.655387, acc: 0.656250] [adversarial loss: 1.426164, acc: 0.109375]\n",
      "10452: [discriminator loss: 0.561908, acc: 0.742188] [adversarial loss: 1.177437, acc: 0.234375]\n",
      "10453: [discriminator loss: 0.578282, acc: 0.648438] [adversarial loss: 1.268417, acc: 0.171875]\n",
      "10454: [discriminator loss: 0.525600, acc: 0.742188] [adversarial loss: 0.920335, acc: 0.406250]\n",
      "10455: [discriminator loss: 0.548688, acc: 0.734375] [adversarial loss: 1.464150, acc: 0.203125]\n",
      "10456: [discriminator loss: 0.563753, acc: 0.671875] [adversarial loss: 1.053046, acc: 0.281250]\n",
      "10457: [discriminator loss: 0.574008, acc: 0.695312] [adversarial loss: 1.387298, acc: 0.125000]\n",
      "10458: [discriminator loss: 0.539733, acc: 0.718750] [adversarial loss: 0.931009, acc: 0.468750]\n",
      "10459: [discriminator loss: 0.622281, acc: 0.640625] [adversarial loss: 1.598957, acc: 0.140625]\n",
      "10460: [discriminator loss: 0.558532, acc: 0.734375] [adversarial loss: 1.204516, acc: 0.250000]\n",
      "10461: [discriminator loss: 0.447460, acc: 0.843750] [adversarial loss: 1.391004, acc: 0.265625]\n",
      "10462: [discriminator loss: 0.529144, acc: 0.726562] [adversarial loss: 1.014892, acc: 0.343750]\n",
      "10463: [discriminator loss: 0.553509, acc: 0.695312] [adversarial loss: 0.982563, acc: 0.250000]\n",
      "10464: [discriminator loss: 0.529263, acc: 0.765625] [adversarial loss: 1.206451, acc: 0.265625]\n",
      "10465: [discriminator loss: 0.585745, acc: 0.679688] [adversarial loss: 0.895696, acc: 0.406250]\n",
      "10466: [discriminator loss: 0.547519, acc: 0.718750] [adversarial loss: 1.134540, acc: 0.265625]\n",
      "10467: [discriminator loss: 0.574164, acc: 0.710938] [adversarial loss: 0.926972, acc: 0.390625]\n",
      "10468: [discriminator loss: 0.539413, acc: 0.703125] [adversarial loss: 1.164201, acc: 0.203125]\n",
      "10469: [discriminator loss: 0.470933, acc: 0.796875] [adversarial loss: 0.911484, acc: 0.421875]\n",
      "10470: [discriminator loss: 0.589188, acc: 0.710938] [adversarial loss: 1.405232, acc: 0.140625]\n",
      "10471: [discriminator loss: 0.523158, acc: 0.757812] [adversarial loss: 0.759934, acc: 0.500000]\n",
      "10472: [discriminator loss: 0.609500, acc: 0.625000] [adversarial loss: 1.518168, acc: 0.125000]\n",
      "10473: [discriminator loss: 0.502617, acc: 0.742188] [adversarial loss: 1.281843, acc: 0.140625]\n",
      "10474: [discriminator loss: 0.564097, acc: 0.695312] [adversarial loss: 1.097298, acc: 0.250000]\n",
      "10475: [discriminator loss: 0.542267, acc: 0.710938] [adversarial loss: 1.317831, acc: 0.218750]\n",
      "10476: [discriminator loss: 0.600364, acc: 0.671875] [adversarial loss: 0.978007, acc: 0.359375]\n",
      "10477: [discriminator loss: 0.564685, acc: 0.718750] [adversarial loss: 1.452060, acc: 0.218750]\n",
      "10478: [discriminator loss: 0.541477, acc: 0.726562] [adversarial loss: 1.068527, acc: 0.343750]\n",
      "10479: [discriminator loss: 0.510981, acc: 0.742188] [adversarial loss: 1.469473, acc: 0.109375]\n",
      "10480: [discriminator loss: 0.597225, acc: 0.726562] [adversarial loss: 1.070728, acc: 0.218750]\n",
      "10481: [discriminator loss: 0.518930, acc: 0.734375] [adversarial loss: 1.149719, acc: 0.359375]\n",
      "10482: [discriminator loss: 0.542789, acc: 0.765625] [adversarial loss: 1.218419, acc: 0.250000]\n",
      "10483: [discriminator loss: 0.619484, acc: 0.632812] [adversarial loss: 1.156160, acc: 0.250000]\n",
      "10484: [discriminator loss: 0.578019, acc: 0.703125] [adversarial loss: 0.912610, acc: 0.343750]\n",
      "10485: [discriminator loss: 0.566393, acc: 0.703125] [adversarial loss: 1.413560, acc: 0.125000]\n",
      "10486: [discriminator loss: 0.526849, acc: 0.703125] [adversarial loss: 0.773248, acc: 0.515625]\n",
      "10487: [discriminator loss: 0.576726, acc: 0.710938] [adversarial loss: 1.436773, acc: 0.171875]\n",
      "10488: [discriminator loss: 0.586762, acc: 0.718750] [adversarial loss: 0.891195, acc: 0.390625]\n",
      "10489: [discriminator loss: 0.611158, acc: 0.687500] [adversarial loss: 1.469757, acc: 0.140625]\n",
      "10490: [discriminator loss: 0.595679, acc: 0.664062] [adversarial loss: 1.062630, acc: 0.296875]\n",
      "10491: [discriminator loss: 0.544256, acc: 0.726562] [adversarial loss: 1.038846, acc: 0.234375]\n",
      "10492: [discriminator loss: 0.580763, acc: 0.671875] [adversarial loss: 1.088059, acc: 0.281250]\n",
      "10493: [discriminator loss: 0.491569, acc: 0.742188] [adversarial loss: 1.147776, acc: 0.218750]\n",
      "10494: [discriminator loss: 0.506154, acc: 0.710938] [adversarial loss: 0.998204, acc: 0.421875]\n",
      "10495: [discriminator loss: 0.593607, acc: 0.679688] [adversarial loss: 1.132238, acc: 0.250000]\n",
      "10496: [discriminator loss: 0.493268, acc: 0.726562] [adversarial loss: 1.055724, acc: 0.312500]\n",
      "10497: [discriminator loss: 0.527470, acc: 0.734375] [adversarial loss: 1.152348, acc: 0.250000]\n",
      "10498: [discriminator loss: 0.516613, acc: 0.718750] [adversarial loss: 1.072173, acc: 0.359375]\n",
      "10499: [discriminator loss: 0.586286, acc: 0.695312] [adversarial loss: 1.377828, acc: 0.140625]\n",
      "10500: [discriminator loss: 0.557599, acc: 0.687500] [adversarial loss: 0.997123, acc: 0.390625]\n",
      "10501: [discriminator loss: 0.580903, acc: 0.734375] [adversarial loss: 1.227399, acc: 0.156250]\n",
      "10502: [discriminator loss: 0.512843, acc: 0.718750] [adversarial loss: 0.907416, acc: 0.343750]\n",
      "10503: [discriminator loss: 0.530571, acc: 0.757812] [adversarial loss: 1.224979, acc: 0.281250]\n",
      "10504: [discriminator loss: 0.470540, acc: 0.742188] [adversarial loss: 1.031649, acc: 0.312500]\n",
      "10505: [discriminator loss: 0.550152, acc: 0.703125] [adversarial loss: 0.954937, acc: 0.390625]\n",
      "10506: [discriminator loss: 0.531057, acc: 0.695312] [adversarial loss: 1.345536, acc: 0.156250]\n",
      "10507: [discriminator loss: 0.507747, acc: 0.750000] [adversarial loss: 0.809771, acc: 0.484375]\n",
      "10508: [discriminator loss: 0.597121, acc: 0.648438] [adversarial loss: 1.325044, acc: 0.171875]\n",
      "10509: [discriminator loss: 0.524297, acc: 0.718750] [adversarial loss: 0.811775, acc: 0.500000]\n",
      "10510: [discriminator loss: 0.563366, acc: 0.664062] [adversarial loss: 1.533348, acc: 0.156250]\n",
      "10511: [discriminator loss: 0.595539, acc: 0.671875] [adversarial loss: 1.008083, acc: 0.328125]\n",
      "10512: [discriminator loss: 0.605949, acc: 0.671875] [adversarial loss: 0.971304, acc: 0.359375]\n",
      "10513: [discriminator loss: 0.551677, acc: 0.734375] [adversarial loss: 1.207141, acc: 0.218750]\n",
      "10514: [discriminator loss: 0.583912, acc: 0.656250] [adversarial loss: 1.239047, acc: 0.218750]\n",
      "10515: [discriminator loss: 0.520674, acc: 0.757812] [adversarial loss: 1.036540, acc: 0.312500]\n",
      "10516: [discriminator loss: 0.575231, acc: 0.664062] [adversarial loss: 1.501797, acc: 0.125000]\n",
      "10517: [discriminator loss: 0.579584, acc: 0.679688] [adversarial loss: 0.801069, acc: 0.500000]\n",
      "10518: [discriminator loss: 0.484710, acc: 0.789062] [adversarial loss: 1.096400, acc: 0.296875]\n",
      "10519: [discriminator loss: 0.540293, acc: 0.726562] [adversarial loss: 1.126050, acc: 0.296875]\n",
      "10520: [discriminator loss: 0.574886, acc: 0.695312] [adversarial loss: 1.346286, acc: 0.218750]\n",
      "10521: [discriminator loss: 0.531853, acc: 0.687500] [adversarial loss: 0.891161, acc: 0.406250]\n",
      "10522: [discriminator loss: 0.554488, acc: 0.703125] [adversarial loss: 1.667246, acc: 0.140625]\n",
      "10523: [discriminator loss: 0.587358, acc: 0.710938] [adversarial loss: 1.238812, acc: 0.171875]\n",
      "10524: [discriminator loss: 0.582670, acc: 0.679688] [adversarial loss: 0.987727, acc: 0.406250]\n",
      "10525: [discriminator loss: 0.559120, acc: 0.695312] [adversarial loss: 1.252794, acc: 0.156250]\n",
      "10526: [discriminator loss: 0.526384, acc: 0.781250] [adversarial loss: 1.087993, acc: 0.250000]\n",
      "10527: [discriminator loss: 0.593472, acc: 0.695312] [adversarial loss: 1.136194, acc: 0.250000]\n",
      "10528: [discriminator loss: 0.574730, acc: 0.734375] [adversarial loss: 1.097757, acc: 0.343750]\n",
      "10529: [discriminator loss: 0.550320, acc: 0.687500] [adversarial loss: 1.107611, acc: 0.296875]\n",
      "10530: [discriminator loss: 0.512083, acc: 0.773438] [adversarial loss: 1.210477, acc: 0.234375]\n",
      "10531: [discriminator loss: 0.533894, acc: 0.703125] [adversarial loss: 1.223078, acc: 0.343750]\n",
      "10532: [discriminator loss: 0.594842, acc: 0.703125] [adversarial loss: 1.184489, acc: 0.250000]\n",
      "10533: [discriminator loss: 0.497185, acc: 0.757812] [adversarial loss: 1.162047, acc: 0.250000]\n",
      "10534: [discriminator loss: 0.545847, acc: 0.687500] [adversarial loss: 1.041872, acc: 0.359375]\n",
      "10535: [discriminator loss: 0.560251, acc: 0.664062] [adversarial loss: 1.292998, acc: 0.171875]\n",
      "10536: [discriminator loss: 0.542137, acc: 0.734375] [adversarial loss: 0.808697, acc: 0.468750]\n",
      "10537: [discriminator loss: 0.521313, acc: 0.710938] [adversarial loss: 1.413611, acc: 0.109375]\n",
      "10538: [discriminator loss: 0.596335, acc: 0.671875] [adversarial loss: 0.713905, acc: 0.546875]\n",
      "10539: [discriminator loss: 0.563679, acc: 0.687500] [adversarial loss: 1.731217, acc: 0.078125]\n",
      "10540: [discriminator loss: 0.605343, acc: 0.671875] [adversarial loss: 0.894093, acc: 0.406250]\n",
      "10541: [discriminator loss: 0.572364, acc: 0.703125] [adversarial loss: 1.448968, acc: 0.125000]\n",
      "10542: [discriminator loss: 0.590740, acc: 0.648438] [adversarial loss: 0.929431, acc: 0.437500]\n",
      "10543: [discriminator loss: 0.546405, acc: 0.718750] [adversarial loss: 1.373203, acc: 0.187500]\n",
      "10544: [discriminator loss: 0.562766, acc: 0.718750] [adversarial loss: 0.904479, acc: 0.421875]\n",
      "10545: [discriminator loss: 0.542091, acc: 0.742188] [adversarial loss: 1.146378, acc: 0.250000]\n",
      "10546: [discriminator loss: 0.500002, acc: 0.757812] [adversarial loss: 1.092117, acc: 0.328125]\n",
      "10547: [discriminator loss: 0.455087, acc: 0.773438] [adversarial loss: 1.271011, acc: 0.156250]\n",
      "10548: [discriminator loss: 0.527413, acc: 0.765625] [adversarial loss: 1.135352, acc: 0.281250]\n",
      "10549: [discriminator loss: 0.531495, acc: 0.726562] [adversarial loss: 0.986676, acc: 0.421875]\n",
      "10550: [discriminator loss: 0.485469, acc: 0.781250] [adversarial loss: 1.210180, acc: 0.218750]\n",
      "10551: [discriminator loss: 0.591322, acc: 0.640625] [adversarial loss: 1.000522, acc: 0.343750]\n",
      "10552: [discriminator loss: 0.532277, acc: 0.726562] [adversarial loss: 1.353386, acc: 0.140625]\n",
      "10553: [discriminator loss: 0.548192, acc: 0.710938] [adversarial loss: 1.090981, acc: 0.312500]\n",
      "10554: [discriminator loss: 0.549320, acc: 0.757812] [adversarial loss: 1.557297, acc: 0.078125]\n",
      "10555: [discriminator loss: 0.533766, acc: 0.710938] [adversarial loss: 0.934365, acc: 0.375000]\n",
      "10556: [discriminator loss: 0.530428, acc: 0.703125] [adversarial loss: 1.449792, acc: 0.078125]\n",
      "10557: [discriminator loss: 0.575741, acc: 0.679688] [adversarial loss: 0.959889, acc: 0.343750]\n",
      "10558: [discriminator loss: 0.521582, acc: 0.742188] [adversarial loss: 1.367175, acc: 0.140625]\n",
      "10559: [discriminator loss: 0.532123, acc: 0.765625] [adversarial loss: 0.800777, acc: 0.468750]\n",
      "10560: [discriminator loss: 0.613548, acc: 0.664062] [adversarial loss: 1.176731, acc: 0.203125]\n",
      "10561: [discriminator loss: 0.548666, acc: 0.671875] [adversarial loss: 1.052461, acc: 0.343750]\n",
      "10562: [discriminator loss: 0.575131, acc: 0.703125] [adversarial loss: 1.339640, acc: 0.171875]\n",
      "10563: [discriminator loss: 0.608723, acc: 0.664062] [adversarial loss: 0.899332, acc: 0.406250]\n",
      "10564: [discriminator loss: 0.552725, acc: 0.710938] [adversarial loss: 1.179907, acc: 0.250000]\n",
      "10565: [discriminator loss: 0.543295, acc: 0.726562] [adversarial loss: 1.201877, acc: 0.218750]\n",
      "10566: [discriminator loss: 0.562382, acc: 0.703125] [adversarial loss: 1.206991, acc: 0.171875]\n",
      "10567: [discriminator loss: 0.554192, acc: 0.679688] [adversarial loss: 1.038036, acc: 0.328125]\n",
      "10568: [discriminator loss: 0.551456, acc: 0.726562] [adversarial loss: 1.365820, acc: 0.093750]\n",
      "10569: [discriminator loss: 0.602080, acc: 0.695312] [adversarial loss: 0.883257, acc: 0.437500]\n",
      "10570: [discriminator loss: 0.536785, acc: 0.734375] [adversarial loss: 1.606179, acc: 0.109375]\n",
      "10571: [discriminator loss: 0.510796, acc: 0.773438] [adversarial loss: 1.085955, acc: 0.312500]\n",
      "10572: [discriminator loss: 0.505823, acc: 0.734375] [adversarial loss: 1.234179, acc: 0.218750]\n",
      "10573: [discriminator loss: 0.494293, acc: 0.718750] [adversarial loss: 0.970804, acc: 0.421875]\n",
      "10574: [discriminator loss: 0.616324, acc: 0.695312] [adversarial loss: 1.322475, acc: 0.203125]\n",
      "10575: [discriminator loss: 0.487348, acc: 0.773438] [adversarial loss: 1.075355, acc: 0.375000]\n",
      "10576: [discriminator loss: 0.578858, acc: 0.664062] [adversarial loss: 1.341208, acc: 0.203125]\n",
      "10577: [discriminator loss: 0.525996, acc: 0.726562] [adversarial loss: 1.084370, acc: 0.359375]\n",
      "10578: [discriminator loss: 0.573029, acc: 0.679688] [adversarial loss: 1.193993, acc: 0.281250]\n",
      "10579: [discriminator loss: 0.584151, acc: 0.664062] [adversarial loss: 1.253415, acc: 0.156250]\n",
      "10580: [discriminator loss: 0.600315, acc: 0.648438] [adversarial loss: 1.161210, acc: 0.312500]\n",
      "10581: [discriminator loss: 0.600040, acc: 0.632812] [adversarial loss: 1.077847, acc: 0.265625]\n",
      "10582: [discriminator loss: 0.529810, acc: 0.703125] [adversarial loss: 1.024603, acc: 0.359375]\n",
      "10583: [discriminator loss: 0.584485, acc: 0.664062] [adversarial loss: 1.236284, acc: 0.171875]\n",
      "10584: [discriminator loss: 0.511842, acc: 0.750000] [adversarial loss: 1.044098, acc: 0.343750]\n",
      "10585: [discriminator loss: 0.462627, acc: 0.796875] [adversarial loss: 1.280764, acc: 0.187500]\n",
      "10586: [discriminator loss: 0.578918, acc: 0.710938] [adversarial loss: 1.008000, acc: 0.328125]\n",
      "10587: [discriminator loss: 0.629668, acc: 0.671875] [adversarial loss: 1.384586, acc: 0.171875]\n",
      "10588: [discriminator loss: 0.531241, acc: 0.710938] [adversarial loss: 1.079741, acc: 0.203125]\n",
      "10589: [discriminator loss: 0.543834, acc: 0.750000] [adversarial loss: 1.281260, acc: 0.171875]\n",
      "10590: [discriminator loss: 0.608157, acc: 0.671875] [adversarial loss: 0.830173, acc: 0.484375]\n",
      "10591: [discriminator loss: 0.636112, acc: 0.718750] [adversarial loss: 1.440436, acc: 0.109375]\n",
      "10592: [discriminator loss: 0.570811, acc: 0.695312] [adversarial loss: 0.854837, acc: 0.468750]\n",
      "10593: [discriminator loss: 0.613232, acc: 0.734375] [adversarial loss: 1.531839, acc: 0.093750]\n",
      "10594: [discriminator loss: 0.625486, acc: 0.679688] [adversarial loss: 0.821086, acc: 0.453125]\n",
      "10595: [discriminator loss: 0.510399, acc: 0.742188] [adversarial loss: 1.305874, acc: 0.156250]\n",
      "10596: [discriminator loss: 0.557725, acc: 0.679688] [adversarial loss: 1.159767, acc: 0.281250]\n",
      "10597: [discriminator loss: 0.513778, acc: 0.750000] [adversarial loss: 1.358343, acc: 0.218750]\n",
      "10598: [discriminator loss: 0.562721, acc: 0.687500] [adversarial loss: 1.061513, acc: 0.312500]\n",
      "10599: [discriminator loss: 0.550464, acc: 0.695312] [adversarial loss: 1.371465, acc: 0.140625]\n",
      "10600: [discriminator loss: 0.509292, acc: 0.750000] [adversarial loss: 0.879085, acc: 0.375000]\n",
      "10601: [discriminator loss: 0.607919, acc: 0.656250] [adversarial loss: 1.225968, acc: 0.171875]\n",
      "10602: [discriminator loss: 0.606369, acc: 0.703125] [adversarial loss: 0.970082, acc: 0.312500]\n",
      "10603: [discriminator loss: 0.567126, acc: 0.710938] [adversarial loss: 1.429114, acc: 0.187500]\n",
      "10604: [discriminator loss: 0.533349, acc: 0.718750] [adversarial loss: 1.194000, acc: 0.203125]\n",
      "10605: [discriminator loss: 0.523750, acc: 0.726562] [adversarial loss: 1.204051, acc: 0.281250]\n",
      "10606: [discriminator loss: 0.489367, acc: 0.765625] [adversarial loss: 0.845460, acc: 0.437500]\n",
      "10607: [discriminator loss: 0.555184, acc: 0.734375] [adversarial loss: 1.418583, acc: 0.109375]\n",
      "10608: [discriminator loss: 0.598669, acc: 0.632812] [adversarial loss: 1.085377, acc: 0.296875]\n",
      "10609: [discriminator loss: 0.544423, acc: 0.710938] [adversarial loss: 1.111688, acc: 0.312500]\n",
      "10610: [discriminator loss: 0.531517, acc: 0.726562] [adversarial loss: 1.094115, acc: 0.296875]\n",
      "10611: [discriminator loss: 0.568610, acc: 0.695312] [adversarial loss: 1.528294, acc: 0.156250]\n",
      "10612: [discriminator loss: 0.579606, acc: 0.703125] [adversarial loss: 0.924978, acc: 0.390625]\n",
      "10613: [discriminator loss: 0.632925, acc: 0.640625] [adversarial loss: 1.216627, acc: 0.156250]\n",
      "10614: [discriminator loss: 0.542656, acc: 0.687500] [adversarial loss: 1.167789, acc: 0.250000]\n",
      "10615: [discriminator loss: 0.493820, acc: 0.742188] [adversarial loss: 1.171731, acc: 0.281250]\n",
      "10616: [discriminator loss: 0.537024, acc: 0.703125] [adversarial loss: 1.392368, acc: 0.171875]\n",
      "10617: [discriminator loss: 0.610611, acc: 0.664062] [adversarial loss: 0.889707, acc: 0.437500]\n",
      "10618: [discriminator loss: 0.598680, acc: 0.664062] [adversarial loss: 1.373358, acc: 0.171875]\n",
      "10619: [discriminator loss: 0.569894, acc: 0.750000] [adversarial loss: 1.322622, acc: 0.125000]\n",
      "10620: [discriminator loss: 0.562675, acc: 0.695312] [adversarial loss: 1.063959, acc: 0.218750]\n",
      "10621: [discriminator loss: 0.481835, acc: 0.765625] [adversarial loss: 1.251254, acc: 0.187500]\n",
      "10622: [discriminator loss: 0.511678, acc: 0.742188] [adversarial loss: 1.110814, acc: 0.218750]\n",
      "10623: [discriminator loss: 0.520964, acc: 0.726562] [adversarial loss: 1.207500, acc: 0.187500]\n",
      "10624: [discriminator loss: 0.535868, acc: 0.687500] [adversarial loss: 1.131272, acc: 0.203125]\n",
      "10625: [discriminator loss: 0.554196, acc: 0.734375] [adversarial loss: 1.280021, acc: 0.218750]\n",
      "10626: [discriminator loss: 0.580352, acc: 0.703125] [adversarial loss: 0.906760, acc: 0.406250]\n",
      "10627: [discriminator loss: 0.603147, acc: 0.687500] [adversarial loss: 1.548827, acc: 0.093750]\n",
      "10628: [discriminator loss: 0.605555, acc: 0.664062] [adversarial loss: 0.988790, acc: 0.312500]\n",
      "10629: [discriminator loss: 0.544334, acc: 0.710938] [adversarial loss: 1.303468, acc: 0.203125]\n",
      "10630: [discriminator loss: 0.577859, acc: 0.718750] [adversarial loss: 0.934234, acc: 0.375000]\n",
      "10631: [discriminator loss: 0.545271, acc: 0.710938] [adversarial loss: 1.461568, acc: 0.078125]\n",
      "10632: [discriminator loss: 0.540437, acc: 0.695312] [adversarial loss: 0.854500, acc: 0.421875]\n",
      "10633: [discriminator loss: 0.595042, acc: 0.617188] [adversarial loss: 1.358697, acc: 0.109375]\n",
      "10634: [discriminator loss: 0.512985, acc: 0.750000] [adversarial loss: 1.084000, acc: 0.343750]\n",
      "10635: [discriminator loss: 0.566581, acc: 0.671875] [adversarial loss: 1.427680, acc: 0.140625]\n",
      "10636: [discriminator loss: 0.520825, acc: 0.703125] [adversarial loss: 1.112480, acc: 0.218750]\n",
      "10637: [discriminator loss: 0.512086, acc: 0.773438] [adversarial loss: 1.262411, acc: 0.171875]\n",
      "10638: [discriminator loss: 0.605087, acc: 0.679688] [adversarial loss: 1.112881, acc: 0.250000]\n",
      "10639: [discriminator loss: 0.509089, acc: 0.757812] [adversarial loss: 1.253942, acc: 0.187500]\n",
      "10640: [discriminator loss: 0.583235, acc: 0.679688] [adversarial loss: 1.026592, acc: 0.328125]\n",
      "10641: [discriminator loss: 0.492326, acc: 0.765625] [adversarial loss: 1.328729, acc: 0.203125]\n",
      "10642: [discriminator loss: 0.552936, acc: 0.664062] [adversarial loss: 1.126642, acc: 0.250000]\n",
      "10643: [discriminator loss: 0.554040, acc: 0.703125] [adversarial loss: 1.361755, acc: 0.203125]\n",
      "10644: [discriminator loss: 0.514135, acc: 0.710938] [adversarial loss: 1.000046, acc: 0.375000]\n",
      "10645: [discriminator loss: 0.556517, acc: 0.734375] [adversarial loss: 1.384236, acc: 0.187500]\n",
      "10646: [discriminator loss: 0.558764, acc: 0.679688] [adversarial loss: 0.879641, acc: 0.437500]\n",
      "10647: [discriminator loss: 0.636205, acc: 0.679688] [adversarial loss: 1.414871, acc: 0.078125]\n",
      "10648: [discriminator loss: 0.542467, acc: 0.710938] [adversarial loss: 0.984732, acc: 0.375000]\n",
      "10649: [discriminator loss: 0.618316, acc: 0.640625] [adversarial loss: 1.211245, acc: 0.156250]\n",
      "10650: [discriminator loss: 0.578146, acc: 0.664062] [adversarial loss: 1.169029, acc: 0.218750]\n",
      "10651: [discriminator loss: 0.602881, acc: 0.671875] [adversarial loss: 1.341051, acc: 0.125000]\n",
      "10652: [discriminator loss: 0.552435, acc: 0.703125] [adversarial loss: 0.958309, acc: 0.406250]\n",
      "10653: [discriminator loss: 0.501123, acc: 0.718750] [adversarial loss: 1.403030, acc: 0.203125]\n",
      "10654: [discriminator loss: 0.548316, acc: 0.718750] [adversarial loss: 0.917346, acc: 0.359375]\n",
      "10655: [discriminator loss: 0.586178, acc: 0.710938] [adversarial loss: 1.213008, acc: 0.218750]\n",
      "10656: [discriminator loss: 0.581407, acc: 0.695312] [adversarial loss: 0.777074, acc: 0.468750]\n",
      "10657: [discriminator loss: 0.584755, acc: 0.695312] [adversarial loss: 1.419638, acc: 0.078125]\n",
      "10658: [discriminator loss: 0.534488, acc: 0.710938] [adversarial loss: 0.939860, acc: 0.390625]\n",
      "10659: [discriminator loss: 0.566026, acc: 0.703125] [adversarial loss: 1.245995, acc: 0.187500]\n",
      "10660: [discriminator loss: 0.625905, acc: 0.640625] [adversarial loss: 1.307009, acc: 0.125000]\n",
      "10661: [discriminator loss: 0.557542, acc: 0.726562] [adversarial loss: 0.853584, acc: 0.390625]\n",
      "10662: [discriminator loss: 0.528201, acc: 0.703125] [adversarial loss: 1.267118, acc: 0.187500]\n",
      "10663: [discriminator loss: 0.485742, acc: 0.750000] [adversarial loss: 1.079869, acc: 0.187500]\n",
      "10664: [discriminator loss: 0.485776, acc: 0.757812] [adversarial loss: 1.349294, acc: 0.125000]\n",
      "10665: [discriminator loss: 0.505555, acc: 0.726562] [adversarial loss: 1.220907, acc: 0.250000]\n",
      "10666: [discriminator loss: 0.488406, acc: 0.804688] [adversarial loss: 1.178852, acc: 0.296875]\n",
      "10667: [discriminator loss: 0.582856, acc: 0.625000] [adversarial loss: 1.457473, acc: 0.187500]\n",
      "10668: [discriminator loss: 0.565088, acc: 0.664062] [adversarial loss: 0.994707, acc: 0.296875]\n",
      "10669: [discriminator loss: 0.516188, acc: 0.757812] [adversarial loss: 1.099996, acc: 0.281250]\n",
      "10670: [discriminator loss: 0.480269, acc: 0.765625] [adversarial loss: 1.280599, acc: 0.250000]\n",
      "10671: [discriminator loss: 0.502603, acc: 0.765625] [adversarial loss: 0.881016, acc: 0.406250]\n",
      "10672: [discriminator loss: 0.531808, acc: 0.695312] [adversarial loss: 1.451152, acc: 0.125000]\n",
      "10673: [discriminator loss: 0.553277, acc: 0.632812] [adversarial loss: 0.668436, acc: 0.593750]\n",
      "10674: [discriminator loss: 0.684065, acc: 0.625000] [adversarial loss: 1.659972, acc: 0.046875]\n",
      "10675: [discriminator loss: 0.616438, acc: 0.656250] [adversarial loss: 0.806531, acc: 0.406250]\n",
      "10676: [discriminator loss: 0.511448, acc: 0.757812] [adversarial loss: 1.545326, acc: 0.093750]\n",
      "10677: [discriminator loss: 0.596390, acc: 0.687500] [adversarial loss: 0.978875, acc: 0.328125]\n",
      "10678: [discriminator loss: 0.600390, acc: 0.687500] [adversarial loss: 1.415868, acc: 0.171875]\n",
      "10679: [discriminator loss: 0.558994, acc: 0.671875] [adversarial loss: 1.127040, acc: 0.218750]\n",
      "10680: [discriminator loss: 0.492043, acc: 0.765625] [adversarial loss: 1.199118, acc: 0.343750]\n",
      "10681: [discriminator loss: 0.542300, acc: 0.664062] [adversarial loss: 1.150847, acc: 0.250000]\n",
      "10682: [discriminator loss: 0.535531, acc: 0.773438] [adversarial loss: 1.116850, acc: 0.218750]\n",
      "10683: [discriminator loss: 0.538851, acc: 0.710938] [adversarial loss: 1.028350, acc: 0.406250]\n",
      "10684: [discriminator loss: 0.605156, acc: 0.632812] [adversarial loss: 1.009650, acc: 0.265625]\n",
      "10685: [discriminator loss: 0.558161, acc: 0.726562] [adversarial loss: 1.278189, acc: 0.234375]\n",
      "10686: [discriminator loss: 0.620861, acc: 0.617188] [adversarial loss: 1.128717, acc: 0.234375]\n",
      "10687: [discriminator loss: 0.606083, acc: 0.671875] [adversarial loss: 1.418865, acc: 0.109375]\n",
      "10688: [discriminator loss: 0.544942, acc: 0.695312] [adversarial loss: 1.032916, acc: 0.187500]\n",
      "10689: [discriminator loss: 0.599731, acc: 0.664062] [adversarial loss: 1.143344, acc: 0.156250]\n",
      "10690: [discriminator loss: 0.469708, acc: 0.773438] [adversarial loss: 1.053550, acc: 0.359375]\n",
      "10691: [discriminator loss: 0.527655, acc: 0.726562] [adversarial loss: 1.302788, acc: 0.156250]\n",
      "10692: [discriminator loss: 0.519717, acc: 0.765625] [adversarial loss: 1.348237, acc: 0.140625]\n",
      "10693: [discriminator loss: 0.556508, acc: 0.710938] [adversarial loss: 0.955460, acc: 0.437500]\n",
      "10694: [discriminator loss: 0.585964, acc: 0.664062] [adversarial loss: 1.620434, acc: 0.109375]\n",
      "10695: [discriminator loss: 0.589721, acc: 0.687500] [adversarial loss: 0.879245, acc: 0.390625]\n",
      "10696: [discriminator loss: 0.532501, acc: 0.718750] [adversarial loss: 1.333339, acc: 0.125000]\n",
      "10697: [discriminator loss: 0.588254, acc: 0.687500] [adversarial loss: 0.831956, acc: 0.453125]\n",
      "10698: [discriminator loss: 0.628531, acc: 0.656250] [adversarial loss: 1.554122, acc: 0.093750]\n",
      "10699: [discriminator loss: 0.558574, acc: 0.710938] [adversarial loss: 1.232590, acc: 0.234375]\n",
      "10700: [discriminator loss: 0.573856, acc: 0.679688] [adversarial loss: 1.026069, acc: 0.312500]\n",
      "10701: [discriminator loss: 0.576426, acc: 0.734375] [adversarial loss: 1.309625, acc: 0.125000]\n",
      "10702: [discriminator loss: 0.536737, acc: 0.726562] [adversarial loss: 0.912340, acc: 0.421875]\n",
      "10703: [discriminator loss: 0.575173, acc: 0.710938] [adversarial loss: 1.295097, acc: 0.234375]\n",
      "10704: [discriminator loss: 0.570182, acc: 0.671875] [adversarial loss: 1.008070, acc: 0.312500]\n",
      "10705: [discriminator loss: 0.607009, acc: 0.625000] [adversarial loss: 1.126901, acc: 0.328125]\n",
      "10706: [discriminator loss: 0.622613, acc: 0.585938] [adversarial loss: 0.997000, acc: 0.375000]\n",
      "10707: [discriminator loss: 0.541704, acc: 0.718750] [adversarial loss: 1.023184, acc: 0.281250]\n",
      "10708: [discriminator loss: 0.495330, acc: 0.773438] [adversarial loss: 1.059752, acc: 0.312500]\n",
      "10709: [discriminator loss: 0.595174, acc: 0.695312] [adversarial loss: 1.270747, acc: 0.187500]\n",
      "10710: [discriminator loss: 0.578784, acc: 0.703125] [adversarial loss: 0.947567, acc: 0.312500]\n",
      "10711: [discriminator loss: 0.583017, acc: 0.617188] [adversarial loss: 1.272997, acc: 0.140625]\n",
      "10712: [discriminator loss: 0.586374, acc: 0.648438] [adversarial loss: 1.059132, acc: 0.265625]\n",
      "10713: [discriminator loss: 0.656272, acc: 0.640625] [adversarial loss: 1.229745, acc: 0.093750]\n",
      "10714: [discriminator loss: 0.520851, acc: 0.703125] [adversarial loss: 1.119219, acc: 0.265625]\n",
      "10715: [discriminator loss: 0.488354, acc: 0.765625] [adversarial loss: 1.125939, acc: 0.281250]\n",
      "10716: [discriminator loss: 0.522341, acc: 0.773438] [adversarial loss: 1.212125, acc: 0.250000]\n",
      "10717: [discriminator loss: 0.590743, acc: 0.742188] [adversarial loss: 1.355271, acc: 0.156250]\n",
      "10718: [discriminator loss: 0.590137, acc: 0.656250] [adversarial loss: 1.182309, acc: 0.203125]\n",
      "10719: [discriminator loss: 0.537886, acc: 0.710938] [adversarial loss: 1.143647, acc: 0.218750]\n",
      "10720: [discriminator loss: 0.525534, acc: 0.734375] [adversarial loss: 1.218088, acc: 0.171875]\n",
      "10721: [discriminator loss: 0.571276, acc: 0.671875] [adversarial loss: 0.948454, acc: 0.312500]\n",
      "10722: [discriminator loss: 0.559646, acc: 0.671875] [adversarial loss: 1.598842, acc: 0.093750]\n",
      "10723: [discriminator loss: 0.605563, acc: 0.656250] [adversarial loss: 0.788555, acc: 0.562500]\n",
      "10724: [discriminator loss: 0.611153, acc: 0.671875] [adversarial loss: 1.412374, acc: 0.046875]\n",
      "10725: [discriminator loss: 0.591534, acc: 0.632812] [adversarial loss: 0.955137, acc: 0.359375]\n",
      "10726: [discriminator loss: 0.539388, acc: 0.734375] [adversarial loss: 1.053101, acc: 0.343750]\n",
      "10727: [discriminator loss: 0.551183, acc: 0.718750] [adversarial loss: 1.236161, acc: 0.109375]\n",
      "10728: [discriminator loss: 0.525348, acc: 0.718750] [adversarial loss: 1.209547, acc: 0.171875]\n",
      "10729: [discriminator loss: 0.585284, acc: 0.671875] [adversarial loss: 1.485445, acc: 0.109375]\n",
      "10730: [discriminator loss: 0.489027, acc: 0.789062] [adversarial loss: 1.224596, acc: 0.171875]\n",
      "10731: [discriminator loss: 0.534705, acc: 0.718750] [adversarial loss: 1.169617, acc: 0.187500]\n",
      "10732: [discriminator loss: 0.575154, acc: 0.718750] [adversarial loss: 0.972706, acc: 0.421875]\n",
      "10733: [discriminator loss: 0.575076, acc: 0.710938] [adversarial loss: 1.518809, acc: 0.109375]\n",
      "10734: [discriminator loss: 0.580088, acc: 0.679688] [adversarial loss: 0.912884, acc: 0.328125]\n",
      "10735: [discriminator loss: 0.537909, acc: 0.765625] [adversarial loss: 1.375322, acc: 0.156250]\n",
      "10736: [discriminator loss: 0.520302, acc: 0.726562] [adversarial loss: 1.214486, acc: 0.187500]\n",
      "10737: [discriminator loss: 0.533376, acc: 0.679688] [adversarial loss: 0.881635, acc: 0.421875]\n",
      "10738: [discriminator loss: 0.589248, acc: 0.687500] [adversarial loss: 1.227555, acc: 0.171875]\n",
      "10739: [discriminator loss: 0.479600, acc: 0.726562] [adversarial loss: 1.066428, acc: 0.281250]\n",
      "10740: [discriminator loss: 0.562366, acc: 0.718750] [adversarial loss: 1.292853, acc: 0.203125]\n",
      "10741: [discriminator loss: 0.564661, acc: 0.734375] [adversarial loss: 0.971720, acc: 0.343750]\n",
      "10742: [discriminator loss: 0.564986, acc: 0.695312] [adversarial loss: 1.225538, acc: 0.218750]\n",
      "10743: [discriminator loss: 0.549789, acc: 0.710938] [adversarial loss: 0.901629, acc: 0.390625]\n",
      "10744: [discriminator loss: 0.480749, acc: 0.781250] [adversarial loss: 1.104110, acc: 0.343750]\n",
      "10745: [discriminator loss: 0.540684, acc: 0.710938] [adversarial loss: 0.909798, acc: 0.343750]\n",
      "10746: [discriminator loss: 0.526182, acc: 0.750000] [adversarial loss: 1.209989, acc: 0.171875]\n",
      "10747: [discriminator loss: 0.550965, acc: 0.726562] [adversarial loss: 0.915978, acc: 0.437500]\n",
      "10748: [discriminator loss: 0.553499, acc: 0.718750] [adversarial loss: 1.203232, acc: 0.156250]\n",
      "10749: [discriminator loss: 0.539313, acc: 0.718750] [adversarial loss: 0.982733, acc: 0.375000]\n",
      "10750: [discriminator loss: 0.594593, acc: 0.687500] [adversarial loss: 1.521708, acc: 0.187500]\n",
      "10751: [discriminator loss: 0.578712, acc: 0.687500] [adversarial loss: 0.646212, acc: 0.656250]\n",
      "10752: [discriminator loss: 0.598427, acc: 0.648438] [adversarial loss: 1.584604, acc: 0.031250]\n",
      "10753: [discriminator loss: 0.566267, acc: 0.703125] [adversarial loss: 0.837436, acc: 0.390625]\n",
      "10754: [discriminator loss: 0.525904, acc: 0.742188] [adversarial loss: 1.492229, acc: 0.109375]\n",
      "10755: [discriminator loss: 0.550753, acc: 0.726562] [adversarial loss: 1.087777, acc: 0.281250]\n",
      "10756: [discriminator loss: 0.549283, acc: 0.695312] [adversarial loss: 1.186847, acc: 0.312500]\n",
      "10757: [discriminator loss: 0.588244, acc: 0.671875] [adversarial loss: 1.137188, acc: 0.281250]\n",
      "10758: [discriminator loss: 0.554663, acc: 0.710938] [adversarial loss: 1.215834, acc: 0.218750]\n",
      "10759: [discriminator loss: 0.572305, acc: 0.648438] [adversarial loss: 1.076924, acc: 0.312500]\n",
      "10760: [discriminator loss: 0.515594, acc: 0.757812] [adversarial loss: 0.914290, acc: 0.421875]\n",
      "10761: [discriminator loss: 0.585337, acc: 0.664062] [adversarial loss: 1.401185, acc: 0.171875]\n",
      "10762: [discriminator loss: 0.588794, acc: 0.671875] [adversarial loss: 0.930613, acc: 0.375000]\n",
      "10763: [discriminator loss: 0.564226, acc: 0.726562] [adversarial loss: 1.283410, acc: 0.109375]\n",
      "10764: [discriminator loss: 0.561054, acc: 0.679688] [adversarial loss: 1.131844, acc: 0.250000]\n",
      "10765: [discriminator loss: 0.585262, acc: 0.664062] [adversarial loss: 1.191623, acc: 0.140625]\n",
      "10766: [discriminator loss: 0.593941, acc: 0.695312] [adversarial loss: 1.091657, acc: 0.343750]\n",
      "10767: [discriminator loss: 0.507222, acc: 0.742188] [adversarial loss: 1.273072, acc: 0.218750]\n",
      "10768: [discriminator loss: 0.557228, acc: 0.718750] [adversarial loss: 1.234013, acc: 0.250000]\n",
      "10769: [discriminator loss: 0.589152, acc: 0.695312] [adversarial loss: 1.109036, acc: 0.281250]\n",
      "10770: [discriminator loss: 0.557187, acc: 0.718750] [adversarial loss: 1.277719, acc: 0.171875]\n",
      "10771: [discriminator loss: 0.626219, acc: 0.625000] [adversarial loss: 0.976852, acc: 0.390625]\n",
      "10772: [discriminator loss: 0.557117, acc: 0.695312] [adversarial loss: 1.217358, acc: 0.218750]\n",
      "10773: [discriminator loss: 0.557132, acc: 0.726562] [adversarial loss: 0.964997, acc: 0.390625]\n",
      "10774: [discriminator loss: 0.558752, acc: 0.671875] [adversarial loss: 1.492857, acc: 0.125000]\n",
      "10775: [discriminator loss: 0.538154, acc: 0.703125] [adversarial loss: 0.941233, acc: 0.390625]\n",
      "10776: [discriminator loss: 0.624811, acc: 0.664062] [adversarial loss: 1.360938, acc: 0.156250]\n",
      "10777: [discriminator loss: 0.544144, acc: 0.773438] [adversarial loss: 1.244632, acc: 0.203125]\n",
      "10778: [discriminator loss: 0.553573, acc: 0.718750] [adversarial loss: 1.033041, acc: 0.296875]\n",
      "10779: [discriminator loss: 0.590467, acc: 0.703125] [adversarial loss: 0.985405, acc: 0.265625]\n",
      "10780: [discriminator loss: 0.481292, acc: 0.796875] [adversarial loss: 1.220343, acc: 0.218750]\n",
      "10781: [discriminator loss: 0.545054, acc: 0.695312] [adversarial loss: 0.990952, acc: 0.312500]\n",
      "10782: [discriminator loss: 0.524377, acc: 0.742188] [adversarial loss: 0.829960, acc: 0.406250]\n",
      "10783: [discriminator loss: 0.576793, acc: 0.687500] [adversarial loss: 1.339627, acc: 0.109375]\n",
      "10784: [discriminator loss: 0.538796, acc: 0.703125] [adversarial loss: 0.879410, acc: 0.453125]\n",
      "10785: [discriminator loss: 0.539665, acc: 0.742188] [adversarial loss: 1.543618, acc: 0.093750]\n",
      "10786: [discriminator loss: 0.610168, acc: 0.601562] [adversarial loss: 0.791319, acc: 0.531250]\n",
      "10787: [discriminator loss: 0.596742, acc: 0.726562] [adversarial loss: 1.492774, acc: 0.062500]\n",
      "10788: [discriminator loss: 0.566541, acc: 0.648438] [adversarial loss: 0.977597, acc: 0.421875]\n",
      "10789: [discriminator loss: 0.524235, acc: 0.765625] [adversarial loss: 1.118336, acc: 0.250000]\n",
      "10790: [discriminator loss: 0.547398, acc: 0.710938] [adversarial loss: 0.917757, acc: 0.312500]\n",
      "10791: [discriminator loss: 0.572983, acc: 0.687500] [adversarial loss: 1.339326, acc: 0.156250]\n",
      "10792: [discriminator loss: 0.507061, acc: 0.734375] [adversarial loss: 0.890147, acc: 0.390625]\n",
      "10793: [discriminator loss: 0.506416, acc: 0.757812] [adversarial loss: 1.564714, acc: 0.093750]\n",
      "10794: [discriminator loss: 0.511296, acc: 0.742188] [adversarial loss: 0.924840, acc: 0.390625]\n",
      "10795: [discriminator loss: 0.588887, acc: 0.710938] [adversarial loss: 1.472089, acc: 0.078125]\n",
      "10796: [discriminator loss: 0.651982, acc: 0.648438] [adversarial loss: 0.856212, acc: 0.437500]\n",
      "10797: [discriminator loss: 0.580221, acc: 0.718750] [adversarial loss: 1.274214, acc: 0.187500]\n",
      "10798: [discriminator loss: 0.502784, acc: 0.703125] [adversarial loss: 1.158961, acc: 0.281250]\n",
      "10799: [discriminator loss: 0.557812, acc: 0.687500] [adversarial loss: 0.915523, acc: 0.421875]\n",
      "10800: [discriminator loss: 0.609139, acc: 0.656250] [adversarial loss: 1.241869, acc: 0.218750]\n",
      "10801: [discriminator loss: 0.480587, acc: 0.781250] [adversarial loss: 1.057393, acc: 0.234375]\n",
      "10802: [discriminator loss: 0.573304, acc: 0.664062] [adversarial loss: 1.056609, acc: 0.328125]\n",
      "10803: [discriminator loss: 0.512106, acc: 0.765625] [adversarial loss: 1.126701, acc: 0.203125]\n",
      "10804: [discriminator loss: 0.524971, acc: 0.710938] [adversarial loss: 1.221285, acc: 0.234375]\n",
      "10805: [discriminator loss: 0.534057, acc: 0.757812] [adversarial loss: 1.147851, acc: 0.312500]\n",
      "10806: [discriminator loss: 0.516211, acc: 0.765625] [adversarial loss: 1.023098, acc: 0.296875]\n",
      "10807: [discriminator loss: 0.578264, acc: 0.671875] [adversarial loss: 1.329780, acc: 0.125000]\n",
      "10808: [discriminator loss: 0.526303, acc: 0.734375] [adversarial loss: 1.080228, acc: 0.218750]\n",
      "10809: [discriminator loss: 0.626553, acc: 0.656250] [adversarial loss: 1.197939, acc: 0.234375]\n",
      "10810: [discriminator loss: 0.518382, acc: 0.726562] [adversarial loss: 0.976008, acc: 0.328125]\n",
      "10811: [discriminator loss: 0.557870, acc: 0.710938] [adversarial loss: 1.519761, acc: 0.140625]\n",
      "10812: [discriminator loss: 0.491518, acc: 0.726562] [adversarial loss: 0.968554, acc: 0.328125]\n",
      "10813: [discriminator loss: 0.538078, acc: 0.750000] [adversarial loss: 1.046234, acc: 0.328125]\n",
      "10814: [discriminator loss: 0.578541, acc: 0.695312] [adversarial loss: 1.028389, acc: 0.281250]\n",
      "10815: [discriminator loss: 0.532120, acc: 0.757812] [adversarial loss: 1.239326, acc: 0.187500]\n",
      "10816: [discriminator loss: 0.578169, acc: 0.695312] [adversarial loss: 0.875779, acc: 0.421875]\n",
      "10817: [discriminator loss: 0.536773, acc: 0.710938] [adversarial loss: 1.420666, acc: 0.125000]\n",
      "10818: [discriminator loss: 0.524970, acc: 0.710938] [adversarial loss: 1.005072, acc: 0.343750]\n",
      "10819: [discriminator loss: 0.581751, acc: 0.679688] [adversarial loss: 0.996867, acc: 0.312500]\n",
      "10820: [discriminator loss: 0.509523, acc: 0.726562] [adversarial loss: 1.061249, acc: 0.250000]\n",
      "10821: [discriminator loss: 0.485165, acc: 0.742188] [adversarial loss: 1.037063, acc: 0.281250]\n",
      "10822: [discriminator loss: 0.592011, acc: 0.687500] [adversarial loss: 1.179722, acc: 0.234375]\n",
      "10823: [discriminator loss: 0.536086, acc: 0.710938] [adversarial loss: 1.015501, acc: 0.312500]\n",
      "10824: [discriminator loss: 0.552604, acc: 0.671875] [adversarial loss: 0.943747, acc: 0.359375]\n",
      "10825: [discriminator loss: 0.502108, acc: 0.718750] [adversarial loss: 1.295038, acc: 0.187500]\n",
      "10826: [discriminator loss: 0.534619, acc: 0.710938] [adversarial loss: 0.872210, acc: 0.500000]\n",
      "10827: [discriminator loss: 0.614625, acc: 0.687500] [adversarial loss: 1.412977, acc: 0.109375]\n",
      "10828: [discriminator loss: 0.515198, acc: 0.734375] [adversarial loss: 0.778063, acc: 0.546875]\n",
      "10829: [discriminator loss: 0.528754, acc: 0.664062] [adversarial loss: 1.459979, acc: 0.125000]\n",
      "10830: [discriminator loss: 0.531804, acc: 0.726562] [adversarial loss: 0.744857, acc: 0.515625]\n",
      "10831: [discriminator loss: 0.666282, acc: 0.593750] [adversarial loss: 1.503258, acc: 0.125000]\n",
      "10832: [discriminator loss: 0.572385, acc: 0.718750] [adversarial loss: 0.936649, acc: 0.375000]\n",
      "10833: [discriminator loss: 0.572657, acc: 0.656250] [adversarial loss: 1.478747, acc: 0.062500]\n",
      "10834: [discriminator loss: 0.621272, acc: 0.648438] [adversarial loss: 0.744373, acc: 0.593750]\n",
      "10835: [discriminator loss: 0.644130, acc: 0.617188] [adversarial loss: 1.285359, acc: 0.125000]\n",
      "10836: [discriminator loss: 0.536560, acc: 0.695312] [adversarial loss: 0.925212, acc: 0.343750]\n",
      "10837: [discriminator loss: 0.549331, acc: 0.734375] [adversarial loss: 1.423831, acc: 0.093750]\n",
      "10838: [discriminator loss: 0.619698, acc: 0.664062] [adversarial loss: 1.008927, acc: 0.312500]\n",
      "10839: [discriminator loss: 0.544124, acc: 0.742188] [adversarial loss: 1.530907, acc: 0.125000]\n",
      "10840: [discriminator loss: 0.515003, acc: 0.718750] [adversarial loss: 0.980609, acc: 0.390625]\n",
      "10841: [discriminator loss: 0.554904, acc: 0.726562] [adversarial loss: 1.389881, acc: 0.156250]\n",
      "10842: [discriminator loss: 0.542180, acc: 0.710938] [adversarial loss: 1.298043, acc: 0.234375]\n",
      "10843: [discriminator loss: 0.589260, acc: 0.656250] [adversarial loss: 0.956879, acc: 0.359375]\n",
      "10844: [discriminator loss: 0.523224, acc: 0.703125] [adversarial loss: 1.087429, acc: 0.390625]\n",
      "10845: [discriminator loss: 0.569862, acc: 0.687500] [adversarial loss: 0.966526, acc: 0.328125]\n",
      "10846: [discriminator loss: 0.567959, acc: 0.703125] [adversarial loss: 1.378735, acc: 0.156250]\n",
      "10847: [discriminator loss: 0.531600, acc: 0.695312] [adversarial loss: 0.951498, acc: 0.343750]\n",
      "10848: [discriminator loss: 0.508217, acc: 0.750000] [adversarial loss: 1.318927, acc: 0.140625]\n",
      "10849: [discriminator loss: 0.634020, acc: 0.656250] [adversarial loss: 1.024410, acc: 0.343750]\n",
      "10850: [discriminator loss: 0.539333, acc: 0.703125] [adversarial loss: 1.236141, acc: 0.265625]\n",
      "10851: [discriminator loss: 0.548805, acc: 0.703125] [adversarial loss: 1.232294, acc: 0.218750]\n",
      "10852: [discriminator loss: 0.590851, acc: 0.687500] [adversarial loss: 0.847518, acc: 0.390625]\n",
      "10853: [discriminator loss: 0.575768, acc: 0.695312] [adversarial loss: 1.232673, acc: 0.187500]\n",
      "10854: [discriminator loss: 0.642233, acc: 0.625000] [adversarial loss: 0.804522, acc: 0.437500]\n",
      "10855: [discriminator loss: 0.547840, acc: 0.726562] [adversarial loss: 1.276584, acc: 0.203125]\n",
      "10856: [discriminator loss: 0.500308, acc: 0.765625] [adversarial loss: 1.515126, acc: 0.156250]\n",
      "10857: [discriminator loss: 0.620257, acc: 0.656250] [adversarial loss: 0.951270, acc: 0.359375]\n",
      "10858: [discriminator loss: 0.559587, acc: 0.757812] [adversarial loss: 1.230301, acc: 0.171875]\n",
      "10859: [discriminator loss: 0.559536, acc: 0.687500] [adversarial loss: 1.102471, acc: 0.296875]\n",
      "10860: [discriminator loss: 0.521052, acc: 0.718750] [adversarial loss: 1.089894, acc: 0.281250]\n",
      "10861: [discriminator loss: 0.611972, acc: 0.632812] [adversarial loss: 1.020588, acc: 0.296875]\n",
      "10862: [discriminator loss: 0.520256, acc: 0.750000] [adversarial loss: 1.035626, acc: 0.281250]\n",
      "10863: [discriminator loss: 0.499747, acc: 0.726562] [adversarial loss: 1.267389, acc: 0.187500]\n",
      "10864: [discriminator loss: 0.532973, acc: 0.726562] [adversarial loss: 0.957691, acc: 0.390625]\n",
      "10865: [discriminator loss: 0.628125, acc: 0.617188] [adversarial loss: 1.394453, acc: 0.125000]\n",
      "10866: [discriminator loss: 0.555969, acc: 0.703125] [adversarial loss: 0.882563, acc: 0.421875]\n",
      "10867: [discriminator loss: 0.541676, acc: 0.750000] [adversarial loss: 1.441265, acc: 0.125000]\n",
      "10868: [discriminator loss: 0.569959, acc: 0.679688] [adversarial loss: 1.053135, acc: 0.343750]\n",
      "10869: [discriminator loss: 0.496216, acc: 0.734375] [adversarial loss: 1.379405, acc: 0.187500]\n",
      "10870: [discriminator loss: 0.475118, acc: 0.789062] [adversarial loss: 0.904252, acc: 0.328125]\n",
      "10871: [discriminator loss: 0.593135, acc: 0.648438] [adversarial loss: 1.379585, acc: 0.156250]\n",
      "10872: [discriminator loss: 0.525002, acc: 0.726562] [adversarial loss: 1.132037, acc: 0.218750]\n",
      "10873: [discriminator loss: 0.577910, acc: 0.640625] [adversarial loss: 1.095233, acc: 0.281250]\n",
      "10874: [discriminator loss: 0.554005, acc: 0.695312] [adversarial loss: 1.106863, acc: 0.312500]\n",
      "10875: [discriminator loss: 0.568943, acc: 0.710938] [adversarial loss: 1.094220, acc: 0.234375]\n",
      "10876: [discriminator loss: 0.517112, acc: 0.695312] [adversarial loss: 1.235979, acc: 0.265625]\n",
      "10877: [discriminator loss: 0.562555, acc: 0.679688] [adversarial loss: 1.047219, acc: 0.343750]\n",
      "10878: [discriminator loss: 0.507945, acc: 0.750000] [adversarial loss: 1.351128, acc: 0.140625]\n",
      "10879: [discriminator loss: 0.564660, acc: 0.656250] [adversarial loss: 1.373908, acc: 0.109375]\n",
      "10880: [discriminator loss: 0.515297, acc: 0.765625] [adversarial loss: 1.357128, acc: 0.187500]\n",
      "10881: [discriminator loss: 0.556040, acc: 0.695312] [adversarial loss: 1.318317, acc: 0.234375]\n",
      "10882: [discriminator loss: 0.648594, acc: 0.648438] [adversarial loss: 0.939750, acc: 0.312500]\n",
      "10883: [discriminator loss: 0.579930, acc: 0.695312] [adversarial loss: 1.731015, acc: 0.187500]\n",
      "10884: [discriminator loss: 0.724797, acc: 0.625000] [adversarial loss: 0.887100, acc: 0.421875]\n",
      "10885: [discriminator loss: 0.546910, acc: 0.726562] [adversarial loss: 1.364335, acc: 0.171875]\n",
      "10886: [discriminator loss: 0.501156, acc: 0.773438] [adversarial loss: 1.205789, acc: 0.265625]\n",
      "10887: [discriminator loss: 0.621233, acc: 0.648438] [adversarial loss: 1.283048, acc: 0.171875]\n",
      "10888: [discriminator loss: 0.529014, acc: 0.703125] [adversarial loss: 1.000586, acc: 0.281250]\n",
      "10889: [discriminator loss: 0.541679, acc: 0.726562] [adversarial loss: 1.280873, acc: 0.140625]\n",
      "10890: [discriminator loss: 0.557241, acc: 0.718750] [adversarial loss: 1.335907, acc: 0.109375]\n",
      "10891: [discriminator loss: 0.583416, acc: 0.671875] [adversarial loss: 0.927611, acc: 0.437500]\n",
      "10892: [discriminator loss: 0.589529, acc: 0.671875] [adversarial loss: 1.263126, acc: 0.234375]\n",
      "10893: [discriminator loss: 0.524138, acc: 0.734375] [adversarial loss: 1.261762, acc: 0.171875]\n",
      "10894: [discriminator loss: 0.590049, acc: 0.703125] [adversarial loss: 1.140533, acc: 0.328125]\n",
      "10895: [discriminator loss: 0.534687, acc: 0.750000] [adversarial loss: 1.088609, acc: 0.312500]\n",
      "10896: [discriminator loss: 0.499315, acc: 0.718750] [adversarial loss: 1.149059, acc: 0.250000]\n",
      "10897: [discriminator loss: 0.570922, acc: 0.664062] [adversarial loss: 1.296950, acc: 0.125000]\n",
      "10898: [discriminator loss: 0.545451, acc: 0.750000] [adversarial loss: 0.770091, acc: 0.546875]\n",
      "10899: [discriminator loss: 0.572021, acc: 0.671875] [adversarial loss: 1.393847, acc: 0.093750]\n",
      "10900: [discriminator loss: 0.653167, acc: 0.625000] [adversarial loss: 0.847240, acc: 0.390625]\n",
      "10901: [discriminator loss: 0.580432, acc: 0.734375] [adversarial loss: 1.475266, acc: 0.109375]\n",
      "10902: [discriminator loss: 0.540797, acc: 0.710938] [adversarial loss: 0.916774, acc: 0.421875]\n",
      "10903: [discriminator loss: 0.611548, acc: 0.664062] [adversarial loss: 1.521002, acc: 0.140625]\n",
      "10904: [discriminator loss: 0.605769, acc: 0.632812] [adversarial loss: 0.883286, acc: 0.406250]\n",
      "10905: [discriminator loss: 0.562034, acc: 0.687500] [adversarial loss: 1.546034, acc: 0.125000]\n",
      "10906: [discriminator loss: 0.564180, acc: 0.687500] [adversarial loss: 1.021605, acc: 0.359375]\n",
      "10907: [discriminator loss: 0.537225, acc: 0.687500] [adversarial loss: 1.448447, acc: 0.062500]\n",
      "10908: [discriminator loss: 0.548713, acc: 0.695312] [adversarial loss: 1.093606, acc: 0.234375]\n",
      "10909: [discriminator loss: 0.528512, acc: 0.710938] [adversarial loss: 1.248166, acc: 0.218750]\n",
      "10910: [discriminator loss: 0.520938, acc: 0.750000] [adversarial loss: 1.139698, acc: 0.250000]\n",
      "10911: [discriminator loss: 0.506066, acc: 0.742188] [adversarial loss: 0.982335, acc: 0.265625]\n",
      "10912: [discriminator loss: 0.513218, acc: 0.789062] [adversarial loss: 1.227432, acc: 0.281250]\n",
      "10913: [discriminator loss: 0.575565, acc: 0.664062] [adversarial loss: 0.976672, acc: 0.296875]\n",
      "10914: [discriminator loss: 0.591333, acc: 0.640625] [adversarial loss: 1.231155, acc: 0.203125]\n",
      "10915: [discriminator loss: 0.505095, acc: 0.757812] [adversarial loss: 1.171003, acc: 0.203125]\n",
      "10916: [discriminator loss: 0.534672, acc: 0.718750] [adversarial loss: 0.929459, acc: 0.343750]\n",
      "10917: [discriminator loss: 0.580372, acc: 0.656250] [adversarial loss: 1.529986, acc: 0.046875]\n",
      "10918: [discriminator loss: 0.519706, acc: 0.710938] [adversarial loss: 1.319725, acc: 0.203125]\n",
      "10919: [discriminator loss: 0.578624, acc: 0.695312] [adversarial loss: 0.856352, acc: 0.390625]\n",
      "10920: [discriminator loss: 0.563618, acc: 0.656250] [adversarial loss: 1.297785, acc: 0.156250]\n",
      "10921: [discriminator loss: 0.576374, acc: 0.640625] [adversarial loss: 0.960540, acc: 0.375000]\n",
      "10922: [discriminator loss: 0.529619, acc: 0.734375] [adversarial loss: 1.336736, acc: 0.171875]\n",
      "10923: [discriminator loss: 0.587511, acc: 0.671875] [adversarial loss: 1.035317, acc: 0.312500]\n",
      "10924: [discriminator loss: 0.520905, acc: 0.718750] [adversarial loss: 1.181380, acc: 0.218750]\n",
      "10925: [discriminator loss: 0.559437, acc: 0.718750] [adversarial loss: 1.144001, acc: 0.187500]\n",
      "10926: [discriminator loss: 0.561366, acc: 0.742188] [adversarial loss: 1.115592, acc: 0.281250]\n",
      "10927: [discriminator loss: 0.550417, acc: 0.718750] [adversarial loss: 1.279756, acc: 0.171875]\n",
      "10928: [discriminator loss: 0.558985, acc: 0.664062] [adversarial loss: 0.993648, acc: 0.343750]\n",
      "10929: [discriminator loss: 0.539311, acc: 0.718750] [adversarial loss: 1.205063, acc: 0.218750]\n",
      "10930: [discriminator loss: 0.561241, acc: 0.742188] [adversarial loss: 1.024343, acc: 0.281250]\n",
      "10931: [discriminator loss: 0.521672, acc: 0.734375] [adversarial loss: 1.500051, acc: 0.140625]\n",
      "10932: [discriminator loss: 0.470650, acc: 0.726562] [adversarial loss: 0.981182, acc: 0.312500]\n",
      "10933: [discriminator loss: 0.543453, acc: 0.695312] [adversarial loss: 1.560101, acc: 0.093750]\n",
      "10934: [discriminator loss: 0.645948, acc: 0.656250] [adversarial loss: 0.875980, acc: 0.406250]\n",
      "10935: [discriminator loss: 0.546545, acc: 0.671875] [adversarial loss: 1.742974, acc: 0.078125]\n",
      "10936: [discriminator loss: 0.574118, acc: 0.710938] [adversarial loss: 0.891944, acc: 0.453125]\n",
      "10937: [discriminator loss: 0.585026, acc: 0.695312] [adversarial loss: 1.220374, acc: 0.187500]\n",
      "10938: [discriminator loss: 0.628091, acc: 0.601562] [adversarial loss: 0.864158, acc: 0.500000]\n",
      "10939: [discriminator loss: 0.563623, acc: 0.718750] [adversarial loss: 1.129298, acc: 0.203125]\n",
      "10940: [discriminator loss: 0.512206, acc: 0.726562] [adversarial loss: 1.249738, acc: 0.265625]\n",
      "10941: [discriminator loss: 0.635387, acc: 0.625000] [adversarial loss: 1.070665, acc: 0.265625]\n",
      "10942: [discriminator loss: 0.587047, acc: 0.703125] [adversarial loss: 1.231939, acc: 0.250000]\n",
      "10943: [discriminator loss: 0.608169, acc: 0.640625] [adversarial loss: 1.013082, acc: 0.328125]\n",
      "10944: [discriminator loss: 0.497235, acc: 0.757812] [adversarial loss: 1.000527, acc: 0.375000]\n",
      "10945: [discriminator loss: 0.521810, acc: 0.750000] [adversarial loss: 1.076663, acc: 0.218750]\n",
      "10946: [discriminator loss: 0.545764, acc: 0.726562] [adversarial loss: 1.043811, acc: 0.265625]\n",
      "10947: [discriminator loss: 0.564259, acc: 0.679688] [adversarial loss: 1.115647, acc: 0.281250]\n",
      "10948: [discriminator loss: 0.503649, acc: 0.726562] [adversarial loss: 0.992193, acc: 0.328125]\n",
      "10949: [discriminator loss: 0.519482, acc: 0.718750] [adversarial loss: 1.029799, acc: 0.218750]\n",
      "10950: [discriminator loss: 0.560182, acc: 0.687500] [adversarial loss: 1.256734, acc: 0.234375]\n",
      "10951: [discriminator loss: 0.547630, acc: 0.687500] [adversarial loss: 1.048906, acc: 0.312500]\n",
      "10952: [discriminator loss: 0.542742, acc: 0.710938] [adversarial loss: 1.371418, acc: 0.140625]\n",
      "10953: [discriminator loss: 0.436237, acc: 0.820312] [adversarial loss: 1.052501, acc: 0.265625]\n",
      "10954: [discriminator loss: 0.528706, acc: 0.757812] [adversarial loss: 1.242509, acc: 0.187500]\n",
      "10955: [discriminator loss: 0.578941, acc: 0.648438] [adversarial loss: 1.127474, acc: 0.265625]\n",
      "10956: [discriminator loss: 0.509109, acc: 0.757812] [adversarial loss: 1.230161, acc: 0.250000]\n",
      "10957: [discriminator loss: 0.633778, acc: 0.671875] [adversarial loss: 1.155291, acc: 0.265625]\n",
      "10958: [discriminator loss: 0.467503, acc: 0.765625] [adversarial loss: 1.303520, acc: 0.187500]\n",
      "10959: [discriminator loss: 0.503420, acc: 0.773438] [adversarial loss: 1.100523, acc: 0.375000]\n",
      "10960: [discriminator loss: 0.531941, acc: 0.750000] [adversarial loss: 1.427176, acc: 0.125000]\n",
      "10961: [discriminator loss: 0.579711, acc: 0.687500] [adversarial loss: 1.061859, acc: 0.328125]\n",
      "10962: [discriminator loss: 0.500818, acc: 0.750000] [adversarial loss: 1.465066, acc: 0.140625]\n",
      "10963: [discriminator loss: 0.626795, acc: 0.625000] [adversarial loss: 1.052789, acc: 0.296875]\n",
      "10964: [discriminator loss: 0.546945, acc: 0.718750] [adversarial loss: 1.443889, acc: 0.171875]\n",
      "10965: [discriminator loss: 0.574827, acc: 0.703125] [adversarial loss: 0.813424, acc: 0.468750]\n",
      "10966: [discriminator loss: 0.577269, acc: 0.703125] [adversarial loss: 1.109563, acc: 0.203125]\n",
      "10967: [discriminator loss: 0.589710, acc: 0.687500] [adversarial loss: 0.984260, acc: 0.359375]\n",
      "10968: [discriminator loss: 0.549062, acc: 0.695312] [adversarial loss: 1.415911, acc: 0.218750]\n",
      "10969: [discriminator loss: 0.603735, acc: 0.656250] [adversarial loss: 0.798169, acc: 0.609375]\n",
      "10970: [discriminator loss: 0.570569, acc: 0.671875] [adversarial loss: 1.318227, acc: 0.187500]\n",
      "10971: [discriminator loss: 0.588163, acc: 0.664062] [adversarial loss: 1.256442, acc: 0.250000]\n",
      "10972: [discriminator loss: 0.586191, acc: 0.632812] [adversarial loss: 1.225604, acc: 0.218750]\n",
      "10973: [discriminator loss: 0.511211, acc: 0.757812] [adversarial loss: 0.914904, acc: 0.406250]\n",
      "10974: [discriminator loss: 0.529738, acc: 0.757812] [adversarial loss: 1.155903, acc: 0.250000]\n",
      "10975: [discriminator loss: 0.566731, acc: 0.687500] [adversarial loss: 1.466560, acc: 0.171875]\n",
      "10976: [discriminator loss: 0.683156, acc: 0.617188] [adversarial loss: 0.976681, acc: 0.343750]\n",
      "10977: [discriminator loss: 0.606127, acc: 0.664062] [adversarial loss: 1.107856, acc: 0.203125]\n",
      "10978: [discriminator loss: 0.530908, acc: 0.734375] [adversarial loss: 1.036609, acc: 0.343750]\n",
      "10979: [discriminator loss: 0.530042, acc: 0.773438] [adversarial loss: 1.038740, acc: 0.265625]\n",
      "10980: [discriminator loss: 0.576871, acc: 0.679688] [adversarial loss: 0.918774, acc: 0.437500]\n",
      "10981: [discriminator loss: 0.545345, acc: 0.695312] [adversarial loss: 1.136511, acc: 0.281250]\n",
      "10982: [discriminator loss: 0.549642, acc: 0.718750] [adversarial loss: 1.059378, acc: 0.343750]\n",
      "10983: [discriminator loss: 0.546743, acc: 0.734375] [adversarial loss: 1.201189, acc: 0.328125]\n",
      "10984: [discriminator loss: 0.542428, acc: 0.765625] [adversarial loss: 1.313022, acc: 0.187500]\n",
      "10985: [discriminator loss: 0.551704, acc: 0.703125] [adversarial loss: 0.993847, acc: 0.406250]\n",
      "10986: [discriminator loss: 0.491564, acc: 0.757812] [adversarial loss: 1.321371, acc: 0.218750]\n",
      "10987: [discriminator loss: 0.597928, acc: 0.679688] [adversarial loss: 1.168610, acc: 0.218750]\n",
      "10988: [discriminator loss: 0.488434, acc: 0.750000] [adversarial loss: 1.078438, acc: 0.359375]\n",
      "10989: [discriminator loss: 0.545552, acc: 0.734375] [adversarial loss: 1.186150, acc: 0.296875]\n",
      "10990: [discriminator loss: 0.651044, acc: 0.625000] [adversarial loss: 0.837503, acc: 0.484375]\n",
      "10991: [discriminator loss: 0.550661, acc: 0.687500] [adversarial loss: 1.439360, acc: 0.156250]\n",
      "10992: [discriminator loss: 0.559695, acc: 0.710938] [adversarial loss: 0.937890, acc: 0.406250]\n",
      "10993: [discriminator loss: 0.606587, acc: 0.656250] [adversarial loss: 1.692956, acc: 0.093750]\n",
      "10994: [discriminator loss: 0.570169, acc: 0.632812] [adversarial loss: 0.894202, acc: 0.406250]\n",
      "10995: [discriminator loss: 0.587031, acc: 0.640625] [adversarial loss: 1.564102, acc: 0.125000]\n",
      "10996: [discriminator loss: 0.589912, acc: 0.679688] [adversarial loss: 0.914843, acc: 0.406250]\n",
      "10997: [discriminator loss: 0.501620, acc: 0.781250] [adversarial loss: 1.455858, acc: 0.140625]\n",
      "10998: [discriminator loss: 0.585315, acc: 0.687500] [adversarial loss: 0.790542, acc: 0.500000]\n",
      "10999: [discriminator loss: 0.572257, acc: 0.671875] [adversarial loss: 1.861435, acc: 0.062500]\n",
      "11000: [discriminator loss: 0.651851, acc: 0.609375] [adversarial loss: 0.878574, acc: 0.375000]\n",
      "11001: [discriminator loss: 0.573762, acc: 0.734375] [adversarial loss: 1.178342, acc: 0.203125]\n",
      "11002: [discriminator loss: 0.562078, acc: 0.695312] [adversarial loss: 1.161802, acc: 0.218750]\n",
      "11003: [discriminator loss: 0.575835, acc: 0.695312] [adversarial loss: 1.294751, acc: 0.203125]\n",
      "11004: [discriminator loss: 0.598753, acc: 0.726562] [adversarial loss: 0.914523, acc: 0.421875]\n",
      "11005: [discriminator loss: 0.575007, acc: 0.734375] [adversarial loss: 1.348415, acc: 0.187500]\n",
      "11006: [discriminator loss: 0.564963, acc: 0.679688] [adversarial loss: 1.151029, acc: 0.187500]\n",
      "11007: [discriminator loss: 0.582870, acc: 0.671875] [adversarial loss: 1.160235, acc: 0.296875]\n",
      "11008: [discriminator loss: 0.518211, acc: 0.757812] [adversarial loss: 1.121712, acc: 0.203125]\n",
      "11009: [discriminator loss: 0.525534, acc: 0.750000] [adversarial loss: 1.147244, acc: 0.140625]\n",
      "11010: [discriminator loss: 0.618377, acc: 0.617188] [adversarial loss: 1.003380, acc: 0.375000]\n",
      "11011: [discriminator loss: 0.570456, acc: 0.710938] [adversarial loss: 1.024210, acc: 0.281250]\n",
      "11012: [discriminator loss: 0.531712, acc: 0.734375] [adversarial loss: 1.208166, acc: 0.234375]\n",
      "11013: [discriminator loss: 0.514789, acc: 0.726562] [adversarial loss: 1.179421, acc: 0.265625]\n",
      "11014: [discriminator loss: 0.568545, acc: 0.656250] [adversarial loss: 1.175368, acc: 0.156250]\n",
      "11015: [discriminator loss: 0.596992, acc: 0.671875] [adversarial loss: 1.076818, acc: 0.218750]\n",
      "11016: [discriminator loss: 0.610174, acc: 0.648438] [adversarial loss: 1.169707, acc: 0.281250]\n",
      "11017: [discriminator loss: 0.547937, acc: 0.703125] [adversarial loss: 1.116881, acc: 0.203125]\n",
      "11018: [discriminator loss: 0.529809, acc: 0.757812] [adversarial loss: 1.264690, acc: 0.171875]\n",
      "11019: [discriminator loss: 0.564702, acc: 0.718750] [adversarial loss: 0.744516, acc: 0.468750]\n",
      "11020: [discriminator loss: 0.581912, acc: 0.703125] [adversarial loss: 1.527936, acc: 0.062500]\n",
      "11021: [discriminator loss: 0.555963, acc: 0.703125] [adversarial loss: 0.787605, acc: 0.468750]\n",
      "11022: [discriminator loss: 0.516653, acc: 0.773438] [adversarial loss: 1.383947, acc: 0.171875]\n",
      "11023: [discriminator loss: 0.509812, acc: 0.726562] [adversarial loss: 0.997092, acc: 0.343750]\n",
      "11024: [discriminator loss: 0.506280, acc: 0.765625] [adversarial loss: 1.451419, acc: 0.171875]\n",
      "11025: [discriminator loss: 0.555231, acc: 0.671875] [adversarial loss: 1.202262, acc: 0.156250]\n",
      "11026: [discriminator loss: 0.538163, acc: 0.718750] [adversarial loss: 1.011761, acc: 0.281250]\n",
      "11027: [discriminator loss: 0.588258, acc: 0.648438] [adversarial loss: 0.981299, acc: 0.359375]\n",
      "11028: [discriminator loss: 0.547032, acc: 0.726562] [adversarial loss: 0.996085, acc: 0.343750]\n",
      "11029: [discriminator loss: 0.574914, acc: 0.710938] [adversarial loss: 1.352437, acc: 0.109375]\n",
      "11030: [discriminator loss: 0.527179, acc: 0.710938] [adversarial loss: 1.047043, acc: 0.265625]\n",
      "11031: [discriminator loss: 0.528551, acc: 0.734375] [adversarial loss: 1.275684, acc: 0.125000]\n",
      "11032: [discriminator loss: 0.531069, acc: 0.734375] [adversarial loss: 0.960773, acc: 0.281250]\n",
      "11033: [discriminator loss: 0.548494, acc: 0.742188] [adversarial loss: 1.426088, acc: 0.187500]\n",
      "11034: [discriminator loss: 0.570853, acc: 0.703125] [adversarial loss: 0.988382, acc: 0.359375]\n",
      "11035: [discriminator loss: 0.543099, acc: 0.703125] [adversarial loss: 1.201808, acc: 0.171875]\n",
      "11036: [discriminator loss: 0.533827, acc: 0.679688] [adversarial loss: 1.208576, acc: 0.265625]\n",
      "11037: [discriminator loss: 0.578126, acc: 0.687500] [adversarial loss: 1.225223, acc: 0.359375]\n",
      "11038: [discriminator loss: 0.567876, acc: 0.679688] [adversarial loss: 0.950194, acc: 0.359375]\n",
      "11039: [discriminator loss: 0.592225, acc: 0.695312] [adversarial loss: 1.374200, acc: 0.171875]\n",
      "11040: [discriminator loss: 0.613292, acc: 0.648438] [adversarial loss: 1.087064, acc: 0.359375]\n",
      "11041: [discriminator loss: 0.595958, acc: 0.703125] [adversarial loss: 1.279263, acc: 0.171875]\n",
      "11042: [discriminator loss: 0.565866, acc: 0.679688] [adversarial loss: 1.407354, acc: 0.109375]\n",
      "11043: [discriminator loss: 0.573447, acc: 0.679688] [adversarial loss: 0.863735, acc: 0.406250]\n",
      "11044: [discriminator loss: 0.615989, acc: 0.671875] [adversarial loss: 1.358760, acc: 0.109375]\n",
      "11045: [discriminator loss: 0.502506, acc: 0.742188] [adversarial loss: 0.843189, acc: 0.453125]\n",
      "11046: [discriminator loss: 0.533262, acc: 0.750000] [adversarial loss: 1.341334, acc: 0.187500]\n",
      "11047: [discriminator loss: 0.557202, acc: 0.679688] [adversarial loss: 1.047475, acc: 0.265625]\n",
      "11048: [discriminator loss: 0.562548, acc: 0.726562] [adversarial loss: 1.118232, acc: 0.218750]\n",
      "11049: [discriminator loss: 0.508353, acc: 0.726562] [adversarial loss: 0.976268, acc: 0.343750]\n",
      "11050: [discriminator loss: 0.498378, acc: 0.757812] [adversarial loss: 1.692083, acc: 0.062500]\n",
      "11051: [discriminator loss: 0.532009, acc: 0.742188] [adversarial loss: 0.808908, acc: 0.453125]\n",
      "11052: [discriminator loss: 0.515489, acc: 0.742188] [adversarial loss: 1.114895, acc: 0.296875]\n",
      "11053: [discriminator loss: 0.550497, acc: 0.710938] [adversarial loss: 1.193417, acc: 0.250000]\n",
      "11054: [discriminator loss: 0.512760, acc: 0.734375] [adversarial loss: 1.096281, acc: 0.296875]\n",
      "11055: [discriminator loss: 0.565677, acc: 0.726562] [adversarial loss: 1.105459, acc: 0.265625]\n",
      "11056: [discriminator loss: 0.581985, acc: 0.679688] [adversarial loss: 1.415468, acc: 0.140625]\n",
      "11057: [discriminator loss: 0.493637, acc: 0.750000] [adversarial loss: 1.106706, acc: 0.250000]\n",
      "11058: [discriminator loss: 0.587570, acc: 0.648438] [adversarial loss: 1.457832, acc: 0.156250]\n",
      "11059: [discriminator loss: 0.500726, acc: 0.734375] [adversarial loss: 1.140311, acc: 0.281250]\n",
      "11060: [discriminator loss: 0.518338, acc: 0.750000] [adversarial loss: 1.363258, acc: 0.156250]\n",
      "11061: [discriminator loss: 0.516681, acc: 0.703125] [adversarial loss: 1.134500, acc: 0.234375]\n",
      "11062: [discriminator loss: 0.585162, acc: 0.726562] [adversarial loss: 0.978627, acc: 0.296875]\n",
      "11063: [discriminator loss: 0.547088, acc: 0.718750] [adversarial loss: 1.138909, acc: 0.296875]\n",
      "11064: [discriminator loss: 0.512367, acc: 0.742188] [adversarial loss: 1.302184, acc: 0.265625]\n",
      "11065: [discriminator loss: 0.571655, acc: 0.726562] [adversarial loss: 1.224741, acc: 0.234375]\n",
      "11066: [discriminator loss: 0.490664, acc: 0.750000] [adversarial loss: 1.021513, acc: 0.312500]\n",
      "11067: [discriminator loss: 0.466048, acc: 0.796875] [adversarial loss: 1.346128, acc: 0.140625]\n",
      "11068: [discriminator loss: 0.612984, acc: 0.679688] [adversarial loss: 1.012134, acc: 0.312500]\n",
      "11069: [discriminator loss: 0.572517, acc: 0.695312] [adversarial loss: 1.226509, acc: 0.171875]\n",
      "11070: [discriminator loss: 0.575658, acc: 0.687500] [adversarial loss: 0.835779, acc: 0.406250]\n",
      "11071: [discriminator loss: 0.532522, acc: 0.726562] [adversarial loss: 1.337645, acc: 0.125000]\n",
      "11072: [discriminator loss: 0.535787, acc: 0.750000] [adversarial loss: 0.877950, acc: 0.343750]\n",
      "11073: [discriminator loss: 0.566117, acc: 0.703125] [adversarial loss: 1.550650, acc: 0.093750]\n",
      "11074: [discriminator loss: 0.641297, acc: 0.671875] [adversarial loss: 0.767573, acc: 0.468750]\n",
      "11075: [discriminator loss: 0.569356, acc: 0.695312] [adversarial loss: 1.679763, acc: 0.031250]\n",
      "11076: [discriminator loss: 0.578616, acc: 0.695312] [adversarial loss: 0.841575, acc: 0.421875]\n",
      "11077: [discriminator loss: 0.585485, acc: 0.695312] [adversarial loss: 1.328357, acc: 0.125000]\n",
      "11078: [discriminator loss: 0.550024, acc: 0.734375] [adversarial loss: 0.921005, acc: 0.343750]\n",
      "11079: [discriminator loss: 0.529453, acc: 0.734375] [adversarial loss: 1.282755, acc: 0.171875]\n",
      "11080: [discriminator loss: 0.561483, acc: 0.671875] [adversarial loss: 1.044106, acc: 0.281250]\n",
      "11081: [discriminator loss: 0.526404, acc: 0.718750] [adversarial loss: 1.360516, acc: 0.140625]\n",
      "11082: [discriminator loss: 0.491241, acc: 0.773438] [adversarial loss: 1.157213, acc: 0.234375]\n",
      "11083: [discriminator loss: 0.517612, acc: 0.742188] [adversarial loss: 1.299962, acc: 0.250000]\n",
      "11084: [discriminator loss: 0.595352, acc: 0.679688] [adversarial loss: 1.204465, acc: 0.250000]\n",
      "11085: [discriminator loss: 0.547155, acc: 0.757812] [adversarial loss: 1.208008, acc: 0.187500]\n",
      "11086: [discriminator loss: 0.595757, acc: 0.679688] [adversarial loss: 0.990569, acc: 0.375000]\n",
      "11087: [discriminator loss: 0.599913, acc: 0.656250] [adversarial loss: 1.632252, acc: 0.093750]\n",
      "11088: [discriminator loss: 0.507550, acc: 0.757812] [adversarial loss: 0.896077, acc: 0.421875]\n",
      "11089: [discriminator loss: 0.502622, acc: 0.742188] [adversarial loss: 1.308681, acc: 0.187500]\n",
      "11090: [discriminator loss: 0.645802, acc: 0.648438] [adversarial loss: 0.936543, acc: 0.437500]\n",
      "11091: [discriminator loss: 0.602370, acc: 0.679688] [adversarial loss: 1.336573, acc: 0.093750]\n",
      "11092: [discriminator loss: 0.537991, acc: 0.757812] [adversarial loss: 1.225173, acc: 0.265625]\n",
      "11093: [discriminator loss: 0.558832, acc: 0.726562] [adversarial loss: 1.318915, acc: 0.156250]\n",
      "11094: [discriminator loss: 0.588773, acc: 0.671875] [adversarial loss: 1.078621, acc: 0.312500]\n",
      "11095: [discriminator loss: 0.581267, acc: 0.679688] [adversarial loss: 1.267609, acc: 0.109375]\n",
      "11096: [discriminator loss: 0.481180, acc: 0.789062] [adversarial loss: 1.021739, acc: 0.281250]\n",
      "11097: [discriminator loss: 0.572963, acc: 0.656250] [adversarial loss: 1.115932, acc: 0.296875]\n",
      "11098: [discriminator loss: 0.547721, acc: 0.710938] [adversarial loss: 1.173049, acc: 0.234375]\n",
      "11099: [discriminator loss: 0.572400, acc: 0.695312] [adversarial loss: 0.913831, acc: 0.421875]\n",
      "11100: [discriminator loss: 0.485003, acc: 0.742188] [adversarial loss: 1.271500, acc: 0.171875]\n",
      "11101: [discriminator loss: 0.482327, acc: 0.765625] [adversarial loss: 1.154076, acc: 0.250000]\n",
      "11102: [discriminator loss: 0.559207, acc: 0.703125] [adversarial loss: 1.202080, acc: 0.125000]\n",
      "11103: [discriminator loss: 0.516594, acc: 0.773438] [adversarial loss: 1.122208, acc: 0.250000]\n",
      "11104: [discriminator loss: 0.557538, acc: 0.679688] [adversarial loss: 0.934661, acc: 0.421875]\n",
      "11105: [discriminator loss: 0.605803, acc: 0.664062] [adversarial loss: 1.446836, acc: 0.187500]\n",
      "11106: [discriminator loss: 0.574127, acc: 0.695312] [adversarial loss: 0.838669, acc: 0.468750]\n",
      "11107: [discriminator loss: 0.578855, acc: 0.695312] [adversarial loss: 1.232320, acc: 0.187500]\n",
      "11108: [discriminator loss: 0.542403, acc: 0.710938] [adversarial loss: 1.030674, acc: 0.343750]\n",
      "11109: [discriminator loss: 0.567223, acc: 0.726562] [adversarial loss: 1.022681, acc: 0.312500]\n",
      "11110: [discriminator loss: 0.540792, acc: 0.742188] [adversarial loss: 1.352936, acc: 0.156250]\n",
      "11111: [discriminator loss: 0.527409, acc: 0.726562] [adversarial loss: 0.963292, acc: 0.421875]\n",
      "11112: [discriminator loss: 0.599206, acc: 0.648438] [adversarial loss: 1.668150, acc: 0.156250]\n",
      "11113: [discriminator loss: 0.652040, acc: 0.601562] [adversarial loss: 0.845297, acc: 0.468750]\n",
      "11114: [discriminator loss: 0.532008, acc: 0.726562] [adversarial loss: 1.429450, acc: 0.093750]\n",
      "11115: [discriminator loss: 0.628560, acc: 0.640625] [adversarial loss: 0.846991, acc: 0.437500]\n",
      "11116: [discriminator loss: 0.556252, acc: 0.703125] [adversarial loss: 1.263941, acc: 0.093750]\n",
      "11117: [discriminator loss: 0.486444, acc: 0.789062] [adversarial loss: 1.119536, acc: 0.203125]\n",
      "11118: [discriminator loss: 0.519873, acc: 0.726562] [adversarial loss: 1.403020, acc: 0.140625]\n",
      "11119: [discriminator loss: 0.646403, acc: 0.656250] [adversarial loss: 0.974179, acc: 0.328125]\n",
      "11120: [discriminator loss: 0.573838, acc: 0.648438] [adversarial loss: 1.446671, acc: 0.062500]\n",
      "11121: [discriminator loss: 0.622055, acc: 0.648438] [adversarial loss: 0.900241, acc: 0.406250]\n",
      "11122: [discriminator loss: 0.593270, acc: 0.679688] [adversarial loss: 1.274255, acc: 0.187500]\n",
      "11123: [discriminator loss: 0.515819, acc: 0.734375] [adversarial loss: 0.799827, acc: 0.453125]\n",
      "11124: [discriminator loss: 0.469938, acc: 0.765625] [adversarial loss: 1.356502, acc: 0.218750]\n",
      "11125: [discriminator loss: 0.563748, acc: 0.679688] [adversarial loss: 1.001278, acc: 0.359375]\n",
      "11126: [discriminator loss: 0.577097, acc: 0.671875] [adversarial loss: 1.114950, acc: 0.281250]\n",
      "11127: [discriminator loss: 0.495325, acc: 0.765625] [adversarial loss: 1.170277, acc: 0.250000]\n",
      "11128: [discriminator loss: 0.584082, acc: 0.742188] [adversarial loss: 1.466386, acc: 0.203125]\n",
      "11129: [discriminator loss: 0.587379, acc: 0.703125] [adversarial loss: 0.866778, acc: 0.468750]\n",
      "11130: [discriminator loss: 0.565114, acc: 0.734375] [adversarial loss: 1.381139, acc: 0.171875]\n",
      "11131: [discriminator loss: 0.534901, acc: 0.734375] [adversarial loss: 0.996021, acc: 0.375000]\n",
      "11132: [discriminator loss: 0.595728, acc: 0.687500] [adversarial loss: 0.873078, acc: 0.406250]\n",
      "11133: [discriminator loss: 0.553289, acc: 0.664062] [adversarial loss: 1.394087, acc: 0.156250]\n",
      "11134: [discriminator loss: 0.504494, acc: 0.734375] [adversarial loss: 1.016227, acc: 0.296875]\n",
      "11135: [discriminator loss: 0.559083, acc: 0.664062] [adversarial loss: 1.206722, acc: 0.218750]\n",
      "11136: [discriminator loss: 0.549944, acc: 0.695312] [adversarial loss: 1.269225, acc: 0.234375]\n",
      "11137: [discriminator loss: 0.480601, acc: 0.773438] [adversarial loss: 1.200918, acc: 0.250000]\n",
      "11138: [discriminator loss: 0.569276, acc: 0.625000] [adversarial loss: 1.135354, acc: 0.328125]\n",
      "11139: [discriminator loss: 0.498649, acc: 0.718750] [adversarial loss: 1.274691, acc: 0.156250]\n",
      "11140: [discriminator loss: 0.606866, acc: 0.640625] [adversarial loss: 0.894086, acc: 0.468750]\n",
      "11141: [discriminator loss: 0.581029, acc: 0.710938] [adversarial loss: 1.802664, acc: 0.125000]\n",
      "11142: [discriminator loss: 0.660675, acc: 0.640625] [adversarial loss: 0.815856, acc: 0.421875]\n",
      "11143: [discriminator loss: 0.698934, acc: 0.617188] [adversarial loss: 1.498760, acc: 0.078125]\n",
      "11144: [discriminator loss: 0.632968, acc: 0.640625] [adversarial loss: 1.003080, acc: 0.312500]\n",
      "11145: [discriminator loss: 0.526366, acc: 0.734375] [adversarial loss: 1.225365, acc: 0.140625]\n",
      "11146: [discriminator loss: 0.563843, acc: 0.687500] [adversarial loss: 1.024477, acc: 0.234375]\n",
      "11147: [discriminator loss: 0.527403, acc: 0.726562] [adversarial loss: 1.212009, acc: 0.296875]\n",
      "11148: [discriminator loss: 0.521257, acc: 0.750000] [adversarial loss: 0.916497, acc: 0.468750]\n",
      "11149: [discriminator loss: 0.534324, acc: 0.750000] [adversarial loss: 1.282547, acc: 0.218750]\n",
      "11150: [discriminator loss: 0.500238, acc: 0.750000] [adversarial loss: 1.101779, acc: 0.312500]\n",
      "11151: [discriminator loss: 0.539417, acc: 0.671875] [adversarial loss: 1.387166, acc: 0.187500]\n",
      "11152: [discriminator loss: 0.507840, acc: 0.742188] [adversarial loss: 0.976293, acc: 0.390625]\n",
      "11153: [discriminator loss: 0.589933, acc: 0.687500] [adversarial loss: 1.349811, acc: 0.156250]\n",
      "11154: [discriminator loss: 0.543717, acc: 0.695312] [adversarial loss: 1.133754, acc: 0.265625]\n",
      "11155: [discriminator loss: 0.516834, acc: 0.742188] [adversarial loss: 1.224862, acc: 0.171875]\n",
      "11156: [discriminator loss: 0.528210, acc: 0.703125] [adversarial loss: 0.984934, acc: 0.250000]\n",
      "11157: [discriminator loss: 0.539456, acc: 0.710938] [adversarial loss: 1.322969, acc: 0.156250]\n",
      "11158: [discriminator loss: 0.545274, acc: 0.718750] [adversarial loss: 0.928833, acc: 0.328125]\n",
      "11159: [discriminator loss: 0.565230, acc: 0.679688] [adversarial loss: 1.351592, acc: 0.109375]\n",
      "11160: [discriminator loss: 0.561591, acc: 0.718750] [adversarial loss: 0.865424, acc: 0.468750]\n",
      "11161: [discriminator loss: 0.591709, acc: 0.664062] [adversarial loss: 1.216073, acc: 0.156250]\n",
      "11162: [discriminator loss: 0.556139, acc: 0.726562] [adversarial loss: 1.279092, acc: 0.218750]\n",
      "11163: [discriminator loss: 0.662005, acc: 0.617188] [adversarial loss: 1.274017, acc: 0.093750]\n",
      "11164: [discriminator loss: 0.628394, acc: 0.625000] [adversarial loss: 0.942132, acc: 0.328125]\n",
      "11165: [discriminator loss: 0.592661, acc: 0.703125] [adversarial loss: 1.356476, acc: 0.187500]\n",
      "11166: [discriminator loss: 0.561165, acc: 0.703125] [adversarial loss: 0.780868, acc: 0.562500]\n",
      "11167: [discriminator loss: 0.625050, acc: 0.648438] [adversarial loss: 1.451664, acc: 0.109375]\n",
      "11168: [discriminator loss: 0.625038, acc: 0.671875] [adversarial loss: 0.953439, acc: 0.359375]\n",
      "11169: [discriminator loss: 0.511630, acc: 0.773438] [adversarial loss: 1.144266, acc: 0.187500]\n",
      "11170: [discriminator loss: 0.513896, acc: 0.734375] [adversarial loss: 1.208747, acc: 0.265625]\n",
      "11171: [discriminator loss: 0.593071, acc: 0.671875] [adversarial loss: 1.037815, acc: 0.375000]\n",
      "11172: [discriminator loss: 0.570033, acc: 0.726562] [adversarial loss: 1.448904, acc: 0.093750]\n",
      "11173: [discriminator loss: 0.521257, acc: 0.625000] [adversarial loss: 0.847277, acc: 0.437500]\n",
      "11174: [discriminator loss: 0.588907, acc: 0.656250] [adversarial loss: 1.479949, acc: 0.125000]\n",
      "11175: [discriminator loss: 0.588853, acc: 0.671875] [adversarial loss: 0.823277, acc: 0.468750]\n",
      "11176: [discriminator loss: 0.570815, acc: 0.703125] [adversarial loss: 1.457268, acc: 0.156250]\n",
      "11177: [discriminator loss: 0.669610, acc: 0.671875] [adversarial loss: 0.735918, acc: 0.468750]\n",
      "11178: [discriminator loss: 0.536955, acc: 0.757812] [adversarial loss: 1.197370, acc: 0.265625]\n",
      "11179: [discriminator loss: 0.505687, acc: 0.781250] [adversarial loss: 1.146292, acc: 0.265625]\n",
      "11180: [discriminator loss: 0.536037, acc: 0.726562] [adversarial loss: 0.980020, acc: 0.359375]\n",
      "11181: [discriminator loss: 0.489675, acc: 0.742188] [adversarial loss: 1.102607, acc: 0.281250]\n",
      "11182: [discriminator loss: 0.547577, acc: 0.734375] [adversarial loss: 1.008906, acc: 0.328125]\n",
      "11183: [discriminator loss: 0.542924, acc: 0.750000] [adversarial loss: 1.350718, acc: 0.140625]\n",
      "11184: [discriminator loss: 0.539271, acc: 0.695312] [adversarial loss: 1.120549, acc: 0.296875]\n",
      "11185: [discriminator loss: 0.536109, acc: 0.710938] [adversarial loss: 0.977558, acc: 0.281250]\n",
      "11186: [discriminator loss: 0.589529, acc: 0.632812] [adversarial loss: 1.056844, acc: 0.218750]\n",
      "11187: [discriminator loss: 0.522059, acc: 0.773438] [adversarial loss: 1.163087, acc: 0.234375]\n",
      "11188: [discriminator loss: 0.473442, acc: 0.820312] [adversarial loss: 0.959948, acc: 0.312500]\n",
      "11189: [discriminator loss: 0.622351, acc: 0.695312] [adversarial loss: 1.171370, acc: 0.171875]\n",
      "11190: [discriminator loss: 0.505336, acc: 0.773438] [adversarial loss: 1.405403, acc: 0.093750]\n",
      "11191: [discriminator loss: 0.612313, acc: 0.687500] [adversarial loss: 1.086252, acc: 0.265625]\n",
      "11192: [discriminator loss: 0.590321, acc: 0.695312] [adversarial loss: 1.196209, acc: 0.203125]\n",
      "11193: [discriminator loss: 0.468832, acc: 0.796875] [adversarial loss: 1.209577, acc: 0.281250]\n",
      "11194: [discriminator loss: 0.577472, acc: 0.679688] [adversarial loss: 1.130004, acc: 0.218750]\n",
      "11195: [discriminator loss: 0.543980, acc: 0.679688] [adversarial loss: 0.960185, acc: 0.328125]\n",
      "11196: [discriminator loss: 0.533697, acc: 0.726562] [adversarial loss: 1.298283, acc: 0.203125]\n",
      "11197: [discriminator loss: 0.571622, acc: 0.710938] [adversarial loss: 0.829681, acc: 0.500000]\n",
      "11198: [discriminator loss: 0.539486, acc: 0.687500] [adversarial loss: 1.482004, acc: 0.140625]\n",
      "11199: [discriminator loss: 0.535313, acc: 0.726562] [adversarial loss: 0.852015, acc: 0.468750]\n",
      "11200: [discriminator loss: 0.618592, acc: 0.671875] [adversarial loss: 1.294229, acc: 0.156250]\n",
      "11201: [discriminator loss: 0.644524, acc: 0.617188] [adversarial loss: 1.014944, acc: 0.406250]\n",
      "11202: [discriminator loss: 0.582979, acc: 0.640625] [adversarial loss: 1.517622, acc: 0.109375]\n",
      "11203: [discriminator loss: 0.587827, acc: 0.679688] [adversarial loss: 0.940680, acc: 0.375000]\n",
      "11204: [discriminator loss: 0.568367, acc: 0.703125] [adversarial loss: 1.378008, acc: 0.234375]\n",
      "11205: [discriminator loss: 0.572893, acc: 0.671875] [adversarial loss: 1.011483, acc: 0.375000]\n",
      "11206: [discriminator loss: 0.519265, acc: 0.765625] [adversarial loss: 0.907677, acc: 0.453125]\n",
      "11207: [discriminator loss: 0.547830, acc: 0.695312] [adversarial loss: 1.430433, acc: 0.140625]\n",
      "11208: [discriminator loss: 0.581930, acc: 0.695312] [adversarial loss: 1.047913, acc: 0.265625]\n",
      "11209: [discriminator loss: 0.525776, acc: 0.750000] [adversarial loss: 1.079484, acc: 0.218750]\n",
      "11210: [discriminator loss: 0.511153, acc: 0.726562] [adversarial loss: 1.479614, acc: 0.093750]\n",
      "11211: [discriminator loss: 0.495310, acc: 0.750000] [adversarial loss: 0.902369, acc: 0.421875]\n",
      "11212: [discriminator loss: 0.556710, acc: 0.726562] [adversarial loss: 1.516657, acc: 0.171875]\n",
      "11213: [discriminator loss: 0.622310, acc: 0.671875] [adversarial loss: 0.780382, acc: 0.468750]\n",
      "11214: [discriminator loss: 0.556214, acc: 0.734375] [adversarial loss: 1.365646, acc: 0.171875]\n",
      "11215: [discriminator loss: 0.572344, acc: 0.687500] [adversarial loss: 1.060250, acc: 0.281250]\n",
      "11216: [discriminator loss: 0.531217, acc: 0.726562] [adversarial loss: 1.186062, acc: 0.250000]\n",
      "11217: [discriminator loss: 0.529052, acc: 0.718750] [adversarial loss: 1.082984, acc: 0.328125]\n",
      "11218: [discriminator loss: 0.528199, acc: 0.742188] [adversarial loss: 1.445950, acc: 0.171875]\n",
      "11219: [discriminator loss: 0.619358, acc: 0.648438] [adversarial loss: 0.839835, acc: 0.468750]\n",
      "11220: [discriminator loss: 0.547037, acc: 0.750000] [adversarial loss: 1.510787, acc: 0.140625]\n",
      "11221: [discriminator loss: 0.603074, acc: 0.687500] [adversarial loss: 0.853408, acc: 0.468750]\n",
      "11222: [discriminator loss: 0.620153, acc: 0.687500] [adversarial loss: 1.433314, acc: 0.109375]\n",
      "11223: [discriminator loss: 0.544481, acc: 0.687500] [adversarial loss: 1.067243, acc: 0.250000]\n",
      "11224: [discriminator loss: 0.553188, acc: 0.718750] [adversarial loss: 1.207257, acc: 0.218750]\n",
      "11225: [discriminator loss: 0.588623, acc: 0.632812] [adversarial loss: 0.807335, acc: 0.437500]\n",
      "11226: [discriminator loss: 0.556430, acc: 0.718750] [adversarial loss: 1.337170, acc: 0.203125]\n",
      "11227: [discriminator loss: 0.630221, acc: 0.632812] [adversarial loss: 0.872822, acc: 0.421875]\n",
      "11228: [discriminator loss: 0.589526, acc: 0.742188] [adversarial loss: 1.494909, acc: 0.140625]\n",
      "11229: [discriminator loss: 0.533597, acc: 0.710938] [adversarial loss: 1.107731, acc: 0.234375]\n",
      "11230: [discriminator loss: 0.477620, acc: 0.796875] [adversarial loss: 1.521373, acc: 0.093750]\n",
      "11231: [discriminator loss: 0.650134, acc: 0.632812] [adversarial loss: 0.931162, acc: 0.359375]\n",
      "11232: [discriminator loss: 0.505004, acc: 0.773438] [adversarial loss: 1.429242, acc: 0.140625]\n",
      "11233: [discriminator loss: 0.480753, acc: 0.773438] [adversarial loss: 1.178067, acc: 0.281250]\n",
      "11234: [discriminator loss: 0.522366, acc: 0.726562] [adversarial loss: 0.867671, acc: 0.421875]\n",
      "11235: [discriminator loss: 0.538156, acc: 0.703125] [adversarial loss: 1.314213, acc: 0.171875]\n",
      "11236: [discriminator loss: 0.527025, acc: 0.703125] [adversarial loss: 1.076513, acc: 0.296875]\n",
      "11237: [discriminator loss: 0.533674, acc: 0.703125] [adversarial loss: 1.194347, acc: 0.203125]\n",
      "11238: [discriminator loss: 0.519511, acc: 0.718750] [adversarial loss: 1.061787, acc: 0.281250]\n",
      "11239: [discriminator loss: 0.555228, acc: 0.687500] [adversarial loss: 1.245477, acc: 0.218750]\n",
      "11240: [discriminator loss: 0.559993, acc: 0.687500] [adversarial loss: 0.901538, acc: 0.437500]\n",
      "11241: [discriminator loss: 0.536077, acc: 0.765625] [adversarial loss: 1.232307, acc: 0.250000]\n",
      "11242: [discriminator loss: 0.611859, acc: 0.671875] [adversarial loss: 1.010857, acc: 0.343750]\n",
      "11243: [discriminator loss: 0.509704, acc: 0.734375] [adversarial loss: 1.822512, acc: 0.078125]\n",
      "11244: [discriminator loss: 0.559827, acc: 0.718750] [adversarial loss: 0.937365, acc: 0.359375]\n",
      "11245: [discriminator loss: 0.506014, acc: 0.726562] [adversarial loss: 1.433086, acc: 0.125000]\n",
      "11246: [discriminator loss: 0.581427, acc: 0.679688] [adversarial loss: 1.000509, acc: 0.281250]\n",
      "11247: [discriminator loss: 0.544808, acc: 0.726562] [adversarial loss: 1.071327, acc: 0.203125]\n",
      "11248: [discriminator loss: 0.576881, acc: 0.710938] [adversarial loss: 1.269304, acc: 0.203125]\n",
      "11249: [discriminator loss: 0.494246, acc: 0.789062] [adversarial loss: 0.916027, acc: 0.406250]\n",
      "11250: [discriminator loss: 0.582448, acc: 0.679688] [adversarial loss: 1.348424, acc: 0.125000]\n",
      "11251: [discriminator loss: 0.472158, acc: 0.820312] [adversarial loss: 0.749039, acc: 0.593750]\n",
      "11252: [discriminator loss: 0.509349, acc: 0.679688] [adversarial loss: 1.514736, acc: 0.140625]\n",
      "11253: [discriminator loss: 0.641718, acc: 0.640625] [adversarial loss: 0.961766, acc: 0.359375]\n",
      "11254: [discriminator loss: 0.541708, acc: 0.710938] [adversarial loss: 1.180766, acc: 0.250000]\n",
      "11255: [discriminator loss: 0.625471, acc: 0.617188] [adversarial loss: 1.111078, acc: 0.281250]\n",
      "11256: [discriminator loss: 0.549654, acc: 0.703125] [adversarial loss: 1.444999, acc: 0.187500]\n",
      "11257: [discriminator loss: 0.568467, acc: 0.703125] [adversarial loss: 1.186628, acc: 0.218750]\n",
      "11258: [discriminator loss: 0.601922, acc: 0.648438] [adversarial loss: 1.331619, acc: 0.171875]\n",
      "11259: [discriminator loss: 0.511753, acc: 0.757812] [adversarial loss: 0.899826, acc: 0.406250]\n",
      "11260: [discriminator loss: 0.517074, acc: 0.742188] [adversarial loss: 1.076678, acc: 0.296875]\n",
      "11261: [discriminator loss: 0.578250, acc: 0.726562] [adversarial loss: 1.226563, acc: 0.250000]\n",
      "11262: [discriminator loss: 0.624073, acc: 0.640625] [adversarial loss: 0.780167, acc: 0.500000]\n",
      "11263: [discriminator loss: 0.583713, acc: 0.671875] [adversarial loss: 1.634217, acc: 0.093750]\n",
      "11264: [discriminator loss: 0.610247, acc: 0.664062] [adversarial loss: 0.810800, acc: 0.468750]\n",
      "11265: [discriminator loss: 0.540696, acc: 0.734375] [adversarial loss: 1.538302, acc: 0.140625]\n",
      "11266: [discriminator loss: 0.554618, acc: 0.664062] [adversarial loss: 0.806462, acc: 0.546875]\n",
      "11267: [discriminator loss: 0.558875, acc: 0.726562] [adversarial loss: 1.337549, acc: 0.171875]\n",
      "11268: [discriminator loss: 0.550328, acc: 0.734375] [adversarial loss: 0.980734, acc: 0.359375]\n",
      "11269: [discriminator loss: 0.608553, acc: 0.687500] [adversarial loss: 1.321210, acc: 0.187500]\n",
      "11270: [discriminator loss: 0.513467, acc: 0.710938] [adversarial loss: 1.052549, acc: 0.359375]\n",
      "11271: [discriminator loss: 0.561184, acc: 0.687500] [adversarial loss: 1.109583, acc: 0.281250]\n",
      "11272: [discriminator loss: 0.540033, acc: 0.695312] [adversarial loss: 1.146563, acc: 0.250000]\n",
      "11273: [discriminator loss: 0.554698, acc: 0.703125] [adversarial loss: 1.062155, acc: 0.359375]\n",
      "11274: [discriminator loss: 0.528353, acc: 0.734375] [adversarial loss: 0.948071, acc: 0.390625]\n",
      "11275: [discriminator loss: 0.618427, acc: 0.648438] [adversarial loss: 1.089444, acc: 0.312500]\n",
      "11276: [discriminator loss: 0.535607, acc: 0.726562] [adversarial loss: 1.141733, acc: 0.265625]\n",
      "11277: [discriminator loss: 0.538779, acc: 0.710938] [adversarial loss: 1.393352, acc: 0.156250]\n",
      "11278: [discriminator loss: 0.566443, acc: 0.726562] [adversarial loss: 0.949133, acc: 0.375000]\n",
      "11279: [discriminator loss: 0.543507, acc: 0.734375] [adversarial loss: 1.229698, acc: 0.265625]\n",
      "11280: [discriminator loss: 0.574032, acc: 0.671875] [adversarial loss: 1.284315, acc: 0.203125]\n",
      "11281: [discriminator loss: 0.522316, acc: 0.796875] [adversarial loss: 1.200763, acc: 0.187500]\n",
      "11282: [discriminator loss: 0.535050, acc: 0.742188] [adversarial loss: 1.077697, acc: 0.250000]\n",
      "11283: [discriminator loss: 0.548236, acc: 0.726562] [adversarial loss: 1.256026, acc: 0.187500]\n",
      "11284: [discriminator loss: 0.638406, acc: 0.625000] [adversarial loss: 1.091756, acc: 0.250000]\n",
      "11285: [discriminator loss: 0.586360, acc: 0.640625] [adversarial loss: 1.135626, acc: 0.312500]\n",
      "11286: [discriminator loss: 0.508965, acc: 0.781250] [adversarial loss: 1.182230, acc: 0.265625]\n",
      "11287: [discriminator loss: 0.559726, acc: 0.703125] [adversarial loss: 0.895553, acc: 0.343750]\n",
      "11288: [discriminator loss: 0.620322, acc: 0.695312] [adversarial loss: 1.381007, acc: 0.187500]\n",
      "11289: [discriminator loss: 0.575828, acc: 0.726562] [adversarial loss: 0.812908, acc: 0.421875]\n",
      "11290: [discriminator loss: 0.550004, acc: 0.710938] [adversarial loss: 1.643174, acc: 0.078125]\n",
      "11291: [discriminator loss: 0.508750, acc: 0.726562] [adversarial loss: 0.964346, acc: 0.453125]\n",
      "11292: [discriminator loss: 0.556462, acc: 0.710938] [adversarial loss: 1.285070, acc: 0.234375]\n",
      "11293: [discriminator loss: 0.570971, acc: 0.710938] [adversarial loss: 0.955040, acc: 0.375000]\n",
      "11294: [discriminator loss: 0.489750, acc: 0.726562] [adversarial loss: 1.288342, acc: 0.203125]\n",
      "11295: [discriminator loss: 0.509936, acc: 0.718750] [adversarial loss: 1.095679, acc: 0.250000]\n",
      "11296: [discriminator loss: 0.537342, acc: 0.710938] [adversarial loss: 1.465655, acc: 0.156250]\n",
      "11297: [discriminator loss: 0.582169, acc: 0.664062] [adversarial loss: 1.052281, acc: 0.265625]\n",
      "11298: [discriminator loss: 0.589119, acc: 0.718750] [adversarial loss: 1.288316, acc: 0.171875]\n",
      "11299: [discriminator loss: 0.588568, acc: 0.671875] [adversarial loss: 0.908746, acc: 0.421875]\n",
      "11300: [discriminator loss: 0.539039, acc: 0.710938] [adversarial loss: 1.277209, acc: 0.140625]\n",
      "11301: [discriminator loss: 0.611902, acc: 0.664062] [adversarial loss: 0.917292, acc: 0.453125]\n",
      "11302: [discriminator loss: 0.622726, acc: 0.695312] [adversarial loss: 1.306636, acc: 0.250000]\n",
      "11303: [discriminator loss: 0.532203, acc: 0.742188] [adversarial loss: 1.062020, acc: 0.296875]\n",
      "11304: [discriminator loss: 0.532452, acc: 0.718750] [adversarial loss: 1.310017, acc: 0.218750]\n",
      "11305: [discriminator loss: 0.584475, acc: 0.679688] [adversarial loss: 1.037200, acc: 0.375000]\n",
      "11306: [discriminator loss: 0.528682, acc: 0.742188] [adversarial loss: 1.139111, acc: 0.265625]\n",
      "11307: [discriminator loss: 0.646313, acc: 0.648438] [adversarial loss: 1.119951, acc: 0.234375]\n",
      "11308: [discriminator loss: 0.520725, acc: 0.734375] [adversarial loss: 1.237148, acc: 0.234375]\n",
      "11309: [discriminator loss: 0.530881, acc: 0.726562] [adversarial loss: 1.164149, acc: 0.234375]\n",
      "11310: [discriminator loss: 0.616575, acc: 0.656250] [adversarial loss: 1.039810, acc: 0.312500]\n",
      "11311: [discriminator loss: 0.593338, acc: 0.679688] [adversarial loss: 1.277179, acc: 0.171875]\n",
      "11312: [discriminator loss: 0.490433, acc: 0.750000] [adversarial loss: 1.115630, acc: 0.250000]\n",
      "11313: [discriminator loss: 0.510224, acc: 0.750000] [adversarial loss: 1.230416, acc: 0.265625]\n",
      "11314: [discriminator loss: 0.572860, acc: 0.687500] [adversarial loss: 1.087745, acc: 0.281250]\n",
      "11315: [discriminator loss: 0.520902, acc: 0.789062] [adversarial loss: 1.115207, acc: 0.250000]\n",
      "11316: [discriminator loss: 0.553366, acc: 0.687500] [adversarial loss: 1.027209, acc: 0.312500]\n",
      "11317: [discriminator loss: 0.516996, acc: 0.703125] [adversarial loss: 1.006005, acc: 0.234375]\n",
      "11318: [discriminator loss: 0.610303, acc: 0.671875] [adversarial loss: 0.978960, acc: 0.359375]\n",
      "11319: [discriminator loss: 0.563076, acc: 0.687500] [adversarial loss: 1.653208, acc: 0.062500]\n",
      "11320: [discriminator loss: 0.585442, acc: 0.671875] [adversarial loss: 0.782473, acc: 0.484375]\n",
      "11321: [discriminator loss: 0.642949, acc: 0.648438] [adversarial loss: 1.482426, acc: 0.203125]\n",
      "11322: [discriminator loss: 0.614557, acc: 0.664062] [adversarial loss: 0.918559, acc: 0.406250]\n",
      "11323: [discriminator loss: 0.572774, acc: 0.679688] [adversarial loss: 1.226484, acc: 0.203125]\n",
      "11324: [discriminator loss: 0.517597, acc: 0.703125] [adversarial loss: 0.953578, acc: 0.296875]\n",
      "11325: [discriminator loss: 0.570444, acc: 0.664062] [adversarial loss: 1.237507, acc: 0.187500]\n",
      "11326: [discriminator loss: 0.609098, acc: 0.679688] [adversarial loss: 1.425879, acc: 0.109375]\n",
      "11327: [discriminator loss: 0.589622, acc: 0.687500] [adversarial loss: 0.895007, acc: 0.359375]\n",
      "11328: [discriminator loss: 0.593511, acc: 0.695312] [adversarial loss: 1.308174, acc: 0.171875]\n",
      "11329: [discriminator loss: 0.560852, acc: 0.656250] [adversarial loss: 1.053101, acc: 0.296875]\n",
      "11330: [discriminator loss: 0.633265, acc: 0.617188] [adversarial loss: 1.045598, acc: 0.234375]\n",
      "11331: [discriminator loss: 0.505869, acc: 0.726562] [adversarial loss: 1.004692, acc: 0.281250]\n",
      "11332: [discriminator loss: 0.561443, acc: 0.710938] [adversarial loss: 1.208295, acc: 0.250000]\n",
      "11333: [discriminator loss: 0.488398, acc: 0.757812] [adversarial loss: 1.142479, acc: 0.265625]\n",
      "11334: [discriminator loss: 0.569686, acc: 0.703125] [adversarial loss: 1.300774, acc: 0.218750]\n",
      "11335: [discriminator loss: 0.572282, acc: 0.726562] [adversarial loss: 1.025963, acc: 0.343750]\n",
      "11336: [discriminator loss: 0.563065, acc: 0.710938] [adversarial loss: 1.282518, acc: 0.250000]\n",
      "11337: [discriminator loss: 0.538426, acc: 0.750000] [adversarial loss: 0.975082, acc: 0.390625]\n",
      "11338: [discriminator loss: 0.555519, acc: 0.703125] [adversarial loss: 1.249976, acc: 0.156250]\n",
      "11339: [discriminator loss: 0.519784, acc: 0.718750] [adversarial loss: 0.897036, acc: 0.421875]\n",
      "11340: [discriminator loss: 0.656781, acc: 0.625000] [adversarial loss: 1.211501, acc: 0.203125]\n",
      "11341: [discriminator loss: 0.523218, acc: 0.734375] [adversarial loss: 0.984164, acc: 0.296875]\n",
      "11342: [discriminator loss: 0.468979, acc: 0.781250] [adversarial loss: 1.513911, acc: 0.187500]\n",
      "11343: [discriminator loss: 0.625560, acc: 0.664062] [adversarial loss: 1.241526, acc: 0.218750]\n",
      "11344: [discriminator loss: 0.521788, acc: 0.695312] [adversarial loss: 1.165468, acc: 0.250000]\n",
      "11345: [discriminator loss: 0.484687, acc: 0.781250] [adversarial loss: 1.073885, acc: 0.281250]\n",
      "11346: [discriminator loss: 0.553765, acc: 0.710938] [adversarial loss: 1.330511, acc: 0.218750]\n",
      "11347: [discriminator loss: 0.550467, acc: 0.710938] [adversarial loss: 1.065167, acc: 0.218750]\n",
      "11348: [discriminator loss: 0.575397, acc: 0.718750] [adversarial loss: 1.214574, acc: 0.265625]\n",
      "11349: [discriminator loss: 0.580882, acc: 0.664062] [adversarial loss: 1.070207, acc: 0.296875]\n",
      "11350: [discriminator loss: 0.611622, acc: 0.656250] [adversarial loss: 1.195681, acc: 0.187500]\n",
      "11351: [discriminator loss: 0.550629, acc: 0.695312] [adversarial loss: 1.047692, acc: 0.328125]\n",
      "11352: [discriminator loss: 0.622727, acc: 0.632812] [adversarial loss: 1.280174, acc: 0.187500]\n",
      "11353: [discriminator loss: 0.522798, acc: 0.742188] [adversarial loss: 1.044989, acc: 0.281250]\n",
      "11354: [discriminator loss: 0.534027, acc: 0.726562] [adversarial loss: 1.300066, acc: 0.125000]\n",
      "11355: [discriminator loss: 0.529555, acc: 0.734375] [adversarial loss: 0.921081, acc: 0.421875]\n",
      "11356: [discriminator loss: 0.548416, acc: 0.695312] [adversarial loss: 1.348337, acc: 0.125000]\n",
      "11357: [discriminator loss: 0.584923, acc: 0.648438] [adversarial loss: 1.058631, acc: 0.312500]\n",
      "11358: [discriminator loss: 0.593265, acc: 0.687500] [adversarial loss: 1.235375, acc: 0.218750]\n",
      "11359: [discriminator loss: 0.570592, acc: 0.640625] [adversarial loss: 1.000248, acc: 0.375000]\n",
      "11360: [discriminator loss: 0.575010, acc: 0.656250] [adversarial loss: 1.274817, acc: 0.109375]\n",
      "11361: [discriminator loss: 0.546953, acc: 0.671875] [adversarial loss: 1.099238, acc: 0.281250]\n",
      "11362: [discriminator loss: 0.554995, acc: 0.703125] [adversarial loss: 1.338329, acc: 0.187500]\n",
      "11363: [discriminator loss: 0.595122, acc: 0.632812] [adversarial loss: 0.779342, acc: 0.562500]\n",
      "11364: [discriminator loss: 0.656092, acc: 0.617188] [adversarial loss: 1.385394, acc: 0.156250]\n",
      "11365: [discriminator loss: 0.550279, acc: 0.695312] [adversarial loss: 0.884464, acc: 0.421875]\n",
      "11366: [discriminator loss: 0.526739, acc: 0.773438] [adversarial loss: 1.362969, acc: 0.171875]\n",
      "11367: [discriminator loss: 0.591880, acc: 0.648438] [adversarial loss: 0.960265, acc: 0.390625]\n",
      "11368: [discriminator loss: 0.534224, acc: 0.726562] [adversarial loss: 1.317137, acc: 0.203125]\n",
      "11369: [discriminator loss: 0.529174, acc: 0.726562] [adversarial loss: 1.163504, acc: 0.218750]\n",
      "11370: [discriminator loss: 0.612599, acc: 0.656250] [adversarial loss: 0.960607, acc: 0.359375]\n",
      "11371: [discriminator loss: 0.507649, acc: 0.773438] [adversarial loss: 1.299131, acc: 0.140625]\n",
      "11372: [discriminator loss: 0.565488, acc: 0.664062] [adversarial loss: 1.121049, acc: 0.265625]\n",
      "11373: [discriminator loss: 0.526950, acc: 0.742188] [adversarial loss: 1.348128, acc: 0.171875]\n",
      "11374: [discriminator loss: 0.594335, acc: 0.703125] [adversarial loss: 0.926406, acc: 0.328125]\n",
      "11375: [discriminator loss: 0.545911, acc: 0.757812] [adversarial loss: 1.243848, acc: 0.156250]\n",
      "11376: [discriminator loss: 0.527765, acc: 0.726562] [adversarial loss: 1.017916, acc: 0.343750]\n",
      "11377: [discriminator loss: 0.508346, acc: 0.789062] [adversarial loss: 1.023624, acc: 0.437500]\n",
      "11378: [discriminator loss: 0.537156, acc: 0.742188] [adversarial loss: 1.160604, acc: 0.265625]\n",
      "11379: [discriminator loss: 0.528762, acc: 0.718750] [adversarial loss: 1.031768, acc: 0.343750]\n",
      "11380: [discriminator loss: 0.542599, acc: 0.695312] [adversarial loss: 0.979324, acc: 0.359375]\n",
      "11381: [discriminator loss: 0.537694, acc: 0.726562] [adversarial loss: 1.587046, acc: 0.093750]\n",
      "11382: [discriminator loss: 0.537875, acc: 0.734375] [adversarial loss: 0.893716, acc: 0.406250]\n",
      "11383: [discriminator loss: 0.610327, acc: 0.632812] [adversarial loss: 1.545046, acc: 0.093750]\n",
      "11384: [discriminator loss: 0.595371, acc: 0.648438] [adversarial loss: 0.745041, acc: 0.515625]\n",
      "11385: [discriminator loss: 0.610422, acc: 0.679688] [adversarial loss: 1.282746, acc: 0.187500]\n",
      "11386: [discriminator loss: 0.521999, acc: 0.679688] [adversarial loss: 0.749675, acc: 0.500000]\n",
      "11387: [discriminator loss: 0.633689, acc: 0.625000] [adversarial loss: 1.587367, acc: 0.062500]\n",
      "11388: [discriminator loss: 0.603509, acc: 0.664062] [adversarial loss: 0.911891, acc: 0.390625]\n",
      "11389: [discriminator loss: 0.573660, acc: 0.656250] [adversarial loss: 1.519384, acc: 0.093750]\n",
      "11390: [discriminator loss: 0.517131, acc: 0.750000] [adversarial loss: 1.071504, acc: 0.296875]\n",
      "11391: [discriminator loss: 0.563454, acc: 0.703125] [adversarial loss: 1.278120, acc: 0.156250]\n",
      "11392: [discriminator loss: 0.529727, acc: 0.710938] [adversarial loss: 1.081165, acc: 0.265625]\n",
      "11393: [discriminator loss: 0.578346, acc: 0.710938] [adversarial loss: 1.009606, acc: 0.250000]\n",
      "11394: [discriminator loss: 0.574826, acc: 0.687500] [adversarial loss: 1.219326, acc: 0.171875]\n",
      "11395: [discriminator loss: 0.559447, acc: 0.695312] [adversarial loss: 1.102554, acc: 0.218750]\n",
      "11396: [discriminator loss: 0.540090, acc: 0.656250] [adversarial loss: 1.349142, acc: 0.171875]\n",
      "11397: [discriminator loss: 0.634008, acc: 0.656250] [adversarial loss: 0.915625, acc: 0.343750]\n",
      "11398: [discriminator loss: 0.650434, acc: 0.593750] [adversarial loss: 1.595954, acc: 0.031250]\n",
      "11399: [discriminator loss: 0.548179, acc: 0.710938] [adversarial loss: 0.830037, acc: 0.453125]\n",
      "11400: [discriminator loss: 0.587201, acc: 0.679688] [adversarial loss: 1.591712, acc: 0.031250]\n",
      "11401: [discriminator loss: 0.562490, acc: 0.703125] [adversarial loss: 0.922777, acc: 0.375000]\n",
      "11402: [discriminator loss: 0.556514, acc: 0.671875] [adversarial loss: 1.479533, acc: 0.093750]\n",
      "11403: [discriminator loss: 0.508761, acc: 0.742188] [adversarial loss: 1.013401, acc: 0.312500]\n",
      "11404: [discriminator loss: 0.558631, acc: 0.718750] [adversarial loss: 1.231734, acc: 0.203125]\n",
      "11405: [discriminator loss: 0.548486, acc: 0.695312] [adversarial loss: 1.299452, acc: 0.187500]\n",
      "11406: [discriminator loss: 0.539736, acc: 0.718750] [adversarial loss: 0.992121, acc: 0.296875]\n",
      "11407: [discriminator loss: 0.493564, acc: 0.757812] [adversarial loss: 1.171185, acc: 0.218750]\n",
      "11408: [discriminator loss: 0.519912, acc: 0.671875] [adversarial loss: 0.986675, acc: 0.328125]\n",
      "11409: [discriminator loss: 0.544936, acc: 0.703125] [adversarial loss: 1.165571, acc: 0.234375]\n",
      "11410: [discriminator loss: 0.579267, acc: 0.687500] [adversarial loss: 1.168561, acc: 0.187500]\n",
      "11411: [discriminator loss: 0.560129, acc: 0.687500] [adversarial loss: 1.065132, acc: 0.281250]\n",
      "11412: [discriminator loss: 0.566012, acc: 0.695312] [adversarial loss: 1.338619, acc: 0.109375]\n",
      "11413: [discriminator loss: 0.523061, acc: 0.742188] [adversarial loss: 0.890166, acc: 0.484375]\n",
      "11414: [discriminator loss: 0.503093, acc: 0.734375] [adversarial loss: 1.481250, acc: 0.140625]\n",
      "11415: [discriminator loss: 0.515038, acc: 0.718750] [adversarial loss: 0.971730, acc: 0.328125]\n",
      "11416: [discriminator loss: 0.604249, acc: 0.671875] [adversarial loss: 0.954098, acc: 0.328125]\n",
      "11417: [discriminator loss: 0.517992, acc: 0.734375] [adversarial loss: 1.023584, acc: 0.359375]\n",
      "11418: [discriminator loss: 0.572186, acc: 0.679688] [adversarial loss: 1.049905, acc: 0.250000]\n",
      "11419: [discriminator loss: 0.521421, acc: 0.781250] [adversarial loss: 1.048972, acc: 0.328125]\n",
      "11420: [discriminator loss: 0.534271, acc: 0.687500] [adversarial loss: 1.113552, acc: 0.296875]\n",
      "11421: [discriminator loss: 0.533178, acc: 0.710938] [adversarial loss: 1.215136, acc: 0.109375]\n",
      "11422: [discriminator loss: 0.605001, acc: 0.640625] [adversarial loss: 1.006212, acc: 0.343750]\n",
      "11423: [discriminator loss: 0.543881, acc: 0.734375] [adversarial loss: 0.899090, acc: 0.390625]\n",
      "11424: [discriminator loss: 0.550136, acc: 0.710938] [adversarial loss: 1.384465, acc: 0.109375]\n",
      "11425: [discriminator loss: 0.478486, acc: 0.757812] [adversarial loss: 0.998299, acc: 0.281250]\n",
      "11426: [discriminator loss: 0.655369, acc: 0.632812] [adversarial loss: 1.075951, acc: 0.281250]\n",
      "11427: [discriminator loss: 0.551934, acc: 0.718750] [adversarial loss: 1.006287, acc: 0.328125]\n",
      "11428: [discriminator loss: 0.595945, acc: 0.695312] [adversarial loss: 1.532362, acc: 0.171875]\n",
      "11429: [discriminator loss: 0.466560, acc: 0.804688] [adversarial loss: 1.192686, acc: 0.296875]\n",
      "11430: [discriminator loss: 0.572216, acc: 0.734375] [adversarial loss: 1.166906, acc: 0.218750]\n",
      "11431: [discriminator loss: 0.523956, acc: 0.687500] [adversarial loss: 1.249825, acc: 0.093750]\n",
      "11432: [discriminator loss: 0.627809, acc: 0.656250] [adversarial loss: 0.926613, acc: 0.328125]\n",
      "11433: [discriminator loss: 0.525438, acc: 0.750000] [adversarial loss: 1.336084, acc: 0.203125]\n",
      "11434: [discriminator loss: 0.556909, acc: 0.710938] [adversarial loss: 0.818493, acc: 0.546875]\n",
      "11435: [discriminator loss: 0.625789, acc: 0.632812] [adversarial loss: 1.488774, acc: 0.093750]\n",
      "11436: [discriminator loss: 0.605754, acc: 0.625000] [adversarial loss: 0.876356, acc: 0.375000]\n",
      "11437: [discriminator loss: 0.519466, acc: 0.742188] [adversarial loss: 1.360771, acc: 0.140625]\n",
      "11438: [discriminator loss: 0.679614, acc: 0.609375] [adversarial loss: 0.908506, acc: 0.406250]\n",
      "11439: [discriminator loss: 0.552422, acc: 0.679688] [adversarial loss: 1.280867, acc: 0.109375]\n",
      "11440: [discriminator loss: 0.519011, acc: 0.734375] [adversarial loss: 1.177098, acc: 0.203125]\n",
      "11441: [discriminator loss: 0.514238, acc: 0.710938] [adversarial loss: 1.175911, acc: 0.156250]\n",
      "11442: [discriminator loss: 0.494858, acc: 0.750000] [adversarial loss: 1.140849, acc: 0.187500]\n",
      "11443: [discriminator loss: 0.564752, acc: 0.664062] [adversarial loss: 1.211280, acc: 0.203125]\n",
      "11444: [discriminator loss: 0.615907, acc: 0.664062] [adversarial loss: 0.841233, acc: 0.437500]\n",
      "11445: [discriminator loss: 0.532199, acc: 0.695312] [adversarial loss: 1.151879, acc: 0.250000]\n",
      "11446: [discriminator loss: 0.544952, acc: 0.703125] [adversarial loss: 1.173644, acc: 0.281250]\n",
      "11447: [discriminator loss: 0.541921, acc: 0.710938] [adversarial loss: 1.320474, acc: 0.203125]\n",
      "11448: [discriminator loss: 0.545147, acc: 0.734375] [adversarial loss: 0.819770, acc: 0.453125]\n",
      "11449: [discriminator loss: 0.584027, acc: 0.671875] [adversarial loss: 1.401356, acc: 0.109375]\n",
      "11450: [discriminator loss: 0.514687, acc: 0.718750] [adversarial loss: 0.795979, acc: 0.468750]\n",
      "11451: [discriminator loss: 0.436198, acc: 0.781250] [adversarial loss: 1.212454, acc: 0.203125]\n",
      "11452: [discriminator loss: 0.548651, acc: 0.679688] [adversarial loss: 1.233160, acc: 0.187500]\n",
      "11453: [discriminator loss: 0.514212, acc: 0.695312] [adversarial loss: 1.227554, acc: 0.218750]\n",
      "11454: [discriminator loss: 0.507357, acc: 0.718750] [adversarial loss: 1.005257, acc: 0.359375]\n",
      "11455: [discriminator loss: 0.510010, acc: 0.781250] [adversarial loss: 1.314355, acc: 0.140625]\n",
      "11456: [discriminator loss: 0.540558, acc: 0.695312] [adversarial loss: 0.891198, acc: 0.437500]\n",
      "11457: [discriminator loss: 0.588750, acc: 0.609375] [adversarial loss: 1.362864, acc: 0.125000]\n",
      "11458: [discriminator loss: 0.522762, acc: 0.726562] [adversarial loss: 1.135838, acc: 0.203125]\n",
      "11459: [discriminator loss: 0.549009, acc: 0.695312] [adversarial loss: 1.425804, acc: 0.171875]\n",
      "11460: [discriminator loss: 0.496131, acc: 0.765625] [adversarial loss: 0.893211, acc: 0.390625]\n",
      "11461: [discriminator loss: 0.576582, acc: 0.687500] [adversarial loss: 1.404790, acc: 0.109375]\n",
      "11462: [discriminator loss: 0.534381, acc: 0.718750] [adversarial loss: 0.874767, acc: 0.453125]\n",
      "11463: [discriminator loss: 0.492200, acc: 0.734375] [adversarial loss: 1.350432, acc: 0.234375]\n",
      "11464: [discriminator loss: 0.572580, acc: 0.664062] [adversarial loss: 0.988736, acc: 0.406250]\n",
      "11465: [discriminator loss: 0.603238, acc: 0.664062] [adversarial loss: 1.290924, acc: 0.171875]\n",
      "11466: [discriminator loss: 0.562051, acc: 0.726562] [adversarial loss: 1.055885, acc: 0.359375]\n",
      "11467: [discriminator loss: 0.588030, acc: 0.687500] [adversarial loss: 1.288106, acc: 0.171875]\n",
      "11468: [discriminator loss: 0.545085, acc: 0.750000] [adversarial loss: 1.002847, acc: 0.250000]\n",
      "11469: [discriminator loss: 0.532192, acc: 0.726562] [adversarial loss: 1.170053, acc: 0.203125]\n",
      "11470: [discriminator loss: 0.595423, acc: 0.671875] [adversarial loss: 1.293051, acc: 0.218750]\n",
      "11471: [discriminator loss: 0.652180, acc: 0.640625] [adversarial loss: 0.961389, acc: 0.390625]\n",
      "11472: [discriminator loss: 0.593289, acc: 0.679688] [adversarial loss: 1.457102, acc: 0.109375]\n",
      "11473: [discriminator loss: 0.637271, acc: 0.648438] [adversarial loss: 0.988771, acc: 0.359375]\n",
      "11474: [discriminator loss: 0.552030, acc: 0.679688] [adversarial loss: 1.329566, acc: 0.203125]\n",
      "11475: [discriminator loss: 0.568319, acc: 0.718750] [adversarial loss: 0.878796, acc: 0.421875]\n",
      "11476: [discriminator loss: 0.453953, acc: 0.773438] [adversarial loss: 1.247471, acc: 0.156250]\n",
      "11477: [discriminator loss: 0.571180, acc: 0.671875] [adversarial loss: 1.036593, acc: 0.312500]\n",
      "11478: [discriminator loss: 0.545198, acc: 0.695312] [adversarial loss: 1.020554, acc: 0.312500]\n",
      "11479: [discriminator loss: 0.517745, acc: 0.734375] [adversarial loss: 1.482684, acc: 0.156250]\n",
      "11480: [discriminator loss: 0.613558, acc: 0.656250] [adversarial loss: 0.799470, acc: 0.531250]\n",
      "11481: [discriminator loss: 0.578940, acc: 0.687500] [adversarial loss: 1.489330, acc: 0.093750]\n",
      "11482: [discriminator loss: 0.622089, acc: 0.656250] [adversarial loss: 1.061751, acc: 0.343750]\n",
      "11483: [discriminator loss: 0.585248, acc: 0.679688] [adversarial loss: 1.254369, acc: 0.187500]\n",
      "11484: [discriminator loss: 0.586774, acc: 0.710938] [adversarial loss: 0.867017, acc: 0.437500]\n",
      "11485: [discriminator loss: 0.590870, acc: 0.695312] [adversarial loss: 1.246115, acc: 0.171875]\n",
      "11486: [discriminator loss: 0.676064, acc: 0.640625] [adversarial loss: 0.819873, acc: 0.484375]\n",
      "11487: [discriminator loss: 0.544038, acc: 0.695312] [adversarial loss: 1.433587, acc: 0.156250]\n",
      "11488: [discriminator loss: 0.553874, acc: 0.710938] [adversarial loss: 0.852845, acc: 0.421875]\n",
      "11489: [discriminator loss: 0.572904, acc: 0.632812] [adversarial loss: 1.360058, acc: 0.203125]\n",
      "11490: [discriminator loss: 0.542854, acc: 0.750000] [adversarial loss: 0.968281, acc: 0.359375]\n",
      "11491: [discriminator loss: 0.561673, acc: 0.710938] [adversarial loss: 1.072082, acc: 0.312500]\n",
      "11492: [discriminator loss: 0.519524, acc: 0.773438] [adversarial loss: 1.212344, acc: 0.218750]\n",
      "11493: [discriminator loss: 0.526791, acc: 0.718750] [adversarial loss: 1.428633, acc: 0.140625]\n",
      "11494: [discriminator loss: 0.564893, acc: 0.710938] [adversarial loss: 0.913553, acc: 0.390625]\n",
      "11495: [discriminator loss: 0.518604, acc: 0.734375] [adversarial loss: 0.955535, acc: 0.375000]\n",
      "11496: [discriminator loss: 0.558135, acc: 0.718750] [adversarial loss: 1.148115, acc: 0.328125]\n",
      "11497: [discriminator loss: 0.577984, acc: 0.664062] [adversarial loss: 1.032423, acc: 0.296875]\n",
      "11498: [discriminator loss: 0.622419, acc: 0.687500] [adversarial loss: 1.291292, acc: 0.140625]\n",
      "11499: [discriminator loss: 0.539308, acc: 0.718750] [adversarial loss: 1.108916, acc: 0.281250]\n",
      "11500: [discriminator loss: 0.611204, acc: 0.664062] [adversarial loss: 1.210797, acc: 0.203125]\n",
      "11501: [discriminator loss: 0.605535, acc: 0.679688] [adversarial loss: 1.208040, acc: 0.234375]\n",
      "11502: [discriminator loss: 0.525403, acc: 0.726562] [adversarial loss: 1.148444, acc: 0.234375]\n",
      "11503: [discriminator loss: 0.475635, acc: 0.765625] [adversarial loss: 1.154077, acc: 0.250000]\n",
      "11504: [discriminator loss: 0.543402, acc: 0.687500] [adversarial loss: 1.006766, acc: 0.281250]\n",
      "11505: [discriminator loss: 0.677152, acc: 0.562500] [adversarial loss: 1.355053, acc: 0.156250]\n",
      "11506: [discriminator loss: 0.523249, acc: 0.703125] [adversarial loss: 0.896037, acc: 0.406250]\n",
      "11507: [discriminator loss: 0.529736, acc: 0.703125] [adversarial loss: 0.906062, acc: 0.281250]\n",
      "11508: [discriminator loss: 0.542948, acc: 0.726562] [adversarial loss: 1.056442, acc: 0.250000]\n",
      "11509: [discriminator loss: 0.527172, acc: 0.726562] [adversarial loss: 1.103876, acc: 0.296875]\n",
      "11510: [discriminator loss: 0.479582, acc: 0.773438] [adversarial loss: 1.084237, acc: 0.343750]\n",
      "11511: [discriminator loss: 0.554439, acc: 0.734375] [adversarial loss: 1.094128, acc: 0.296875]\n",
      "11512: [discriminator loss: 0.497635, acc: 0.765625] [adversarial loss: 1.288429, acc: 0.156250]\n",
      "11513: [discriminator loss: 0.457686, acc: 0.781250] [adversarial loss: 1.288834, acc: 0.218750]\n",
      "11514: [discriminator loss: 0.485274, acc: 0.734375] [adversarial loss: 0.979631, acc: 0.312500]\n",
      "11515: [discriminator loss: 0.635870, acc: 0.664062] [adversarial loss: 1.362512, acc: 0.109375]\n",
      "11516: [discriminator loss: 0.635427, acc: 0.632812] [adversarial loss: 0.706289, acc: 0.562500]\n",
      "11517: [discriminator loss: 0.614469, acc: 0.679688] [adversarial loss: 1.531870, acc: 0.125000]\n",
      "11518: [discriminator loss: 0.588926, acc: 0.679688] [adversarial loss: 0.769794, acc: 0.531250]\n",
      "11519: [discriminator loss: 0.544059, acc: 0.687500] [adversarial loss: 1.336087, acc: 0.125000]\n",
      "11520: [discriminator loss: 0.508849, acc: 0.765625] [adversarial loss: 1.151399, acc: 0.265625]\n",
      "11521: [discriminator loss: 0.564431, acc: 0.742188] [adversarial loss: 1.106989, acc: 0.312500]\n",
      "11522: [discriminator loss: 0.548937, acc: 0.757812] [adversarial loss: 1.034262, acc: 0.250000]\n",
      "11523: [discriminator loss: 0.544811, acc: 0.718750] [adversarial loss: 1.175982, acc: 0.265625]\n",
      "11524: [discriminator loss: 0.520080, acc: 0.757812] [adversarial loss: 1.265930, acc: 0.156250]\n",
      "11525: [discriminator loss: 0.577608, acc: 0.710938] [adversarial loss: 1.001243, acc: 0.250000]\n",
      "11526: [discriminator loss: 0.560991, acc: 0.679688] [adversarial loss: 1.166500, acc: 0.250000]\n",
      "11527: [discriminator loss: 0.554901, acc: 0.734375] [adversarial loss: 1.022610, acc: 0.296875]\n",
      "11528: [discriminator loss: 0.517745, acc: 0.750000] [adversarial loss: 1.111647, acc: 0.250000]\n",
      "11529: [discriminator loss: 0.559834, acc: 0.742188] [adversarial loss: 1.234713, acc: 0.171875]\n",
      "11530: [discriminator loss: 0.529278, acc: 0.734375] [adversarial loss: 1.004256, acc: 0.328125]\n",
      "11531: [discriminator loss: 0.544419, acc: 0.750000] [adversarial loss: 1.375443, acc: 0.093750]\n",
      "11532: [discriminator loss: 0.496772, acc: 0.757812] [adversarial loss: 1.112755, acc: 0.265625]\n",
      "11533: [discriminator loss: 0.519948, acc: 0.718750] [adversarial loss: 1.317787, acc: 0.171875]\n",
      "11534: [discriminator loss: 0.569234, acc: 0.726562] [adversarial loss: 1.006943, acc: 0.312500]\n",
      "11535: [discriminator loss: 0.571081, acc: 0.679688] [adversarial loss: 1.547196, acc: 0.125000]\n",
      "11536: [discriminator loss: 0.493966, acc: 0.718750] [adversarial loss: 0.775762, acc: 0.546875]\n",
      "11537: [discriminator loss: 0.592676, acc: 0.679688] [adversarial loss: 1.516499, acc: 0.140625]\n",
      "11538: [discriminator loss: 0.479172, acc: 0.773438] [adversarial loss: 0.936885, acc: 0.390625]\n",
      "11539: [discriminator loss: 0.524553, acc: 0.742188] [adversarial loss: 1.624515, acc: 0.109375]\n",
      "11540: [discriminator loss: 0.534423, acc: 0.734375] [adversarial loss: 1.030268, acc: 0.359375]\n",
      "11541: [discriminator loss: 0.494928, acc: 0.734375] [adversarial loss: 1.236351, acc: 0.265625]\n",
      "11542: [discriminator loss: 0.567828, acc: 0.750000] [adversarial loss: 1.308047, acc: 0.234375]\n",
      "11543: [discriminator loss: 0.544435, acc: 0.703125] [adversarial loss: 1.224517, acc: 0.218750]\n",
      "11544: [discriminator loss: 0.485264, acc: 0.765625] [adversarial loss: 0.903843, acc: 0.406250]\n",
      "11545: [discriminator loss: 0.535243, acc: 0.789062] [adversarial loss: 1.469010, acc: 0.140625]\n",
      "11546: [discriminator loss: 0.538995, acc: 0.710938] [adversarial loss: 0.690094, acc: 0.593750]\n",
      "11547: [discriminator loss: 0.555990, acc: 0.703125] [adversarial loss: 1.532292, acc: 0.109375]\n",
      "11548: [discriminator loss: 0.635478, acc: 0.632812] [adversarial loss: 0.956438, acc: 0.421875]\n",
      "11549: [discriminator loss: 0.604334, acc: 0.687500] [adversarial loss: 1.057384, acc: 0.375000]\n",
      "11550: [discriminator loss: 0.497996, acc: 0.773438] [adversarial loss: 1.138467, acc: 0.250000]\n",
      "11551: [discriminator loss: 0.612193, acc: 0.671875] [adversarial loss: 1.135149, acc: 0.296875]\n",
      "11552: [discriminator loss: 0.546200, acc: 0.750000] [adversarial loss: 1.167205, acc: 0.218750]\n",
      "11553: [discriminator loss: 0.606135, acc: 0.656250] [adversarial loss: 0.766913, acc: 0.578125]\n",
      "11554: [discriminator loss: 0.664379, acc: 0.625000] [adversarial loss: 1.502822, acc: 0.109375]\n",
      "11555: [discriminator loss: 0.528735, acc: 0.726562] [adversarial loss: 1.179787, acc: 0.187500]\n",
      "11556: [discriminator loss: 0.488074, acc: 0.781250] [adversarial loss: 0.954668, acc: 0.343750]\n",
      "11557: [discriminator loss: 0.530230, acc: 0.695312] [adversarial loss: 0.885405, acc: 0.406250]\n",
      "11558: [discriminator loss: 0.553443, acc: 0.710938] [adversarial loss: 1.194253, acc: 0.187500]\n",
      "11559: [discriminator loss: 0.568478, acc: 0.679688] [adversarial loss: 1.139926, acc: 0.203125]\n",
      "11560: [discriminator loss: 0.501268, acc: 0.757812] [adversarial loss: 1.301274, acc: 0.171875]\n",
      "11561: [discriminator loss: 0.563550, acc: 0.671875] [adversarial loss: 0.951518, acc: 0.328125]\n",
      "11562: [discriminator loss: 0.558379, acc: 0.757812] [adversarial loss: 1.213298, acc: 0.218750]\n",
      "11563: [discriminator loss: 0.556266, acc: 0.687500] [adversarial loss: 0.866896, acc: 0.453125]\n",
      "11564: [discriminator loss: 0.528853, acc: 0.765625] [adversarial loss: 1.400154, acc: 0.203125]\n",
      "11565: [discriminator loss: 0.566681, acc: 0.679688] [adversarial loss: 1.097204, acc: 0.328125]\n",
      "11566: [discriminator loss: 0.502691, acc: 0.757812] [adversarial loss: 1.405821, acc: 0.125000]\n",
      "11567: [discriminator loss: 0.546733, acc: 0.718750] [adversarial loss: 0.894274, acc: 0.390625]\n",
      "11568: [discriminator loss: 0.589451, acc: 0.710938] [adversarial loss: 1.324768, acc: 0.140625]\n",
      "11569: [discriminator loss: 0.460273, acc: 0.812500] [adversarial loss: 1.070906, acc: 0.312500]\n",
      "11570: [discriminator loss: 0.612044, acc: 0.671875] [adversarial loss: 1.270766, acc: 0.234375]\n",
      "11571: [discriminator loss: 0.551707, acc: 0.664062] [adversarial loss: 1.217071, acc: 0.359375]\n",
      "11572: [discriminator loss: 0.521927, acc: 0.679688] [adversarial loss: 1.299563, acc: 0.140625]\n",
      "11573: [discriminator loss: 0.482343, acc: 0.750000] [adversarial loss: 0.774229, acc: 0.515625]\n",
      "11574: [discriminator loss: 0.635596, acc: 0.640625] [adversarial loss: 1.806957, acc: 0.031250]\n",
      "11575: [discriminator loss: 0.580439, acc: 0.687500] [adversarial loss: 0.903012, acc: 0.406250]\n",
      "11576: [discriminator loss: 0.563442, acc: 0.703125] [adversarial loss: 1.285169, acc: 0.140625]\n",
      "11577: [discriminator loss: 0.557164, acc: 0.695312] [adversarial loss: 1.109033, acc: 0.250000]\n",
      "11578: [discriminator loss: 0.517421, acc: 0.718750] [adversarial loss: 1.270899, acc: 0.203125]\n",
      "11579: [discriminator loss: 0.540700, acc: 0.750000] [adversarial loss: 1.083891, acc: 0.296875]\n",
      "11580: [discriminator loss: 0.541662, acc: 0.726562] [adversarial loss: 1.238731, acc: 0.234375]\n",
      "11581: [discriminator loss: 0.460549, acc: 0.789062] [adversarial loss: 1.075624, acc: 0.265625]\n",
      "11582: [discriminator loss: 0.656121, acc: 0.632812] [adversarial loss: 1.429589, acc: 0.140625]\n",
      "11583: [discriminator loss: 0.545906, acc: 0.726562] [adversarial loss: 1.070256, acc: 0.312500]\n",
      "11584: [discriminator loss: 0.577285, acc: 0.726562] [adversarial loss: 1.261701, acc: 0.140625]\n",
      "11585: [discriminator loss: 0.532105, acc: 0.757812] [adversarial loss: 1.028648, acc: 0.328125]\n",
      "11586: [discriminator loss: 0.475948, acc: 0.781250] [adversarial loss: 1.329256, acc: 0.187500]\n",
      "11587: [discriminator loss: 0.519503, acc: 0.679688] [adversarial loss: 1.148466, acc: 0.296875]\n",
      "11588: [discriminator loss: 0.637043, acc: 0.664062] [adversarial loss: 1.281729, acc: 0.218750]\n",
      "11589: [discriminator loss: 0.611157, acc: 0.664062] [adversarial loss: 1.275283, acc: 0.140625]\n",
      "11590: [discriminator loss: 0.527638, acc: 0.734375] [adversarial loss: 1.430701, acc: 0.140625]\n",
      "11591: [discriminator loss: 0.563866, acc: 0.687500] [adversarial loss: 0.870118, acc: 0.468750]\n",
      "11592: [discriminator loss: 0.481426, acc: 0.781250] [adversarial loss: 1.395842, acc: 0.140625]\n",
      "11593: [discriminator loss: 0.569084, acc: 0.703125] [adversarial loss: 0.890618, acc: 0.406250]\n",
      "11594: [discriminator loss: 0.547474, acc: 0.718750] [adversarial loss: 1.106025, acc: 0.328125]\n",
      "11595: [discriminator loss: 0.548138, acc: 0.734375] [adversarial loss: 0.983577, acc: 0.343750]\n",
      "11596: [discriminator loss: 0.511451, acc: 0.695312] [adversarial loss: 1.159265, acc: 0.234375]\n",
      "11597: [discriminator loss: 0.517816, acc: 0.757812] [adversarial loss: 1.216627, acc: 0.203125]\n",
      "11598: [discriminator loss: 0.547337, acc: 0.710938] [adversarial loss: 0.996021, acc: 0.312500]\n",
      "11599: [discriminator loss: 0.532070, acc: 0.742188] [adversarial loss: 1.719878, acc: 0.046875]\n",
      "11600: [discriminator loss: 0.496476, acc: 0.703125] [adversarial loss: 0.837528, acc: 0.484375]\n",
      "11601: [discriminator loss: 0.579216, acc: 0.710938] [adversarial loss: 1.513942, acc: 0.171875]\n",
      "11602: [discriminator loss: 0.623808, acc: 0.671875] [adversarial loss: 1.143471, acc: 0.296875]\n",
      "11603: [discriminator loss: 0.630910, acc: 0.648438] [adversarial loss: 1.069353, acc: 0.343750]\n",
      "11604: [discriminator loss: 0.500028, acc: 0.726562] [adversarial loss: 1.140935, acc: 0.234375]\n",
      "11605: [discriminator loss: 0.557210, acc: 0.726562] [adversarial loss: 0.922135, acc: 0.375000]\n",
      "11606: [discriminator loss: 0.548302, acc: 0.718750] [adversarial loss: 1.240724, acc: 0.171875]\n",
      "11607: [discriminator loss: 0.551560, acc: 0.703125] [adversarial loss: 0.945453, acc: 0.406250]\n",
      "11608: [discriminator loss: 0.555687, acc: 0.687500] [adversarial loss: 1.447554, acc: 0.125000]\n",
      "11609: [discriminator loss: 0.597160, acc: 0.687500] [adversarial loss: 0.818088, acc: 0.468750]\n",
      "11610: [discriminator loss: 0.586784, acc: 0.687500] [adversarial loss: 1.196029, acc: 0.265625]\n",
      "11611: [discriminator loss: 0.574990, acc: 0.671875] [adversarial loss: 1.154613, acc: 0.296875]\n",
      "11612: [discriminator loss: 0.524432, acc: 0.734375] [adversarial loss: 0.974214, acc: 0.343750]\n",
      "11613: [discriminator loss: 0.556271, acc: 0.687500] [adversarial loss: 1.417286, acc: 0.140625]\n",
      "11614: [discriminator loss: 0.562565, acc: 0.750000] [adversarial loss: 1.094108, acc: 0.265625]\n",
      "11615: [discriminator loss: 0.533660, acc: 0.742188] [adversarial loss: 1.094616, acc: 0.328125]\n",
      "11616: [discriminator loss: 0.602972, acc: 0.687500] [adversarial loss: 1.121148, acc: 0.281250]\n",
      "11617: [discriminator loss: 0.557292, acc: 0.718750] [adversarial loss: 1.308438, acc: 0.125000]\n",
      "11618: [discriminator loss: 0.541317, acc: 0.703125] [adversarial loss: 0.966888, acc: 0.375000]\n",
      "11619: [discriminator loss: 0.523832, acc: 0.789062] [adversarial loss: 1.263546, acc: 0.203125]\n",
      "11620: [discriminator loss: 0.607101, acc: 0.687500] [adversarial loss: 1.244943, acc: 0.218750]\n",
      "11621: [discriminator loss: 0.579048, acc: 0.671875] [adversarial loss: 0.997828, acc: 0.343750]\n",
      "11622: [discriminator loss: 0.623701, acc: 0.703125] [adversarial loss: 1.005431, acc: 0.312500]\n",
      "11623: [discriminator loss: 0.477223, acc: 0.742188] [adversarial loss: 1.483023, acc: 0.234375]\n",
      "11624: [discriminator loss: 0.618543, acc: 0.648438] [adversarial loss: 0.783631, acc: 0.500000]\n",
      "11625: [discriminator loss: 0.547387, acc: 0.710938] [adversarial loss: 1.974611, acc: 0.015625]\n",
      "11626: [discriminator loss: 0.653541, acc: 0.609375] [adversarial loss: 0.968886, acc: 0.328125]\n",
      "11627: [discriminator loss: 0.475219, acc: 0.750000] [adversarial loss: 1.346769, acc: 0.156250]\n",
      "11628: [discriminator loss: 0.579863, acc: 0.671875] [adversarial loss: 1.070894, acc: 0.359375]\n",
      "11629: [discriminator loss: 0.537749, acc: 0.757812] [adversarial loss: 1.199227, acc: 0.265625]\n",
      "11630: [discriminator loss: 0.514404, acc: 0.734375] [adversarial loss: 1.310668, acc: 0.265625]\n",
      "11631: [discriminator loss: 0.557067, acc: 0.734375] [adversarial loss: 1.114268, acc: 0.171875]\n",
      "11632: [discriminator loss: 0.500926, acc: 0.726562] [adversarial loss: 0.989820, acc: 0.296875]\n",
      "11633: [discriminator loss: 0.519030, acc: 0.757812] [adversarial loss: 1.162977, acc: 0.156250]\n",
      "11634: [discriminator loss: 0.517838, acc: 0.757812] [adversarial loss: 1.013299, acc: 0.343750]\n",
      "11635: [discriminator loss: 0.546978, acc: 0.703125] [adversarial loss: 1.375992, acc: 0.125000]\n",
      "11636: [discriminator loss: 0.530750, acc: 0.734375] [adversarial loss: 0.916271, acc: 0.390625]\n",
      "11637: [discriminator loss: 0.565960, acc: 0.648438] [adversarial loss: 1.208653, acc: 0.234375]\n",
      "11638: [discriminator loss: 0.572518, acc: 0.726562] [adversarial loss: 1.145704, acc: 0.218750]\n",
      "11639: [discriminator loss: 0.548154, acc: 0.671875] [adversarial loss: 1.090871, acc: 0.171875]\n",
      "11640: [discriminator loss: 0.597472, acc: 0.648438] [adversarial loss: 0.934072, acc: 0.375000]\n",
      "11641: [discriminator loss: 0.469581, acc: 0.789062] [adversarial loss: 1.206681, acc: 0.234375]\n",
      "11642: [discriminator loss: 0.584167, acc: 0.687500] [adversarial loss: 1.108640, acc: 0.250000]\n",
      "11643: [discriminator loss: 0.522392, acc: 0.679688] [adversarial loss: 1.325010, acc: 0.125000]\n",
      "11644: [discriminator loss: 0.532296, acc: 0.703125] [adversarial loss: 1.050177, acc: 0.312500]\n",
      "11645: [discriminator loss: 0.532724, acc: 0.742188] [adversarial loss: 1.016366, acc: 0.390625]\n",
      "11646: [discriminator loss: 0.622916, acc: 0.625000] [adversarial loss: 1.337915, acc: 0.109375]\n",
      "11647: [discriminator loss: 0.537436, acc: 0.734375] [adversarial loss: 1.121252, acc: 0.250000]\n",
      "11648: [discriminator loss: 0.551211, acc: 0.742188] [adversarial loss: 0.917374, acc: 0.375000]\n",
      "11649: [discriminator loss: 0.547956, acc: 0.742188] [adversarial loss: 1.101768, acc: 0.296875]\n",
      "11650: [discriminator loss: 0.537067, acc: 0.695312] [adversarial loss: 1.168398, acc: 0.187500]\n",
      "11651: [discriminator loss: 0.540634, acc: 0.695312] [adversarial loss: 1.162635, acc: 0.234375]\n",
      "11652: [discriminator loss: 0.503753, acc: 0.750000] [adversarial loss: 1.825964, acc: 0.031250]\n",
      "11653: [discriminator loss: 0.517772, acc: 0.750000] [adversarial loss: 1.111627, acc: 0.265625]\n",
      "11654: [discriminator loss: 0.489435, acc: 0.789062] [adversarial loss: 1.345240, acc: 0.187500]\n",
      "11655: [discriminator loss: 0.515229, acc: 0.718750] [adversarial loss: 0.901570, acc: 0.437500]\n",
      "11656: [discriminator loss: 0.509489, acc: 0.804688] [adversarial loss: 1.501901, acc: 0.078125]\n",
      "11657: [discriminator loss: 0.553734, acc: 0.703125] [adversarial loss: 0.735385, acc: 0.578125]\n",
      "11658: [discriminator loss: 0.568259, acc: 0.695312] [adversarial loss: 1.559704, acc: 0.109375]\n",
      "11659: [discriminator loss: 0.551259, acc: 0.687500] [adversarial loss: 1.087775, acc: 0.312500]\n",
      "11660: [discriminator loss: 0.543675, acc: 0.703125] [adversarial loss: 1.163984, acc: 0.281250]\n",
      "11661: [discriminator loss: 0.556952, acc: 0.695312] [adversarial loss: 1.297235, acc: 0.187500]\n",
      "11662: [discriminator loss: 0.504480, acc: 0.765625] [adversarial loss: 1.111453, acc: 0.312500]\n",
      "11663: [discriminator loss: 0.588198, acc: 0.679688] [adversarial loss: 1.198441, acc: 0.187500]\n",
      "11664: [discriminator loss: 0.500168, acc: 0.773438] [adversarial loss: 1.143669, acc: 0.203125]\n",
      "11665: [discriminator loss: 0.450566, acc: 0.812500] [adversarial loss: 1.397635, acc: 0.218750]\n",
      "11666: [discriminator loss: 0.533903, acc: 0.687500] [adversarial loss: 1.266694, acc: 0.203125]\n",
      "11667: [discriminator loss: 0.512130, acc: 0.734375] [adversarial loss: 1.346942, acc: 0.140625]\n",
      "11668: [discriminator loss: 0.552220, acc: 0.703125] [adversarial loss: 0.992280, acc: 0.328125]\n",
      "11669: [discriminator loss: 0.625038, acc: 0.664062] [adversarial loss: 1.354473, acc: 0.156250]\n",
      "11670: [discriminator loss: 0.588145, acc: 0.726562] [adversarial loss: 0.774114, acc: 0.484375]\n",
      "11671: [discriminator loss: 0.542339, acc: 0.695312] [adversarial loss: 1.445115, acc: 0.078125]\n",
      "11672: [discriminator loss: 0.520589, acc: 0.742188] [adversarial loss: 1.188375, acc: 0.281250]\n",
      "11673: [discriminator loss: 0.513319, acc: 0.781250] [adversarial loss: 1.284871, acc: 0.265625]\n",
      "11674: [discriminator loss: 0.539154, acc: 0.734375] [adversarial loss: 1.125962, acc: 0.234375]\n",
      "11675: [discriminator loss: 0.529088, acc: 0.695312] [adversarial loss: 1.265894, acc: 0.265625]\n",
      "11676: [discriminator loss: 0.575483, acc: 0.726562] [adversarial loss: 0.987667, acc: 0.390625]\n",
      "11677: [discriminator loss: 0.609926, acc: 0.695312] [adversarial loss: 1.539438, acc: 0.125000]\n",
      "11678: [discriminator loss: 0.556748, acc: 0.671875] [adversarial loss: 0.929066, acc: 0.390625]\n",
      "11679: [discriminator loss: 0.608944, acc: 0.617188] [adversarial loss: 1.435655, acc: 0.109375]\n",
      "11680: [discriminator loss: 0.519199, acc: 0.703125] [adversarial loss: 1.003481, acc: 0.343750]\n",
      "11681: [discriminator loss: 0.554592, acc: 0.710938] [adversarial loss: 1.313661, acc: 0.234375]\n",
      "11682: [discriminator loss: 0.541859, acc: 0.710938] [adversarial loss: 0.922745, acc: 0.406250]\n",
      "11683: [discriminator loss: 0.597610, acc: 0.671875] [adversarial loss: 1.324003, acc: 0.140625]\n",
      "11684: [discriminator loss: 0.560863, acc: 0.695312] [adversarial loss: 0.871451, acc: 0.375000]\n",
      "11685: [discriminator loss: 0.488567, acc: 0.765625] [adversarial loss: 1.593557, acc: 0.125000]\n",
      "11686: [discriminator loss: 0.713780, acc: 0.609375] [adversarial loss: 0.721503, acc: 0.593750]\n",
      "11687: [discriminator loss: 0.626631, acc: 0.703125] [adversarial loss: 1.539222, acc: 0.093750]\n",
      "11688: [discriminator loss: 0.583655, acc: 0.695312] [adversarial loss: 1.070195, acc: 0.265625]\n",
      "11689: [discriminator loss: 0.573884, acc: 0.687500] [adversarial loss: 1.220250, acc: 0.171875]\n",
      "11690: [discriminator loss: 0.464427, acc: 0.742188] [adversarial loss: 1.107417, acc: 0.234375]\n",
      "11691: [discriminator loss: 0.550835, acc: 0.757812] [adversarial loss: 1.326654, acc: 0.109375]\n",
      "11692: [discriminator loss: 0.554662, acc: 0.703125] [adversarial loss: 0.953030, acc: 0.328125]\n",
      "11693: [discriminator loss: 0.618705, acc: 0.671875] [adversarial loss: 1.387921, acc: 0.062500]\n",
      "11694: [discriminator loss: 0.517880, acc: 0.718750] [adversarial loss: 1.074329, acc: 0.328125]\n",
      "11695: [discriminator loss: 0.540775, acc: 0.718750] [adversarial loss: 1.207212, acc: 0.250000]\n",
      "11696: [discriminator loss: 0.517567, acc: 0.726562] [adversarial loss: 1.087677, acc: 0.296875]\n",
      "11697: [discriminator loss: 0.649974, acc: 0.640625] [adversarial loss: 0.967247, acc: 0.375000]\n",
      "11698: [discriminator loss: 0.575683, acc: 0.679688] [adversarial loss: 1.335014, acc: 0.140625]\n",
      "11699: [discriminator loss: 0.586638, acc: 0.710938] [adversarial loss: 0.940507, acc: 0.406250]\n",
      "11700: [discriminator loss: 0.553588, acc: 0.695312] [adversarial loss: 1.652162, acc: 0.062500]\n",
      "11701: [discriminator loss: 0.553355, acc: 0.664062] [adversarial loss: 0.960521, acc: 0.343750]\n",
      "11702: [discriminator loss: 0.495385, acc: 0.804688] [adversarial loss: 1.235912, acc: 0.265625]\n",
      "11703: [discriminator loss: 0.546538, acc: 0.734375] [adversarial loss: 1.073251, acc: 0.296875]\n",
      "11704: [discriminator loss: 0.555559, acc: 0.695312] [adversarial loss: 1.210680, acc: 0.203125]\n",
      "11705: [discriminator loss: 0.539045, acc: 0.726562] [adversarial loss: 0.954092, acc: 0.421875]\n",
      "11706: [discriminator loss: 0.553213, acc: 0.726562] [adversarial loss: 1.191331, acc: 0.203125]\n",
      "11707: [discriminator loss: 0.555990, acc: 0.710938] [adversarial loss: 1.179837, acc: 0.218750]\n",
      "11708: [discriminator loss: 0.500253, acc: 0.750000] [adversarial loss: 0.991586, acc: 0.359375]\n",
      "11709: [discriminator loss: 0.530655, acc: 0.734375] [adversarial loss: 1.141310, acc: 0.250000]\n",
      "11710: [discriminator loss: 0.542781, acc: 0.718750] [adversarial loss: 1.371863, acc: 0.125000]\n",
      "11711: [discriminator loss: 0.619939, acc: 0.648438] [adversarial loss: 0.878283, acc: 0.437500]\n",
      "11712: [discriminator loss: 0.560716, acc: 0.750000] [adversarial loss: 1.495277, acc: 0.093750]\n",
      "11713: [discriminator loss: 0.647036, acc: 0.664062] [adversarial loss: 0.835487, acc: 0.421875]\n",
      "11714: [discriminator loss: 0.550940, acc: 0.742188] [adversarial loss: 1.305722, acc: 0.125000]\n",
      "11715: [discriminator loss: 0.573853, acc: 0.687500] [adversarial loss: 0.924706, acc: 0.343750]\n",
      "11716: [discriminator loss: 0.541858, acc: 0.765625] [adversarial loss: 1.111395, acc: 0.281250]\n",
      "11717: [discriminator loss: 0.580407, acc: 0.687500] [adversarial loss: 0.950403, acc: 0.390625]\n",
      "11718: [discriminator loss: 0.552418, acc: 0.710938] [adversarial loss: 1.581124, acc: 0.125000]\n",
      "11719: [discriminator loss: 0.635108, acc: 0.609375] [adversarial loss: 0.988750, acc: 0.406250]\n",
      "11720: [discriminator loss: 0.619750, acc: 0.648438] [adversarial loss: 1.141373, acc: 0.250000]\n",
      "11721: [discriminator loss: 0.535208, acc: 0.726562] [adversarial loss: 0.977117, acc: 0.421875]\n",
      "11722: [discriminator loss: 0.627008, acc: 0.679688] [adversarial loss: 1.375209, acc: 0.187500]\n",
      "11723: [discriminator loss: 0.625870, acc: 0.648438] [adversarial loss: 0.919539, acc: 0.375000]\n",
      "11724: [discriminator loss: 0.583005, acc: 0.648438] [adversarial loss: 1.413990, acc: 0.093750]\n",
      "11725: [discriminator loss: 0.577953, acc: 0.679688] [adversarial loss: 1.228883, acc: 0.250000]\n",
      "11726: [discriminator loss: 0.559729, acc: 0.726562] [adversarial loss: 1.107940, acc: 0.218750]\n",
      "11727: [discriminator loss: 0.525757, acc: 0.718750] [adversarial loss: 1.046445, acc: 0.343750]\n",
      "11728: [discriminator loss: 0.493085, acc: 0.750000] [adversarial loss: 1.016208, acc: 0.296875]\n",
      "11729: [discriminator loss: 0.556444, acc: 0.703125] [adversarial loss: 1.361057, acc: 0.296875]\n",
      "11730: [discriminator loss: 0.554954, acc: 0.695312] [adversarial loss: 1.018932, acc: 0.328125]\n",
      "11731: [discriminator loss: 0.546442, acc: 0.718750] [adversarial loss: 1.033869, acc: 0.281250]\n",
      "11732: [discriminator loss: 0.540027, acc: 0.695312] [adversarial loss: 1.381052, acc: 0.125000]\n",
      "11733: [discriminator loss: 0.506473, acc: 0.773438] [adversarial loss: 1.076540, acc: 0.250000]\n",
      "11734: [discriminator loss: 0.601468, acc: 0.632812] [adversarial loss: 1.162173, acc: 0.203125]\n",
      "11735: [discriminator loss: 0.533328, acc: 0.726562] [adversarial loss: 1.188753, acc: 0.250000]\n",
      "11736: [discriminator loss: 0.593164, acc: 0.726562] [adversarial loss: 1.321235, acc: 0.234375]\n",
      "11737: [discriminator loss: 0.539953, acc: 0.695312] [adversarial loss: 1.276331, acc: 0.203125]\n",
      "11738: [discriminator loss: 0.533244, acc: 0.734375] [adversarial loss: 1.121616, acc: 0.281250]\n",
      "11739: [discriminator loss: 0.527533, acc: 0.710938] [adversarial loss: 1.062406, acc: 0.265625]\n",
      "11740: [discriminator loss: 0.502557, acc: 0.718750] [adversarial loss: 1.086345, acc: 0.296875]\n",
      "11741: [discriminator loss: 0.514261, acc: 0.734375] [adversarial loss: 1.184854, acc: 0.156250]\n",
      "11742: [discriminator loss: 0.574306, acc: 0.726562] [adversarial loss: 1.248945, acc: 0.171875]\n",
      "11743: [discriminator loss: 0.483733, acc: 0.765625] [adversarial loss: 1.165534, acc: 0.171875]\n",
      "11744: [discriminator loss: 0.588110, acc: 0.695312] [adversarial loss: 0.990707, acc: 0.281250]\n",
      "11745: [discriminator loss: 0.506318, acc: 0.757812] [adversarial loss: 1.352200, acc: 0.187500]\n",
      "11746: [discriminator loss: 0.577286, acc: 0.656250] [adversarial loss: 0.845041, acc: 0.500000]\n",
      "11747: [discriminator loss: 0.604512, acc: 0.648438] [adversarial loss: 1.644280, acc: 0.093750]\n",
      "11748: [discriminator loss: 0.519737, acc: 0.742188] [adversarial loss: 1.011896, acc: 0.296875]\n",
      "11749: [discriminator loss: 0.509666, acc: 0.734375] [adversarial loss: 1.422184, acc: 0.156250]\n",
      "11750: [discriminator loss: 0.605179, acc: 0.671875] [adversarial loss: 0.860984, acc: 0.421875]\n",
      "11751: [discriminator loss: 0.568335, acc: 0.695312] [adversarial loss: 1.386126, acc: 0.234375]\n",
      "11752: [discriminator loss: 0.609001, acc: 0.648438] [adversarial loss: 0.821240, acc: 0.515625]\n",
      "11753: [discriminator loss: 0.633759, acc: 0.648438] [adversarial loss: 1.612085, acc: 0.109375]\n",
      "11754: [discriminator loss: 0.566554, acc: 0.695312] [adversarial loss: 0.870711, acc: 0.328125]\n",
      "11755: [discriminator loss: 0.596632, acc: 0.648438] [adversarial loss: 1.375722, acc: 0.125000]\n",
      "11756: [discriminator loss: 0.531533, acc: 0.750000] [adversarial loss: 1.004115, acc: 0.375000]\n",
      "11757: [discriminator loss: 0.518344, acc: 0.742188] [adversarial loss: 1.140809, acc: 0.296875]\n",
      "11758: [discriminator loss: 0.562267, acc: 0.718750] [adversarial loss: 1.394486, acc: 0.125000]\n",
      "11759: [discriminator loss: 0.528947, acc: 0.718750] [adversarial loss: 1.092865, acc: 0.328125]\n",
      "11760: [discriminator loss: 0.490987, acc: 0.750000] [adversarial loss: 1.272762, acc: 0.187500]\n",
      "11761: [discriminator loss: 0.507251, acc: 0.726562] [adversarial loss: 1.151325, acc: 0.312500]\n",
      "11762: [discriminator loss: 0.515522, acc: 0.742188] [adversarial loss: 1.095751, acc: 0.296875]\n",
      "11763: [discriminator loss: 0.491652, acc: 0.820312] [adversarial loss: 1.348340, acc: 0.218750]\n",
      "11764: [discriminator loss: 0.642101, acc: 0.617188] [adversarial loss: 1.135983, acc: 0.218750]\n",
      "11765: [discriminator loss: 0.590768, acc: 0.656250] [adversarial loss: 1.190746, acc: 0.171875]\n",
      "11766: [discriminator loss: 0.548377, acc: 0.718750] [adversarial loss: 1.154901, acc: 0.281250]\n",
      "11767: [discriminator loss: 0.591579, acc: 0.632812] [adversarial loss: 1.338246, acc: 0.171875]\n",
      "11768: [discriminator loss: 0.577088, acc: 0.695312] [adversarial loss: 0.867704, acc: 0.390625]\n",
      "11769: [discriminator loss: 0.549827, acc: 0.742188] [adversarial loss: 1.471575, acc: 0.125000]\n",
      "11770: [discriminator loss: 0.571376, acc: 0.687500] [adversarial loss: 0.989645, acc: 0.437500]\n",
      "11771: [discriminator loss: 0.502423, acc: 0.718750] [adversarial loss: 1.214528, acc: 0.218750]\n",
      "11772: [discriminator loss: 0.535338, acc: 0.742188] [adversarial loss: 0.960785, acc: 0.390625]\n",
      "11773: [discriminator loss: 0.530364, acc: 0.734375] [adversarial loss: 1.347204, acc: 0.203125]\n",
      "11774: [discriminator loss: 0.584715, acc: 0.734375] [adversarial loss: 1.015722, acc: 0.390625]\n",
      "11775: [discriminator loss: 0.536358, acc: 0.695312] [adversarial loss: 1.296935, acc: 0.203125]\n",
      "11776: [discriminator loss: 0.576132, acc: 0.632812] [adversarial loss: 0.757227, acc: 0.484375]\n",
      "11777: [discriminator loss: 0.600808, acc: 0.632812] [adversarial loss: 1.388938, acc: 0.109375]\n",
      "11778: [discriminator loss: 0.588447, acc: 0.687500] [adversarial loss: 0.999981, acc: 0.296875]\n",
      "11779: [discriminator loss: 0.506731, acc: 0.718750] [adversarial loss: 1.161772, acc: 0.312500]\n",
      "11780: [discriminator loss: 0.554339, acc: 0.703125] [adversarial loss: 0.968060, acc: 0.250000]\n",
      "11781: [discriminator loss: 0.622293, acc: 0.648438] [adversarial loss: 1.514649, acc: 0.078125]\n",
      "11782: [discriminator loss: 0.549440, acc: 0.726562] [adversarial loss: 0.854569, acc: 0.500000]\n",
      "11783: [discriminator loss: 0.561118, acc: 0.703125] [adversarial loss: 1.321151, acc: 0.218750]\n",
      "11784: [discriminator loss: 0.566053, acc: 0.687500] [adversarial loss: 1.081997, acc: 0.281250]\n",
      "11785: [discriminator loss: 0.518280, acc: 0.726562] [adversarial loss: 1.010395, acc: 0.328125]\n",
      "11786: [discriminator loss: 0.563825, acc: 0.718750] [adversarial loss: 1.123465, acc: 0.218750]\n",
      "11787: [discriminator loss: 0.551736, acc: 0.718750] [adversarial loss: 1.148530, acc: 0.312500]\n",
      "11788: [discriminator loss: 0.515046, acc: 0.750000] [adversarial loss: 1.216543, acc: 0.250000]\n",
      "11789: [discriminator loss: 0.507874, acc: 0.734375] [adversarial loss: 1.200600, acc: 0.234375]\n",
      "11790: [discriminator loss: 0.588902, acc: 0.703125] [adversarial loss: 1.048820, acc: 0.359375]\n",
      "11791: [discriminator loss: 0.555626, acc: 0.710938] [adversarial loss: 1.410650, acc: 0.187500]\n",
      "11792: [discriminator loss: 0.538039, acc: 0.679688] [adversarial loss: 1.086056, acc: 0.250000]\n",
      "11793: [discriminator loss: 0.544156, acc: 0.671875] [adversarial loss: 0.976113, acc: 0.343750]\n",
      "11794: [discriminator loss: 0.513032, acc: 0.789062] [adversarial loss: 1.200273, acc: 0.171875]\n",
      "11795: [discriminator loss: 0.579118, acc: 0.648438] [adversarial loss: 1.017110, acc: 0.296875]\n",
      "11796: [discriminator loss: 0.519593, acc: 0.726562] [adversarial loss: 1.487250, acc: 0.156250]\n",
      "11797: [discriminator loss: 0.605140, acc: 0.679688] [adversarial loss: 0.709115, acc: 0.578125]\n",
      "11798: [discriminator loss: 0.709153, acc: 0.578125] [adversarial loss: 1.608701, acc: 0.093750]\n",
      "11799: [discriminator loss: 0.603688, acc: 0.679688] [adversarial loss: 0.839114, acc: 0.468750]\n",
      "11800: [discriminator loss: 0.550471, acc: 0.718750] [adversarial loss: 1.133887, acc: 0.250000]\n",
      "11801: [discriminator loss: 0.510961, acc: 0.671875] [adversarial loss: 1.052159, acc: 0.296875]\n",
      "11802: [discriminator loss: 0.519078, acc: 0.718750] [adversarial loss: 0.990319, acc: 0.375000]\n",
      "11803: [discriminator loss: 0.563910, acc: 0.679688] [adversarial loss: 1.144826, acc: 0.218750]\n",
      "11804: [discriminator loss: 0.566828, acc: 0.703125] [adversarial loss: 1.123754, acc: 0.281250]\n",
      "11805: [discriminator loss: 0.534092, acc: 0.734375] [adversarial loss: 1.275069, acc: 0.203125]\n",
      "11806: [discriminator loss: 0.606673, acc: 0.609375] [adversarial loss: 1.190683, acc: 0.187500]\n",
      "11807: [discriminator loss: 0.538492, acc: 0.750000] [adversarial loss: 1.119133, acc: 0.218750]\n",
      "11808: [discriminator loss: 0.512145, acc: 0.718750] [adversarial loss: 1.162590, acc: 0.234375]\n",
      "11809: [discriminator loss: 0.615749, acc: 0.664062] [adversarial loss: 1.200521, acc: 0.156250]\n",
      "11810: [discriminator loss: 0.508366, acc: 0.734375] [adversarial loss: 0.851947, acc: 0.453125]\n",
      "11811: [discriminator loss: 0.551136, acc: 0.656250] [adversarial loss: 1.581442, acc: 0.093750]\n",
      "11812: [discriminator loss: 0.630454, acc: 0.656250] [adversarial loss: 0.811957, acc: 0.468750]\n",
      "11813: [discriminator loss: 0.552802, acc: 0.718750] [adversarial loss: 1.325089, acc: 0.187500]\n",
      "11814: [discriminator loss: 0.553723, acc: 0.726562] [adversarial loss: 1.073764, acc: 0.281250]\n",
      "11815: [discriminator loss: 0.578812, acc: 0.632812] [adversarial loss: 1.296990, acc: 0.218750]\n",
      "11816: [discriminator loss: 0.557589, acc: 0.703125] [adversarial loss: 0.983642, acc: 0.359375]\n",
      "11817: [discriminator loss: 0.565332, acc: 0.703125] [adversarial loss: 1.433084, acc: 0.140625]\n",
      "11818: [discriminator loss: 0.544443, acc: 0.695312] [adversarial loss: 1.025710, acc: 0.296875]\n",
      "11819: [discriminator loss: 0.649687, acc: 0.640625] [adversarial loss: 1.240618, acc: 0.187500]\n",
      "11820: [discriminator loss: 0.553654, acc: 0.726562] [adversarial loss: 1.273793, acc: 0.140625]\n",
      "11821: [discriminator loss: 0.527538, acc: 0.742188] [adversarial loss: 0.961779, acc: 0.421875]\n",
      "11822: [discriminator loss: 0.605765, acc: 0.656250] [adversarial loss: 1.260847, acc: 0.203125]\n",
      "11823: [discriminator loss: 0.476718, acc: 0.789062] [adversarial loss: 1.155681, acc: 0.234375]\n",
      "11824: [discriminator loss: 0.475172, acc: 0.750000] [adversarial loss: 0.860983, acc: 0.468750]\n",
      "11825: [discriminator loss: 0.563795, acc: 0.726562] [adversarial loss: 1.367168, acc: 0.203125]\n",
      "11826: [discriminator loss: 0.568063, acc: 0.687500] [adversarial loss: 1.068864, acc: 0.296875]\n",
      "11827: [discriminator loss: 0.583988, acc: 0.656250] [adversarial loss: 1.456369, acc: 0.156250]\n",
      "11828: [discriminator loss: 0.556872, acc: 0.687500] [adversarial loss: 0.811375, acc: 0.500000]\n",
      "11829: [discriminator loss: 0.583524, acc: 0.703125] [adversarial loss: 1.632377, acc: 0.109375]\n",
      "11830: [discriminator loss: 0.588466, acc: 0.687500] [adversarial loss: 0.954950, acc: 0.406250]\n",
      "11831: [discriminator loss: 0.459616, acc: 0.820312] [adversarial loss: 1.322941, acc: 0.187500]\n",
      "11832: [discriminator loss: 0.607946, acc: 0.617188] [adversarial loss: 1.150488, acc: 0.140625]\n",
      "11833: [discriminator loss: 0.493855, acc: 0.804688] [adversarial loss: 1.357607, acc: 0.156250]\n",
      "11834: [discriminator loss: 0.556184, acc: 0.703125] [adversarial loss: 1.276986, acc: 0.093750]\n",
      "11835: [discriminator loss: 0.515716, acc: 0.757812] [adversarial loss: 0.967110, acc: 0.390625]\n",
      "11836: [discriminator loss: 0.651469, acc: 0.617188] [adversarial loss: 1.143359, acc: 0.171875]\n",
      "11837: [discriminator loss: 0.558043, acc: 0.710938] [adversarial loss: 1.125201, acc: 0.171875]\n",
      "11838: [discriminator loss: 0.509097, acc: 0.726562] [adversarial loss: 0.960728, acc: 0.390625]\n",
      "11839: [discriminator loss: 0.578090, acc: 0.703125] [adversarial loss: 1.518887, acc: 0.109375]\n",
      "11840: [discriminator loss: 0.545879, acc: 0.734375] [adversarial loss: 1.223527, acc: 0.250000]\n",
      "11841: [discriminator loss: 0.571093, acc: 0.703125] [adversarial loss: 1.275734, acc: 0.187500]\n",
      "11842: [discriminator loss: 0.574748, acc: 0.710938] [adversarial loss: 1.057995, acc: 0.203125]\n",
      "11843: [discriminator loss: 0.479678, acc: 0.781250] [adversarial loss: 1.482492, acc: 0.140625]\n",
      "11844: [discriminator loss: 0.593438, acc: 0.656250] [adversarial loss: 0.717705, acc: 0.593750]\n",
      "11845: [discriminator loss: 0.577640, acc: 0.664062] [adversarial loss: 1.490509, acc: 0.109375]\n",
      "11846: [discriminator loss: 0.582081, acc: 0.664062] [adversarial loss: 0.707216, acc: 0.640625]\n",
      "11847: [discriminator loss: 0.570145, acc: 0.664062] [adversarial loss: 1.533906, acc: 0.093750]\n",
      "11848: [discriminator loss: 0.512091, acc: 0.726562] [adversarial loss: 1.043421, acc: 0.343750]\n",
      "11849: [discriminator loss: 0.569979, acc: 0.687500] [adversarial loss: 1.087269, acc: 0.281250]\n",
      "11850: [discriminator loss: 0.621101, acc: 0.671875] [adversarial loss: 1.007122, acc: 0.265625]\n",
      "11851: [discriminator loss: 0.577265, acc: 0.664062] [adversarial loss: 1.450086, acc: 0.125000]\n",
      "11852: [discriminator loss: 0.557520, acc: 0.718750] [adversarial loss: 1.121480, acc: 0.265625]\n",
      "11853: [discriminator loss: 0.499872, acc: 0.726562] [adversarial loss: 1.241809, acc: 0.187500]\n",
      "11854: [discriminator loss: 0.546966, acc: 0.695312] [adversarial loss: 0.953942, acc: 0.359375]\n",
      "11855: [discriminator loss: 0.548337, acc: 0.757812] [adversarial loss: 1.027583, acc: 0.250000]\n",
      "11856: [discriminator loss: 0.543423, acc: 0.750000] [adversarial loss: 1.225221, acc: 0.187500]\n",
      "11857: [discriminator loss: 0.570453, acc: 0.710938] [adversarial loss: 1.348300, acc: 0.171875]\n",
      "11858: [discriminator loss: 0.582483, acc: 0.687500] [adversarial loss: 0.923268, acc: 0.406250]\n",
      "11859: [discriminator loss: 0.491579, acc: 0.750000] [adversarial loss: 1.302641, acc: 0.171875]\n",
      "11860: [discriminator loss: 0.558532, acc: 0.710938] [adversarial loss: 1.162553, acc: 0.296875]\n",
      "11861: [discriminator loss: 0.533786, acc: 0.734375] [adversarial loss: 0.990590, acc: 0.390625]\n",
      "11862: [discriminator loss: 0.505904, acc: 0.742188] [adversarial loss: 1.505723, acc: 0.109375]\n",
      "11863: [discriminator loss: 0.547354, acc: 0.726562] [adversarial loss: 1.070574, acc: 0.281250]\n",
      "11864: [discriminator loss: 0.498971, acc: 0.757812] [adversarial loss: 1.060810, acc: 0.296875]\n",
      "11865: [discriminator loss: 0.539136, acc: 0.671875] [adversarial loss: 1.091991, acc: 0.281250]\n",
      "11866: [discriminator loss: 0.555223, acc: 0.695312] [adversarial loss: 1.033365, acc: 0.343750]\n",
      "11867: [discriminator loss: 0.515274, acc: 0.726562] [adversarial loss: 1.108585, acc: 0.265625]\n",
      "11868: [discriminator loss: 0.635032, acc: 0.656250] [adversarial loss: 1.047650, acc: 0.296875]\n",
      "11869: [discriminator loss: 0.511434, acc: 0.820312] [adversarial loss: 0.875010, acc: 0.484375]\n",
      "11870: [discriminator loss: 0.584779, acc: 0.648438] [adversarial loss: 1.412215, acc: 0.171875]\n",
      "11871: [discriminator loss: 0.515124, acc: 0.679688] [adversarial loss: 0.952144, acc: 0.390625]\n",
      "11872: [discriminator loss: 0.564931, acc: 0.718750] [adversarial loss: 1.300932, acc: 0.187500]\n",
      "11873: [discriminator loss: 0.620435, acc: 0.632812] [adversarial loss: 0.807889, acc: 0.406250]\n",
      "11874: [discriminator loss: 0.645914, acc: 0.617188] [adversarial loss: 1.713401, acc: 0.078125]\n",
      "11875: [discriminator loss: 0.656811, acc: 0.601562] [adversarial loss: 0.830943, acc: 0.406250]\n",
      "11876: [discriminator loss: 0.604804, acc: 0.632812] [adversarial loss: 1.365263, acc: 0.171875]\n",
      "11877: [discriminator loss: 0.493866, acc: 0.773438] [adversarial loss: 0.989082, acc: 0.328125]\n",
      "11878: [discriminator loss: 0.596399, acc: 0.664062] [adversarial loss: 1.379165, acc: 0.218750]\n",
      "11879: [discriminator loss: 0.565128, acc: 0.687500] [adversarial loss: 0.992602, acc: 0.359375]\n",
      "11880: [discriminator loss: 0.517251, acc: 0.757812] [adversarial loss: 1.355495, acc: 0.187500]\n",
      "11881: [discriminator loss: 0.567641, acc: 0.648438] [adversarial loss: 0.870311, acc: 0.453125]\n",
      "11882: [discriminator loss: 0.608706, acc: 0.703125] [adversarial loss: 1.073098, acc: 0.328125]\n",
      "11883: [discriminator loss: 0.540181, acc: 0.718750] [adversarial loss: 1.421310, acc: 0.156250]\n",
      "11884: [discriminator loss: 0.524831, acc: 0.742188] [adversarial loss: 1.124365, acc: 0.265625]\n",
      "11885: [discriminator loss: 0.601711, acc: 0.656250] [adversarial loss: 1.234328, acc: 0.203125]\n",
      "11886: [discriminator loss: 0.525028, acc: 0.734375] [adversarial loss: 1.200521, acc: 0.265625]\n",
      "11887: [discriminator loss: 0.651365, acc: 0.664062] [adversarial loss: 1.199501, acc: 0.265625]\n",
      "11888: [discriminator loss: 0.624466, acc: 0.687500] [adversarial loss: 0.872343, acc: 0.406250]\n",
      "11889: [discriminator loss: 0.538170, acc: 0.742188] [adversarial loss: 1.176655, acc: 0.250000]\n",
      "11890: [discriminator loss: 0.600576, acc: 0.671875] [adversarial loss: 1.002701, acc: 0.296875]\n",
      "11891: [discriminator loss: 0.532837, acc: 0.742188] [adversarial loss: 1.396623, acc: 0.171875]\n",
      "11892: [discriminator loss: 0.645230, acc: 0.554688] [adversarial loss: 0.968136, acc: 0.328125]\n",
      "11893: [discriminator loss: 0.529639, acc: 0.750000] [adversarial loss: 1.327665, acc: 0.203125]\n",
      "11894: [discriminator loss: 0.495620, acc: 0.796875] [adversarial loss: 1.097606, acc: 0.218750]\n",
      "11895: [discriminator loss: 0.525649, acc: 0.750000] [adversarial loss: 1.161232, acc: 0.296875]\n",
      "11896: [discriminator loss: 0.573573, acc: 0.671875] [adversarial loss: 1.173295, acc: 0.171875]\n",
      "11897: [discriminator loss: 0.464695, acc: 0.789062] [adversarial loss: 1.221849, acc: 0.187500]\n",
      "11898: [discriminator loss: 0.482323, acc: 0.789062] [adversarial loss: 1.098790, acc: 0.281250]\n",
      "11899: [discriminator loss: 0.566639, acc: 0.703125] [adversarial loss: 0.865646, acc: 0.437500]\n",
      "11900: [discriminator loss: 0.532890, acc: 0.742188] [adversarial loss: 1.597205, acc: 0.109375]\n",
      "11901: [discriminator loss: 0.551127, acc: 0.695312] [adversarial loss: 0.959772, acc: 0.406250]\n",
      "11902: [discriminator loss: 0.582522, acc: 0.695312] [adversarial loss: 1.184129, acc: 0.250000]\n",
      "11903: [discriminator loss: 0.502782, acc: 0.734375] [adversarial loss: 1.187704, acc: 0.250000]\n",
      "11904: [discriminator loss: 0.530390, acc: 0.679688] [adversarial loss: 0.917398, acc: 0.468750]\n",
      "11905: [discriminator loss: 0.573602, acc: 0.671875] [adversarial loss: 1.387939, acc: 0.125000]\n",
      "11906: [discriminator loss: 0.598568, acc: 0.640625] [adversarial loss: 0.889912, acc: 0.406250]\n",
      "11907: [discriminator loss: 0.525741, acc: 0.703125] [adversarial loss: 1.461496, acc: 0.125000]\n",
      "11908: [discriminator loss: 0.556557, acc: 0.710938] [adversarial loss: 0.686914, acc: 0.546875]\n",
      "11909: [discriminator loss: 0.613260, acc: 0.617188] [adversarial loss: 1.375119, acc: 0.140625]\n",
      "11910: [discriminator loss: 0.558248, acc: 0.664062] [adversarial loss: 0.960015, acc: 0.406250]\n",
      "11911: [discriminator loss: 0.647201, acc: 0.640625] [adversarial loss: 1.793880, acc: 0.078125]\n",
      "11912: [discriminator loss: 0.590439, acc: 0.679688] [adversarial loss: 0.982603, acc: 0.312500]\n",
      "11913: [discriminator loss: 0.494259, acc: 0.765625] [adversarial loss: 1.356036, acc: 0.171875]\n",
      "11914: [discriminator loss: 0.471362, acc: 0.773438] [adversarial loss: 0.888160, acc: 0.406250]\n",
      "11915: [discriminator loss: 0.564686, acc: 0.695312] [adversarial loss: 1.428514, acc: 0.140625]\n",
      "11916: [discriminator loss: 0.494486, acc: 0.757812] [adversarial loss: 1.001151, acc: 0.343750]\n",
      "11917: [discriminator loss: 0.621118, acc: 0.656250] [adversarial loss: 1.346992, acc: 0.156250]\n",
      "11918: [discriminator loss: 0.551959, acc: 0.710938] [adversarial loss: 0.972243, acc: 0.359375]\n",
      "11919: [discriminator loss: 0.596743, acc: 0.656250] [adversarial loss: 1.580116, acc: 0.093750]\n",
      "11920: [discriminator loss: 0.593632, acc: 0.648438] [adversarial loss: 0.876294, acc: 0.390625]\n",
      "11921: [discriminator loss: 0.564953, acc: 0.742188] [adversarial loss: 1.405339, acc: 0.203125]\n",
      "11922: [discriminator loss: 0.586163, acc: 0.671875] [adversarial loss: 1.044548, acc: 0.265625]\n",
      "11923: [discriminator loss: 0.624099, acc: 0.640625] [adversarial loss: 1.224411, acc: 0.171875]\n",
      "11924: [discriminator loss: 0.498747, acc: 0.726562] [adversarial loss: 1.186897, acc: 0.234375]\n",
      "11925: [discriminator loss: 0.593480, acc: 0.703125] [adversarial loss: 1.182824, acc: 0.203125]\n",
      "11926: [discriminator loss: 0.579597, acc: 0.687500] [adversarial loss: 1.136157, acc: 0.265625]\n",
      "11927: [discriminator loss: 0.499666, acc: 0.710938] [adversarial loss: 1.231047, acc: 0.250000]\n",
      "11928: [discriminator loss: 0.581314, acc: 0.656250] [adversarial loss: 0.926347, acc: 0.375000]\n",
      "11929: [discriminator loss: 0.532149, acc: 0.757812] [adversarial loss: 1.514266, acc: 0.093750]\n",
      "11930: [discriminator loss: 0.592551, acc: 0.679688] [adversarial loss: 0.725159, acc: 0.593750]\n",
      "11931: [discriminator loss: 0.582989, acc: 0.703125] [adversarial loss: 1.691529, acc: 0.109375]\n",
      "11932: [discriminator loss: 0.610676, acc: 0.687500] [adversarial loss: 0.878352, acc: 0.468750]\n",
      "11933: [discriminator loss: 0.604655, acc: 0.632812] [adversarial loss: 1.193940, acc: 0.250000]\n",
      "11934: [discriminator loss: 0.529118, acc: 0.703125] [adversarial loss: 1.142474, acc: 0.343750]\n",
      "11935: [discriminator loss: 0.608313, acc: 0.640625] [adversarial loss: 1.093632, acc: 0.250000]\n",
      "11936: [discriminator loss: 0.496626, acc: 0.742188] [adversarial loss: 1.132665, acc: 0.281250]\n",
      "11937: [discriminator loss: 0.533892, acc: 0.679688] [adversarial loss: 1.180630, acc: 0.171875]\n",
      "11938: [discriminator loss: 0.557078, acc: 0.695312] [adversarial loss: 0.851096, acc: 0.437500]\n",
      "11939: [discriminator loss: 0.540385, acc: 0.703125] [adversarial loss: 1.199776, acc: 0.234375]\n",
      "11940: [discriminator loss: 0.447146, acc: 0.765625] [adversarial loss: 1.214898, acc: 0.187500]\n",
      "11941: [discriminator loss: 0.583010, acc: 0.687500] [adversarial loss: 1.009574, acc: 0.343750]\n",
      "11942: [discriminator loss: 0.546244, acc: 0.742188] [adversarial loss: 1.295208, acc: 0.234375]\n",
      "11943: [discriminator loss: 0.588905, acc: 0.664062] [adversarial loss: 0.921097, acc: 0.343750]\n",
      "11944: [discriminator loss: 0.557452, acc: 0.679688] [adversarial loss: 1.202762, acc: 0.218750]\n",
      "11945: [discriminator loss: 0.519234, acc: 0.718750] [adversarial loss: 0.914919, acc: 0.406250]\n",
      "11946: [discriminator loss: 0.559330, acc: 0.671875] [adversarial loss: 1.509772, acc: 0.109375]\n",
      "11947: [discriminator loss: 0.601955, acc: 0.640625] [adversarial loss: 0.872397, acc: 0.515625]\n",
      "11948: [discriminator loss: 0.516229, acc: 0.734375] [adversarial loss: 1.504117, acc: 0.171875]\n",
      "11949: [discriminator loss: 0.608006, acc: 0.632812] [adversarial loss: 0.940379, acc: 0.375000]\n",
      "11950: [discriminator loss: 0.574225, acc: 0.671875] [adversarial loss: 1.196344, acc: 0.281250]\n",
      "11951: [discriminator loss: 0.542629, acc: 0.750000] [adversarial loss: 1.104757, acc: 0.265625]\n",
      "11952: [discriminator loss: 0.576559, acc: 0.671875] [adversarial loss: 1.214780, acc: 0.234375]\n",
      "11953: [discriminator loss: 0.504507, acc: 0.734375] [adversarial loss: 1.113539, acc: 0.250000]\n",
      "11954: [discriminator loss: 0.500346, acc: 0.757812] [adversarial loss: 1.142565, acc: 0.296875]\n",
      "11955: [discriminator loss: 0.554539, acc: 0.710938] [adversarial loss: 1.308633, acc: 0.171875]\n",
      "11956: [discriminator loss: 0.529756, acc: 0.703125] [adversarial loss: 1.089382, acc: 0.281250]\n",
      "11957: [discriminator loss: 0.587819, acc: 0.671875] [adversarial loss: 0.918445, acc: 0.375000]\n",
      "11958: [discriminator loss: 0.567981, acc: 0.687500] [adversarial loss: 1.266358, acc: 0.265625]\n",
      "11959: [discriminator loss: 0.519250, acc: 0.765625] [adversarial loss: 0.781684, acc: 0.531250]\n",
      "11960: [discriminator loss: 0.559062, acc: 0.664062] [adversarial loss: 1.631099, acc: 0.078125]\n",
      "11961: [discriminator loss: 0.605001, acc: 0.664062] [adversarial loss: 0.694039, acc: 0.578125]\n",
      "11962: [discriminator loss: 0.595026, acc: 0.679688] [adversarial loss: 1.565631, acc: 0.062500]\n",
      "11963: [discriminator loss: 0.603660, acc: 0.640625] [adversarial loss: 0.974496, acc: 0.343750]\n",
      "11964: [discriminator loss: 0.648912, acc: 0.625000] [adversarial loss: 1.085433, acc: 0.250000]\n",
      "11965: [discriminator loss: 0.514352, acc: 0.718750] [adversarial loss: 1.135489, acc: 0.218750]\n",
      "11966: [discriminator loss: 0.628571, acc: 0.648438] [adversarial loss: 1.018345, acc: 0.359375]\n",
      "11967: [discriminator loss: 0.490191, acc: 0.765625] [adversarial loss: 1.203395, acc: 0.187500]\n",
      "11968: [discriminator loss: 0.564399, acc: 0.703125] [adversarial loss: 0.988376, acc: 0.281250]\n",
      "11969: [discriminator loss: 0.558095, acc: 0.718750] [adversarial loss: 1.291783, acc: 0.171875]\n",
      "11970: [discriminator loss: 0.570293, acc: 0.679688] [adversarial loss: 1.251172, acc: 0.125000]\n",
      "11971: [discriminator loss: 0.522592, acc: 0.718750] [adversarial loss: 0.888184, acc: 0.406250]\n",
      "11972: [discriminator loss: 0.525091, acc: 0.742188] [adversarial loss: 1.174671, acc: 0.187500]\n",
      "11973: [discriminator loss: 0.539077, acc: 0.687500] [adversarial loss: 1.231852, acc: 0.140625]\n",
      "11974: [discriminator loss: 0.542417, acc: 0.710938] [adversarial loss: 0.894607, acc: 0.421875]\n",
      "11975: [discriminator loss: 0.590915, acc: 0.695312] [adversarial loss: 1.392719, acc: 0.171875]\n",
      "11976: [discriminator loss: 0.578399, acc: 0.664062] [adversarial loss: 0.842464, acc: 0.375000]\n",
      "11977: [discriminator loss: 0.558785, acc: 0.687500] [adversarial loss: 1.385752, acc: 0.062500]\n",
      "11978: [discriminator loss: 0.579733, acc: 0.648438] [adversarial loss: 1.024116, acc: 0.343750]\n",
      "11979: [discriminator loss: 0.557951, acc: 0.703125] [adversarial loss: 1.224704, acc: 0.171875]\n",
      "11980: [discriminator loss: 0.560444, acc: 0.679688] [adversarial loss: 1.049734, acc: 0.250000]\n",
      "11981: [discriminator loss: 0.575013, acc: 0.687500] [adversarial loss: 1.180075, acc: 0.234375]\n",
      "11982: [discriminator loss: 0.533118, acc: 0.734375] [adversarial loss: 1.102257, acc: 0.265625]\n",
      "11983: [discriminator loss: 0.567752, acc: 0.695312] [adversarial loss: 1.182123, acc: 0.296875]\n",
      "11984: [discriminator loss: 0.557805, acc: 0.664062] [adversarial loss: 1.111545, acc: 0.312500]\n",
      "11985: [discriminator loss: 0.514298, acc: 0.718750] [adversarial loss: 1.322133, acc: 0.187500]\n",
      "11986: [discriminator loss: 0.620426, acc: 0.640625] [adversarial loss: 0.996969, acc: 0.328125]\n",
      "11987: [discriminator loss: 0.581872, acc: 0.703125] [adversarial loss: 1.299629, acc: 0.156250]\n",
      "11988: [discriminator loss: 0.591579, acc: 0.687500] [adversarial loss: 1.071993, acc: 0.296875]\n",
      "11989: [discriminator loss: 0.573731, acc: 0.726562] [adversarial loss: 1.152368, acc: 0.250000]\n",
      "11990: [discriminator loss: 0.557123, acc: 0.710938] [adversarial loss: 1.004718, acc: 0.328125]\n",
      "11991: [discriminator loss: 0.576593, acc: 0.757812] [adversarial loss: 1.472722, acc: 0.156250]\n",
      "11992: [discriminator loss: 0.575736, acc: 0.695312] [adversarial loss: 0.881178, acc: 0.453125]\n",
      "11993: [discriminator loss: 0.563847, acc: 0.679688] [adversarial loss: 1.325100, acc: 0.140625]\n",
      "11994: [discriminator loss: 0.532882, acc: 0.687500] [adversarial loss: 0.985325, acc: 0.359375]\n",
      "11995: [discriminator loss: 0.470740, acc: 0.796875] [adversarial loss: 1.417982, acc: 0.093750]\n",
      "11996: [discriminator loss: 0.639867, acc: 0.609375] [adversarial loss: 1.003788, acc: 0.265625]\n",
      "11997: [discriminator loss: 0.594560, acc: 0.679688] [adversarial loss: 1.419266, acc: 0.125000]\n",
      "11998: [discriminator loss: 0.566267, acc: 0.656250] [adversarial loss: 0.903942, acc: 0.328125]\n",
      "11999: [discriminator loss: 0.513770, acc: 0.773438] [adversarial loss: 1.254233, acc: 0.093750]\n",
      "12000: [discriminator loss: 0.554730, acc: 0.718750] [adversarial loss: 0.954394, acc: 0.343750]\n",
      "12001: [discriminator loss: 0.513630, acc: 0.765625] [adversarial loss: 1.318892, acc: 0.171875]\n",
      "12002: [discriminator loss: 0.673289, acc: 0.585938] [adversarial loss: 0.893153, acc: 0.421875]\n",
      "12003: [discriminator loss: 0.522801, acc: 0.718750] [adversarial loss: 1.221325, acc: 0.171875]\n",
      "12004: [discriminator loss: 0.570133, acc: 0.679688] [adversarial loss: 1.230137, acc: 0.203125]\n",
      "12005: [discriminator loss: 0.576648, acc: 0.718750] [adversarial loss: 0.996619, acc: 0.343750]\n",
      "12006: [discriminator loss: 0.522018, acc: 0.757812] [adversarial loss: 1.175602, acc: 0.234375]\n",
      "12007: [discriminator loss: 0.527483, acc: 0.718750] [adversarial loss: 0.907019, acc: 0.453125]\n",
      "12008: [discriminator loss: 0.608032, acc: 0.710938] [adversarial loss: 1.189051, acc: 0.171875]\n",
      "12009: [discriminator loss: 0.478733, acc: 0.765625] [adversarial loss: 0.964916, acc: 0.328125]\n",
      "12010: [discriminator loss: 0.543172, acc: 0.695312] [adversarial loss: 1.398384, acc: 0.156250]\n",
      "12011: [discriminator loss: 0.540634, acc: 0.734375] [adversarial loss: 0.754692, acc: 0.562500]\n",
      "12012: [discriminator loss: 0.510910, acc: 0.726562] [adversarial loss: 1.170131, acc: 0.265625]\n",
      "12013: [discriminator loss: 0.602326, acc: 0.695312] [adversarial loss: 1.167260, acc: 0.187500]\n",
      "12014: [discriminator loss: 0.538541, acc: 0.773438] [adversarial loss: 0.894270, acc: 0.406250]\n",
      "12015: [discriminator loss: 0.558010, acc: 0.726562] [adversarial loss: 1.217520, acc: 0.234375]\n",
      "12016: [discriminator loss: 0.536916, acc: 0.765625] [adversarial loss: 1.256449, acc: 0.171875]\n",
      "12017: [discriminator loss: 0.573514, acc: 0.671875] [adversarial loss: 1.255288, acc: 0.218750]\n",
      "12018: [discriminator loss: 0.608525, acc: 0.671875] [adversarial loss: 1.124499, acc: 0.187500]\n",
      "12019: [discriminator loss: 0.572469, acc: 0.687500] [adversarial loss: 1.478185, acc: 0.093750]\n",
      "12020: [discriminator loss: 0.614814, acc: 0.671875] [adversarial loss: 0.765308, acc: 0.515625]\n",
      "12021: [discriminator loss: 0.602686, acc: 0.648438] [adversarial loss: 1.385986, acc: 0.093750]\n",
      "12022: [discriminator loss: 0.559766, acc: 0.648438] [adversarial loss: 0.744242, acc: 0.578125]\n",
      "12023: [discriminator loss: 0.547012, acc: 0.773438] [adversarial loss: 1.215225, acc: 0.125000]\n",
      "12024: [discriminator loss: 0.554131, acc: 0.710938] [adversarial loss: 1.152496, acc: 0.218750]\n",
      "12025: [discriminator loss: 0.542813, acc: 0.726562] [adversarial loss: 1.177766, acc: 0.203125]\n",
      "12026: [discriminator loss: 0.542416, acc: 0.703125] [adversarial loss: 1.019726, acc: 0.281250]\n",
      "12027: [discriminator loss: 0.583965, acc: 0.656250] [adversarial loss: 1.373937, acc: 0.109375]\n",
      "12028: [discriminator loss: 0.610011, acc: 0.609375] [adversarial loss: 0.815899, acc: 0.406250]\n",
      "12029: [discriminator loss: 0.576681, acc: 0.656250] [adversarial loss: 1.465532, acc: 0.125000]\n",
      "12030: [discriminator loss: 0.639052, acc: 0.640625] [adversarial loss: 0.943864, acc: 0.375000]\n",
      "12031: [discriminator loss: 0.571541, acc: 0.679688] [adversarial loss: 1.466624, acc: 0.125000]\n",
      "12032: [discriminator loss: 0.556271, acc: 0.679688] [adversarial loss: 0.901838, acc: 0.359375]\n",
      "12033: [discriminator loss: 0.574034, acc: 0.687500] [adversarial loss: 1.163075, acc: 0.250000]\n",
      "12034: [discriminator loss: 0.572408, acc: 0.687500] [adversarial loss: 1.193337, acc: 0.250000]\n",
      "12035: [discriminator loss: 0.586886, acc: 0.687500] [adversarial loss: 1.219983, acc: 0.296875]\n",
      "12036: [discriminator loss: 0.513360, acc: 0.734375] [adversarial loss: 1.132525, acc: 0.250000]\n",
      "12037: [discriminator loss: 0.563702, acc: 0.703125] [adversarial loss: 1.264842, acc: 0.203125]\n",
      "12038: [discriminator loss: 0.595269, acc: 0.679688] [adversarial loss: 0.906053, acc: 0.437500]\n",
      "12039: [discriminator loss: 0.585597, acc: 0.656250] [adversarial loss: 1.539088, acc: 0.078125]\n",
      "12040: [discriminator loss: 0.606019, acc: 0.625000] [adversarial loss: 0.968989, acc: 0.390625]\n",
      "12041: [discriminator loss: 0.519183, acc: 0.750000] [adversarial loss: 1.060889, acc: 0.296875]\n",
      "12042: [discriminator loss: 0.573245, acc: 0.710938] [adversarial loss: 1.236819, acc: 0.265625]\n",
      "12043: [discriminator loss: 0.493882, acc: 0.734375] [adversarial loss: 1.218086, acc: 0.171875]\n",
      "12044: [discriminator loss: 0.595635, acc: 0.679688] [adversarial loss: 1.113662, acc: 0.250000]\n",
      "12045: [discriminator loss: 0.570813, acc: 0.695312] [adversarial loss: 1.184357, acc: 0.171875]\n",
      "12046: [discriminator loss: 0.547110, acc: 0.726562] [adversarial loss: 1.071142, acc: 0.296875]\n",
      "12047: [discriminator loss: 0.573935, acc: 0.718750] [adversarial loss: 1.205044, acc: 0.296875]\n",
      "12048: [discriminator loss: 0.580098, acc: 0.695312] [adversarial loss: 1.103345, acc: 0.296875]\n",
      "12049: [discriminator loss: 0.564340, acc: 0.695312] [adversarial loss: 1.345156, acc: 0.093750]\n",
      "12050: [discriminator loss: 0.554129, acc: 0.695312] [adversarial loss: 0.849038, acc: 0.500000]\n",
      "12051: [discriminator loss: 0.559426, acc: 0.695312] [adversarial loss: 1.394918, acc: 0.156250]\n",
      "12052: [discriminator loss: 0.548552, acc: 0.703125] [adversarial loss: 0.907793, acc: 0.421875]\n",
      "12053: [discriminator loss: 0.514967, acc: 0.796875] [adversarial loss: 1.341020, acc: 0.171875]\n",
      "12054: [discriminator loss: 0.511482, acc: 0.710938] [adversarial loss: 0.966452, acc: 0.375000]\n",
      "12055: [discriminator loss: 0.573308, acc: 0.695312] [adversarial loss: 1.216631, acc: 0.203125]\n",
      "12056: [discriminator loss: 0.599634, acc: 0.648438] [adversarial loss: 0.828134, acc: 0.468750]\n",
      "12057: [discriminator loss: 0.546251, acc: 0.664062] [adversarial loss: 1.623041, acc: 0.046875]\n",
      "12058: [discriminator loss: 0.522435, acc: 0.710938] [adversarial loss: 0.868479, acc: 0.421875]\n",
      "12059: [discriminator loss: 0.505329, acc: 0.726562] [adversarial loss: 1.581044, acc: 0.078125]\n",
      "12060: [discriminator loss: 0.592389, acc: 0.679688] [adversarial loss: 0.750085, acc: 0.562500]\n",
      "12061: [discriminator loss: 0.589333, acc: 0.671875] [adversarial loss: 1.476970, acc: 0.093750]\n",
      "12062: [discriminator loss: 0.567948, acc: 0.671875] [adversarial loss: 0.915183, acc: 0.453125]\n",
      "12063: [discriminator loss: 0.601876, acc: 0.687500] [adversarial loss: 1.364819, acc: 0.187500]\n",
      "12064: [discriminator loss: 0.554131, acc: 0.695312] [adversarial loss: 1.031483, acc: 0.343750]\n",
      "12065: [discriminator loss: 0.559209, acc: 0.703125] [adversarial loss: 1.164106, acc: 0.265625]\n",
      "12066: [discriminator loss: 0.514014, acc: 0.742188] [adversarial loss: 1.111999, acc: 0.234375]\n",
      "12067: [discriminator loss: 0.526096, acc: 0.710938] [adversarial loss: 1.093883, acc: 0.265625]\n",
      "12068: [discriminator loss: 0.523911, acc: 0.765625] [adversarial loss: 1.306004, acc: 0.250000]\n",
      "12069: [discriminator loss: 0.597579, acc: 0.687500] [adversarial loss: 0.961067, acc: 0.375000]\n",
      "12070: [discriminator loss: 0.533694, acc: 0.734375] [adversarial loss: 1.504493, acc: 0.109375]\n",
      "12071: [discriminator loss: 0.553707, acc: 0.695312] [adversarial loss: 1.209473, acc: 0.203125]\n",
      "12072: [discriminator loss: 0.556507, acc: 0.687500] [adversarial loss: 1.213929, acc: 0.187500]\n",
      "12073: [discriminator loss: 0.484699, acc: 0.734375] [adversarial loss: 1.419527, acc: 0.140625]\n",
      "12074: [discriminator loss: 0.557542, acc: 0.703125] [adversarial loss: 1.024509, acc: 0.312500]\n",
      "12075: [discriminator loss: 0.582831, acc: 0.679688] [adversarial loss: 1.432449, acc: 0.093750]\n",
      "12076: [discriminator loss: 0.555638, acc: 0.687500] [adversarial loss: 0.949572, acc: 0.359375]\n",
      "12077: [discriminator loss: 0.602187, acc: 0.648438] [adversarial loss: 1.249633, acc: 0.187500]\n",
      "12078: [discriminator loss: 0.548752, acc: 0.687500] [adversarial loss: 1.151400, acc: 0.125000]\n",
      "12079: [discriminator loss: 0.570308, acc: 0.726562] [adversarial loss: 1.218760, acc: 0.281250]\n",
      "12080: [discriminator loss: 0.565382, acc: 0.695312] [adversarial loss: 1.086763, acc: 0.281250]\n",
      "12081: [discriminator loss: 0.515650, acc: 0.765625] [adversarial loss: 1.161710, acc: 0.218750]\n",
      "12082: [discriminator loss: 0.508050, acc: 0.804688] [adversarial loss: 1.081398, acc: 0.312500]\n",
      "12083: [discriminator loss: 0.581414, acc: 0.664062] [adversarial loss: 1.055611, acc: 0.328125]\n",
      "12084: [discriminator loss: 0.604483, acc: 0.648438] [adversarial loss: 1.420237, acc: 0.140625]\n",
      "12085: [discriminator loss: 0.551802, acc: 0.734375] [adversarial loss: 0.820690, acc: 0.406250]\n",
      "12086: [discriminator loss: 0.540236, acc: 0.742188] [adversarial loss: 1.387531, acc: 0.156250]\n",
      "12087: [discriminator loss: 0.603128, acc: 0.656250] [adversarial loss: 0.859131, acc: 0.406250]\n",
      "12088: [discriminator loss: 0.604501, acc: 0.656250] [adversarial loss: 1.516254, acc: 0.156250]\n",
      "12089: [discriminator loss: 0.567005, acc: 0.718750] [adversarial loss: 1.003961, acc: 0.359375]\n",
      "12090: [discriminator loss: 0.537695, acc: 0.742188] [adversarial loss: 1.319905, acc: 0.125000]\n",
      "12091: [discriminator loss: 0.581590, acc: 0.695312] [adversarial loss: 1.064292, acc: 0.296875]\n",
      "12092: [discriminator loss: 0.600865, acc: 0.679688] [adversarial loss: 1.137362, acc: 0.218750]\n",
      "12093: [discriminator loss: 0.502025, acc: 0.726562] [adversarial loss: 1.067797, acc: 0.187500]\n",
      "12094: [discriminator loss: 0.517886, acc: 0.734375] [adversarial loss: 0.883300, acc: 0.468750]\n",
      "12095: [discriminator loss: 0.566015, acc: 0.703125] [adversarial loss: 1.364767, acc: 0.140625]\n",
      "12096: [discriminator loss: 0.601608, acc: 0.664062] [adversarial loss: 1.012212, acc: 0.375000]\n",
      "12097: [discriminator loss: 0.530991, acc: 0.765625] [adversarial loss: 1.443269, acc: 0.156250]\n",
      "12098: [discriminator loss: 0.587844, acc: 0.742188] [adversarial loss: 1.376139, acc: 0.296875]\n",
      "12099: [discriminator loss: 0.592759, acc: 0.679688] [adversarial loss: 1.138958, acc: 0.156250]\n",
      "12100: [discriminator loss: 0.647924, acc: 0.632812] [adversarial loss: 1.157971, acc: 0.234375]\n",
      "12101: [discriminator loss: 0.535582, acc: 0.703125] [adversarial loss: 1.075714, acc: 0.312500]\n",
      "12102: [discriminator loss: 0.497257, acc: 0.726562] [adversarial loss: 1.044047, acc: 0.312500]\n",
      "12103: [discriminator loss: 0.548150, acc: 0.695312] [adversarial loss: 1.425316, acc: 0.125000]\n",
      "12104: [discriminator loss: 0.549990, acc: 0.750000] [adversarial loss: 0.867480, acc: 0.375000]\n",
      "12105: [discriminator loss: 0.541012, acc: 0.648438] [adversarial loss: 1.700469, acc: 0.046875]\n",
      "12106: [discriminator loss: 0.545735, acc: 0.710938] [adversarial loss: 0.885082, acc: 0.437500]\n",
      "12107: [discriminator loss: 0.562980, acc: 0.703125] [adversarial loss: 1.453361, acc: 0.156250]\n",
      "12108: [discriminator loss: 0.525082, acc: 0.742188] [adversarial loss: 0.917258, acc: 0.390625]\n",
      "12109: [discriminator loss: 0.527362, acc: 0.742188] [adversarial loss: 1.089726, acc: 0.265625]\n",
      "12110: [discriminator loss: 0.479165, acc: 0.773438] [adversarial loss: 1.302811, acc: 0.156250]\n",
      "12111: [discriminator loss: 0.513551, acc: 0.742188] [adversarial loss: 0.896321, acc: 0.468750]\n",
      "12112: [discriminator loss: 0.502376, acc: 0.773438] [adversarial loss: 1.467089, acc: 0.140625]\n",
      "12113: [discriminator loss: 0.594637, acc: 0.679688] [adversarial loss: 0.999250, acc: 0.250000]\n",
      "12114: [discriminator loss: 0.513822, acc: 0.773438] [adversarial loss: 1.369429, acc: 0.156250]\n",
      "12115: [discriminator loss: 0.593795, acc: 0.679688] [adversarial loss: 1.034864, acc: 0.328125]\n",
      "12116: [discriminator loss: 0.613867, acc: 0.687500] [adversarial loss: 1.145378, acc: 0.250000]\n",
      "12117: [discriminator loss: 0.520488, acc: 0.773438] [adversarial loss: 0.841261, acc: 0.406250]\n",
      "12118: [discriminator loss: 0.558827, acc: 0.695312] [adversarial loss: 1.363653, acc: 0.218750]\n",
      "12119: [discriminator loss: 0.565102, acc: 0.710938] [adversarial loss: 0.983698, acc: 0.375000]\n",
      "12120: [discriminator loss: 0.596771, acc: 0.671875] [adversarial loss: 1.287317, acc: 0.203125]\n",
      "12121: [discriminator loss: 0.566041, acc: 0.695312] [adversarial loss: 1.008359, acc: 0.437500]\n",
      "12122: [discriminator loss: 0.663471, acc: 0.625000] [adversarial loss: 1.475738, acc: 0.171875]\n",
      "12123: [discriminator loss: 0.583722, acc: 0.656250] [adversarial loss: 0.898234, acc: 0.359375]\n",
      "12124: [discriminator loss: 0.556898, acc: 0.773438] [adversarial loss: 1.374222, acc: 0.125000]\n",
      "12125: [discriminator loss: 0.494477, acc: 0.765625] [adversarial loss: 0.997175, acc: 0.390625]\n",
      "12126: [discriminator loss: 0.563458, acc: 0.664062] [adversarial loss: 1.475639, acc: 0.109375]\n",
      "12127: [discriminator loss: 0.591420, acc: 0.656250] [adversarial loss: 0.727089, acc: 0.593750]\n",
      "12128: [discriminator loss: 0.555700, acc: 0.726562] [adversarial loss: 1.229126, acc: 0.140625]\n",
      "12129: [discriminator loss: 0.547356, acc: 0.718750] [adversarial loss: 0.975450, acc: 0.359375]\n",
      "12130: [discriminator loss: 0.595990, acc: 0.617188] [adversarial loss: 1.160710, acc: 0.234375]\n",
      "12131: [discriminator loss: 0.568833, acc: 0.687500] [adversarial loss: 0.992966, acc: 0.343750]\n",
      "12132: [discriminator loss: 0.499063, acc: 0.773438] [adversarial loss: 1.308768, acc: 0.125000]\n",
      "12133: [discriminator loss: 0.551765, acc: 0.687500] [adversarial loss: 0.898064, acc: 0.406250]\n",
      "12134: [discriminator loss: 0.584954, acc: 0.695312] [adversarial loss: 1.305719, acc: 0.156250]\n",
      "12135: [discriminator loss: 0.611738, acc: 0.671875] [adversarial loss: 1.244907, acc: 0.250000]\n",
      "12136: [discriminator loss: 0.607305, acc: 0.671875] [adversarial loss: 0.879090, acc: 0.437500]\n",
      "12137: [discriminator loss: 0.573974, acc: 0.679688] [adversarial loss: 1.428349, acc: 0.093750]\n",
      "12138: [discriminator loss: 0.552087, acc: 0.656250] [adversarial loss: 1.157944, acc: 0.187500]\n",
      "12139: [discriminator loss: 0.513956, acc: 0.765625] [adversarial loss: 1.241856, acc: 0.187500]\n",
      "12140: [discriminator loss: 0.540253, acc: 0.718750] [adversarial loss: 0.866743, acc: 0.437500]\n",
      "12141: [discriminator loss: 0.669230, acc: 0.570312] [adversarial loss: 1.462664, acc: 0.062500]\n",
      "12142: [discriminator loss: 0.565914, acc: 0.687500] [adversarial loss: 0.798805, acc: 0.500000]\n",
      "12143: [discriminator loss: 0.563267, acc: 0.718750] [adversarial loss: 1.185273, acc: 0.296875]\n",
      "12144: [discriminator loss: 0.518615, acc: 0.703125] [adversarial loss: 1.325758, acc: 0.171875]\n",
      "12145: [discriminator loss: 0.504340, acc: 0.710938] [adversarial loss: 1.014445, acc: 0.343750]\n",
      "12146: [discriminator loss: 0.530565, acc: 0.710938] [adversarial loss: 1.198071, acc: 0.218750]\n",
      "12147: [discriminator loss: 0.560583, acc: 0.734375] [adversarial loss: 1.145797, acc: 0.265625]\n",
      "12148: [discriminator loss: 0.505988, acc: 0.734375] [adversarial loss: 1.203894, acc: 0.234375]\n",
      "12149: [discriminator loss: 0.475169, acc: 0.773438] [adversarial loss: 1.138869, acc: 0.281250]\n",
      "12150: [discriminator loss: 0.621665, acc: 0.671875] [adversarial loss: 1.373983, acc: 0.250000]\n",
      "12151: [discriminator loss: 0.475967, acc: 0.750000] [adversarial loss: 0.922698, acc: 0.421875]\n",
      "12152: [discriminator loss: 0.530076, acc: 0.726562] [adversarial loss: 1.196953, acc: 0.250000]\n",
      "12153: [discriminator loss: 0.550528, acc: 0.726562] [adversarial loss: 1.081449, acc: 0.203125]\n",
      "12154: [discriminator loss: 0.556859, acc: 0.742188] [adversarial loss: 1.119373, acc: 0.281250]\n",
      "12155: [discriminator loss: 0.593368, acc: 0.671875] [adversarial loss: 0.957984, acc: 0.406250]\n",
      "12156: [discriminator loss: 0.509230, acc: 0.750000] [adversarial loss: 1.176265, acc: 0.218750]\n",
      "12157: [discriminator loss: 0.623551, acc: 0.648438] [adversarial loss: 1.157755, acc: 0.234375]\n",
      "12158: [discriminator loss: 0.489171, acc: 0.781250] [adversarial loss: 0.956108, acc: 0.359375]\n",
      "12159: [discriminator loss: 0.523349, acc: 0.750000] [adversarial loss: 1.438128, acc: 0.171875]\n",
      "12160: [discriminator loss: 0.542436, acc: 0.726562] [adversarial loss: 0.834432, acc: 0.468750]\n",
      "12161: [discriminator loss: 0.548427, acc: 0.742188] [adversarial loss: 1.421586, acc: 0.156250]\n",
      "12162: [discriminator loss: 0.601630, acc: 0.671875] [adversarial loss: 1.109064, acc: 0.281250]\n",
      "12163: [discriminator loss: 0.543074, acc: 0.671875] [adversarial loss: 1.262203, acc: 0.156250]\n",
      "12164: [discriminator loss: 0.551447, acc: 0.742188] [adversarial loss: 1.164156, acc: 0.250000]\n",
      "12165: [discriminator loss: 0.601688, acc: 0.640625] [adversarial loss: 1.183902, acc: 0.296875]\n",
      "12166: [discriminator loss: 0.550795, acc: 0.679688] [adversarial loss: 1.132234, acc: 0.265625]\n",
      "12167: [discriminator loss: 0.529944, acc: 0.710938] [adversarial loss: 1.142221, acc: 0.265625]\n",
      "12168: [discriminator loss: 0.543642, acc: 0.718750] [adversarial loss: 1.062226, acc: 0.281250]\n",
      "12169: [discriminator loss: 0.624151, acc: 0.593750] [adversarial loss: 1.152861, acc: 0.125000]\n",
      "12170: [discriminator loss: 0.546210, acc: 0.703125] [adversarial loss: 1.038828, acc: 0.328125]\n",
      "12171: [discriminator loss: 0.596015, acc: 0.648438] [adversarial loss: 0.969718, acc: 0.406250]\n",
      "12172: [discriminator loss: 0.599878, acc: 0.671875] [adversarial loss: 0.981571, acc: 0.359375]\n",
      "12173: [discriminator loss: 0.565104, acc: 0.679688] [adversarial loss: 1.347062, acc: 0.171875]\n",
      "12174: [discriminator loss: 0.558524, acc: 0.742188] [adversarial loss: 1.303963, acc: 0.140625]\n",
      "12175: [discriminator loss: 0.598955, acc: 0.687500] [adversarial loss: 0.968082, acc: 0.312500]\n",
      "12176: [discriminator loss: 0.538887, acc: 0.734375] [adversarial loss: 1.361234, acc: 0.125000]\n",
      "12177: [discriminator loss: 0.594504, acc: 0.695312] [adversarial loss: 1.067123, acc: 0.375000]\n",
      "12178: [discriminator loss: 0.525638, acc: 0.710938] [adversarial loss: 1.000326, acc: 0.312500]\n",
      "12179: [discriminator loss: 0.617272, acc: 0.625000] [adversarial loss: 1.017458, acc: 0.312500]\n",
      "12180: [discriminator loss: 0.530961, acc: 0.695312] [adversarial loss: 1.100308, acc: 0.296875]\n",
      "12181: [discriminator loss: 0.512450, acc: 0.734375] [adversarial loss: 1.141069, acc: 0.187500]\n",
      "12182: [discriminator loss: 0.597340, acc: 0.679688] [adversarial loss: 1.154056, acc: 0.218750]\n",
      "12183: [discriminator loss: 0.573479, acc: 0.734375] [adversarial loss: 1.073574, acc: 0.250000]\n",
      "12184: [discriminator loss: 0.544843, acc: 0.734375] [adversarial loss: 1.482719, acc: 0.093750]\n",
      "12185: [discriminator loss: 0.645835, acc: 0.664062] [adversarial loss: 0.661819, acc: 0.593750]\n",
      "12186: [discriminator loss: 0.563905, acc: 0.710938] [adversarial loss: 1.581282, acc: 0.062500]\n",
      "12187: [discriminator loss: 0.584420, acc: 0.703125] [adversarial loss: 1.199371, acc: 0.218750]\n",
      "12188: [discriminator loss: 0.468518, acc: 0.750000] [adversarial loss: 1.020429, acc: 0.375000]\n",
      "12189: [discriminator loss: 0.581795, acc: 0.671875] [adversarial loss: 1.146647, acc: 0.265625]\n",
      "12190: [discriminator loss: 0.522472, acc: 0.687500] [adversarial loss: 1.211022, acc: 0.203125]\n",
      "12191: [discriminator loss: 0.555252, acc: 0.718750] [adversarial loss: 1.302521, acc: 0.156250]\n",
      "12192: [discriminator loss: 0.556487, acc: 0.695312] [adversarial loss: 0.974624, acc: 0.406250]\n",
      "12193: [discriminator loss: 0.572799, acc: 0.687500] [adversarial loss: 1.258252, acc: 0.218750]\n",
      "12194: [discriminator loss: 0.600477, acc: 0.687500] [adversarial loss: 0.874980, acc: 0.484375]\n",
      "12195: [discriminator loss: 0.476530, acc: 0.789062] [adversarial loss: 1.161463, acc: 0.250000]\n",
      "12196: [discriminator loss: 0.556443, acc: 0.742188] [adversarial loss: 1.046734, acc: 0.281250]\n",
      "12197: [discriminator loss: 0.621147, acc: 0.617188] [adversarial loss: 1.208017, acc: 0.281250]\n",
      "12198: [discriminator loss: 0.565928, acc: 0.703125] [adversarial loss: 1.035991, acc: 0.250000]\n",
      "12199: [discriminator loss: 0.596297, acc: 0.687500] [adversarial loss: 0.997722, acc: 0.375000]\n",
      "12200: [discriminator loss: 0.557153, acc: 0.726562] [adversarial loss: 1.062233, acc: 0.250000]\n",
      "12201: [discriminator loss: 0.591782, acc: 0.648438] [adversarial loss: 1.254810, acc: 0.281250]\n",
      "12202: [discriminator loss: 0.606328, acc: 0.617188] [adversarial loss: 1.004954, acc: 0.328125]\n",
      "12203: [discriminator loss: 0.593753, acc: 0.687500] [adversarial loss: 1.346723, acc: 0.125000]\n",
      "12204: [discriminator loss: 0.602617, acc: 0.687500] [adversarial loss: 0.888634, acc: 0.406250]\n",
      "12205: [discriminator loss: 0.574277, acc: 0.671875] [adversarial loss: 1.525248, acc: 0.125000]\n",
      "12206: [discriminator loss: 0.664850, acc: 0.656250] [adversarial loss: 0.767752, acc: 0.578125]\n",
      "12207: [discriminator loss: 0.654853, acc: 0.578125] [adversarial loss: 1.639098, acc: 0.062500]\n",
      "12208: [discriminator loss: 0.580039, acc: 0.687500] [adversarial loss: 0.819662, acc: 0.437500]\n",
      "12209: [discriminator loss: 0.524016, acc: 0.757812] [adversarial loss: 1.422982, acc: 0.093750]\n",
      "12210: [discriminator loss: 0.481757, acc: 0.789062] [adversarial loss: 1.045864, acc: 0.296875]\n",
      "12211: [discriminator loss: 0.571672, acc: 0.687500] [adversarial loss: 1.192338, acc: 0.218750]\n",
      "12212: [discriminator loss: 0.549034, acc: 0.687500] [adversarial loss: 0.714815, acc: 0.578125]\n",
      "12213: [discriminator loss: 0.554411, acc: 0.710938] [adversarial loss: 1.360403, acc: 0.156250]\n",
      "12214: [discriminator loss: 0.644094, acc: 0.617188] [adversarial loss: 0.913500, acc: 0.390625]\n",
      "12215: [discriminator loss: 0.584720, acc: 0.734375] [adversarial loss: 1.116563, acc: 0.265625]\n",
      "12216: [discriminator loss: 0.516483, acc: 0.742188] [adversarial loss: 1.004866, acc: 0.390625]\n",
      "12217: [discriminator loss: 0.531259, acc: 0.757812] [adversarial loss: 0.955141, acc: 0.343750]\n",
      "12218: [discriminator loss: 0.534594, acc: 0.750000] [adversarial loss: 1.273924, acc: 0.093750]\n",
      "12219: [discriminator loss: 0.582599, acc: 0.679688] [adversarial loss: 1.089867, acc: 0.328125]\n",
      "12220: [discriminator loss: 0.576269, acc: 0.726562] [adversarial loss: 1.050017, acc: 0.375000]\n",
      "12221: [discriminator loss: 0.530811, acc: 0.765625] [adversarial loss: 1.312141, acc: 0.140625]\n",
      "12222: [discriminator loss: 0.518529, acc: 0.757812] [adversarial loss: 0.980343, acc: 0.281250]\n",
      "12223: [discriminator loss: 0.445026, acc: 0.781250] [adversarial loss: 1.091164, acc: 0.296875]\n",
      "12224: [discriminator loss: 0.493988, acc: 0.726562] [adversarial loss: 0.850184, acc: 0.453125]\n",
      "12225: [discriminator loss: 0.549061, acc: 0.718750] [adversarial loss: 1.957736, acc: 0.062500]\n",
      "12226: [discriminator loss: 0.629891, acc: 0.671875] [adversarial loss: 0.747326, acc: 0.609375]\n",
      "12227: [discriminator loss: 0.534166, acc: 0.750000] [adversarial loss: 1.138386, acc: 0.265625]\n",
      "12228: [discriminator loss: 0.540708, acc: 0.703125] [adversarial loss: 1.010547, acc: 0.328125]\n",
      "12229: [discriminator loss: 0.545265, acc: 0.734375] [adversarial loss: 1.305264, acc: 0.187500]\n",
      "12230: [discriminator loss: 0.570818, acc: 0.742188] [adversarial loss: 1.039352, acc: 0.375000]\n",
      "12231: [discriminator loss: 0.554958, acc: 0.734375] [adversarial loss: 1.065041, acc: 0.312500]\n",
      "12232: [discriminator loss: 0.492756, acc: 0.742188] [adversarial loss: 1.434431, acc: 0.093750]\n",
      "12233: [discriminator loss: 0.553649, acc: 0.687500] [adversarial loss: 1.069728, acc: 0.312500]\n",
      "12234: [discriminator loss: 0.521462, acc: 0.734375] [adversarial loss: 1.119234, acc: 0.265625]\n",
      "12235: [discriminator loss: 0.592516, acc: 0.640625] [adversarial loss: 1.024356, acc: 0.328125]\n",
      "12236: [discriminator loss: 0.523217, acc: 0.734375] [adversarial loss: 1.593594, acc: 0.125000]\n",
      "12237: [discriminator loss: 0.520488, acc: 0.718750] [adversarial loss: 0.834702, acc: 0.453125]\n",
      "12238: [discriminator loss: 0.589206, acc: 0.695312] [adversarial loss: 1.196081, acc: 0.218750]\n",
      "12239: [discriminator loss: 0.542718, acc: 0.679688] [adversarial loss: 1.174132, acc: 0.187500]\n",
      "12240: [discriminator loss: 0.514850, acc: 0.765625] [adversarial loss: 1.140713, acc: 0.250000]\n",
      "12241: [discriminator loss: 0.565475, acc: 0.671875] [adversarial loss: 1.143050, acc: 0.328125]\n",
      "12242: [discriminator loss: 0.546837, acc: 0.687500] [adversarial loss: 1.306991, acc: 0.218750]\n",
      "12243: [discriminator loss: 0.626346, acc: 0.617188] [adversarial loss: 1.228147, acc: 0.265625]\n",
      "12244: [discriminator loss: 0.564314, acc: 0.710938] [adversarial loss: 0.994959, acc: 0.390625]\n",
      "12245: [discriminator loss: 0.617669, acc: 0.648438] [adversarial loss: 1.236542, acc: 0.250000]\n",
      "12246: [discriminator loss: 0.602448, acc: 0.656250] [adversarial loss: 0.938307, acc: 0.343750]\n",
      "12247: [discriminator loss: 0.514913, acc: 0.757812] [adversarial loss: 1.304165, acc: 0.218750]\n",
      "12248: [discriminator loss: 0.521890, acc: 0.773438] [adversarial loss: 0.889335, acc: 0.359375]\n",
      "12249: [discriminator loss: 0.576630, acc: 0.734375] [adversarial loss: 1.388040, acc: 0.125000]\n",
      "12250: [discriminator loss: 0.556195, acc: 0.718750] [adversarial loss: 0.921402, acc: 0.453125]\n",
      "12251: [discriminator loss: 0.583265, acc: 0.695312] [adversarial loss: 1.394410, acc: 0.109375]\n",
      "12252: [discriminator loss: 0.593440, acc: 0.671875] [adversarial loss: 0.844518, acc: 0.484375]\n",
      "12253: [discriminator loss: 0.583779, acc: 0.687500] [adversarial loss: 1.312186, acc: 0.218750]\n",
      "12254: [discriminator loss: 0.531429, acc: 0.726562] [adversarial loss: 1.074427, acc: 0.281250]\n",
      "12255: [discriminator loss: 0.510291, acc: 0.757812] [adversarial loss: 1.200094, acc: 0.250000]\n",
      "12256: [discriminator loss: 0.563337, acc: 0.671875] [adversarial loss: 1.232815, acc: 0.328125]\n",
      "12257: [discriminator loss: 0.536050, acc: 0.710938] [adversarial loss: 1.311696, acc: 0.156250]\n",
      "12258: [discriminator loss: 0.586266, acc: 0.695312] [adversarial loss: 0.951416, acc: 0.375000]\n",
      "12259: [discriminator loss: 0.543481, acc: 0.718750] [adversarial loss: 1.198344, acc: 0.187500]\n",
      "12260: [discriminator loss: 0.507131, acc: 0.750000] [adversarial loss: 1.206838, acc: 0.234375]\n",
      "12261: [discriminator loss: 0.490246, acc: 0.757812] [adversarial loss: 1.219021, acc: 0.234375]\n",
      "12262: [discriminator loss: 0.549413, acc: 0.703125] [adversarial loss: 1.087108, acc: 0.328125]\n",
      "12263: [discriminator loss: 0.538611, acc: 0.726562] [adversarial loss: 1.273791, acc: 0.218750]\n",
      "12264: [discriminator loss: 0.587397, acc: 0.648438] [adversarial loss: 1.030629, acc: 0.265625]\n",
      "12265: [discriminator loss: 0.583026, acc: 0.671875] [adversarial loss: 1.247320, acc: 0.234375]\n",
      "12266: [discriminator loss: 0.496584, acc: 0.750000] [adversarial loss: 1.006224, acc: 0.406250]\n",
      "12267: [discriminator loss: 0.536099, acc: 0.718750] [adversarial loss: 1.577551, acc: 0.093750]\n",
      "12268: [discriminator loss: 0.557482, acc: 0.718750] [adversarial loss: 0.857696, acc: 0.500000]\n",
      "12269: [discriminator loss: 0.519329, acc: 0.718750] [adversarial loss: 1.606014, acc: 0.125000]\n",
      "12270: [discriminator loss: 0.502646, acc: 0.726562] [adversarial loss: 0.961380, acc: 0.359375]\n",
      "12271: [discriminator loss: 0.575350, acc: 0.703125] [adversarial loss: 1.201258, acc: 0.218750]\n",
      "12272: [discriminator loss: 0.491120, acc: 0.734375] [adversarial loss: 1.215055, acc: 0.203125]\n",
      "12273: [discriminator loss: 0.565372, acc: 0.679688] [adversarial loss: 1.080639, acc: 0.328125]\n",
      "12274: [discriminator loss: 0.502387, acc: 0.757812] [adversarial loss: 1.342303, acc: 0.140625]\n",
      "12275: [discriminator loss: 0.584908, acc: 0.679688] [adversarial loss: 1.010181, acc: 0.406250]\n",
      "12276: [discriminator loss: 0.588275, acc: 0.726562] [adversarial loss: 1.322798, acc: 0.234375]\n",
      "12277: [discriminator loss: 0.637301, acc: 0.656250] [adversarial loss: 0.868661, acc: 0.375000]\n",
      "12278: [discriminator loss: 0.533777, acc: 0.734375] [adversarial loss: 1.192644, acc: 0.234375]\n",
      "12279: [discriminator loss: 0.587673, acc: 0.710938] [adversarial loss: 1.112253, acc: 0.250000]\n",
      "12280: [discriminator loss: 0.565403, acc: 0.648438] [adversarial loss: 1.272937, acc: 0.140625]\n",
      "12281: [discriminator loss: 0.546585, acc: 0.710938] [adversarial loss: 0.766861, acc: 0.468750]\n",
      "12282: [discriminator loss: 0.589246, acc: 0.710938] [adversarial loss: 1.472700, acc: 0.109375]\n",
      "12283: [discriminator loss: 0.576810, acc: 0.679688] [adversarial loss: 1.048403, acc: 0.265625]\n",
      "12284: [discriminator loss: 0.546582, acc: 0.710938] [adversarial loss: 1.226592, acc: 0.250000]\n",
      "12285: [discriminator loss: 0.596613, acc: 0.679688] [adversarial loss: 1.102442, acc: 0.265625]\n",
      "12286: [discriminator loss: 0.499385, acc: 0.734375] [adversarial loss: 1.195395, acc: 0.265625]\n",
      "12287: [discriminator loss: 0.584990, acc: 0.679688] [adversarial loss: 0.791879, acc: 0.500000]\n",
      "12288: [discriminator loss: 0.544058, acc: 0.710938] [adversarial loss: 1.255286, acc: 0.218750]\n",
      "12289: [discriminator loss: 0.495124, acc: 0.757812] [adversarial loss: 1.147897, acc: 0.250000]\n",
      "12290: [discriminator loss: 0.474958, acc: 0.750000] [adversarial loss: 1.337560, acc: 0.125000]\n",
      "12291: [discriminator loss: 0.471553, acc: 0.804688] [adversarial loss: 1.018850, acc: 0.343750]\n",
      "12292: [discriminator loss: 0.546010, acc: 0.742188] [adversarial loss: 1.125406, acc: 0.265625]\n",
      "12293: [discriminator loss: 0.671149, acc: 0.617188] [adversarial loss: 0.980964, acc: 0.296875]\n",
      "12294: [discriminator loss: 0.503119, acc: 0.742188] [adversarial loss: 1.425719, acc: 0.125000]\n",
      "12295: [discriminator loss: 0.619042, acc: 0.640625] [adversarial loss: 0.808792, acc: 0.437500]\n",
      "12296: [discriminator loss: 0.571266, acc: 0.726562] [adversarial loss: 1.213229, acc: 0.203125]\n",
      "12297: [discriminator loss: 0.501195, acc: 0.726562] [adversarial loss: 0.993436, acc: 0.359375]\n",
      "12298: [discriminator loss: 0.532741, acc: 0.750000] [adversarial loss: 1.295715, acc: 0.265625]\n",
      "12299: [discriminator loss: 0.527052, acc: 0.710938] [adversarial loss: 0.983845, acc: 0.375000]\n",
      "12300: [discriminator loss: 0.538272, acc: 0.710938] [adversarial loss: 1.254760, acc: 0.312500]\n",
      "12301: [discriminator loss: 0.585779, acc: 0.671875] [adversarial loss: 0.857146, acc: 0.437500]\n",
      "12302: [discriminator loss: 0.546150, acc: 0.726562] [adversarial loss: 1.258196, acc: 0.218750]\n",
      "12303: [discriminator loss: 0.466624, acc: 0.781250] [adversarial loss: 1.076383, acc: 0.328125]\n",
      "12304: [discriminator loss: 0.543716, acc: 0.703125] [adversarial loss: 1.111143, acc: 0.234375]\n",
      "12305: [discriminator loss: 0.546020, acc: 0.742188] [adversarial loss: 1.196295, acc: 0.296875]\n",
      "12306: [discriminator loss: 0.569320, acc: 0.640625] [adversarial loss: 1.185816, acc: 0.265625]\n",
      "12307: [discriminator loss: 0.639161, acc: 0.625000] [adversarial loss: 1.034144, acc: 0.265625]\n",
      "12308: [discriminator loss: 0.517225, acc: 0.773438] [adversarial loss: 1.255169, acc: 0.250000]\n",
      "12309: [discriminator loss: 0.571626, acc: 0.656250] [adversarial loss: 1.112547, acc: 0.312500]\n",
      "12310: [discriminator loss: 0.484476, acc: 0.781250] [adversarial loss: 1.219274, acc: 0.265625]\n",
      "12311: [discriminator loss: 0.495336, acc: 0.812500] [adversarial loss: 0.930687, acc: 0.390625]\n",
      "12312: [discriminator loss: 0.517297, acc: 0.726562] [adversarial loss: 0.974798, acc: 0.328125]\n",
      "12313: [discriminator loss: 0.514211, acc: 0.734375] [adversarial loss: 1.045770, acc: 0.312500]\n",
      "12314: [discriminator loss: 0.568120, acc: 0.679688] [adversarial loss: 0.927005, acc: 0.437500]\n",
      "12315: [discriminator loss: 0.523849, acc: 0.718750] [adversarial loss: 1.367558, acc: 0.140625]\n",
      "12316: [discriminator loss: 0.586162, acc: 0.679688] [adversarial loss: 1.193989, acc: 0.250000]\n",
      "12317: [discriminator loss: 0.588346, acc: 0.695312] [adversarial loss: 1.190449, acc: 0.281250]\n",
      "12318: [discriminator loss: 0.627708, acc: 0.679688] [adversarial loss: 1.509549, acc: 0.109375]\n",
      "12319: [discriminator loss: 0.570762, acc: 0.656250] [adversarial loss: 0.514429, acc: 0.796875]\n",
      "12320: [discriminator loss: 0.656070, acc: 0.617188] [adversarial loss: 1.798265, acc: 0.046875]\n",
      "12321: [discriminator loss: 0.705295, acc: 0.617188] [adversarial loss: 0.711467, acc: 0.593750]\n",
      "12322: [discriminator loss: 0.632243, acc: 0.656250] [adversarial loss: 1.255247, acc: 0.156250]\n",
      "12323: [discriminator loss: 0.540078, acc: 0.695312] [adversarial loss: 0.872104, acc: 0.468750]\n",
      "12324: [discriminator loss: 0.545764, acc: 0.742188] [adversarial loss: 1.345452, acc: 0.156250]\n",
      "12325: [discriminator loss: 0.598315, acc: 0.695312] [adversarial loss: 0.797080, acc: 0.562500]\n",
      "12326: [discriminator loss: 0.528133, acc: 0.679688] [adversarial loss: 1.381183, acc: 0.062500]\n",
      "12327: [discriminator loss: 0.596863, acc: 0.726562] [adversarial loss: 1.072366, acc: 0.265625]\n",
      "12328: [discriminator loss: 0.571908, acc: 0.718750] [adversarial loss: 1.190791, acc: 0.218750]\n",
      "12329: [discriminator loss: 0.556927, acc: 0.710938] [adversarial loss: 1.073383, acc: 0.312500]\n",
      "12330: [discriminator loss: 0.574707, acc: 0.679688] [adversarial loss: 0.999430, acc: 0.312500]\n",
      "12331: [discriminator loss: 0.563363, acc: 0.718750] [adversarial loss: 1.281691, acc: 0.140625]\n",
      "12332: [discriminator loss: 0.536262, acc: 0.718750] [adversarial loss: 1.086139, acc: 0.203125]\n",
      "12333: [discriminator loss: 0.590900, acc: 0.679688] [adversarial loss: 0.961943, acc: 0.406250]\n",
      "12334: [discriminator loss: 0.522617, acc: 0.757812] [adversarial loss: 0.923071, acc: 0.421875]\n",
      "12335: [discriminator loss: 0.606470, acc: 0.640625] [adversarial loss: 1.215991, acc: 0.218750]\n",
      "12336: [discriminator loss: 0.492348, acc: 0.757812] [adversarial loss: 1.265792, acc: 0.203125]\n",
      "12337: [discriminator loss: 0.560109, acc: 0.687500] [adversarial loss: 1.084965, acc: 0.375000]\n",
      "12338: [discriminator loss: 0.542027, acc: 0.773438] [adversarial loss: 1.499586, acc: 0.109375]\n",
      "12339: [discriminator loss: 0.524423, acc: 0.781250] [adversarial loss: 0.930278, acc: 0.421875]\n",
      "12340: [discriminator loss: 0.526988, acc: 0.718750] [adversarial loss: 1.168153, acc: 0.187500]\n",
      "12341: [discriminator loss: 0.503150, acc: 0.750000] [adversarial loss: 1.256194, acc: 0.265625]\n",
      "12342: [discriminator loss: 0.577465, acc: 0.664062] [adversarial loss: 1.168327, acc: 0.328125]\n",
      "12343: [discriminator loss: 0.470935, acc: 0.734375] [adversarial loss: 1.010179, acc: 0.328125]\n",
      "12344: [discriminator loss: 0.569587, acc: 0.742188] [adversarial loss: 1.161186, acc: 0.156250]\n",
      "12345: [discriminator loss: 0.625251, acc: 0.601562] [adversarial loss: 1.065982, acc: 0.281250]\n",
      "12346: [discriminator loss: 0.532386, acc: 0.781250] [adversarial loss: 0.976076, acc: 0.328125]\n",
      "12347: [discriminator loss: 0.514781, acc: 0.742188] [adversarial loss: 1.252544, acc: 0.187500]\n",
      "12348: [discriminator loss: 0.570341, acc: 0.695312] [adversarial loss: 1.225177, acc: 0.234375]\n",
      "12349: [discriminator loss: 0.534914, acc: 0.734375] [adversarial loss: 1.029728, acc: 0.343750]\n",
      "12350: [discriminator loss: 0.511514, acc: 0.773438] [adversarial loss: 1.309251, acc: 0.171875]\n",
      "12351: [discriminator loss: 0.601075, acc: 0.656250] [adversarial loss: 0.912287, acc: 0.453125]\n",
      "12352: [discriminator loss: 0.624914, acc: 0.640625] [adversarial loss: 1.578708, acc: 0.062500]\n",
      "12353: [discriminator loss: 0.520483, acc: 0.757812] [adversarial loss: 0.868235, acc: 0.468750]\n",
      "12354: [discriminator loss: 0.691267, acc: 0.609375] [adversarial loss: 1.496905, acc: 0.171875]\n",
      "12355: [discriminator loss: 0.597688, acc: 0.664062] [adversarial loss: 1.097337, acc: 0.265625]\n",
      "12356: [discriminator loss: 0.570712, acc: 0.718750] [adversarial loss: 1.169350, acc: 0.250000]\n",
      "12357: [discriminator loss: 0.546536, acc: 0.742188] [adversarial loss: 1.001579, acc: 0.406250]\n",
      "12358: [discriminator loss: 0.528062, acc: 0.773438] [adversarial loss: 1.245942, acc: 0.187500]\n",
      "12359: [discriminator loss: 0.503360, acc: 0.750000] [adversarial loss: 1.155955, acc: 0.250000]\n",
      "12360: [discriminator loss: 0.504182, acc: 0.726562] [adversarial loss: 1.195934, acc: 0.250000]\n",
      "12361: [discriminator loss: 0.521729, acc: 0.742188] [adversarial loss: 1.103671, acc: 0.296875]\n",
      "12362: [discriminator loss: 0.563146, acc: 0.632812] [adversarial loss: 1.113926, acc: 0.312500]\n",
      "12363: [discriminator loss: 0.592903, acc: 0.679688] [adversarial loss: 1.197821, acc: 0.328125]\n",
      "12364: [discriminator loss: 0.555143, acc: 0.726562] [adversarial loss: 1.130413, acc: 0.296875]\n",
      "12365: [discriminator loss: 0.548755, acc: 0.703125] [adversarial loss: 1.462395, acc: 0.093750]\n",
      "12366: [discriminator loss: 0.583649, acc: 0.687500] [adversarial loss: 1.020602, acc: 0.375000]\n",
      "12367: [discriminator loss: 0.572259, acc: 0.703125] [adversarial loss: 1.151520, acc: 0.234375]\n",
      "12368: [discriminator loss: 0.562672, acc: 0.664062] [adversarial loss: 1.013200, acc: 0.281250]\n",
      "12369: [discriminator loss: 0.492133, acc: 0.773438] [adversarial loss: 1.051096, acc: 0.375000]\n",
      "12370: [discriminator loss: 0.611059, acc: 0.648438] [adversarial loss: 1.066784, acc: 0.250000]\n",
      "12371: [discriminator loss: 0.594575, acc: 0.687500] [adversarial loss: 0.700468, acc: 0.625000]\n",
      "12372: [discriminator loss: 0.612645, acc: 0.695312] [adversarial loss: 1.310407, acc: 0.156250]\n",
      "12373: [discriminator loss: 0.523155, acc: 0.718750] [adversarial loss: 0.758243, acc: 0.562500]\n",
      "12374: [discriminator loss: 0.502796, acc: 0.726562] [adversarial loss: 1.412482, acc: 0.109375]\n",
      "12375: [discriminator loss: 0.562760, acc: 0.734375] [adversarial loss: 0.903566, acc: 0.375000]\n",
      "12376: [discriminator loss: 0.577463, acc: 0.703125] [adversarial loss: 1.298231, acc: 0.140625]\n",
      "12377: [discriminator loss: 0.490991, acc: 0.734375] [adversarial loss: 1.094218, acc: 0.234375]\n",
      "12378: [discriminator loss: 0.525527, acc: 0.710938] [adversarial loss: 0.876501, acc: 0.421875]\n",
      "12379: [discriminator loss: 0.547745, acc: 0.671875] [adversarial loss: 1.131485, acc: 0.281250]\n",
      "12380: [discriminator loss: 0.551643, acc: 0.726562] [adversarial loss: 0.851629, acc: 0.468750]\n",
      "12381: [discriminator loss: 0.651598, acc: 0.570312] [adversarial loss: 1.780551, acc: 0.078125]\n",
      "12382: [discriminator loss: 0.633355, acc: 0.625000] [adversarial loss: 0.780504, acc: 0.468750]\n",
      "12383: [discriminator loss: 0.594067, acc: 0.726562] [adversarial loss: 1.324958, acc: 0.156250]\n",
      "12384: [discriminator loss: 0.563174, acc: 0.679688] [adversarial loss: 1.162120, acc: 0.328125]\n",
      "12385: [discriminator loss: 0.489753, acc: 0.796875] [adversarial loss: 1.302110, acc: 0.218750]\n",
      "12386: [discriminator loss: 0.556805, acc: 0.710938] [adversarial loss: 0.989633, acc: 0.359375]\n",
      "12387: [discriminator loss: 0.524261, acc: 0.765625] [adversarial loss: 1.260265, acc: 0.250000]\n",
      "12388: [discriminator loss: 0.495750, acc: 0.742188] [adversarial loss: 1.139951, acc: 0.250000]\n",
      "12389: [discriminator loss: 0.546913, acc: 0.765625] [adversarial loss: 1.404374, acc: 0.125000]\n",
      "12390: [discriminator loss: 0.555406, acc: 0.679688] [adversarial loss: 0.860703, acc: 0.390625]\n",
      "12391: [discriminator loss: 0.564542, acc: 0.726562] [adversarial loss: 1.378636, acc: 0.140625]\n",
      "12392: [discriminator loss: 0.538793, acc: 0.742188] [adversarial loss: 0.942238, acc: 0.359375]\n",
      "12393: [discriminator loss: 0.511315, acc: 0.773438] [adversarial loss: 1.461055, acc: 0.125000]\n",
      "12394: [discriminator loss: 0.503980, acc: 0.734375] [adversarial loss: 1.233333, acc: 0.187500]\n",
      "12395: [discriminator loss: 0.496508, acc: 0.718750] [adversarial loss: 1.193750, acc: 0.234375]\n",
      "12396: [discriminator loss: 0.556577, acc: 0.687500] [adversarial loss: 1.128891, acc: 0.250000]\n",
      "12397: [discriminator loss: 0.580714, acc: 0.703125] [adversarial loss: 1.004858, acc: 0.359375]\n",
      "12398: [discriminator loss: 0.500555, acc: 0.804688] [adversarial loss: 1.174292, acc: 0.234375]\n",
      "12399: [discriminator loss: 0.575296, acc: 0.687500] [adversarial loss: 1.054820, acc: 0.312500]\n",
      "12400: [discriminator loss: 0.599673, acc: 0.710938] [adversarial loss: 0.902589, acc: 0.468750]\n",
      "12401: [discriminator loss: 0.558396, acc: 0.742188] [adversarial loss: 1.251649, acc: 0.234375]\n",
      "12402: [discriminator loss: 0.556879, acc: 0.718750] [adversarial loss: 0.958853, acc: 0.406250]\n",
      "12403: [discriminator loss: 0.538257, acc: 0.703125] [adversarial loss: 1.455082, acc: 0.093750]\n",
      "12404: [discriminator loss: 0.515265, acc: 0.734375] [adversarial loss: 0.895765, acc: 0.468750]\n",
      "12405: [discriminator loss: 0.613208, acc: 0.664062] [adversarial loss: 1.731089, acc: 0.078125]\n",
      "12406: [discriminator loss: 0.583110, acc: 0.671875] [adversarial loss: 0.878031, acc: 0.390625]\n",
      "12407: [discriminator loss: 0.552393, acc: 0.742188] [adversarial loss: 1.220211, acc: 0.203125]\n",
      "12408: [discriminator loss: 0.541942, acc: 0.710938] [adversarial loss: 1.043077, acc: 0.359375]\n",
      "12409: [discriminator loss: 0.551799, acc: 0.671875] [adversarial loss: 1.105070, acc: 0.296875]\n",
      "12410: [discriminator loss: 0.575006, acc: 0.671875] [adversarial loss: 1.028398, acc: 0.328125]\n",
      "12411: [discriminator loss: 0.557827, acc: 0.695312] [adversarial loss: 1.157245, acc: 0.250000]\n",
      "12412: [discriminator loss: 0.568689, acc: 0.718750] [adversarial loss: 0.971676, acc: 0.390625]\n",
      "12413: [discriminator loss: 0.621740, acc: 0.648438] [adversarial loss: 1.327621, acc: 0.140625]\n",
      "12414: [discriminator loss: 0.549351, acc: 0.695312] [adversarial loss: 1.053964, acc: 0.328125]\n",
      "12415: [discriminator loss: 0.479542, acc: 0.796875] [adversarial loss: 1.467734, acc: 0.156250]\n",
      "12416: [discriminator loss: 0.644525, acc: 0.648438] [adversarial loss: 0.641712, acc: 0.656250]\n",
      "12417: [discriminator loss: 0.612854, acc: 0.648438] [adversarial loss: 1.748287, acc: 0.078125]\n",
      "12418: [discriminator loss: 0.564724, acc: 0.703125] [adversarial loss: 0.929917, acc: 0.375000]\n",
      "12419: [discriminator loss: 0.593624, acc: 0.679688] [adversarial loss: 1.387937, acc: 0.125000]\n",
      "12420: [discriminator loss: 0.545811, acc: 0.695312] [adversarial loss: 0.993387, acc: 0.343750]\n",
      "12421: [discriminator loss: 0.543791, acc: 0.718750] [adversarial loss: 1.135657, acc: 0.203125]\n",
      "12422: [discriminator loss: 0.524369, acc: 0.734375] [adversarial loss: 1.252156, acc: 0.203125]\n",
      "12423: [discriminator loss: 0.527818, acc: 0.734375] [adversarial loss: 0.937510, acc: 0.312500]\n",
      "12424: [discriminator loss: 0.485680, acc: 0.781250] [adversarial loss: 1.327989, acc: 0.140625]\n",
      "12425: [discriminator loss: 0.503338, acc: 0.789062] [adversarial loss: 1.035185, acc: 0.343750]\n",
      "12426: [discriminator loss: 0.535110, acc: 0.687500] [adversarial loss: 1.237195, acc: 0.203125]\n",
      "12427: [discriminator loss: 0.561400, acc: 0.679688] [adversarial loss: 1.066478, acc: 0.375000]\n",
      "12428: [discriminator loss: 0.529167, acc: 0.718750] [adversarial loss: 1.275170, acc: 0.218750]\n",
      "12429: [discriminator loss: 0.572984, acc: 0.703125] [adversarial loss: 1.102037, acc: 0.343750]\n",
      "12430: [discriminator loss: 0.558252, acc: 0.687500] [adversarial loss: 1.362295, acc: 0.125000]\n",
      "12431: [discriminator loss: 0.635214, acc: 0.679688] [adversarial loss: 0.906032, acc: 0.375000]\n",
      "12432: [discriminator loss: 0.507445, acc: 0.710938] [adversarial loss: 1.296624, acc: 0.062500]\n",
      "12433: [discriminator loss: 0.545279, acc: 0.695312] [adversarial loss: 1.055102, acc: 0.265625]\n",
      "12434: [discriminator loss: 0.530358, acc: 0.695312] [adversarial loss: 1.299047, acc: 0.109375]\n",
      "12435: [discriminator loss: 0.533768, acc: 0.726562] [adversarial loss: 0.860835, acc: 0.421875]\n",
      "12436: [discriminator loss: 0.512890, acc: 0.757812] [adversarial loss: 1.179199, acc: 0.203125]\n",
      "12437: [discriminator loss: 0.498524, acc: 0.757812] [adversarial loss: 1.127841, acc: 0.250000]\n",
      "12438: [discriminator loss: 0.508005, acc: 0.742188] [adversarial loss: 0.994062, acc: 0.359375]\n",
      "12439: [discriminator loss: 0.600643, acc: 0.648438] [adversarial loss: 0.975879, acc: 0.375000]\n",
      "12440: [discriminator loss: 0.537374, acc: 0.734375] [adversarial loss: 1.150799, acc: 0.281250]\n",
      "12441: [discriminator loss: 0.556312, acc: 0.726562] [adversarial loss: 1.138616, acc: 0.203125]\n",
      "12442: [discriminator loss: 0.601835, acc: 0.671875] [adversarial loss: 1.340526, acc: 0.109375]\n",
      "12443: [discriminator loss: 0.580124, acc: 0.695312] [adversarial loss: 1.028104, acc: 0.281250]\n",
      "12444: [discriminator loss: 0.514459, acc: 0.734375] [adversarial loss: 1.258070, acc: 0.156250]\n",
      "12445: [discriminator loss: 0.509964, acc: 0.742188] [adversarial loss: 1.093333, acc: 0.312500]\n",
      "12446: [discriminator loss: 0.502655, acc: 0.781250] [adversarial loss: 1.049487, acc: 0.281250]\n",
      "12447: [discriminator loss: 0.511791, acc: 0.710938] [adversarial loss: 1.050627, acc: 0.296875]\n",
      "12448: [discriminator loss: 0.556840, acc: 0.726562] [adversarial loss: 1.108713, acc: 0.296875]\n",
      "12449: [discriminator loss: 0.539974, acc: 0.757812] [adversarial loss: 1.111593, acc: 0.281250]\n",
      "12450: [discriminator loss: 0.514133, acc: 0.734375] [adversarial loss: 1.198431, acc: 0.203125]\n",
      "12451: [discriminator loss: 0.525985, acc: 0.710938] [adversarial loss: 1.226751, acc: 0.187500]\n",
      "12452: [discriminator loss: 0.582868, acc: 0.671875] [adversarial loss: 0.919787, acc: 0.406250]\n",
      "12453: [discriminator loss: 0.598039, acc: 0.710938] [adversarial loss: 1.964282, acc: 0.015625]\n",
      "12454: [discriminator loss: 0.689652, acc: 0.625000] [adversarial loss: 0.821215, acc: 0.421875]\n",
      "12455: [discriminator loss: 0.602230, acc: 0.671875] [adversarial loss: 1.496950, acc: 0.078125]\n",
      "12456: [discriminator loss: 0.645807, acc: 0.648438] [adversarial loss: 0.866434, acc: 0.453125]\n",
      "12457: [discriminator loss: 0.611350, acc: 0.656250] [adversarial loss: 1.206062, acc: 0.187500]\n",
      "12458: [discriminator loss: 0.578261, acc: 0.679688] [adversarial loss: 0.944663, acc: 0.343750]\n",
      "12459: [discriminator loss: 0.453880, acc: 0.820312] [adversarial loss: 1.275814, acc: 0.187500]\n",
      "12460: [discriminator loss: 0.564652, acc: 0.695312] [adversarial loss: 1.099433, acc: 0.234375]\n",
      "12461: [discriminator loss: 0.592169, acc: 0.656250] [adversarial loss: 1.028256, acc: 0.375000]\n",
      "12462: [discriminator loss: 0.625626, acc: 0.648438] [adversarial loss: 1.345693, acc: 0.203125]\n",
      "12463: [discriminator loss: 0.551751, acc: 0.656250] [adversarial loss: 0.956780, acc: 0.390625]\n",
      "12464: [discriminator loss: 0.566903, acc: 0.695312] [adversarial loss: 1.065077, acc: 0.296875]\n",
      "12465: [discriminator loss: 0.509537, acc: 0.750000] [adversarial loss: 0.977936, acc: 0.265625]\n",
      "12466: [discriminator loss: 0.628446, acc: 0.640625] [adversarial loss: 1.220315, acc: 0.187500]\n",
      "12467: [discriminator loss: 0.565720, acc: 0.726562] [adversarial loss: 1.109045, acc: 0.343750]\n",
      "12468: [discriminator loss: 0.545257, acc: 0.718750] [adversarial loss: 0.942207, acc: 0.359375]\n",
      "12469: [discriminator loss: 0.563202, acc: 0.671875] [adversarial loss: 1.172920, acc: 0.203125]\n",
      "12470: [discriminator loss: 0.554351, acc: 0.710938] [adversarial loss: 1.143675, acc: 0.250000]\n",
      "12471: [discriminator loss: 0.550015, acc: 0.695312] [adversarial loss: 0.994511, acc: 0.343750]\n",
      "12472: [discriminator loss: 0.472609, acc: 0.804688] [adversarial loss: 1.229058, acc: 0.203125]\n",
      "12473: [discriminator loss: 0.538289, acc: 0.742188] [adversarial loss: 0.890080, acc: 0.468750]\n",
      "12474: [discriminator loss: 0.548598, acc: 0.671875] [adversarial loss: 1.383848, acc: 0.125000]\n",
      "12475: [discriminator loss: 0.500204, acc: 0.742188] [adversarial loss: 0.992865, acc: 0.328125]\n",
      "12476: [discriminator loss: 0.583742, acc: 0.687500] [adversarial loss: 1.701904, acc: 0.109375]\n",
      "12477: [discriminator loss: 0.534246, acc: 0.679688] [adversarial loss: 0.877948, acc: 0.375000]\n",
      "12478: [discriminator loss: 0.602942, acc: 0.679688] [adversarial loss: 1.440747, acc: 0.109375]\n",
      "12479: [discriminator loss: 0.574655, acc: 0.687500] [adversarial loss: 0.949089, acc: 0.312500]\n",
      "12480: [discriminator loss: 0.541178, acc: 0.718750] [adversarial loss: 1.281908, acc: 0.265625]\n",
      "12481: [discriminator loss: 0.478093, acc: 0.773438] [adversarial loss: 1.298266, acc: 0.187500]\n",
      "12482: [discriminator loss: 0.588996, acc: 0.664062] [adversarial loss: 0.845149, acc: 0.500000]\n",
      "12483: [discriminator loss: 0.497723, acc: 0.750000] [adversarial loss: 1.273637, acc: 0.250000]\n",
      "12484: [discriminator loss: 0.542193, acc: 0.648438] [adversarial loss: 1.017753, acc: 0.375000]\n",
      "12485: [discriminator loss: 0.579966, acc: 0.625000] [adversarial loss: 1.421450, acc: 0.156250]\n",
      "12486: [discriminator loss: 0.547124, acc: 0.718750] [adversarial loss: 0.910587, acc: 0.390625]\n",
      "12487: [discriminator loss: 0.585639, acc: 0.695312] [adversarial loss: 1.180819, acc: 0.265625]\n",
      "12488: [discriminator loss: 0.555398, acc: 0.703125] [adversarial loss: 1.025770, acc: 0.296875]\n",
      "12489: [discriminator loss: 0.543451, acc: 0.742188] [adversarial loss: 1.256827, acc: 0.218750]\n",
      "12490: [discriminator loss: 0.582859, acc: 0.648438] [adversarial loss: 1.056786, acc: 0.328125]\n",
      "12491: [discriminator loss: 0.580007, acc: 0.687500] [adversarial loss: 1.582407, acc: 0.140625]\n",
      "12492: [discriminator loss: 0.537393, acc: 0.710938] [adversarial loss: 0.706909, acc: 0.578125]\n",
      "12493: [discriminator loss: 0.566905, acc: 0.695312] [adversarial loss: 1.473303, acc: 0.156250]\n",
      "12494: [discriminator loss: 0.505692, acc: 0.750000] [adversarial loss: 1.025987, acc: 0.296875]\n",
      "12495: [discriminator loss: 0.478662, acc: 0.765625] [adversarial loss: 1.230226, acc: 0.234375]\n",
      "12496: [discriminator loss: 0.541783, acc: 0.718750] [adversarial loss: 1.144020, acc: 0.265625]\n",
      "12497: [discriminator loss: 0.590094, acc: 0.679688] [adversarial loss: 1.315881, acc: 0.203125]\n",
      "12498: [discriminator loss: 0.605380, acc: 0.710938] [adversarial loss: 1.121619, acc: 0.265625]\n",
      "12499: [discriminator loss: 0.548434, acc: 0.695312] [adversarial loss: 1.139129, acc: 0.203125]\n",
      "12500: [discriminator loss: 0.507866, acc: 0.781250] [adversarial loss: 1.266619, acc: 0.187500]\n",
      "12501: [discriminator loss: 0.558927, acc: 0.687500] [adversarial loss: 1.098040, acc: 0.296875]\n",
      "12502: [discriminator loss: 0.607940, acc: 0.679688] [adversarial loss: 1.295240, acc: 0.140625]\n",
      "12503: [discriminator loss: 0.466571, acc: 0.773438] [adversarial loss: 1.026797, acc: 0.250000]\n",
      "12504: [discriminator loss: 0.604799, acc: 0.703125] [adversarial loss: 1.562822, acc: 0.078125]\n",
      "12505: [discriminator loss: 0.509982, acc: 0.687500] [adversarial loss: 0.873080, acc: 0.390625]\n",
      "12506: [discriminator loss: 0.551602, acc: 0.679688] [adversarial loss: 1.290037, acc: 0.234375]\n",
      "12507: [discriminator loss: 0.556091, acc: 0.710938] [adversarial loss: 0.902051, acc: 0.406250]\n",
      "12508: [discriminator loss: 0.506449, acc: 0.734375] [adversarial loss: 1.144804, acc: 0.203125]\n",
      "12509: [discriminator loss: 0.516637, acc: 0.765625] [adversarial loss: 1.118165, acc: 0.250000]\n",
      "12510: [discriminator loss: 0.548172, acc: 0.695312] [adversarial loss: 0.903777, acc: 0.359375]\n",
      "12511: [discriminator loss: 0.532471, acc: 0.742188] [adversarial loss: 1.475548, acc: 0.140625]\n",
      "12512: [discriminator loss: 0.531808, acc: 0.718750] [adversarial loss: 1.090838, acc: 0.296875]\n",
      "12513: [discriminator loss: 0.593046, acc: 0.703125] [adversarial loss: 1.253812, acc: 0.234375]\n",
      "12514: [discriminator loss: 0.515355, acc: 0.789062] [adversarial loss: 1.193046, acc: 0.218750]\n",
      "12515: [discriminator loss: 0.557126, acc: 0.726562] [adversarial loss: 0.931902, acc: 0.375000]\n",
      "12516: [discriminator loss: 0.557675, acc: 0.664062] [adversarial loss: 1.575008, acc: 0.046875]\n",
      "12517: [discriminator loss: 0.638699, acc: 0.664062] [adversarial loss: 0.849142, acc: 0.484375]\n",
      "12518: [discriminator loss: 0.583313, acc: 0.687500] [adversarial loss: 1.476981, acc: 0.156250]\n",
      "12519: [discriminator loss: 0.557437, acc: 0.671875] [adversarial loss: 0.920343, acc: 0.375000]\n",
      "12520: [discriminator loss: 0.563281, acc: 0.664062] [adversarial loss: 1.171386, acc: 0.187500]\n",
      "12521: [discriminator loss: 0.513869, acc: 0.726562] [adversarial loss: 0.948099, acc: 0.375000]\n",
      "12522: [discriminator loss: 0.584823, acc: 0.664062] [adversarial loss: 1.253990, acc: 0.234375]\n",
      "12523: [discriminator loss: 0.513912, acc: 0.742188] [adversarial loss: 1.124799, acc: 0.203125]\n",
      "12524: [discriminator loss: 0.525555, acc: 0.734375] [adversarial loss: 0.924686, acc: 0.406250]\n",
      "12525: [discriminator loss: 0.457934, acc: 0.812500] [adversarial loss: 1.353091, acc: 0.156250]\n",
      "12526: [discriminator loss: 0.599049, acc: 0.671875] [adversarial loss: 0.991873, acc: 0.390625]\n",
      "12527: [discriminator loss: 0.550392, acc: 0.710938] [adversarial loss: 1.280823, acc: 0.234375]\n",
      "12528: [discriminator loss: 0.553069, acc: 0.695312] [adversarial loss: 1.062551, acc: 0.359375]\n",
      "12529: [discriminator loss: 0.587274, acc: 0.710938] [adversarial loss: 1.563748, acc: 0.078125]\n",
      "12530: [discriminator loss: 0.626935, acc: 0.664062] [adversarial loss: 0.907033, acc: 0.437500]\n",
      "12531: [discriminator loss: 0.520443, acc: 0.765625] [adversarial loss: 1.435091, acc: 0.093750]\n",
      "12532: [discriminator loss: 0.557566, acc: 0.718750] [adversarial loss: 0.837448, acc: 0.468750]\n",
      "12533: [discriminator loss: 0.506971, acc: 0.718750] [adversarial loss: 1.572130, acc: 0.062500]\n",
      "12534: [discriminator loss: 0.541648, acc: 0.718750] [adversarial loss: 0.695284, acc: 0.562500]\n",
      "12535: [discriminator loss: 0.588433, acc: 0.679688] [adversarial loss: 1.553740, acc: 0.093750]\n",
      "12536: [discriminator loss: 0.651599, acc: 0.648438] [adversarial loss: 0.944432, acc: 0.343750]\n",
      "12537: [discriminator loss: 0.525138, acc: 0.695312] [adversarial loss: 1.142483, acc: 0.265625]\n",
      "12538: [discriminator loss: 0.534940, acc: 0.773438] [adversarial loss: 1.119244, acc: 0.312500]\n",
      "12539: [discriminator loss: 0.595697, acc: 0.632812] [adversarial loss: 1.093930, acc: 0.281250]\n",
      "12540: [discriminator loss: 0.545424, acc: 0.726562] [adversarial loss: 1.058767, acc: 0.359375]\n",
      "12541: [discriminator loss: 0.523279, acc: 0.703125] [adversarial loss: 1.108829, acc: 0.234375]\n",
      "12542: [discriminator loss: 0.527640, acc: 0.718750] [adversarial loss: 1.229323, acc: 0.203125]\n",
      "12543: [discriminator loss: 0.532166, acc: 0.710938] [adversarial loss: 1.246241, acc: 0.218750]\n",
      "12544: [discriminator loss: 0.467708, acc: 0.765625] [adversarial loss: 1.310015, acc: 0.234375]\n",
      "12545: [discriminator loss: 0.494845, acc: 0.773438] [adversarial loss: 1.047284, acc: 0.328125]\n",
      "12546: [discriminator loss: 0.542104, acc: 0.718750] [adversarial loss: 1.518737, acc: 0.109375]\n",
      "12547: [discriminator loss: 0.596693, acc: 0.679688] [adversarial loss: 1.015250, acc: 0.312500]\n",
      "12548: [discriminator loss: 0.614690, acc: 0.664062] [adversarial loss: 1.517485, acc: 0.093750]\n",
      "12549: [discriminator loss: 0.606296, acc: 0.703125] [adversarial loss: 0.886367, acc: 0.359375]\n",
      "12550: [discriminator loss: 0.551836, acc: 0.718750] [adversarial loss: 1.400362, acc: 0.203125]\n",
      "12551: [discriminator loss: 0.554023, acc: 0.718750] [adversarial loss: 1.113454, acc: 0.281250]\n",
      "12552: [discriminator loss: 0.566973, acc: 0.679688] [adversarial loss: 1.078652, acc: 0.250000]\n",
      "12553: [discriminator loss: 0.572721, acc: 0.734375] [adversarial loss: 1.316431, acc: 0.109375]\n",
      "12554: [discriminator loss: 0.547311, acc: 0.710938] [adversarial loss: 0.887288, acc: 0.406250]\n",
      "12555: [discriminator loss: 0.544855, acc: 0.703125] [adversarial loss: 1.264573, acc: 0.281250]\n",
      "12556: [discriminator loss: 0.571972, acc: 0.656250] [adversarial loss: 1.178904, acc: 0.218750]\n",
      "12557: [discriminator loss: 0.502179, acc: 0.781250] [adversarial loss: 1.260210, acc: 0.156250]\n",
      "12558: [discriminator loss: 0.507975, acc: 0.726562] [adversarial loss: 1.058509, acc: 0.312500]\n",
      "12559: [discriminator loss: 0.531681, acc: 0.718750] [adversarial loss: 1.279299, acc: 0.312500]\n",
      "12560: [discriminator loss: 0.493577, acc: 0.742188] [adversarial loss: 1.036824, acc: 0.234375]\n",
      "12561: [discriminator loss: 0.584044, acc: 0.671875] [adversarial loss: 1.035789, acc: 0.312500]\n",
      "12562: [discriminator loss: 0.499001, acc: 0.765625] [adversarial loss: 0.829971, acc: 0.453125]\n",
      "12563: [discriminator loss: 0.589144, acc: 0.718750] [adversarial loss: 1.745841, acc: 0.078125]\n",
      "12564: [discriminator loss: 0.628538, acc: 0.648438] [adversarial loss: 0.902946, acc: 0.406250]\n",
      "12565: [discriminator loss: 0.534023, acc: 0.757812] [adversarial loss: 1.544816, acc: 0.046875]\n",
      "12566: [discriminator loss: 0.536817, acc: 0.734375] [adversarial loss: 0.795875, acc: 0.515625]\n",
      "12567: [discriminator loss: 0.531684, acc: 0.726562] [adversarial loss: 1.306883, acc: 0.218750]\n",
      "12568: [discriminator loss: 0.594980, acc: 0.679688] [adversarial loss: 0.864503, acc: 0.453125]\n",
      "12569: [discriminator loss: 0.565522, acc: 0.718750] [adversarial loss: 1.017780, acc: 0.375000]\n",
      "12570: [discriminator loss: 0.544308, acc: 0.718750] [adversarial loss: 1.215419, acc: 0.187500]\n",
      "12571: [discriminator loss: 0.517503, acc: 0.742188] [adversarial loss: 0.974040, acc: 0.281250]\n",
      "12572: [discriminator loss: 0.563465, acc: 0.687500] [adversarial loss: 1.092835, acc: 0.218750]\n",
      "12573: [discriminator loss: 0.536120, acc: 0.734375] [adversarial loss: 1.380637, acc: 0.125000]\n",
      "12574: [discriminator loss: 0.554081, acc: 0.695312] [adversarial loss: 1.115539, acc: 0.312500]\n",
      "12575: [discriminator loss: 0.535959, acc: 0.734375] [adversarial loss: 1.323349, acc: 0.234375]\n",
      "12576: [discriminator loss: 0.473839, acc: 0.804688] [adversarial loss: 1.507442, acc: 0.156250]\n",
      "12577: [discriminator loss: 0.555902, acc: 0.703125] [adversarial loss: 0.911002, acc: 0.406250]\n",
      "12578: [discriminator loss: 0.568844, acc: 0.718750] [adversarial loss: 1.467719, acc: 0.156250]\n",
      "12579: [discriminator loss: 0.590295, acc: 0.687500] [adversarial loss: 0.698210, acc: 0.578125]\n",
      "12580: [discriminator loss: 0.627623, acc: 0.656250] [adversarial loss: 1.690449, acc: 0.125000]\n",
      "12581: [discriminator loss: 0.550518, acc: 0.703125] [adversarial loss: 0.911896, acc: 0.437500]\n",
      "12582: [discriminator loss: 0.501375, acc: 0.742188] [adversarial loss: 1.191532, acc: 0.281250]\n",
      "12583: [discriminator loss: 0.575577, acc: 0.710938] [adversarial loss: 1.151878, acc: 0.203125]\n",
      "12584: [discriminator loss: 0.576038, acc: 0.687500] [adversarial loss: 1.250670, acc: 0.250000]\n",
      "12585: [discriminator loss: 0.542644, acc: 0.718750] [adversarial loss: 1.092435, acc: 0.250000]\n",
      "12586: [discriminator loss: 0.569035, acc: 0.671875] [adversarial loss: 1.145167, acc: 0.281250]\n",
      "12587: [discriminator loss: 0.582393, acc: 0.718750] [adversarial loss: 1.078842, acc: 0.328125]\n",
      "12588: [discriminator loss: 0.459494, acc: 0.789062] [adversarial loss: 1.137214, acc: 0.203125]\n",
      "12589: [discriminator loss: 0.544500, acc: 0.687500] [adversarial loss: 1.207287, acc: 0.265625]\n",
      "12590: [discriminator loss: 0.549327, acc: 0.710938] [adversarial loss: 1.062990, acc: 0.203125]\n",
      "12591: [discriminator loss: 0.565659, acc: 0.734375] [adversarial loss: 1.412366, acc: 0.093750]\n",
      "12592: [discriminator loss: 0.514582, acc: 0.726562] [adversarial loss: 0.804648, acc: 0.406250]\n",
      "12593: [discriminator loss: 0.593544, acc: 0.656250] [adversarial loss: 1.572638, acc: 0.062500]\n",
      "12594: [discriminator loss: 0.555390, acc: 0.710938] [adversarial loss: 0.825638, acc: 0.406250]\n",
      "12595: [discriminator loss: 0.504676, acc: 0.734375] [adversarial loss: 1.441893, acc: 0.156250]\n",
      "12596: [discriminator loss: 0.533096, acc: 0.718750] [adversarial loss: 0.798735, acc: 0.437500]\n",
      "12597: [discriminator loss: 0.561945, acc: 0.718750] [adversarial loss: 1.639760, acc: 0.093750]\n",
      "12598: [discriminator loss: 0.565020, acc: 0.726562] [adversarial loss: 0.793373, acc: 0.484375]\n",
      "12599: [discriminator loss: 0.587916, acc: 0.687500] [adversarial loss: 1.456585, acc: 0.187500]\n",
      "12600: [discriminator loss: 0.540734, acc: 0.703125] [adversarial loss: 0.986476, acc: 0.390625]\n",
      "12601: [discriminator loss: 0.562788, acc: 0.734375] [adversarial loss: 1.256068, acc: 0.187500]\n",
      "12602: [discriminator loss: 0.577882, acc: 0.632812] [adversarial loss: 1.134469, acc: 0.281250]\n",
      "12603: [discriminator loss: 0.518375, acc: 0.703125] [adversarial loss: 1.475383, acc: 0.171875]\n",
      "12604: [discriminator loss: 0.559793, acc: 0.695312] [adversarial loss: 1.194964, acc: 0.250000]\n",
      "12605: [discriminator loss: 0.582890, acc: 0.710938] [adversarial loss: 1.111459, acc: 0.265625]\n",
      "12606: [discriminator loss: 0.563891, acc: 0.710938] [adversarial loss: 1.103073, acc: 0.296875]\n",
      "12607: [discriminator loss: 0.541704, acc: 0.765625] [adversarial loss: 1.449506, acc: 0.093750]\n",
      "12608: [discriminator loss: 0.596960, acc: 0.671875] [adversarial loss: 1.019842, acc: 0.281250]\n",
      "12609: [discriminator loss: 0.575920, acc: 0.679688] [adversarial loss: 1.512327, acc: 0.171875]\n",
      "12610: [discriminator loss: 0.542388, acc: 0.726562] [adversarial loss: 0.756207, acc: 0.453125]\n",
      "12611: [discriminator loss: 0.585136, acc: 0.679688] [adversarial loss: 1.542818, acc: 0.125000]\n",
      "12612: [discriminator loss: 0.594318, acc: 0.664062] [adversarial loss: 0.842503, acc: 0.453125]\n",
      "12613: [discriminator loss: 0.504136, acc: 0.734375] [adversarial loss: 1.451050, acc: 0.109375]\n",
      "12614: [discriminator loss: 0.613661, acc: 0.664062] [adversarial loss: 0.954397, acc: 0.390625]\n",
      "12615: [discriminator loss: 0.564630, acc: 0.695312] [adversarial loss: 1.148203, acc: 0.250000]\n",
      "12616: [discriminator loss: 0.591277, acc: 0.695312] [adversarial loss: 1.124632, acc: 0.296875]\n",
      "12617: [discriminator loss: 0.556274, acc: 0.718750] [adversarial loss: 1.036962, acc: 0.343750]\n",
      "12618: [discriminator loss: 0.522825, acc: 0.710938] [adversarial loss: 1.240794, acc: 0.203125]\n",
      "12619: [discriminator loss: 0.578600, acc: 0.695312] [adversarial loss: 1.128882, acc: 0.343750]\n",
      "12620: [discriminator loss: 0.503357, acc: 0.750000] [adversarial loss: 1.095732, acc: 0.203125]\n",
      "12621: [discriminator loss: 0.578701, acc: 0.664062] [adversarial loss: 1.427384, acc: 0.140625]\n",
      "12622: [discriminator loss: 0.579026, acc: 0.703125] [adversarial loss: 0.982133, acc: 0.343750]\n",
      "12623: [discriminator loss: 0.533039, acc: 0.734375] [adversarial loss: 1.207861, acc: 0.234375]\n",
      "12624: [discriminator loss: 0.592471, acc: 0.703125] [adversarial loss: 1.396083, acc: 0.203125]\n",
      "12625: [discriminator loss: 0.579684, acc: 0.671875] [adversarial loss: 0.930831, acc: 0.343750]\n",
      "12626: [discriminator loss: 0.518550, acc: 0.750000] [adversarial loss: 1.234470, acc: 0.218750]\n",
      "12627: [discriminator loss: 0.604722, acc: 0.671875] [adversarial loss: 0.865893, acc: 0.437500]\n",
      "12628: [discriminator loss: 0.577803, acc: 0.679688] [adversarial loss: 1.435968, acc: 0.140625]\n",
      "12629: [discriminator loss: 0.577942, acc: 0.718750] [adversarial loss: 1.220762, acc: 0.296875]\n",
      "12630: [discriminator loss: 0.538765, acc: 0.742188] [adversarial loss: 1.178874, acc: 0.218750]\n",
      "12631: [discriminator loss: 0.466895, acc: 0.773438] [adversarial loss: 0.947808, acc: 0.359375]\n",
      "12632: [discriminator loss: 0.525254, acc: 0.742188] [adversarial loss: 1.292667, acc: 0.140625]\n",
      "12633: [discriminator loss: 0.526712, acc: 0.695312] [adversarial loss: 1.061772, acc: 0.296875]\n",
      "12634: [discriminator loss: 0.510480, acc: 0.726562] [adversarial loss: 1.305853, acc: 0.187500]\n",
      "12635: [discriminator loss: 0.523546, acc: 0.734375] [adversarial loss: 1.187307, acc: 0.218750]\n",
      "12636: [discriminator loss: 0.492466, acc: 0.726562] [adversarial loss: 1.134017, acc: 0.265625]\n",
      "12637: [discriminator loss: 0.542785, acc: 0.750000] [adversarial loss: 1.630012, acc: 0.093750]\n",
      "12638: [discriminator loss: 0.571619, acc: 0.687500] [adversarial loss: 0.741274, acc: 0.593750]\n",
      "12639: [discriminator loss: 0.539944, acc: 0.734375] [adversarial loss: 1.736672, acc: 0.062500]\n",
      "12640: [discriminator loss: 0.661636, acc: 0.632812] [adversarial loss: 0.707966, acc: 0.625000]\n",
      "12641: [discriminator loss: 0.624359, acc: 0.703125] [adversarial loss: 1.720188, acc: 0.093750]\n",
      "12642: [discriminator loss: 0.622819, acc: 0.679688] [adversarial loss: 0.795861, acc: 0.484375]\n",
      "12643: [discriminator loss: 0.629405, acc: 0.625000] [adversarial loss: 1.365956, acc: 0.218750]\n",
      "12644: [discriminator loss: 0.527380, acc: 0.734375] [adversarial loss: 1.315835, acc: 0.203125]\n",
      "12645: [discriminator loss: 0.518262, acc: 0.726562] [adversarial loss: 1.046898, acc: 0.390625]\n",
      "12646: [discriminator loss: 0.522792, acc: 0.679688] [adversarial loss: 1.090807, acc: 0.250000]\n",
      "12647: [discriminator loss: 0.572344, acc: 0.671875] [adversarial loss: 0.988485, acc: 0.359375]\n",
      "12648: [discriminator loss: 0.496845, acc: 0.734375] [adversarial loss: 1.439734, acc: 0.140625]\n",
      "12649: [discriminator loss: 0.566258, acc: 0.679688] [adversarial loss: 1.112192, acc: 0.375000]\n",
      "12650: [discriminator loss: 0.531134, acc: 0.742188] [adversarial loss: 0.952957, acc: 0.343750]\n",
      "12651: [discriminator loss: 0.488839, acc: 0.757812] [adversarial loss: 1.275436, acc: 0.250000]\n",
      "12652: [discriminator loss: 0.562468, acc: 0.679688] [adversarial loss: 0.983777, acc: 0.375000]\n",
      "12653: [discriminator loss: 0.530938, acc: 0.750000] [adversarial loss: 1.157447, acc: 0.218750]\n",
      "12654: [discriminator loss: 0.528231, acc: 0.703125] [adversarial loss: 1.353511, acc: 0.156250]\n",
      "12655: [discriminator loss: 0.603777, acc: 0.617188] [adversarial loss: 1.462984, acc: 0.203125]\n",
      "12656: [discriminator loss: 0.596078, acc: 0.695312] [adversarial loss: 1.040319, acc: 0.250000]\n",
      "12657: [discriminator loss: 0.512123, acc: 0.750000] [adversarial loss: 1.386717, acc: 0.187500]\n",
      "12658: [discriminator loss: 0.597736, acc: 0.703125] [adversarial loss: 1.349429, acc: 0.140625]\n",
      "12659: [discriminator loss: 0.641753, acc: 0.632812] [adversarial loss: 0.972352, acc: 0.359375]\n",
      "12660: [discriminator loss: 0.548035, acc: 0.718750] [adversarial loss: 0.993793, acc: 0.281250]\n",
      "12661: [discriminator loss: 0.502763, acc: 0.796875] [adversarial loss: 1.136240, acc: 0.265625]\n",
      "12662: [discriminator loss: 0.491797, acc: 0.757812] [adversarial loss: 0.983172, acc: 0.343750]\n",
      "12663: [discriminator loss: 0.512108, acc: 0.757812] [adversarial loss: 1.038393, acc: 0.281250]\n",
      "12664: [discriminator loss: 0.541114, acc: 0.687500] [adversarial loss: 1.194757, acc: 0.234375]\n",
      "12665: [discriminator loss: 0.625384, acc: 0.671875] [adversarial loss: 0.817923, acc: 0.421875]\n",
      "12666: [discriminator loss: 0.533961, acc: 0.734375] [adversarial loss: 1.314049, acc: 0.125000]\n",
      "12667: [discriminator loss: 0.551443, acc: 0.703125] [adversarial loss: 1.148071, acc: 0.281250]\n",
      "12668: [discriminator loss: 0.579800, acc: 0.687500] [adversarial loss: 1.219041, acc: 0.296875]\n",
      "12669: [discriminator loss: 0.543289, acc: 0.734375] [adversarial loss: 1.063844, acc: 0.265625]\n",
      "12670: [discriminator loss: 0.529148, acc: 0.710938] [adversarial loss: 1.654942, acc: 0.109375]\n",
      "12671: [discriminator loss: 0.634597, acc: 0.648438] [adversarial loss: 0.708351, acc: 0.578125]\n",
      "12672: [discriminator loss: 0.692501, acc: 0.656250] [adversarial loss: 1.773747, acc: 0.062500]\n",
      "12673: [discriminator loss: 0.617239, acc: 0.695312] [adversarial loss: 0.841375, acc: 0.484375]\n",
      "12674: [discriminator loss: 0.551825, acc: 0.671875] [adversarial loss: 1.402594, acc: 0.078125]\n",
      "12675: [discriminator loss: 0.631676, acc: 0.640625] [adversarial loss: 0.934064, acc: 0.328125]\n",
      "12676: [discriminator loss: 0.545599, acc: 0.718750] [adversarial loss: 1.486136, acc: 0.140625]\n",
      "12677: [discriminator loss: 0.533823, acc: 0.710938] [adversarial loss: 1.093003, acc: 0.250000]\n",
      "12678: [discriminator loss: 0.550062, acc: 0.656250] [adversarial loss: 1.154902, acc: 0.187500]\n",
      "12679: [discriminator loss: 0.494453, acc: 0.750000] [adversarial loss: 1.013546, acc: 0.390625]\n",
      "12680: [discriminator loss: 0.558702, acc: 0.679688] [adversarial loss: 1.217478, acc: 0.203125]\n",
      "12681: [discriminator loss: 0.515001, acc: 0.750000] [adversarial loss: 1.063095, acc: 0.281250]\n",
      "12682: [discriminator loss: 0.494451, acc: 0.773438] [adversarial loss: 1.416857, acc: 0.140625]\n",
      "12683: [discriminator loss: 0.537669, acc: 0.687500] [adversarial loss: 1.314384, acc: 0.140625]\n",
      "12684: [discriminator loss: 0.487157, acc: 0.773438] [adversarial loss: 1.075603, acc: 0.296875]\n",
      "12685: [discriminator loss: 0.525609, acc: 0.742188] [adversarial loss: 1.227050, acc: 0.218750]\n",
      "12686: [discriminator loss: 0.499969, acc: 0.796875] [adversarial loss: 1.173227, acc: 0.234375]\n",
      "12687: [discriminator loss: 0.560187, acc: 0.750000] [adversarial loss: 1.371656, acc: 0.187500]\n",
      "12688: [discriminator loss: 0.533699, acc: 0.726562] [adversarial loss: 1.264876, acc: 0.265625]\n",
      "12689: [discriminator loss: 0.568485, acc: 0.671875] [adversarial loss: 1.145205, acc: 0.328125]\n",
      "12690: [discriminator loss: 0.652956, acc: 0.585938] [adversarial loss: 0.988335, acc: 0.406250]\n",
      "12691: [discriminator loss: 0.530171, acc: 0.742188] [adversarial loss: 1.210683, acc: 0.187500]\n",
      "12692: [discriminator loss: 0.522456, acc: 0.710938] [adversarial loss: 1.045359, acc: 0.312500]\n",
      "12693: [discriminator loss: 0.547746, acc: 0.687500] [adversarial loss: 1.409119, acc: 0.109375]\n",
      "12694: [discriminator loss: 0.573392, acc: 0.656250] [adversarial loss: 0.929857, acc: 0.375000]\n",
      "12695: [discriminator loss: 0.549849, acc: 0.718750] [adversarial loss: 1.569929, acc: 0.140625]\n",
      "12696: [discriminator loss: 0.626259, acc: 0.664062] [adversarial loss: 0.783626, acc: 0.500000]\n",
      "12697: [discriminator loss: 0.596330, acc: 0.695312] [adversarial loss: 1.397032, acc: 0.171875]\n",
      "12698: [discriminator loss: 0.545710, acc: 0.734375] [adversarial loss: 0.804352, acc: 0.500000]\n",
      "12699: [discriminator loss: 0.579366, acc: 0.718750] [adversarial loss: 1.626583, acc: 0.109375]\n"
     ]
    }
   ],
   "source": [
    "def build_and_train_models():\n",
    "    # load MNIST dataset\n",
    "    (x_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # reshape data for CNN as (28, 28, 1) and normalize\n",
    "    image_size = x_train.shape[1]\n",
    "    x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "\n",
    "    model_name = \"dcgan_mnist\"\n",
    "    # network parameters\n",
    "    # the latent or z vector is 100-dim\n",
    "    latent_size = 100\n",
    "    batch_size = 64\n",
    "    train_steps = 40000\n",
    "    lr = 2e-4\n",
    "    decay = 6e-8\n",
    "    input_shape = (image_size, image_size, 1)\n",
    "\n",
    "    # build discriminator model\n",
    "    inputs = Input(shape=input_shape, name='discriminator_input')\n",
    "    discriminator = build_discriminator(inputs)\n",
    "    # [1] or original paper uses Adam, \n",
    "    # but discriminator converges easily with RMSprop\n",
    "    optimizer = RMSprop(learning_rate=lr, decay=decay)\n",
    "    discriminator.compile(loss='binary_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=['accuracy'])\n",
    "    discriminator.summary()\n",
    "\n",
    "    # build generator model\n",
    "    input_shape = (latent_size, )\n",
    "    inputs = Input(shape=input_shape, name='z_input')\n",
    "    generator = build_generator(inputs, image_size)\n",
    "    generator.summary()\n",
    "\n",
    "    # build adversarial model\n",
    "    optimizer = RMSprop(learning_rate=lr * 0.5, decay=decay * 0.5)\n",
    "    # åœ¨å¯¹æŠ—è®­ç»ƒæœŸé—´å†»ç»“é‰´åˆ«å™¨çš„æƒé‡\n",
    "    discriminator.trainable = False\n",
    "    # adversarial = generator + discriminator\n",
    "    adversarial = Model(inputs, \n",
    "                        discriminator(generator(inputs)),\n",
    "                        name=model_name)\n",
    "    adversarial.compile(loss='binary_crossentropy',\n",
    "                        optimizer=optimizer,\n",
    "                        metrics=['accuracy'])\n",
    "    adversarial.summary()\n",
    "\n",
    "    # train discriminator and adversarial networks\n",
    "    models = (generator, discriminator, adversarial)\n",
    "    params = (batch_size, latent_size, train_steps, model_name)\n",
    "    train(models, x_train, params)\n",
    "\n",
    "\n",
    "def test_generator(generator):\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "    plot_images(generator,\n",
    "                noise_input=noise_input,\n",
    "                show=True,\n",
    "                model_name=\"test_outputs\")\n",
    "\n",
    "    \n",
    "run_generator =False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#    parser = argparse.ArgumentParser()\n",
    "#    help_ = \"Load generator h5 model with trained weights\"\n",
    "#    parser.add_argument(\"-g\", \"--generator\", help=help_)\n",
    "#    args = parser.parse_args()\n",
    "    if run_generator:\n",
    "        generator = load_model(model_name)\n",
    "        test_generator(generator)\n",
    "    else:\n",
    "        build_and_train_models()\n",
    "        \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# #    parser = argparse.ArgumentParser()\n",
    "# #    help_ = \"Load generator h5 model with trained weights\"\n",
    "# #    parser.add_argument(\"-g\", \"--generator\", help=help_)\n",
    "# #    args = parser.parse_args()\n",
    "#     if args.generator:\n",
    "#         generator = load_model(args.generator)\n",
    "#         test_generator(generator)\n",
    "#     else:\n",
    "#         build_and_train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-heavy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
